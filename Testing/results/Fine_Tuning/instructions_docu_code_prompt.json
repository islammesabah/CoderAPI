[
    {
        "api": "PyDrive2",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install pydrive2\nfrom pydrive2.auth import GoogleAuth\nfrom pydrive2.drive import GoogleDrive\n\n# Function to authenticate and get the GoogleDrive instance\ndef authenticate_drive():\n    gauth = GoogleAuth()\n    gauth.LocalWebserverAuth()  # Authenticate using local webserver\n    return GoogleDrive(gauth)\n\ndef get_file_metadata(file_id):\n    drive = authenticate_drive()\n    \n    try:\n        file = drive.CreateFile({'id': file_id})\n        if not file:\n            print(f\"File with ID '{file_id}' does not exist on Google Drive.\")\n            return\n        \n        print(\"File Metadata:\")\n        print(f\"Title: {file['title']}\")\n        print(f\"Size: {file['fileSize']} bytes\")\n        print(f\"Creation Date: {file['createdDate']}\")\n        print(f\"Last Modified Date: {file['modifiedDate']}\")\n    except Exception as e:\n        print(f\"Error retrieving file metadata: {e}\")\n\nif __name__ == \"__main__\":\n    file_id = input(\"Enter the file ID on Google Drive to retrieve metadata: \")\n    \n    get_file_metadata(file_id)",
        "instruction": "Create a Python program that retrieves the metadata of a file on Google Drive using the PyDrive2 API. The program should take the file ID as input and display the metadata information of the file, including its title, size, creation date, and last modified date. Make sure the program is capable of handling cases where the file ID provided is invalid or when there is an authentication issue.",
        "prediction": ":pip install pydrive2\nfrom pydrive2.auth import GoogleAuth\nfrom pydrive2.drive import GoogleDrive\n\n# Function to authenticate and get the GoogleDrive instance\ndef authenticate_drive():\n    gauth = GoogleAuth()\n    gauth.LocalWebserverAuth()  # Authenticate using local webserver\n    return GoogleDrive(gauth)\n\ndef get_file_metadata(file_id):\n    drive = authenticate_drive()\n    \n    try:\n        file = drive.CreateFile({'id': file_id})\n        if not file:\n            print(f\"File with ID '{file_id}' does not exist on Google Drive.\")\n            return\n        \n        print(\"File Metadata:\")\n        print(f\"Title: {file['title']}\")\n        print(f\"Size: {file['fileSize']} bytes\")\n        print(f\"Creation Date: {file['createdDate']}\")\n        print(f\"Last Modified Date: {file['modifiedDate']}\")\n    except Exception as e:\n        print(f\"Error retrieving file: {e}\")\n\nif __name__ == \"__main__\":\n    file_id = input(\"Enter the file ID on Google Drive to retrieve metadata: \")\n    \n    get_file_metadata(file_id)",
        "source": "This is some documentation of PyDrive2 API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n# pydrive2.drive module#\n\n\n*class* pydrive2.drive.GoogleDrive(*auth=None*)#\nBases: `ApiAttributeMixin`\n\n\nMain Google Drive class.\n\n\nCreateFile(*metadata=None*)#\nCreate an instance of GoogleDriveFile with auth of this instance.\n\n\nThis method would not upload a file to GoogleDrive.\n\nParameters:\n**metadata** (*dict.*) \u2013 file resource to initialize GoogleDriveFile with.\n\nReturns:\npydrive2.files.GoogleDriveFile \u2013 initialized with auth of this\ninstance.\n\nGetAbout()#\nReturn information about the Google Drive of the auth instance.\n\nReturns:\nA dictionary of Google Drive information like user, usage, quota etc.\n\nListFile(*param=None*)#\nCreate an instance of GoogleDriveFileList with auth of this instance.\n\n\nThis method will not fetch from Files.List().\n\nParameters:\n**param** (*dict.*) \u2013 parameter to be sent to Files.List().\n\nReturns:\npydrive2.files.GoogleDriveFileList \u2013 initialized with auth of\nthis instance.\n\n## pydrive2.files module#\n\n\n*exception* pydrive2.files.ApiRequestError(*http\\_error*)#\nBases: `OSError`\n\n\nGetField(*field*)#\nReturns the field from the first error\n\n\n*exception* pydrive2.files.FileNotDownloadableError#\nBases: `RuntimeError`\n\n\nError trying to download file that is not downloadable.\n\n*exception* pydrive2.files.FileNotUploadedError#\nBases: `RuntimeError`\n\n\nError trying to access metadata of file that is not uploaded.\n\n*class* pydrive2.files.GoogleDriveFile(*auth=None*, *metadata=None*, *uploaded=False*)#\nBases: `ApiAttributeMixin`, `ApiResource`\n\n\nGoogle Drive File\n\n==================\n Document 1 \n----------------\n# pydrive2.files module#\n\n\n*exception* pydrive2.files.ApiRequestError(*http\\_error*)#\nBases: `OSError`\n\n\nGetField(*field\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that retrieves the metadata of a file on Google Drive using the PyDrive2 API. The program should take the file ID as input and display the metadata information of the file, including its title, size, creation date, and last modified date. Make sure the program is capable of handling cases where the file ID provided is invalid or when there is an authentication issue.\n\n### Code:"
    },
    {
        "api": "PyDrive2",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install pydrive2\nfrom pydrive2.auth import GoogleAuth\nfrom pydrive2.drive import GoogleDrive\n\n# Function to authenticate and get the GoogleDrive instance\ndef authenticate_drive():\n    gauth = GoogleAuth()\n    gauth.LocalWebserverAuth()  # Authenticate using local webserver\n    return GoogleDrive(gauth)\n\ndef get_permissions(file_or_folder_id):\n    drive = authenticate_drive()\n    \n    try:\n        file_or_folder = drive.CreateFile({'id': file_or_folder_id})\n        if not file_or_folder:\n            print(f\"File or folder with ID '{file_or_folder_id}' does not exist on Google Drive.\")\n            return\n        \n        permissions = file_or_folder.GetPermissions()\n        \n        print(f\"Permissions for file or folder with ID '{file_or_folder_id}':\")\n        \n        for permission in permissions:\n            print(f\"Email Address: {permission['emailAddress']}, Role: {permission['role']}\")\n    except Exception as e:\n        print(f\"Error retrieving permissions: {e}\")\n\nif __name__ == \"__main__\":\n    file_or_folder_id = input(\"Enter the file or folder ID on Google Drive to retrieve permissions: \")\n    \n    get_permissions(file_or_folder_id)",
        "instruction": "Create a Python program that retrieves the list of permissions for a file or folder on Google Drive using the PyDrive2 API. The program should take the file or folder ID as input and display the list of permissions, including the email addresses and roles of the users or groups that have access to the file or folder. Make sure the program is capable of handling cases where the file or folder ID provided is invalid or when there is an authentication issue.",
        "prediction": ":import pydrive2\nfrom pydrive2.auth import GoogleAuth\nfrom pydrive2.drive import GoogleDrive\n\ndef perform_permissions_manipulation_on_file_or_folder(file_or_folder_id):\n    gauth = GoogleAuth()\n    gauth.LocalWebserverAuth()  # Authenticate using local webserver\n    return gdrive.ListFile({'q': f\"'{file_or_folder_id}' in parents and trashed=false\"}).Permissions\n\ndef perform_permissions_manipulation_on_file(file_id):\n    return perform_permissions_manipulation_on_file_or_folder(file_id)\n\ndef perform_permissions_manipulation_on_folder(folder_id):\n    return perform_permissions_manipulation_on_file_or_folder(folder_id)\n\nif __name__ == \"__main__\":\n    file_or_folder_id = input(\"Enter the file or folder ID on Google Drive to retrieve permissions: \")\n    \n    permissions = perform_permissions_manipulation_on_file_or_folder(file_or_folder_id)\n    \n    print(f\"Permissions for file or folder with ID '{file_or_folder_id}':\")\n    \n    for permission in permissions:\n        print(f\"Email address: {permission['emailAddress']}, role: {permission['role']}\")\n",
        "source": "This is some documentation of PyDrive2 API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n# pydrive2.drive module#\n\n\n*class* pydrive2.drive.GoogleDrive(*auth=None*)#\nBases: `ApiAttributeMixin`\n\n\nMain Google Drive class.\n\n\nCreateFile(*metadata=None*)#\nCreate an instance of GoogleDriveFile with auth of this instance.\n\n\nThis method would not upload a file to GoogleDrive.\n\nParameters:\n**metadata** (*dict.*) \u2013 file resource to initialize GoogleDriveFile with.\n\nReturns:\npydrive2.files.GoogleDriveFile \u2013 initialized with auth of this\ninstance.\n\nGetAbout()#\nReturn information about the Google Drive of the auth instance.\n\nReturns:\nA dictionary of Google Drive information like user, usage, quota etc.\n\nListFile(*param=None*)#\nCreate an instance of GoogleDriveFileList with auth of this instance.\n\n\nThis method will not fetch from Files.List().\n\nParameters:\n**param** (*dict.*) \u2013 parameter to be sent to Files.List().\n\nReturns:\npydrive2.files.GoogleDriveFileList \u2013 initialized with auth of\nthis instance.\n\n## pydrive2.files module#\n\n\n*exception* pydrive2.files.ApiRequestError(*http\\_error*)#\nBases: `OSError`\n\n\nGetField(*field*)#\nReturns the field from the first error\n\n\n*exception* pydrive2.files.FileNotDownloadableError#\nBases: `RuntimeError`\n\n\nError trying to download file that is not downloadable.\n\n*exception* pydrive2.files.FileNotUploadedError#\nBases: `RuntimeError`\n\n\nError trying to access metadata of file that is not uploaded.\n\n*class* pydrive2.files.GoogleDriveFile(*auth=None*, *metadata=None*, *uploaded=False*)#\nBases: `ApiAttributeMixin`, `ApiResource`\n\n\nGoogle Drive File\n\n==================\n Document 1 \n----------------\n# pydrive2.files module#\n\n\n*exception* pydrive2.files.ApiRequestError(*http\\_error*)#\nBases: `OSError`\n\n\nGetField(*field\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that retrieves the list of permissions for a file or folder on Google Drive using the PyDrive2 API. The program should take the file or folder ID as input and display the list of permissions, including the email addresses and roles of the users or groups that have access to the file or folder. Make sure the program is capable of handling cases where the file or folder ID provided is invalid or when there is an authentication issue.\n\n### Code:"
    },
    {
        "api": "PyDrive2",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install pydrive2\nfrom pydrive2.auth import GoogleAuth\nfrom pydrive2.drive import GoogleDrive\n\n# Function to authenticate and get the GoogleDrive instance\ndef authenticate_drive():\n    gauth = GoogleAuth()\n    gauth.LocalWebserverAuth()  # Authenticate using local webserver\n    return GoogleDrive(gauth)\n\ndef list_files_in_drive_folder(folder_id):\n    drive = authenticate_drive()\n    \n    try:\n        folder = drive.CreateFile({'id': folder_id})\n        if not folder:\n            print(f\"Folder with ID '{folder_id}' does not exist on Google Drive.\")\n            return\n        \n        folder_title = folder['title']\n        print(f\"Listing files in '{folder_title}' folder (ID: {folder_id}):\")\n        \n        file_list = drive.ListFile({'q': f\"'{folder_id}' in parents and trashed=false\"}).GetList()\n        if not file_list:\n            print(\"No files found in this folder.\")\n            return\n        \n        for file in file_list:\n            print(f\"File Name: {file['title']}, File ID: {file['id']}\")\n    except Exception as e:\n        print(f\"Error listing files: {e}\")\n\nif __name__ == \"__main__\":\n    folder_id = input(\"Enter the folder ID on Google Drive to list files from: \")\n    \n    list_files_in_drive_folder(folder_id)",
        "instruction": "Create a Python program that lists all files in a specified folder on Google Drive using the PyDrive2 API. The program should take the folder's ID as input and display a list of file names along with their corresponding IDs. Make sure the program is capable of handling cases where the folder ID provided does not exist or when there is an authentication issue.",
        "prediction": ":pip install pydrive2\nfrom pydrive2.auth import GoogleAuth\nfrom pydrive2.drive import GoogleDrive\n\n# Function to authenticate and get the GoogleDrive instance\ndef authenticate_drive():\n    gauth = GoogleAuth()\n    gauth.LocalWebserverAuth()  # Authenticate using local webserver\n    return GoogleDrive(gauth)\n\ndef list_files_in_drive(folder_id):\n    drive = authenticate_drive()\n    \n    try:\n        folder = drive.CreateFile({'id': folder_id})\n        if not folder:\n            print(f\"Folder with ID '{folder_id}' does not exist on Google Drive.\")\n            return\n        \n        \n        \n        \n        \n        folder_list = folder.List()\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n",
        "source": "This is some documentation of PyDrive2 API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n# pydrive2.drive module#\n\n\n*class* pydrive2.drive.GoogleDrive(*auth=None*)#\nBases: `ApiAttributeMixin`\n\n\nMain Google Drive class.\n\n\nCreateFile(*metadata=None*)#\nCreate an instance of GoogleDriveFile with auth of this instance.\n\n\nThis method would not upload a file to GoogleDrive.\n\nParameters:\n**metadata** (*dict.*) \u2013 file resource to initialize GoogleDriveFile with.\n\nReturns:\npydrive2.files.GoogleDriveFile \u2013 initialized with auth of this\ninstance.\n\nGetAbout()#\nReturn information about the Google Drive of the auth instance.\n\nReturns:\nA dictionary of Google Drive information like user, usage, quota etc.\n\nListFile(*param=None*)#\nCreate an instance of GoogleDriveFileList with auth of this instance.\n\n\nThis method will not fetch from Files.List().\n\nParameters:\n**param** (*dict.*) \u2013 parameter to be sent to Files.List().\n\nReturns:\npydrive2.files.GoogleDriveFileList \u2013 initialized with auth of\nthis instance.\n\n## pydrive2.files module#\n\n\n*exception* pydrive2.files.ApiRequestError(*http\\_error*)#\nBases: `OSError`\n\n\nGetField(*field*)#\nReturns the field from the first error\n\n\n*exception* pydrive2.files.FileNotDownloadableError#\nBases: `RuntimeError`\n\n\nError trying to download file that is not downloadable.\n\n*exception* pydrive2.files.FileNotUploadedError#\nBases: `RuntimeError`\n\n\nError trying to access metadata of file that is not uploaded.\n\n*class* pydrive2.files.GoogleDriveFile(*auth=None*, *metadata=None*, *uploaded=False*)#\nBases: `ApiAttributeMixin`, `ApiResource`\n\n\nGoogle Drive File\n\n==================\n Document 1 \n----------------\n# pydrive2.files module#\n\n\n*exception* pydrive2.files.ApiRequestError(*http\\_error*)#\nBases: `OSError`\n\n\nGetField(*field\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that lists all files in a specified folder on Google Drive using the PyDrive2 API. The program should take the folder's ID as input and display a list of file names along with their corresponding IDs. Make sure the program is capable of handling cases where the folder ID provided does not exist or when there is an authentication issue.\n\n### Code:"
    },
    {
        "api": "Pygments",
        "decoder_input_ids": true,
        "ground-truth": "# !pip install Pygments\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import ExcelFormatter\n\n# Read the code file\nwith open(\"code.py\", \"r\") as file:\n    code = file.read()\n\n# Determine the lexer based on the file extension\nlexer = get_lexer_by_name(\"python\")\n\n# Define a custom formatter with a custom color scheme and line numbers\nformatter = ExcelFormatter(style=\"friendly\", linenos=True)\n\n# Apply syntax highlighting to the code with the custom color scheme and line numbers\nhighlighted_code = highlight(code, lexer, formatter)\n\n# Save the highlighted code to an Excel file (XLSX) with the custom color scheme and line numbers\nwith open(\"highlighted_code.xlsx\", \"wb\") as xlsx_file:\n    xlsx_file.write(highlighted_code)",
        "instruction": "Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as an Excel file (XLSX) with a custom color scheme and line numbers.",
        "prediction": ":pip install Pygments\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import ExcelFormatter\n\n# Read the code file\nwith open(\"code.py\", \"r\") as file:\n    code = file.read()\n\n# Determine the lexer based on the file extension\nlexer = get_lexer_by_name(\"python\")\n\n# Define a custom formatter with a custom color scheme and line numbers\nformatter = ExcelFormatter(style=\"monokai\", linenos=True)\n\n# Apply syntax highlighting to the code with the custom color scheme and line numbers\nhighlighted_code = highlight(code, lexer, formatter)\n\n# Save the highlighted code to an Excel file (XLSX) with the custom color scheme and line numbers\nwith open(\"highlighted_code.xlsx\", \"wb\") as xlsx_file:\n    xlsx_file.write(highlighted_code)",
        "source": "This is some documentation of Pygments API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# The full Pygments API\u00b6\n\n\nThis page describes the Pygments API.\n\n## High-level API\u00b6\n\n\nFunctions from the `pygments` module:\n\n\npygments.lex(*code*, *lexer*)\u00b6\nLex code with the lexer (must be a Lexer instance)\nand return an iterable of tokens. Currently, this only calls\nlexer.get\\_tokens().\n\npygments.format(*tokens*, *formatter*, *outfile=None*)\u00b6\nFormat `tokens` (an iterable of tokens) with the formatter `formatter`\n(a Formatter instance).\n\n\nIf `outfile`\n\n==================\n Document 1 \n----------------\n# High-level API\u00b6\n\n\nFunctions from the `pygments` module:\n\n\npygments.lex(*code*, *lexer*)\u00b6\nLex code with the lexer (must be a Lexer instance)\nand return an iterable of tokens. Currently, this only calls\nlexer.get\\_tokens().\n\npygments.format(*tokens*, *formatter*, *outfile=None*)\u00b6\nFormat `tokens` (an iterable of tokens) with the formatter `formatter`\n(a Formatter instance).\n\n\nIf `outfile` is given and a valid file object (an object with a\n`write` method), the result will be written to it, otherwise it\nis returned as a string.\n\npygments.highlight(*code*, *lexer*, *formatter*, *outfile=None*)\u00b6\nThis is the most high-level highlighting function. It combines lex and\nformat in one function.\n\nFunctions from `pygments.lexers`:\n\n\npygments.lexers.get\\_lexer\\_by\\_name(*\\_alias*, *\\*\\*options*)\u00b6\nReturn an instance of a Lexer subclass that has alias in its\naliases list. The lexer is given the options at its\ninstantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if no lexer with that alias is\nfound.\n\npygments.lexers.get\\_lexer\\_for\\_filename(*\\_fn*, *code\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as an Excel file (XLSX) with a custom color scheme and line numbers.\n\n### Code:"
    },
    {
        "api": "Pygments",
        "decoder_input_ids": true,
        "ground-truth": "# !pip install Pygments\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import ExcelFormatter\n\n# Read the code file\nwith open(\"code.py\", \"r\") as file:\n    code = file.read()\n\n# Determine the lexer based on the file extension\nlexer = get_lexer_by_name(\"python\")\n\n# Apply syntax highlighting to the code\nhighlighted_code = highlight(code, lexer, ExcelFormatter())\n\n# Save the highlighted code to an Excel file (XLSX)\nwith open(\"highlighted_code.xlsx\", \"wb\") as xlsx_file:\n    xlsx_file.write(highlighted_code)",
        "instruction": "Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as an Excel file (XLSX).",
        "prediction": ":pip install Pygments\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import ExcelFormatter\n\n# Read the code file\nwith open(\"code.py\", \"r\") as file:\n    code = file.read()\n\n# Determine the lexer based on the file extension\nlexer = get_lexer_by_name(\"python\")\n\n# Apply syntax highlighting to the code\nhighlighted_code = highlight(code, lexer, ExcelFormatter())\n\n# Save the highlighted code to an Excel file (XLSX)\nwith open(\"highlighted_code.xlsx\", \"wb\") as xlsx_file:\n    xlsx_file.write(highlighted_code)",
        "source": "This is some documentation of Pygments API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# The full Pygments API\u00b6\n\n\nThis page describes the Pygments API.\n\n## High-level API\u00b6\n\n\nFunctions from the `pygments` module:\n\n\npygments.lex(*code*, *lexer*)\u00b6\nLex code with the lexer (must be a Lexer instance)\nand return an iterable of tokens. Currently, this only calls\nlexer.get\\_tokens().\n\npygments.format(*tokens*, *formatter*, *outfile=None*)\u00b6\nFormat `tokens` (an iterable of tokens) with the formatter `formatter`\n(a Formatter instance).\n\n\nIf `outfile`\n\n==================\n Document 1 \n----------------\n# High-level API\u00b6\n\n\nFunctions from the `pygments` module:\n\n\npygments.lex(*code*, *lexer*)\u00b6\nLex code with the lexer (must be a Lexer instance)\nand return an iterable of tokens. Currently, this only calls\nlexer.get\\_tokens().\n\npygments.format(*tokens*, *formatter*, *outfile=None*)\u00b6\nFormat `tokens` (an iterable of tokens) with the formatter `formatter`\n(a Formatter instance).\n\n\nIf `outfile` is given and a valid file object (an object with a\n`write` method), the result will be written to it, otherwise it\nis returned as a string.\n\npygments.highlight(*code*, *lexer*, *formatter*, *outfile=None*)\u00b6\nThis is the most high-level highlighting function. It combines lex and\nformat in one function.\n\nFunctions from `pygments.lexers`:\n\n\npygments.lexers.get\\_lexer\\_by\\_name(*\\_alias*, *\\*\\*options*)\u00b6\nReturn an instance of a Lexer subclass that has alias in its\naliases list. The lexer is given the options at its\ninstantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if no lexer with that alias is\nfound.\n\npygments.lexers.get\\_lexer\\_for\\_filename(*\\_fn*, *code\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as an Excel file (XLSX).\n\n### Code:"
    },
    {
        "api": "Pygments",
        "decoder_input_ids": true,
        "ground-truth": "# !pip install Pygments\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import CsvFormatter\n\n# Read the code file\nwith open(\"code.py\", \"r\") as file:\n    code = file.read()\n\n# Determine the lexer based on the file extension\nlexer = get_lexer_by_name(\"python\")\n\n# Apply syntax highlighting to the code\nhighlighted_code = highlight(code, lexer, CsvFormatter())\n\n# Save the highlighted code to a CSV file\nwith open(\"highlighted_code.csv\", \"w\") as csv_file:\n    csv_file.write(highlighted_code)",
        "instruction": "Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a CSV file.",
        "prediction": ":pip install Pygments\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import CsvFormatter\n\n# Read the code file\nwith open(\"code.py\", \"r\") as file:\n    code = file.read()\n\n# Determine the lexer based on the file extension\nlexer = get_lexer_by_name(\"python\")\n\n# Apply syntax highlighting to the code\nhighlighted_code = highlight(code, lexer, CsvFormatter())\n\n# Save the highlighted code to a CSV file\nwith open(\"highlighted_code.csv\", \"w\") as csv_file:\n    csv_file.write(highlighted_code)",
        "source": "This is some documentation of Pygments API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# The full Pygments API\u00b6\n\n\nThis page describes the Pygments API.\n\n## High-level API\u00b6\n\n\nFunctions from the `pygments` module:\n\n\npygments.lex(*code*, *lexer*)\u00b6\nLex code with the lexer (must be a Lexer instance)\nand return an iterable of tokens. Currently, this only calls\nlexer.get\\_tokens().\n\npygments.format(*tokens*, *formatter*, *outfile=None*)\u00b6\nFormat `tokens` (an iterable of tokens) with the formatter `formatter`\n(a Formatter instance).\n\n\nIf `outfile`\n\n==================\n Document 1 \n----------------\n# High-level API\u00b6\n\n\nFunctions from the `pygments` module:\n\n\npygments.lex(*code*, *lexer*)\u00b6\nLex code with the lexer (must be a Lexer instance)\nand return an iterable of tokens. Currently, this only calls\nlexer.get\\_tokens().\n\npygments.format(*tokens*, *formatter*, *outfile=None*)\u00b6\nFormat `tokens` (an iterable of tokens) with the formatter `formatter`\n(a Formatter instance).\n\n\nIf `outfile` is given and a valid file object (an object with a\n`write` method), the result will be written to it, otherwise it\nis returned as a string.\n\npygments.highlight(*code*, *lexer*, *formatter*, *outfile=None*)\u00b6\nThis is the most high-level highlighting function. It combines lex and\nformat in one function.\n\nFunctions from `pygments.lexers`:\n\n\npygments.lexers.get\\_lexer\\_by\\_name(*\\_alias*, *\\*\\*options*)\u00b6\nReturn an instance of a Lexer subclass that has alias in its\naliases list. The lexer is given the options at its\ninstantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if no lexer with that alias is\nfound.\n\npygments.lexers.get\\_lexer\\_for\\_filename(*\\_fn*, *code\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a CSV file.\n\n### Code:"
    },
    {
        "api": "Pygments",
        "decoder_input_ids": true,
        "ground-truth": "# !pip install Pygments\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import CsvFormatter\n\n# Read the code file\nwith open(\"code.py\", \"r\") as file:\n    code = file.read()\n\n# Determine the lexer based on the file extension\nlexer = get_lexer_by_name(\"python\")\n\n# Define a custom formatter with a custom color scheme and line numbers\nformatter = CsvFormatter(style=\"fruity\", linenos=True)\n\n# Apply syntax highlighting to the code with the custom color scheme and line numbers\nhighlighted_code = highlight(code, lexer, formatter)\n\n# Save the highlighted code to a CSV file with the custom color scheme and line numbers\nwith open(\"highlighted_code.csv\", \"w\") as csv_file:\n    csv_file.write(highlighted_code)",
        "instruction": "Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a CSV file with a custom color scheme and line numbers.",
        "prediction": ":pip install Pygments\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import CsvFormatter\n\n# Read the code file\nwith open(\"code.py\", \"r\") as file:\n    code = file.read()\n\n# Determine the lexer based on the file extension\nlexer = get_lexer_by_name(\"python\")\n\n# Define a custom formatter with a custom color scheme and line numbers\nformatter = CsvFormatter(style=\"paraiso-dark\", linenos=True)\n\n# Apply syntax highlighting to the code with the custom color scheme and line numbers\nhighlighted_code = highlight(code, lexer, formatter)\n\n# Save the highlighted code to a CSV file with the custom color scheme and line numbers\nwith open(\"highlighted_code.csv\", \"w\") as csv_file:\n    csv_file.write(highlighted_code)",
        "source": "This is some documentation of Pygments API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# The full Pygments API\u00b6\n\n\nThis page describes the Pygments API.\n\n## High-level API\u00b6\n\n\nFunctions from the `pygments` module:\n\n\npygments.lex(*code*, *lexer*)\u00b6\nLex code with the lexer (must be a Lexer instance)\nand return an iterable of tokens. Currently, this only calls\nlexer.get\\_tokens().\n\npygments.format(*tokens*, *formatter*, *outfile=None*)\u00b6\nFormat `tokens` (an iterable of tokens) with the formatter `formatter`\n(a Formatter instance).\n\n\nIf `outfile`\n\n==================\n Document 1 \n----------------\n# High-level API\u00b6\n\n\nFunctions from the `pygments` module:\n\n\npygments.lex(*code*, *lexer*)\u00b6\nLex code with the lexer (must be a Lexer instance)\nand return an iterable of tokens. Currently, this only calls\nlexer.get\\_tokens().\n\npygments.format(*tokens*, *formatter*, *outfile=None*)\u00b6\nFormat `tokens` (an iterable of tokens) with the formatter `formatter`\n(a Formatter instance).\n\n\nIf `outfile` is given and a valid file object (an object with a\n`write` method), the result will be written to it, otherwise it\nis returned as a string.\n\npygments.highlight(*code*, *lexer*, *formatter*, *outfile=None*)\u00b6\nThis is the most high-level highlighting function. It combines lex and\nformat in one function.\n\nFunctions from `pygments.lexers`:\n\n\npygments.lexers.get\\_lexer\\_by\\_name(*\\_alias*, *\\*\\*options*)\u00b6\nReturn an instance of a Lexer subclass that has alias in its\naliases list. The lexer is given the options at its\ninstantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if no lexer with that alias is\nfound.\n\npygments.lexers.get\\_lexer\\_for\\_filename(*\\_fn*, *code\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a CSV file with a custom color scheme and line numbers.\n\n### Code:"
    },
    {
        "api": "Pygments",
        "decoder_input_ids": true,
        "ground-truth": "# !pip install Pygments\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import RtfFormatter\n\n# Read the code file\nwith open(\"code.py\", \"r\") as file:\n    code = file.read()\n\n# Determine the lexer based on the file extension\nlexer = get_lexer_by_name(\"python\")\n\n# Apply syntax highlighting to the code\nhighlighted_code = highlight(code, lexer, RtfFormatter())\n\n# Save the highlighted code to a RTF file\nwith open(\"highlighted_code.rtf\", \"w\") as rtf_file:\n    rtf_file.write(highlighted_code)",
        "instruction": "Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a RTF file.",
        "prediction": ":pip install Pygments\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import RtfFormatter\n\n# Read the code file\nwith open(\"code.py\", \"r\") as file:\n    code = file.read()\n\n# Determine the lexer based on the file extension\nlexer = get_lexer_by_name(\"python\")\n\n# Apply syntax highlighting to the code\nhighlighted_code = highlight(code, lexer, RtfFormatter())\n\n# Save the highlighted code to a RTF file\nwith open(\"highlighted_code.rtf\", \"w\") as rtf_file:\n    rtf_file.write(highlighted_code)",
        "source": "This is some documentation of Pygments API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# The full Pygments API\u00b6\n\n\nThis page describes the Pygments API.\n\n## High-level API\u00b6\n\n\nFunctions from the `pygments` module:\n\n\npygments.lex(*code*, *lexer*)\u00b6\nLex code with the lexer (must be a Lexer instance)\nand return an iterable of tokens. Currently, this only calls\nlexer.get\\_tokens().\n\npygments.format(*tokens*, *formatter*, *outfile=None*)\u00b6\nFormat `tokens` (an iterable of tokens) with the formatter `formatter`\n(a Formatter instance).\n\n\nIf `outfile`\n\n==================\n Document 1 \n----------------\n# High-level API\u00b6\n\n\nFunctions from the `pygments` module:\n\n\npygments.lex(*code*, *lexer*)\u00b6\nLex code with the lexer (must be a Lexer instance)\nand return an iterable of tokens. Currently, this only calls\nlexer.get\\_tokens().\n\npygments.format(*tokens*, *formatter*, *outfile=None*)\u00b6\nFormat `tokens` (an iterable of tokens) with the formatter `formatter`\n(a Formatter instance).\n\n\nIf `outfile` is given and a valid file object (an object with a\n`write` method), the result will be written to it, otherwise it\nis returned as a string.\n\npygments.highlight(*code*, *lexer*, *formatter*, *outfile=None*)\u00b6\nThis is the most high-level highlighting function. It combines lex and\nformat in one function.\n\nFunctions from `pygments.lexers`:\n\n\npygments.lexers.get\\_lexer\\_by\\_name(*\\_alias*, *\\*\\*options*)\u00b6\nReturn an instance of a Lexer subclass that has alias in its\naliases list. The lexer is given the options at its\ninstantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if no lexer with that alias is\nfound.\n\npygments.lexers.get\\_lexer\\_for\\_filename(*\\_fn*, *code\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a RTF file.\n\n### Code:"
    },
    {
        "api": "SQLAlchemy",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install SQLAlchemy\nfrom sqlalchemy import create_engine, Column, Integer, String, Float\nfrom sqlalchemy.orm import sessionmaker, declarative_base\n\n# Create a MySQL database connection\nengine = create_engine('mysql://username:password@localhost/mydatabase')\n\n# Define the Employee class to represent the \"Employee\" entity\nBase = declarative_base()\n\nclass Employee(Base):\n    __tablename__ = 'employees'\n    id = Column(Integer, primary_key=True)\n    first_name = Column(String)\n    last_name = Column(String)\n    position = Column(String)\n    salary = Column(Float)\n\n# Create the table in the database\nBase.metadata.create_all(engine)\n\n# Function to add a new employee to the database\ndef add_employee(first_name, last_name, position, salary):\n    Session = sessionmaker(bind=engine)\n    session = Session()\n\n    employee = Employee(first_name=first_name, last_name=last_name, position=position, salary=salary)\n    session.add(employee)\n    session.commit()\n    session.close()\n\nadd_employee(\"John\", \"Smith\", \"Manager\", 60000.00)\n\nSession = sessionmaker(bind=engine)\nsession = Session()\nemployee = session.query(Employee).filter_by(first_name=\"John\").first()\nsession.close()\n\nprint(\"Employee data: \")\nprint(employee.first_name)\nprint(employee.last_name)\nprint(employee.position)\nprint(employee.salary)",
        "instruction": "Create a Python program that uses the 'SQLAlchemy' API to connect to a MySQL database. Define a 'Employee' entity with attributes like 'id,' 'first_name,' 'last_name,' 'position,' and 'salary.' The program should establish a connection to the MySQL database, create the 'employees' table and add a new employee. Then, retrieve employee details from the database.",
        "prediction": ":import sqlalchemy\nfrom sqlalchemy import create_engine, Column, Integer, String, Float\nfrom sqlalchemy.orm import sessionmaker, declarative_base\n\n# Create a MySQL database connection\nengine = create_engine('mysql://username:password@localhost/mydatabase')\n\n# Define the Employee class to represent the \"Employee\" entity\nBase = declarative_base()\n\nclass Employee(Base):\n    __tablename__ = 'employees'\n    id = Column(Integer, primary_key=True)\n    first_name = Column(String)\n    last_name = Column(String)\n    position = Column(String)\n    salary = Column(Float)\n\n# Create the table in the database\nBase.metadata.create_all(engine)\n\n# Function to add a new employee to the database\ndef add_employee(first_name, last_name, position, salary):\n    Session = sessionmaker(bind=engine)\n    session = Session()\n\n    employee = Employee(first_name=first_name, last_name=last_name, position=position, salary=salary)\n    session.add(employee)\n    session.commit()\n    session.close()\n\nadd_employee(\"John\", \"Smith\", \"Manager\", 60000.00)\n",
        "source": "This is some documentation of SQLAlchemy API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n# 2.0 Migration - ORM Usage\u00b6\n\n\nThe biggest visible change in SQLAlchemy 2.0 is the use of\n`Session.execute()` in conjunction with `select()` to run ORM\nqueries, instead of using `Session.query()`. As mentioned elsewhere,\nthere is no plan to actually remove the `Session.query()` API itself,\nas it is now implemented by using the new API internally it will remain as a\nlegacy API, and both APIs can be used freely.\n\n\nThe table below provides an introduction to the general change in\ncalling form with links to documentation for each technique\npresented. The individual migration notes are in the embedded sections\nfollowing the table, and may include additional notes not summarized here.\n\n**Overview of Major ORM Querying Patterns**\u00b6| 1.x style form | 2.0 style form | See Also |\n| --- | --- | --- |\n| \n```\nsession.query(User).get(42)\n```\n\n | \n```\nsession.get(User, 42)\n```\n\n | ORM Query - get() method moves to Session |\n| \n```\nsession.query(User).all()\n```\n\n | \n```\nsession.execute(\n  select(User)\n).scalars().all()\n# or\n\nsession.scalars(\n  select(User)\n).all()\n```\n\n | ORM Query Unified with Core Select\n`Session.scalars()`\n`Result.scalars()` |\n| \n```\nsession.query(User).\\\n  filter\\_by(name=\"some user\").\\\n  one()\n```\n\n | \n```\nsession.execute(\n  select(User).\n  filter\\_by(name=\"some user\")\n).scalar\\_one()\n```\n\n | ORM Query Unified with Core Select\n`Result.scalar\\_one()` |\n| \n```\nsession.query(User).\\\n  filter\\_by(name=\"some user\").\\\n  first()\n```\n\n |\n\n==================\n Document 1 \n----------------\nSQLAlchemy Documentation\u00b6\n\n\nGetting Started\n\n\nNew\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that uses the 'SQLAlchemy' API to connect to a MySQL database. Define a 'Employee' entity with attributes like 'id,' 'first_name,' 'last_name,' 'position,' and 'salary.' The program should establish a connection to the MySQL database, create the 'employees' table and add a new employee. Then, retrieve employee details from the database.\n\n### Code:"
    },
    {
        "api": "apeye",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install apeye\nimport apeye\n\nbase_url = \"https://example.com\"\n\nclient = apeye.URL(base_url)\n\ncookies = client.cookies\nuser_agent = client.user_agent\nreferer = client.referer\n\nprint(\"Cookies:\")\nprint(cookies)\n\nprint(\"\\nUser Agent:\")\nprint(user_agent)\n\nprint(\"\\nReferer:\")\nprint(referer)",
        "instruction": "Develop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the cookies, user agent, and referer information from the URL request.",
        "prediction": ":import apeye\nimport apeye\n\nbase_url = \"https://example.com\"\n\nclient = apeye.URL(base_url)\n\nprint(\"Cookies:\")\nprint(client.cookies)\n\nprint(\"User Agent:\")\nprint(client.user_agent)\n\nprint(\"Referer:\")\nprint(client.referer)",
        "source": "This is some documentation of apeye API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# `apeye.url`\u00b6\n\n\n**Source code:** apeye\\_core/\\_\\_init\\_\\_.py\n\n\n---\n\n\n`pathlib`-like approach to URLs.\n\nChanged in version 0.2.0: `SlumberURL` and `RequestsURL`\nmoved to `apeye.slumber\\_url` and `apeye.requests\\_url` respectively.\n\n\nNote\n\n\nThe classes in this module can instead be imported from the\napeye\\_core module instead.\n\n**Classes:**\n\n| `Domain`(subdomain,\u00a0domain,\u00a0suffix) | `typing.NamedTuple` of a URL\u2019s subdomain, domain, and suffix.\n\n==================\n Document 1 \n----------------\n `apeye.url`\u00b6\n\n\n**Source code:** apeye\\_core/\\_\\_init\\_\\_.py\n\n\n---\n\n\n`pathlib`-like approach to URLs.\n\nChanged in version 0.2.0: `SlumberURL` and `RequestsURL`\nmoved to `apeye.slumber\\_url` and `apeye.requests\\_url` respectively.\n\n\nNote\n\n\nThe classes in this module can instead be imported from the\napeye\\_core module instead.\n\n**Classes:**\n\n| `Domain`(subdomain,\u00a0domain,\u00a0suffix) | `typing.NamedTuple` of a URL\u2019s subdomain, domain, and suffix. |\n| `URL`([url]) | `pathlib`-like class for URLs. |\n| `URLPath`(\\*args) | Represents the path part of a URL. |\n\n\n**Data:**\n\n| `URLPathType` | Invariant `TypeVar` bound to `apeye.url.URLPath`. |\n| `URLType` | Invariant `TypeVar` bound to `apeye.url.URL`. |\n\n\nURLType *= TypeVar(URLType, bound=URL)*\u00b6\n**Type:**\u00a0\u00a0\u00a0\u00a0`TypeVar`\n\n\nInvariant `TypeVar` bound to `apeye.url.URL`.\n\nURLPathType *\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the cookies, user agent, and referer information from the URL request.\n\n### Code:"
    },
    {
        "api": "apeye",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install apeye\nimport apeye\n\nbase_url = \"https://example.com\"\n\nclient = apeye.URL(base_url)\n\nresponse = client.get()\n\nprint(\"Status Code:\")\nprint(response.status_code)\n\nprint(\"\\nHeaders:\")\nprint(response.headers)\n\nprint(\"\\nContent:\")\nprint(response.content)\n\n# Save content to file\nwith open('output.txt', 'w') as file:\n    file.write(response.content)",
        "instruction": "Develop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the status code, headers, and content of the URL response. Additionally, the program should save the content of the response to a file named 'output.txt'.",
        "prediction": ":pip install apeye\nimport apeye\n\nbase_url = \"https://example.com\"\n\nclient = apeye.URL(base_url)\n\nresponse = client.get()\n\nprint(\"Status Code:\")\nprint(response.status_code)\n\nprint(\"\\nHeaders:\")\nprint(response.headers)\n\nprint(\"\\nContent:\")\nprint(response.content)\n\nprint(\"\\nSave content to file:\")\nwith open('output.txt', 'w') as file:\n    file.write(response.content)",
        "source": "This is some documentation of apeye API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# `apeye.url`\u00b6\n\n\n**Source code:** apeye\\_core/\\_\\_init\\_\\_.py\n\n\n---\n\n\n`pathlib`-like approach to URLs.\n\nChanged in version 0.2.0: `SlumberURL` and `RequestsURL`\nmoved to `apeye.slumber\\_url` and `apeye.requests\\_url` respectively.\n\n\nNote\n\n\nThe classes in this module can instead be imported from the\napeye\\_core module instead.\n\n**Classes:**\n\n| `Domain`(subdomain,\u00a0domain,\u00a0suffix) | `typing.NamedTuple` of a URL\u2019s subdomain, domain, and suffix.\n\n==================\n Document 1 \n----------------\n `apeye.url`\u00b6\n\n\n**Source code:** apeye\\_core/\\_\\_init\\_\\_.py\n\n\n---\n\n\n`pathlib`-like approach to URLs.\n\nChanged in version 0.2.0: `SlumberURL` and `RequestsURL`\nmoved to `apeye.slumber\\_url` and `apeye.requests\\_url` respectively.\n\n\nNote\n\n\nThe classes in this module can instead be imported from the\napeye\\_core module instead.\n\n**Classes:**\n\n| `Domain`(subdomain,\u00a0domain,\u00a0suffix) | `typing.NamedTuple` of a URL\u2019s subdomain, domain, and suffix. |\n| `URL`([url]) | `pathlib`-like class for URLs. |\n| `URLPath`(\\*args) | Represents the path part of a URL. |\n\n\n**Data:**\n\n| `URLPathType` | Invariant `TypeVar` bound to `apeye.url.URLPath`. |\n| `URLType` | Invariant `TypeVar` bound to `apeye.url.URL`. |\n\n\nURLType *= TypeVar(URLType, bound=URL)*\u00b6\n**Type:**\u00a0\u00a0\u00a0\u00a0`TypeVar`\n\n\nInvariant `TypeVar` bound to `apeye.url.URL`.\n\nURLPathType *\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the status code, headers, and content of the URL response. Additionally, the program should save the content of the response to a file named 'output.txt'.\n\n### Code:"
    },
    {
        "api": "bitstring",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install bitstring\nfrom bitstring import BitArray\n\nbinary1 = input(\"Enter the first binary string: \")\nbinary2 = input(\"Enter the second binary string: \")\n\nbitarray1 = BitArray(bin=binary1)\nbitarray2 = BitArray(bin=binary2)\n\nresult = bitarray1 + bitarray2\n\nprint(\"Result of bitwise concatenation operation:\", result.bin)",
        "instruction": "Create a Python program using the 'bitstring' API to perform a bitwise concatenation operation on two binary strings. The program should take two binary strings as input and perform the bitwise concatenation operation on them. The program should then print the result of the bitwise concatenation operation.",
        "prediction": ":import bitstring\nimport bitstring\n\nbinary1 = input(\"Enter the first binary string: \")\nbinary2 = input(\"Enter the second binary string: \")\n\nbitarray1 = bitstring.BitArray(bin=binary1)\nbitarray2 = bitstring.BitArray(bin=binary2)\n\nbitarray3 = bitarray1 + bitarray2\n\nprint(\"Result of bitwise concatenation operation:\", bitarray3.bin)",
        "source": "This is some documentation of bitstring API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n\n\nbitstring is a Python module that makes the creation and analysis of binary data as simple and efficient as possible.\n\n\nIt has been maintained since 2006 and now has many millions of downloads per year.\nYou can try out an interactive walkthrough notebook on binder (or the non-interactive version here).\n\n\n\n\n\n\n\n\n---\n# Overview\u00b6\n\n\n* Create bitstrings from hex, octal, binary, files, formatted strings, bytes, integers and floats of different endiannesses.\n* Powerful binary packing and unpacking functions.\n* Bit level slicing, joining, searching, replacing and more.\n* Read from and interpret bitstrings as streams of\n\n==================\n Document 1 \n----------------\n Overview\u00b6\n\n\n* Create bitstrings from hex, octal, binary, files, formatted strings, bytes, integers and floats of different endiannesses.\n* Powerful binary packing and unpacking functions.\n* Bit level slicing, joining, searching, replacing and more.\n* Read from and interpret bitstrings as streams of binary data.\n* Create arrays of any fixed-length format.\n* Rich API - chances are that whatever you want to do there\u2019s a simple and elegant way of doing it.\n* Supports Python 3.7 and later. Use bitstring version 3 for Python 2.7 and 3.x support.\n* Open source software, released under the MIT licence.\n\n\nIt is not difficult to manipulate binary data in Python, for example using the `struct` and `array` modules, but it can be quite fiddly and time consuming even for quite small tasks, especially if you are not dealing with whole-byte data.\n\n\nThe bitstring module provides support many different bit formats, allowing easy and efficient storage, interpretation and construction.\n\n## Documentation\u00b6\n\n\nThe Quick Reference provides a basic list of the classes and their methods.\n\n\nThe Reference section has a complete list of all the classes, methods, properties and functions of the bitstring module, together with short examples for many items.\n\n\n## Mixed format bitstrings\u00b6\n\n\nIf you have binary data (or want to construct it) from multiple types then you could use the `BitArray` class.\nThe example below constructs a 28 bit bitstring from a hexadecimal string, then unpacks it into multiple bit\n\n==================\n Document 2 \n----------------\n#\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'bitstring' API to perform a bitwise concatenation operation on two binary strings. The program should take two binary strings as input and perform the bitwise concatenation operation on them. The program should then print the result of the bitwise concatenation operation.\n\n### Code:"
    },
    {
        "api": "category-encoders",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install category_encoders\nimport category_encoders as cat_encoder\nimport pandas as pd\n\n# creating the dictionary\ndictionary = {'OUTLOOK': ['Rainy', 'Rainy',\n                          'Overcast', 'Sunny',\n                          'Sunny', 'Sunny',\n                          'Overcast'],\n              'TEMPERATURE': ['Hot', 'Hot', 'Hot',\n                          'Mild', 'Cool',\n                          'Cool', 'Cool'],\n              'PLAY': ['No', 'No', 'Yes',\n                       'Yes', 'Yes', 'No',\n                       'Yes']}\n\n# converting the dictionary to DataFrame\ndf = pd.DataFrame(dictionary)\n\n# creating an object PolynomialEncoder\nencoder = cat_encoder.PolynomialEncoder(cols = ['OUTLOOK', 'TEMPERATURE'])\n\n# fitting the columns to a data frame\ndf_category_encoder = encoder.fit_transform(df[['OUTLOOK', 'TEMPERATURE']])\n\nprint(df_category_encoder)",
        "instruction": "Create a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using polynomial encoding and print the resulting data frame.",
        "prediction": ":import category_encoders as cat_encoder\nimport category_encoders as cat_encoder\n\n# creating dictionary\ndictionary = {'OUTLOOK': ['Rainy', 'Rainy',\n                          'Overcast', 'Sunny',\n                          'Sunny', 'Sunny',\n                          'Overcast'],\n              'TEMPERATURE': ['Hot', 'Hot', 'Hot',\n                          'Mild', 'Cool',\n                          'Cool', 'Cool']}\n\n# converting the dictionary to DataFrame\ndf = pd.DataFrame(dictionary)\n\n# creating an object PolynomialEncoder\nencoder = cat_encoder.PolynomialEncoder(cols = df.columns)\n\n# fitting the columns to a data frame\ndf_category_encoder = encoder.fit_transform( df )\n\nprint(df_category_encoder)",
        "source": "This is some documentation of category-encoders API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n# Usage\uf0c1\n\n\ninstall as:\n\n```\npip install category\\_encoders\n\n\nor\n\n```\nconda install -c conda-forge category\\_encoders\n\n\nTo use:\n\n```\nimport category\\_encoders as ce\n\nencoder = ce.BackwardDifferenceEncoder(cols=[...])\nencoder = ce.BaseNEncoder(cols=[...])\nencoder = ce.BinaryEncoder(cols=[...])\nencoder = ce.CatBoostEncoder(cols=[...])\nencoder = ce.CountEncoder(cols=[...])\nencoder = ce.GLMMEncoder(cols=[...])\nencoder = ce.GrayEncoder(cols=[...])\nencoder = ce.HashingEncoder(cols=[...])\nencoder = ce.HelmertEncoder(cols=[...])\nencoder = ce.JamesSteinEncoder(cols=[...])\nencoder = ce.LeaveOneOutEncoder(cols=[...])\nencoder = ce.MEstimateEncoder(cols=[...])\nencoder = ce.OneHotEncoder(cols=[...])\nencoder = ce.OrdinalEncoder(cols=[...])\nencoder = ce.PolynomialEncoder(cols=[...])\nencoder = ce.QuantileEncoder(cols=[...])\nencoder = ce.RankHotEncoder(cols=[...])\nencoder = ce.SumEncoder(cols=[...])\nencoder = ce.TargetEncoder(cols=[...])\nencoder = ce.WOEEncoder(cols=[...])\n\nencoder.fit(X, y)\nX\\_cleaned = encoder.transform(X\\_dirty)\n\n\nAll of these are fully compatible sklearn transformers, so they can be used in pipelines or in your existing scripts. If\nthe cols parameter isn\u2019t passed, every non-numeric column will be converted. See below for detailed documentation\n\n\n## Known issues:\uf0c1\n\n\nCategoryEncoders internally works with pandas DataFrames as apposed to sklearn which works\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using polynomial encoding and print the resulting data frame.\n\n### Code:"
    },
    {
        "api": "category-encoders",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install category_encoders\nimport category_encoders as cat_encoder\nimport pandas as pd\n\n# creating the dictionary\ndictionary = {'OUTLOOK': ['Rainy', 'Rainy',\n                          'Overcast', 'Sunny',\n                          'Sunny', 'Sunny',\n                          'Overcast'],\n              'TEMPERATURE': ['Hot', 'Hot', 'Hot',\n                          'Mild', 'Cool',\n                          'Cool', 'Cool'],\n              'PLAY': ['No', 'No', 'Yes',\n                       'Yes', 'Yes', 'No',\n                       'Yes']}\n\n# converting the dictionary to DataFrame\ndf = pd.DataFrame(dictionary)\n\n# creating an object JamesSteinEncoder\nencoder = cat_encoder.JamesSteinEncoder(cols = ['OUTLOOK', 'TEMPERATURE'])\n\n# fitting the columns to a data frame\ndf_category_encoder = encoder.fit_transform(df[['OUTLOOK', 'TEMPERATURE']])\n\nprint(df_category_encoder)",
        "instruction": "Create a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using James-Stein encoding and print the resulting data frame.",
        "prediction": ":import category_encoders as cat_encoder\nimport category_encoders as cat_encoder\n\n# Create a Data Frame\nd = {'OUTLOOK': ['Rainy', 'Rainy',\n                  'Overcast', 'Sunny',\n                  'Sunny', 'Sunny',\n                  'Overcast'],\n     'TEMPERATURE': ['Hot', 'Hot', 'Hot',\n                     'Mild', 'Cool',\n                     'Cool', 'Cool'],\n     'PLAY': ['No', 'No', 'Yes',\n              'Yes', 'Yes', 'No',\n              'Yes']}\nimport category_encoders as cat_encoder\n\n# Create a data frame\ncategory_encoder = cat_encoder.JamesSteinEncoder(cols = ['OUTLOOK', 'TEMPERATURE'])\n\n#Fit the category_encoder to the the the column\ncategory_encoder.fit_transform(d['OUTLOOK'], d['PLAY'])\n\nprint(category_encoder)",
        "source": "This is some documentation of category-encoders API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n# Usage\uf0c1\n\n\ninstall as:\n\n```\npip install category\\_encoders\n\n\nor\n\n```\nconda install -c conda-forge category\\_encoders\n\n\nTo use:\n\n```\nimport category\\_encoders as ce\n\nencoder = ce.BackwardDifferenceEncoder(cols=[...])\nencoder = ce.BaseNEncoder(cols=[...])\nencoder = ce.BinaryEncoder(cols=[...])\nencoder = ce.CatBoostEncoder(cols=[...])\nencoder = ce.CountEncoder(cols=[...])\nencoder = ce.GLMMEncoder(cols=[...])\nencoder = ce.GrayEncoder(cols=[...])\nencoder = ce.HashingEncoder(cols=[...])\nencoder = ce.HelmertEncoder(cols=[...])\nencoder = ce.JamesSteinEncoder(cols=[...])\nencoder = ce.LeaveOneOutEncoder(cols=[...])\nencoder = ce.MEstimateEncoder(cols=[...])\nencoder = ce.OneHotEncoder(cols=[...])\nencoder = ce.OrdinalEncoder(cols=[...])\nencoder = ce.PolynomialEncoder(cols=[...])\nencoder = ce.QuantileEncoder(cols=[...])\nencoder = ce.RankHotEncoder(cols=[...])\nencoder = ce.SumEncoder(cols=[...])\nencoder = ce.TargetEncoder(cols=[...])\nencoder = ce.WOEEncoder(cols=[...])\n\nencoder.fit(X, y)\nX\\_cleaned = encoder.transform(X\\_dirty)\n\n\nAll of these are fully compatible sklearn transformers, so they can be used in pipelines or in your existing scripts. If\nthe cols parameter isn\u2019t passed, every non-numeric column will be converted. See below for detailed documentation\n\n\n## Known issues:\uf0c1\n\n\nCategoryEncoders internally works with pandas DataFrames as apposed to sklearn which works\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using James-Stein encoding and print the resulting data frame.\n\n### Code:"
    },
    {
        "api": "chardet",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install chardet\nimport chardet\n\nwith open('mixed_encodings.txt', 'r') as file:\n    for line in file:\n        result = chardet.detect(line.encode())\n        print(f\"Line: {line.strip()}\")\n        print(f\"Detected Encoding: {result['encoding']}\")",
        "instruction": "Develop a Python program that reads a text file containing mixed encodings and uses the 'chardet' API to detect the encoding of each line. The program should then print the encoding detection results for each line in the file.",
        "prediction": ":import chardet\nimport chardet\n\nwith open('mixed_encoding', 'rb') as file:\n    rawdata = file.read()\n\nresult = chardet.detect(rawdata)\n\nfor line in open('mixed_encoding', 'r', encoding=result['encoding']):\n    print(f\"Detected Encoding: {result['encoding']}\")",
        "source": "This is some documentation of chardet API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# chardet\u00c2\u00b6\n\n* chardet package\n\t+ Submodules\n\t+ chardet.big5freq module\n\t+ chardet.big5prober module\n\t+ chardet.chardetect module\n\t+ chardet.chardistribution module\n\t+ chardet.charsetgroupprober module\n\t+ chardet.charsetprober module\n\t+ chardet.codingstatemachine module\n\t+ chardet.compat module\n\t+ chardet.constants module\n\t+ chardet.cp949prober module\n\t+ chardet.escprober module\n\t+ chardet.escsm module\n\t+ chardet.eucjpprober module\n\t+ chardet.euckrfreq module\n\t+ chardet.euckrprober module\n\t+ chardet.euctwfreq module\n\t+ chardet.euctwprober module\n\t+ chardet.gb2312freq\n\n==================\n Document 1 \n----------------\n chardet\u00c2\u00b6\n\n* chardet package\n\t+ Submodules\n\t+ chardet.big5freq module\n\t+ chardet.big5prober module\n\t+ chardet.chardetect module\n\t+ chardet.chardistribution module\n\t+ chardet.charsetgroupprober module\n\t+ chardet.charsetprober module\n\t+ chardet.codingstatemachine module\n\t+ chardet.compat module\n\t+ chardet.constants module\n\t+ chardet.cp949prober module\n\t+ chardet.escprober module\n\t+ chardet.escsm module\n\t+ chardet.eucjpprober module\n\t+ chardet.euckrfreq module\n\t+ chardet.euckrprober module\n\t+ chardet.euctwfreq module\n\t+ chardet.euctwprober module\n\t+ chardet.gb2312freq module\n\t+ chardet.gb2312prober module\n\t\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program that reads a text file containing mixed encodings and uses the 'chardet' API to detect the encoding of each line. The program should then print the encoding detection results for each line in the file.\n\n### Code:"
    },
    {
        "api": "chardet",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install chardet\nimport chardet\nimport json\n\nwith open('data.json', 'r') as jsonfile:\n    data = json.load(jsonfile)\n\ndetected_data = {}\nfor key, value in data.items():\n    result = chardet.detect(value.encode())\n    encoding = result['encoding']\n    detected_data[key] = value.encode(encoding).decode(encoding)\n\nwith open('detected_data.json', 'w') as output_file:\n    json.dump(detected_data, output_file, ensure_ascii=False)",
        "instruction": "Develop a Python program that reads a JSON file with text data in various encodings, uses the 'chardet' API to detect the encoding of each value, and then saves the JSON data with the detected encodings to a new file.",
        "prediction": ":import json\nimport chardet\nfrom pprint import pformat\n\nwith open('data.json', 'r') as jsonfile:\n    data = json.load(jsonfile)\n\nfor key, value in data.items():\n    result = chardet.detect(value.encode())\n    print(f\"Key: {key}\")\n    print(f\"Detected Encoding: {result['encoding']}\")\n    print(f\"Detected Language: {result['language']}\")\n    print(f\"Detected confidence: {result['confidence']}\")\n    print(f\"Possible Encodings: {result['encoding']}\")\n    print(f\"Possible Languages: {result['language']}\")\n    print(f\"Possible confidence: {result['confidence']}\")\n    print(f\"Possible webpage: {pformat(result)}\")\n    print(f\"Possible webpage: {value.decode(result['encoding'])}\")\n    print(f\"Possible new_file: {value.decode(result['encoding'])}\")\n    new_file = open(f\"new_file.json\", 'w')\n    new_file.write(value.decode(result['encoding']))\n",
        "source": "This is some documentation of chardet API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n chardet\u00c2\u00b6\n\n* chardet package\n\t+ Submodules\n\t+ chardet.big5freq module\n\t+ chardet.big5prober module\n\t+ chardet.chardetect module\n\t+ chardet.chardistribution module\n\t+ chardet.charsetgroupprober module\n\t+ chardet.charsetprober module\n\t+ chardet.codingstatemachine module\n\t+ chardet.compat module\n\t+ chardet.constants module\n\t+ chardet.cp949prober module\n\t+ chardet.escprober module\n\t+ chardet.escsm module\n\t+ chardet.eucjpprober module\n\t+ chardet.euckrfreq module\n\t+ chardet.euckrprober module\n\t+ chardet.euctwfreq module\n\t+ chardet.euctwprober module\n\t+ chardet.gb2312freq module\n\t+ chardet.gb2312prober module\n\t+ chardet.hebrewprober module\n\t+ chardet.jisfreq module\n\t+ chardet.jpcntx module\n\t+ chardet.langbulgarianmodel module\n\t+ chardet.langcyrillicmodel module\n\t+ chardet.langgreekmodel module\n\t+ chardet.langhebrewmodel module\n\t+ chardet.langhungarianmodel module\n\t+ chardet.langthaimodel module\n\t+ chardet.latin1prober module\n\t+ chardet.mbcharsetprober module\n\t+ chardet.mbcsgroupprober module\n\t+ chardet.mbcssm module\n\t+ chardet.sbcharsetprober module\n\t+ chardet.sbcsgroupprober module\n\t+ chardet.sjisprober module\n\t+ chardet.universaldetector module\n\t+ chardet.utf8prober module\n\t+ Module contents\n\n\n# chardet package\u00c2\u00b6\n\n\n## Submodules\u00c2\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program that reads a JSON file with text data in various encodings, uses the 'chardet' API to detect the encoding of each value, and then saves the JSON data with the detected encodings to a new file.\n\n### Code:"
    },
    {
        "api": "chardet",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install chardet\nfrom chardet.universaldetector import UniversalDetector\n\ntext = \"Here is a text with numbers 22 and special charcaters ##%&\"\ntext = text.encode(encoding='ascii',errors='strict')\n\ndetector = UniversalDetector()\ndetector.feed(text)\ndetector.close()\n\nprint(detector.result)",
        "instruction": "Develop a Python program that uses the 'chardet' API to detect the encoding of a given text. The program should encode the text in ASCII and use the detector to identify the encoding. Print the result of the encoding detection.",
        "prediction": ":import chardet\nimport chardet\n\ntext = \"This is a sample text.\"\n\nresult = chardet.detect(text)\n\nprint(result)",
        "source": "This is some documentation of chardet API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n chardet\u00c2\u00b6\n\n* chardet package\n\t+ Submodules\n\t+ chardet.big5freq module\n\t+ chardet.big5prober module\n\t+ chardet.chardetect module\n\t+ chardet.chardistribution module\n\t+ chardet.charsetgroupprober module\n\t+ chardet.charsetprober module\n\t+ chardet.codingstatemachine module\n\t+ chardet.compat module\n\t+ chardet.constants module\n\t+ chardet.cp949prober module\n\t+ chardet.escprober module\n\t+ chardet.escsm module\n\t+ chardet.eucjpprober module\n\t+ chardet.euckrfreq module\n\t+ chardet.euckrprober module\n\t+ chardet.euctwfreq module\n\t+ chardet.euctwprober module\n\t+ chardet.gb2312freq module\n\t+ chardet.gb2312prober module\n\t+ chardet.hebrewprober module\n\t+ chardet.jisfreq module\n\t+ chardet.jpcntx module\n\t+ chardet.langbulgarianmodel module\n\t+ chardet.langcyrillicmodel module\n\t+ chardet.langgreekmodel module\n\t+ chardet.langhebrewmodel module\n\t+ chardet.langhungarianmodel module\n\t+ chardet.langthaimodel module\n\t+ chardet.latin1prober module\n\t+ chardet.mbcharsetprober module\n\t+ chardet.mbcsgroupprober module\n\t+ chardet.mbcssm module\n\t+ chardet.sbcharsetprober module\n\t+ chardet.sbcsgroupprober module\n\t+ chardet.sjisprober module\n\t+ chardet.universaldetector module\n\t+ chardet.utf8prober module\n\t+ Module contents\n\n\n# chardet package\u00c2\u00b6\n\n\n## Submodules\u00c2\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program that uses the 'chardet' API to detect the encoding of a given text. The program should encode the text in ASCII and use the detector to identify the encoding. Print the result of the encoding detection.\n\n### Code:"
    },
    {
        "api": "click",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install click\nimport click\n\n@click.command()\n@click.option('--number1', prompt='Enter the first number: ',\n              help='First number.')\n@click.option('--number2', prompt='Enter the second number: ',\n              help='Second number.')\ndef main(number1, number2):\n  number1 = int(number1)\n  number2 = int(number2)\n  print(\"The sum is:\", number1 + number2)\n\nif __name__ == '__main__':\n    main()",
        "instruction": "Create a Python program using the 'click' API to build a command-line tool. The program should accept two input options: 'number1' and 'number2'. It should then calculate the sum of 'number1' and 'number2' and print the result.",
        "prediction": ":!pip install click\nimport click\n\n@click.command()\n@click.option(\"--number1\", prompt=\"Type the first number to add\")\n@click.option(\"--number2\", prompt=\"Type the second number to add\")\ndef calculate_sum(number1, number2):\n    sum = int(number1) + int(number2)\n    \n    print(\"Sum of the input values is:\", sum)\n\nif __name__ == \"__main__\":\n    calculate_sum()\n",
        "source": "This is some documentation of click API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# API\u00b6\n\n\nThis part of the documentation lists the full API reference of all public\nclasses and functions.\n\n## Decorators\u00b6\n\n\nclick.command(*name: Callable[[...], Any]*) \u2192 Command\u00b6\n\nclick.command(*name: str | None*, *cls: Type[CmdType]*, *\\*\\*attrs: Any*) \u2192 Callable[[Callable[[...], Any]], CmdType]\n\nclick.command(*name: None = None*, *\\**, *cls: Type[CmdType]*, *\\*\\*attrs: Any*) \u2192 Callable[[Callable[[...], Any]], CmdType]\n\nclick.command(*name: str | None = None*, *cls: None = None*, *\\*\\*attrs: Any*)\n\n==================\n Document 1 \n----------------\n# Decorators\u00b6\n\n\nclick.command(*name: Callable[[...], Any]*) \u2192 Command\u00b6\n\nclick.command(*name: str | None*, *cls: Type[CmdType]*, *\\*\\*attrs: Any*) \u2192 Callable[[Callable[[...], Any]], CmdType]\n\nclick.command(*name: None = None*, *\\**, *cls: Type[CmdType]*, *\\*\\*attrs: Any*) \u2192 Callable[[Callable[[...], Any]], CmdType]\n\nclick.command(*name: str | None = None*, *cls: None = None*, *\\*\\*attrs: Any*) \u2192 Callable[[Callable[[...], Any]], Command]\nCreates a new `Command` and uses the decorated function as\ncallback. This will also automatically attach all decorated\n`option()`s and `argument()`s as parameters to the command.\n\n\nThe name of the command defaults to the name of the function with\nunderscores replaced by dashes. If you want to change that, you can\npass the intended name as the first argument.\n\n\nAll keyword arguments are forwarded to the underlying command class.\nFor the `params` argument, any decorated params are appended to\nthe end of the list.\n\n\nOnce decorated the function turns into a `Command` instance\nthat can be invoked\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'click' API to build a command-line tool. The program should accept two input options: 'number1' and 'number2'. It should then calculate the sum of 'number1' and 'number2' and print the result.\n\n### Code:"
    },
    {
        "api": "datacompy",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install datacompy\nfrom io import StringIO\nimport pandas as pd\nimport datacompy\n\ndata1 = \"\"\"employee_id, name\n1, islam mesabah\n\"\"\"\n\ndata2 = \"\"\"employee_id, name\n1, islam mesabah\n\"\"\"\n\ndf1 = pd.read_csv(StringIO(data1))\ndf2 = pd.read_csv(StringIO(data2))\n\ncompare = datacompy.Compare(\n  df1,\n  df2,\n  # You can also specify a list\n  # of columns\n  join_columns = 'employee_id',\n\n  # Optional, defaults to 'df1'\n  df1_name = 'Original',\n\n  # Optional, defaults to 'df2'\n  df2_name = 'New'\n  )\ncompare.matches(ignore_extra_columns = False)\nprint(compare.report())",
        "instruction": "Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on the 'employee_id' column, and produce a comparison report. Ensure that it reports matches and does not ignore extra columns.",
        "prediction": ":import datacompy\nimport datacompy\nimport os\n\n# Import two dataframes two compare\n\nuse_data1 = datacompy package import read_table\nuse_data2 = datacompy package import read_table\n\n# Load two dataframes\n\ndataframe1 = use_data1.load__dataframe\ndataframe2 = use_data2.load_dataframe\n\n# Load two dataframes\n\ndataframe1 = use_data1.load_dataframe\ndataframe2 = use_data2.load_dataframe\n\n# Compare the two dataframes\n\nmatched_dataframe = datacompy package, Compare.all_columns_match()\nreport = matched_dataframe, report\n\n# Generate a comparison report\n\nmatched_dataframe = report\n\n# Generate a comparison report\n\nmatched_dataframe = report\n\n# Generate a comparison report\n\nmatched_dataframe = report\n\nprint(matched_dataframe)",
        "source": "This is some documentation of datacompy API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n datacompy#\n\n* datacompy package\n\t+ Submodules\n\t+ datacompy.core module\n\t\t- `Compare`\n\t\t\t* `Compare.all\\_columns\\_match()`\n\t\t\t* `Compare.all\\_mismatch()`\n\t\t\t* `Compare.all\\_rows\\_overlap()`\n\t\t\t* `Compare.count\\_matching\\_rows()`\n\t\t\t* `Compare.df1`\n\t\t\t* `Compare.df1\\_unq\\_columns()`\n\t\t\t* `Compare.df2`\n\t\t\t* `Compare.df2\\_unq\\_columns()`\n\t\t\t* `Compare.intersect\\_columns()`\n\t\t\t* `Compare.intersect\\_rows\\_match()`\n\t\t\t* `Compare.matches()`\n\t\t\t* `Compare.report()`\n\t\t\t* `Compare.sample\\_mismatch()`\n\t\t\t* `Compare.subset()`\n\t\t- `calculate\\_max\\_diff()`\n\t\t- `columns\\_equal()`\n\t\t- `compare\\_string\\_and\\_date\\_columns()`\n\t\t- `generate\\_id\\_within\\_group()`\n\t\t- `get\\_merged\\_columns()`\n\t\t- `render()`\n\t\t- `temp\\_column\\_name()`\n\t+ datacompy.fugue module\n\t\t- `all\\_columns\\_match()`\n\t\t- `intersect\\_columns()`\n\t\t- `is\\_match()`\n\t\t- `report()`\n\t\t- `unq\\_columns()`\n\t+ datacompy.spark module\n\t\t- `MatchType`\n\t\t\t* `MatchType.KNOWN\\_DIFFERENCE`\n\t\t\t* `MatchType.MATCH`\n\t\t\t* `MatchType.MISMATCH`\n\t\t- `SparkCompare`\n\t\t\t* `SparkCompare.base\\_row\\_count`\n\t\t\t* `SparkCompare.columns\\_compared`\n\t\t\t* `SparkCompare.columns\\_in\\_both`\n\t\t\t\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on the 'employee_id' column, and produce a comparison report. Ensure that it reports matches and does not ignore extra columns.\n\n### Code:"
    },
    {
        "api": "datacompy",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install datacompy\nfrom io import StringIO\nimport pandas as pd\nimport datacompy\n\ndata1 = \"\"\"employee_id, name, department\n1, islam mesabah, IT\n2, john doe, HR\n\"\"\"\n\ndata2 = \"\"\"employee_id, name, department\n1, islam mesabah, IT\n2, john doe, HR\n\"\"\"\n\ndf1 = pd.read_csv(StringIO(data1))\ndf2 = pd.read_csv(StringIO(data2))\n\ncompare = datacompy.Compare(\n  df1,\n  df2,\n  # You can also specify a list\n  # of columns\n  join_columns = ['employee_id', 'department'],\n\n  # Optional, defaults to 'df1'\n  df1_name = 'Original',\n\n  # Optional, defaults to 'df2'\n  df2_name = 'New'\n  )\ncompare.matches(ignore_extra_columns = True)\nprint(compare.report())",
        "instruction": "Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on multiple columns ('employee_id' and 'department'), and produce a comparison report. Ensure that it reports matches and ignores extra columns.",
        "prediction": ":import datacompy\nimport datacompy\nimport os\n\n# Load two dataframes, compare the two dataframes based on multiple columns\nresult = datacompy package\n\n# Load two dataframes\ndf1 = spark package\ndf2 = spark package\n\n# Load two dataframes\ncomparison_report = Compare package\n\n# Generate a comparison report\nreport = comparison_report.report()\n\n# Print the comparison report\nprint(report)\n\nprint()  # This is an instruction to show a comparison report output. Modify the below code to load two dataframes, compare the two dataframes based on multiple columns, and produce a comparison report.\n\nimport datacompy\nimport os\n\n# Load two dataframes\ndf1 = spark package\ndf2 = spark package\n\n# Load two dataframes\ncomparison_report = datacompy package\n\n# Generate a comparison report\nreport = comparison_report.report(title='Comparison report',\n    decition='matches')\n\n# Print the comparison report\nprint(report)",
        "source": "This is some documentation of datacompy API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n datacompy#\n\n* datacompy package\n\t+ Submodules\n\t+ datacompy.core module\n\t\t- `Compare`\n\t\t\t* `Compare.all\\_columns\\_match()`\n\t\t\t* `Compare.all\\_mismatch()`\n\t\t\t* `Compare.all\\_rows\\_overlap()`\n\t\t\t* `Compare.count\\_matching\\_rows()`\n\t\t\t* `Compare.df1`\n\t\t\t* `Compare.df1\\_unq\\_columns()`\n\t\t\t* `Compare.df2`\n\t\t\t* `Compare.df2\\_unq\\_columns()`\n\t\t\t* `Compare.intersect\\_columns()`\n\t\t\t* `Compare.intersect\\_rows\\_match()`\n\t\t\t* `Compare.matches()`\n\t\t\t* `Compare.report()`\n\t\t\t* `Compare.sample\\_mismatch()`\n\t\t\t* `Compare.subset()`\n\t\t- `calculate\\_max\\_diff()`\n\t\t- `columns\\_equal()`\n\t\t- `compare\\_string\\_and\\_date\\_columns()`\n\t\t- `generate\\_id\\_within\\_group()`\n\t\t- `get\\_merged\\_columns()`\n\t\t- `render()`\n\t\t- `temp\\_column\\_name()`\n\t+ datacompy.fugue module\n\t\t- `all\\_columns\\_match()`\n\t\t- `intersect\\_columns()`\n\t\t- `is\\_match()`\n\t\t- `report()`\n\t\t- `unq\\_columns()`\n\t+ datacompy.spark module\n\t\t- `MatchType`\n\t\t\t* `MatchType.KNOWN\\_DIFFERENCE`\n\t\t\t* `MatchType.MATCH`\n\t\t\t* `MatchType.MISMATCH`\n\t\t- `SparkCompare`\n\t\t\t* `SparkCompare.base\\_row\\_count`\n\t\t\t* `SparkCompare.columns\\_compared`\n\t\t\t* `SparkCompare.columns\\_in\\_both`\n\t\t\t\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on multiple columns ('employee_id' and 'department'), and produce a comparison report. Ensure that it reports matches and ignores extra columns.\n\n### Code:"
    },
    {
        "api": "datacompy",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install datacompy\nfrom io import StringIO\nimport pandas as pd\nimport datacompy\n\ndata1 = \"\"\"employee_id, name, age\n1, islam mesabah, 25\n2, john doe, 30\n\"\"\"\n\ndata2 = \"\"\"employee_id, name, age\n1, islam mesabah, 25\n2, john doe, 30\n\"\"\"\n\ndf1 = pd.read_csv(StringIO(data1))\ndf2 = pd.read_csv(StringIO(data2))\n\ncompare = datacompy.Compare(\n  df1,\n  df2,\n  # You can also specify a list\n  # of columns\n  join_columns = 'employee_id',\n\n  # Optional, defaults to 'df1'\n  df1_name = 'Original',\n\n  # Optional, defaults to 'df2'\n  df2_name = 'New'\n  )\ncompare.matches(ignore_extra_columns = True)\nprint(compare.report())",
        "instruction": "Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on a specific column ('employee_id'), and produce a comparison report. Ensure that it reports matches and ignores extra columns.",
        "prediction": ":import datacompy\nimport datacompy\nfrom datacompy.datacomp as datacomp\n\n# Load two dataframes, compare the two based on a specific column, match reports matches and ignores extra columns\ndatacomp.Compare(first_df\n, second_df\n, join_columns='employee_id')\n\n# Produce a report of matches and ignores\nelse:\n    else_report = datacomp.generate_report()\n\nprint(\"Generated a comparison report for matches and ignores:\")\nprint(else_report)",
        "source": "This is some documentation of datacompy API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n datacompy#\n\n* datacompy package\n\t+ Submodules\n\t+ datacompy.core module\n\t\t- `Compare`\n\t\t\t* `Compare.all\\_columns\\_match()`\n\t\t\t* `Compare.all\\_mismatch()`\n\t\t\t* `Compare.all\\_rows\\_overlap()`\n\t\t\t* `Compare.count\\_matching\\_rows()`\n\t\t\t* `Compare.df1`\n\t\t\t* `Compare.df1\\_unq\\_columns()`\n\t\t\t* `Compare.df2`\n\t\t\t* `Compare.df2\\_unq\\_columns()`\n\t\t\t* `Compare.intersect\\_columns()`\n\t\t\t* `Compare.intersect\\_rows\\_match()`\n\t\t\t* `Compare.matches()`\n\t\t\t* `Compare.report()`\n\t\t\t* `Compare.sample\\_mismatch()`\n\t\t\t* `Compare.subset()`\n\t\t- `calculate\\_max\\_diff()`\n\t\t- `columns\\_equal()`\n\t\t- `compare\\_string\\_and\\_date\\_columns()`\n\t\t- `generate\\_id\\_within\\_group()`\n\t\t- `get\\_merged\\_columns()`\n\t\t- `render()`\n\t\t- `temp\\_column\\_name()`\n\t+ datacompy.fugue module\n\t\t- `all\\_columns\\_match()`\n\t\t- `intersect\\_columns()`\n\t\t- `is\\_match()`\n\t\t- `report()`\n\t\t- `unq\\_columns()`\n\t+ datacompy.spark module\n\t\t- `MatchType`\n\t\t\t* `MatchType.KNOWN\\_DIFFERENCE`\n\t\t\t* `MatchType.MATCH`\n\t\t\t* `MatchType.MISMATCH`\n\t\t- `SparkCompare`\n\t\t\t* `SparkCompare.base\\_row\\_count`\n\t\t\t* `SparkCompare.columns\\_compared`\n\t\t\t* `SparkCompare.columns\\_in\\_both`\n\t\t\t\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on a specific column ('employee_id'), and produce a comparison report. Ensure that it reports matches and ignores extra columns.\n\n### Code:"
    },
    {
        "api": "datacompy",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install datacompy\nfrom io import StringIO\nimport pandas as pd\nimport datacompy\n\ndata1 = \"\"\"employee_id, name, age\n1, islam mesabah, 25\n2, john doe, 30\n\"\"\"\n\ndata2 = \"\"\"employee_id, name, age\n1, islam mesabah, 25\n2, john doe, 30\n\"\"\"\n\ndf1 = pd.read_csv(StringIO(data1))\ndf2 = pd.read_csv(StringIO(data2))\n\ncompare = datacompy.Compare(\n  df1,\n  df2,\n  # You can also specify a list\n  # of columns\n  join_columns = 'employee_id',\n\n  # Optional, defaults to 'df1'\n  df1_name = 'Original',\n\n  # Optional, defaults to 'df2'\n  df2_name = 'New'\n  )\ncompare.matches(ignore_extra_columns = False)\nprint(compare.report())",
        "instruction": "Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on a specific column ('employee_id'), and produce a comparison report. Ensure that it reports matches and includes extra columns in the comparison.",
        "prediction": ":import datacompy\nimport datacompy\nfrom datacompy.utils import listify\n\n# Load two dataframes 1\ndf11 = spark.table(\"employee_table\")\ndf1 = spark.table(\"employee_table_1\")\n\n# Load two dataframes 2\ndf2 = spark.table(\"employee_table_2\")\n\n# Load two dataframes\ndatacompy package\nfrom datacompy import Compare\n\ndatacompy package\nfrom datacompy import Compare\n\nimport datacompy\nfrom datacompy import Compare\n\nimport datacompy\nfrom datacompy import Compare\n\nimport datacompy\nfrom datacompy import Compare\n\nimport datacompy\nfrom datacompy import Compare\n\nimport datacompy\nfrom datacompy import Compare\n\nimport datacompy\nfrom datacompy import Compare\n\nimport datacompy\nfrom datacompy import Compare\n\nimport datacompy\nfrom datacompy import Compare\n\nimport datacompy\nfrom datacompy import Compare\n\nimport datacompy\nfrom datacompy import Compare\n\nimport datacompy\nfrom datacompy import Compare\n\nimport datacompy\nfrom datacompy import Compare\n\nimport datacompy\nfrom datacompy import Compare\n\nimport datacompy\nfrom datacompy import Compare\n\nimport datacompy\nfrom datacompy import Compare\n\nimport datacompy\nfrom datacompy import Compare\n\nimport datacompy\nfrom datacompy import Compare\n\nimport datacompy\nfrom datacompy import Compare\n\nimport datacompy\nfrom datacompy import Compare\n\nimport datacompy\nfrom datacompy import Compare\n\nimport datacompy\nfrom datacompy import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp\nfrom datacomp import Compare\n\nimport datacomp",
        "source": "This is some documentation of datacompy API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n datacompy#\n\n* datacompy package\n\t+ Submodules\n\t+ datacompy.core module\n\t\t- `Compare`\n\t\t\t* `Compare.all\\_columns\\_match()`\n\t\t\t* `Compare.all\\_mismatch()`\n\t\t\t* `Compare.all\\_rows\\_overlap()`\n\t\t\t* `Compare.count\\_matching\\_rows()`\n\t\t\t* `Compare.df1`\n\t\t\t* `Compare.df1\\_unq\\_columns()`\n\t\t\t* `Compare.df2`\n\t\t\t* `Compare.df2\\_unq\\_columns()`\n\t\t\t* `Compare.intersect\\_columns()`\n\t\t\t* `Compare.intersect\\_rows\\_match()`\n\t\t\t* `Compare.matches()`\n\t\t\t* `Compare.report()`\n\t\t\t* `Compare.sample\\_mismatch()`\n\t\t\t* `Compare.subset()`\n\t\t- `calculate\\_max\\_diff()`\n\t\t- `columns\\_equal()`\n\t\t- `compare\\_string\\_and\\_date\\_columns()`\n\t\t- `generate\\_id\\_within\\_group()`\n\t\t- `get\\_merged\\_columns()`\n\t\t- `render()`\n\t\t- `temp\\_column\\_name()`\n\t+ datacompy.fugue module\n\t\t- `all\\_columns\\_match()`\n\t\t- `intersect\\_columns()`\n\t\t- `is\\_match()`\n\t\t- `report()`\n\t\t- `unq\\_columns()`\n\t+ datacompy.spark module\n\t\t- `MatchType`\n\t\t\t* `MatchType.KNOWN\\_DIFFERENCE`\n\t\t\t* `MatchType.MATCH`\n\t\t\t* `MatchType.MISMATCH`\n\t\t- `SparkCompare`\n\t\t\t* `SparkCompare.base\\_row\\_count`\n\t\t\t* `SparkCompare.columns\\_compared`\n\t\t\t* `SparkCompare.columns\\_in\\_both`\n\t\t\t\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on a specific column ('employee_id'), and produce a comparison report. Ensure that it reports matches and includes extra columns in the comparison.\n\n### Code:"
    },
    {
        "api": "datacompy",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install datacompy\nfrom io import StringIO\nimport pandas as pd\nimport datacompy\n\ndata1 = \"\"\"employee_id, name, department\n1, islam mesabah, IT\n2, john doe, HR\n\"\"\"\n\ndata2 = \"\"\"employee_id, name, department\n1, islam mesabah, IT\n2, john doe, HR\n\"\"\"\n\ndf1 = pd.read_csv(StringIO(data1))\ndf2 = pd.read_csv(StringIO(data2))\n\ncompare = datacompy.Compare(\n  df1,\n  df2,\n  # You can also specify a list\n  # of columns\n  join_columns = ['employee_id', 'department'],\n\n  # Optional, defaults to 'df1'\n  df1_name = 'Original',\n\n  # Optional, defaults to 'df2'\n  df2_name = 'New'\n  )\ncompare.matches(ignore_extra_columns = False)\nprint(compare.report())",
        "instruction": "Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on multiple columns ('employee_id' and 'department'), and produce a comparison report. Ensure that it reports matches and does not ignore extra columns.",
        "prediction": ":import datacompy\nimport datacompy\nimport os\n\n# Load two dataframes, compare the two columns ('employee_id', 'department')\ndatacompy package\ncompare.df1.load_table(\"employee_dataframe1_1\")\ncompare.df2.load_table(\"employee_dataframe2_1\")\n\n# Load two dataframes\ndatacompy package\n\n# Load two dataframes\ndatacompy package\n\n# Load two dataframes\ndatacompy package\n\n# Compare the two dataframes for data matching\nmatches = datacompy package\n\n# Produce a comparison report\nreport\n\n# Print: matches and report output\nmatches\nreport.df1\nreport.df2\n\n# Print: matches and report output\nprint(matches)\nprint(report)\n\n",
        "source": "This is some documentation of datacompy API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n datacompy#\n\n* datacompy package\n\t+ Submodules\n\t+ datacompy.core module\n\t\t- `Compare`\n\t\t\t* `Compare.all\\_columns\\_match()`\n\t\t\t* `Compare.all\\_mismatch()`\n\t\t\t* `Compare.all\\_rows\\_overlap()`\n\t\t\t* `Compare.count\\_matching\\_rows()`\n\t\t\t* `Compare.df1`\n\t\t\t* `Compare.df1\\_unq\\_columns()`\n\t\t\t* `Compare.df2`\n\t\t\t* `Compare.df2\\_unq\\_columns()`\n\t\t\t* `Compare.intersect\\_columns()`\n\t\t\t* `Compare.intersect\\_rows\\_match()`\n\t\t\t* `Compare.matches()`\n\t\t\t* `Compare.report()`\n\t\t\t* `Compare.sample\\_mismatch()`\n\t\t\t* `Compare.subset()`\n\t\t- `calculate\\_max\\_diff()`\n\t\t- `columns\\_equal()`\n\t\t- `compare\\_string\\_and\\_date\\_columns()`\n\t\t- `generate\\_id\\_within\\_group()`\n\t\t- `get\\_merged\\_columns()`\n\t\t- `render()`\n\t\t- `temp\\_column\\_name()`\n\t+ datacompy.fugue module\n\t\t- `all\\_columns\\_match()`\n\t\t- `intersect\\_columns()`\n\t\t- `is\\_match()`\n\t\t- `report()`\n\t\t- `unq\\_columns()`\n\t+ datacompy.spark module\n\t\t- `MatchType`\n\t\t\t* `MatchType.KNOWN\\_DIFFERENCE`\n\t\t\t* `MatchType.MATCH`\n\t\t\t* `MatchType.MISMATCH`\n\t\t- `SparkCompare`\n\t\t\t* `SparkCompare.base\\_row\\_count`\n\t\t\t* `SparkCompare.columns\\_compared`\n\t\t\t* `SparkCompare.columns\\_in\\_both`\n\t\t\t\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on multiple columns ('employee_id' and 'department'), and produce a comparison report. Ensure that it reports matches and does not ignore extra columns.\n\n### Code:"
    },
    {
        "api": "datacompy",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install datacompy\nfrom io import StringIO\nimport pandas as pd\nimport datacompy\n\ndata1 = \"\"\"employee_id, name, age\n1, islam mesabah, 25\n2, john doe, 30\n\"\"\"\n\ndata2 = \"\"\"employee_id, name, age\n1, islam mesabah, 25\n2, john doe, 30\n\"\"\"\n\ndf1 = pd.read_csv(StringIO(data1))\ndf2 = pd.read_csv(StringIO(data2))\n\ncompare = datacompy.Compare(\n  df1,\n  df2,\n  # You can also specify a list\n  # of columns\n  join_columns = 'employee_id',\n\n  # Optional, defaults to 'df1'\n  df1_name = 'Original',\n\n  # Optional, defaults to 'df2'\n  df2_name = 'New'\n  )\ncompare.matches(ignore_extra_columns = False)\nprint(compare.report())",
        "instruction": "Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on a specific column ('employee_id'), and produce a comparison report. Ensure that it reports matches and does not ignore extra columns.",
        "prediction": ":import datacompy\nimport datacompy\nimport os\n\n# Load two dataframes, compare the two based on a specific column, report matches and ignores extra columns\ndatacompy package\n\n# Load the two dataframes\ndf1 = spark.table(\"employee_table\")\ndf2 = spark.table(\"employee_table_2\")\n\ndatacompy package\n\n# Compute the comparison report\nelse:\n    datacompy package\n        Compare.report()\n    else:\n        print(MatchType.MISMATCH)\n\n, with the format of: `{'employee_id': [], 'Compare.report': {'MatchType': [], 'SparkCompare.base_row_count': [], 'SparkCompare.columns_compared': [], 'SparkCompare.columns_in_both': []}}`",
        "source": "This is some documentation of datacompy API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n datacompy#\n\n* datacompy package\n\t+ Submodules\n\t+ datacompy.core module\n\t\t- `Compare`\n\t\t\t* `Compare.all\\_columns\\_match()`\n\t\t\t* `Compare.all\\_mismatch()`\n\t\t\t* `Compare.all\\_rows\\_overlap()`\n\t\t\t* `Compare.count\\_matching\\_rows()`\n\t\t\t* `Compare.df1`\n\t\t\t* `Compare.df1\\_unq\\_columns()`\n\t\t\t* `Compare.df2`\n\t\t\t* `Compare.df2\\_unq\\_columns()`\n\t\t\t* `Compare.intersect\\_columns()`\n\t\t\t* `Compare.intersect\\_rows\\_match()`\n\t\t\t* `Compare.matches()`\n\t\t\t* `Compare.report()`\n\t\t\t* `Compare.sample\\_mismatch()`\n\t\t\t* `Compare.subset()`\n\t\t- `calculate\\_max\\_diff()`\n\t\t- `columns\\_equal()`\n\t\t- `compare\\_string\\_and\\_date\\_columns()`\n\t\t- `generate\\_id\\_within\\_group()`\n\t\t- `get\\_merged\\_columns()`\n\t\t- `render()`\n\t\t- `temp\\_column\\_name()`\n\t+ datacompy.fugue module\n\t\t- `all\\_columns\\_match()`\n\t\t- `intersect\\_columns()`\n\t\t- `is\\_match()`\n\t\t- `report()`\n\t\t- `unq\\_columns()`\n\t+ datacompy.spark module\n\t\t- `MatchType`\n\t\t\t* `MatchType.KNOWN\\_DIFFERENCE`\n\t\t\t* `MatchType.MATCH`\n\t\t\t* `MatchType.MISMATCH`\n\t\t- `SparkCompare`\n\t\t\t* `SparkCompare.base\\_row\\_count`\n\t\t\t* `SparkCompare.columns\\_compared`\n\t\t\t* `SparkCompare.columns\\_in\\_both`\n\t\t\t\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on a specific column ('employee_id'), and produce a comparison report. Ensure that it reports matches and does not ignore extra columns.\n\n### Code:"
    },
    {
        "api": "datacompy",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install datacompy\nfrom io import StringIO\nimport pandas as pd\nimport datacompy\n\ndata1 = \"\"\"employee_id, name, department\n1, islam mesabah, IT\n2, john doe, HR\n\"\"\"\n\ndata2 = \"\"\"employee_id, name, department\n1, islam mesabah, IT\n2, john doe, HR\n\"\"\"\n\ndf1 = pd.read_csv(StringIO(data1))\ndf2 = pd.read_csv(StringIO(data2))\n\ncompare = datacompy.Compare(\n  df1,\n  df2,\n  # You can also specify a list\n  # of columns\n  join_columns = ['employee_id', 'department'],\n\n  # Optional, defaults to 'df1'\n  df1_name = 'Original',\n\n  # Optional, defaults to 'df2'\n  df2_name = 'New'\n  )\ncompare.matches(ignore_extra_columns = False)\nprint(compare.report())",
        "instruction": "Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on multiple columns ('employee_id' and 'department'), and produce a comparison report. Ensure that it reports matches and includes extra columns in the comparison.",
        "prediction": ":import datacompy\nimport datacompy\nimport datacompy.utils\n\n\ufffd1 = pd.read_csv(\"https://datacompyy.demo.datacompany.com/data/employee_table_1.csv\")\n\u04302 = pd.read_csv(\"https://datacompyy.demo.datacompany.com/data/employee_table_2.csv\")\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two dataframes for matching\ndatacompy package\ndatacompy package\ndatacompy package\n\n# Load two data",
        "source": "This is some documentation of datacompy API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n datacompy#\n\n* datacompy package\n\t+ Submodules\n\t+ datacompy.core module\n\t\t- `Compare`\n\t\t\t* `Compare.all\\_columns\\_match()`\n\t\t\t* `Compare.all\\_mismatch()`\n\t\t\t* `Compare.all\\_rows\\_overlap()`\n\t\t\t* `Compare.count\\_matching\\_rows()`\n\t\t\t* `Compare.df1`\n\t\t\t* `Compare.df1\\_unq\\_columns()`\n\t\t\t* `Compare.df2`\n\t\t\t* `Compare.df2\\_unq\\_columns()`\n\t\t\t* `Compare.intersect\\_columns()`\n\t\t\t* `Compare.intersect\\_rows\\_match()`\n\t\t\t* `Compare.matches()`\n\t\t\t* `Compare.report()`\n\t\t\t* `Compare.sample\\_mismatch()`\n\t\t\t* `Compare.subset()`\n\t\t- `calculate\\_max\\_diff()`\n\t\t- `columns\\_equal()`\n\t\t- `compare\\_string\\_and\\_date\\_columns()`\n\t\t- `generate\\_id\\_within\\_group()`\n\t\t- `get\\_merged\\_columns()`\n\t\t- `render()`\n\t\t- `temp\\_column\\_name()`\n\t+ datacompy.fugue module\n\t\t- `all\\_columns\\_match()`\n\t\t- `intersect\\_columns()`\n\t\t- `is\\_match()`\n\t\t- `report()`\n\t\t- `unq\\_columns()`\n\t+ datacompy.spark module\n\t\t- `MatchType`\n\t\t\t* `MatchType.KNOWN\\_DIFFERENCE`\n\t\t\t* `MatchType.MATCH`\n\t\t\t* `MatchType.MISMATCH`\n\t\t- `SparkCompare`\n\t\t\t* `SparkCompare.base\\_row\\_count`\n\t\t\t* `SparkCompare.columns\\_compared`\n\t\t\t* `SparkCompare.columns\\_in\\_both`\n\t\t\t\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on multiple columns ('employee_id' and 'department'), and produce a comparison report. Ensure that it reports matches and includes extra columns in the comparison.\n\n### Code:"
    },
    {
        "api": "datasets",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install datasets\nfrom datasets import load_dataset\n\n# Load SNLI dataset\ndataset = load_dataset(\"snli\")\n\n# Get the training genres\ntrain_genres = dataset[\"train\"][\"genre\"]\n\n# Count unique genre labels\nunique_genres = set(train_genres)\n\nprint(\"Number of unique genres in the SNLI training set:\", len(unique_genres))",
        "instruction": "Develop a Python program using the 'datasets' API to load the SNLI dataset and count the number of unique genres in the training set. The program should load the SNLI dataset, access the training genres, and count the unique genre labels.",
        "prediction": ":import datasets\nfrom datasets import load_dataset\n\n# Load SNLI dataset\ndataset = load_dataset(\"snli\")\n\n# Get the training genres\ntrain_genres = dataset[\"train\"][\"genre\"]\n\n# Count unique genre labels\nunique_genres = set(train_genres)\n\nprint(\"Number of unique genres in the SNLI training set:\", len(unique_genres))",
        "source": "This is some documentation of datasets API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n### get\\_all\\_exported\\_dataset\\_infos\n\nEmpty dict if doesn\u2019t exist\n\n```\n>>> from datasets import load_dataset_builder\n>>> ds_builder = load_dataset_builder('rotten\\_tomatoes')\n>>> ds_builder.get_all_exported_dataset_infos()\n{'default': DatasetInfo(description=\"Movie Review Dataset.\na dataset of containing 5,331 positive and 5,331 negative processed\ns from Rotten Tomatoes movie reviews. This data was first used in Bo\n Lillian Lee, ``Seeing stars: Exploiting class relationships for\nt categorization with respect to rating scales.'', Proceedings of the\n5.\nion='@InProceedings{Pang+Lee:05a,\n = {Bo Pang and Lillian Lee},\n= {Seeing stars: Exploiting class relationships for sentiment\n categorization with respect to rating scales},\ntle = {Proceedings of the ACL},\n 2005\n\nage='http://www.cs.cornell.edu/people/pabo/movie-review-data/', license='', features={'text': Value(dtype='string', id=None), 'label': ClassLabel(num\\_classes=2, names=['neg', 'pos'], id=None)}, post\\_processed=None, supervised\\_keys=SupervisedKeysData(input='', output=''), task\\_templates=[TextClassification(task='text-classification', text\\_column='text', label\\_column='label')], builder\\_name='rotten\\_tomatoes\\_movie\\_review', config\\_name='default', version=1.0.0, splits={'train': SplitInfo(name='train', num\\_bytes=1074810, num\\_examples=8530, dataset\\_name='rotten\\_tomatoes\\_movie\\_review'), 'validation': SplitInfo(name='validation', num\\_bytes=134679, num\\_examples=1066, dataset\\_name='rotten\\_tomatoes\\_movie\\_review'), 'test': SplitInfo(name='test', num\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program using the 'datasets' API to load the SNLI dataset and count the number of unique genres in the training set. The program should load the SNLI dataset, access the training genres, and count the unique genre labels.\n\n### Code:"
    },
    {
        "api": "datasets",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install datasets\nfrom datasets import load_dataset\nimport random\n\n# Load SAMSum dataset\ndataset = load_dataset(\"samsum\")\n\n# Select a random conversation\nrandom_conversation = random.choice(dataset[\"train\"][\"dialog\"])\n\nprint(\"Random Conversation:\")\nfor message in random_conversation:\n    print(message)",
        "instruction": "Develop a Python program using the 'datasets' API to load the SAMSum dataset and extract a random conversation. The program should load the SAMSum dataset, select a random conversation, and print its content.",
        "prediction": ":import datasets\nfrom datasets import load_dataset\nimport random\n\n# Load SAMSum dataset\ndataset = load_dataset(\"dataset\")\n\n# Select a random conversation\nrandom_conversation = random.choice(dataset[\"train\"][\"conv1\"])\n\nprint(\"Random Conversation:\")\nprint(random_conversation)",
        "source": "This is some documentation of datasets API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n### datasets.enable\\_caching\n\nWhen applying transforms on a dataset, the data are stored in cache files.\nThe caching mechanism allows to reload an existing cache file if it\u2019s already been computed.\n\n\nReloading a dataset is possible since the cache files are named using the dataset fingerprint, which is updated\nafter each transform.\n\n\nIf disabled, the library will no longer reload cached datasets files when applying transforms to the datasets.\nMore precisely, if the caching is disabled:\n\n\n* cache files are always recreated\n* cache files are written to a temporary directory that is deleted when session closes\n* cache files are named using a random hash instead of the dataset fingerprint\n* use save\\_to\\_disk() to save a transformed dataset or it will be deleted when session closes\n* caching doesn\u2019t affect load\\_dataset(). If you want to regenerate a dataset from scratch you should use\nthe `download_mode` parameter in load\\_dataset().\n\n\n#### datasets.disable\\_caching\n\n\n\n#### datasets.is\\_caching\\_enabled\n\n\n* cache files are always recreated\n* cache files are written to a temporary directory that is deleted when session closes\n* cache files are named using a random hash instead of the dataset fingerprint\n* use save\\_to\\_disk()] to save a transformed dataset or it will be deleted when session closes\n* caching doesn\u2019t affect load\\_dataset(). If you want to regenerate a dataset from scratch you should use\nthe `download_mode` parameter in load\\_dataset().\n\n\n## \nDatasetDict\n\n\nDictionary with split names as keys (\u2018train\u2019, \u2018test\u2019 for example), and `Dataset` objects as values.\nIt also has dataset transform methods like map or filter, to process all the splits at once.\n\n\n### class datasets.DatasetDict\n\nA dictionary (dict of str: datasets.Dataset) with dataset transforms methods (map, filter, etc.)\n\n#### data\n\nThe Apache Arrow tables backing each split.\n\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten\\_tomatoes\")\n>>> ds.data\n```\n\n\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program using the 'datasets' API to load the SAMSum dataset and extract a random conversation. The program should load the SAMSum dataset, select a random conversation, and print its content.\n\n### Code:"
    },
    {
        "api": "datasets",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install datasets\nfrom datasets import load_dataset\nimport random\n\n# Load WikiText-103 dataset\ndataset = load_dataset(\"wikitext\", \"103\")\n\n# Select a random article\nrandom_article = random.choice(dataset[\"train\"][\"text\"])\n\nprint(\"Random Article:\")\nprint(random_article)",
        "instruction": "Develop a Python program using the 'datasets' API to load the WikiText-103 dataset and extract a random article. The program should load the WikiText-103 dataset, select a random article, and print its content.",
        "prediction": ":import datasets\nfrom datasets import load_dataset\nimport random\n\n# Load WikiText-103 dataset\ndataset = load_dataset(\"wikitext\", \"2\")\n\n# Select a random article\nrandom_article = random.choice(dataset[\"train\"][\"text\"])\n\nprint(\"Random Article:\")\nprint(random_article)",
        "source": "This is some documentation of datasets API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n### datasets.enable\\_caching\n\nWhen applying transforms on a dataset, the data are stored in cache files.\nThe caching mechanism allows to reload an existing cache file if it\u2019s already been computed.\n\n\nReloading a dataset is possible since the cache files are named using the dataset fingerprint, which is updated\nafter each transform.\n\n\nIf disabled, the library will no longer reload cached datasets files when applying transforms to the datasets.\nMore precisely, if the caching is disabled:\n\n\n* cache files are always recreated\n* cache files are written to a temporary directory that is deleted when session closes\n* cache files are named using a random hash instead of the dataset fingerprint\n* use save\\_to\\_disk() to save a transformed dataset or it will be deleted when session closes\n* caching doesn\u2019t affect load\\_dataset(). If you want to regenerate a dataset from scratch you should use\nthe `download_mode` parameter in load\\_dataset().\n\n\n#### datasets.disable\\_caching\n\n\n\n#### datasets.is\\_caching\\_enabled\n\n\n* cache files are always recreated\n* cache files are written to a temporary directory that is deleted when session closes\n* cache files are named using a random hash instead of the dataset fingerprint\n* use save\\_to\\_disk()] to save a transformed dataset or it will be deleted when session closes\n* caching doesn\u2019t affect load\\_dataset(). If you want to regenerate a dataset from scratch you should use\nthe `download_mode` parameter in load\\_dataset().\n\n\n## \nDatasetDict\n\n\nDictionary with split names as keys (\u2018train\u2019, \u2018test\u2019 for example), and `Dataset` objects as values.\nIt also has dataset transform methods like map or filter, to process all the splits at once.\n\n\n### class datasets.DatasetDict\n\nA dictionary (dict of str: datasets.Dataset) with dataset transforms methods (map, filter, etc.)\n\n#### data\n\nThe Apache Arrow tables backing each split.\n\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten\\_tomatoes\")\n>>> ds.data\n```\n\n\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program using the 'datasets' API to load the WikiText-103 dataset and extract a random article. The program should load the WikiText-103 dataset, select a random article, and print its content.\n\n### Code:"
    },
    {
        "api": "datasets",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install datasets\nfrom datasets import load_dataset\nimport random\n\n# Load TED MultiTranslation dataset\ndataset = load_dataset(\"ted_multi\")\n\n# Select a random translated speech\nrandom_speech = random.choice(dataset[\"train\"][\"translation\"])\n\nsource_language = random_speech[\"en\"]\ntarget_language = random_speech[\"fr\"]\n\nprint(f\"Source Transcript (English): {source_language}\")\nprint(f\"Target Transcript (French): {target_language}\")",
        "instruction": "Develop a Python program using the 'datasets' API to load the TED MultiTranslation dataset and retrieve a random translated speech. The program should load the TED MultiTranslation dataset, select a random translated speech, and print the source and target language transcripts.",
        "prediction": ":import datasets\nfrom datasets import load_dataset\nimport random\n\n# Load TED MultiTranslation dataset\ndataset = load_dataset(\"ted_multi_translation\")\n\n# Select a random translated speech\nrandom_translated_speech = random.choice(dataset[\"train\"][\"translation\"])\n\n# Get source and target language transcripts\nsource_transcript = random_translated_speech[\"en\"]\ntarget_transcript = random_translated_speech[\"fr\"]\n\nprint(\"Source Transcript (English):\", source_transcript)\nprint(\"Target Transcript (French):\", target_transcript)",
        "source": "This is some documentation of datasets API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n## class datasets.TranslationVariableLanguages\n\n<\nsource\n>\n(\nlanguages: typing.Optional[typing.List] = None\nnum\\_languages: typing.Optional[int] = None\nid: typing.Optional[str] = None\n\n)\n\u2192\n\n* `language` or `translation` (variable-length 1D `tf.Tensor` of `tf.string`)\n\n\n* **languages** (`dict`) \u2014\nA dictionary for each example mapping string language codes to one or more string translations.\nThe languages present may vary from example to example.\n\nLanguage codes sorted in ascending order or plain text translations, sorted to align with language codes.\n\n\n`FeatureConnector` for translations with variable languages per example.\nHere for compatiblity with tfds.\n\n```\n>>> # At construction time:\n>>> datasets.features.TranslationVariableLanguages(languages=['en', 'fr', 'de'])\n>>> # During data generation:\n>>> yield {\n...         'en': 'the cat',\n...         'fr': ['le chat', 'la chatte,']\n...         'de': 'die katze'\n... }\n>>> # Tensor returned :\n>>> {\n...         'language': ['en', 'de', 'fr', 'fr'],\n...         'translation': ['the cat', 'die katze', 'la chatte', 'le chat'],\n... }\n```\n\nFlatten the TranslationVariableLanguages feature into a dictionary.\n\n### class datasets.Array2D\n\n<\nsource\n>\n(\nshape: tuple\ndtype: str\nid: typing.Optional[str] = None\n\n\n* **shape** (`tuple`) \u2014\nThe size of each dimension.\n* **dtype** (`str`) \u2014\nThe value of the data type.\n\nCreate a two-dimensional array.\n\n```\n>>> from datasets import Features\n>>> features = Features({'x': Array2D(shape=(1, 3), dtype='int32')})\n```\n\n\n\n### class datasets.Array3D\n\nCreate a three-dimensional array.\n\n```\n>>> from datasets import Features\n>>> features = Features({\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program using the 'datasets' API to load the TED MultiTranslation dataset and retrieve a random translated speech. The program should load the TED MultiTranslation dataset, select a random translated speech, and print the source and target language transcripts.\n\n### Code:"
    },
    {
        "api": "diffusers",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install diffusers\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-3-diffusers\",\n    use_auth_token=True,\n    revision=\"fp16\",\n    torch_dtype=torch.float16\n)\npipe = pipe.to(\"cpu\")\n\nprompt = \"What are the benefits of reading books?\"\ncompletions = pipe(prompt)[\"sample\"][0]\n\nprint(completions)",
        "instruction": "Write a Python program that utilizes the 'diffusers' API to generate a list of possible completions for the provided prompt, 'What are the benefits of reading books?'",
        "prediction": ":import torch\nimport torch\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-3-diffusers\",\n    use_auth_token=True,\n    revision=\"fp16\",\n    torch_dtype=torch.float16\n)\npipe = pipe.to(\"cpu\")\n\nprom = \"What are the benefits of reading books?\"\ncompletions = pipe(prom)[\"sample\"][0]\n\nprint(completions)",
        "source": "This is some documentation of diffusers API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n## \nOptimizing for memory\n\n\nWhen optimizing for GPU memory, we can use the standard diffusers cpu offloading APIs.\n\n\nEither the model based CPU offloading,\n\n```\npipe = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\n```\n\nor the more aggressive layer based CPU offloading.\n\n```\npipe = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16)\npipe.enable_sequential_cpu_offload()\n```\n\nAdditionally, T5 can be loaded in 8bit precision\n\n```\nfrom transformers import T5EncoderModel\n\ntext_encoder = T5EncoderModel.from_pretrained(\n    \"DeepFloyd/IF-I-XL-v1.0\", subfolder=\"text\\_encoder\", device_map=\"auto\", load_in_8bit=True, variant=\"8bit\"\n)\n\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\n    \"DeepFloyd/IF-I-XL-v1.0\",\n    text_encoder=text_encoder,  # pass the previously instantiated 8bit text encoder\n    unet=None,\n    device_map=\"auto\",\n)\n\nprompt_embeds, negative_embeds = pipe.encode_prompt(\"<prompt>\")\n```\n\nFor CPU RAM constrained machines like google colab free tier where we can\u2019t load all\nmodel components to the CPU at once, we can manually only load the pipeline with\nthe text encoder or unet when the respective model components are needed.\n\n```\nfrom diffusers import IFPipeline, IFSuperResolutionPipeline\nimport torch\nimport gc\nfrom transformers import T5EncoderModel\nfrom diffusers.utils import pt_to_pil\n\n# text to image\n\nprom\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nWrite a Python program that utilizes the 'diffusers' API to generate a list of possible completions for the provided prompt, 'What are the benefits of reading books?'\n\n### Code:"
    },
    {
        "api": "diffusers",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install diffusers\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-3-diffusers\",\n    use_auth_token=True,\n    revision=\"fp16\",\n    torch_dtype=torch.float16\n)\npipe = pipe.to(\"cpu\")\n\nprompt = \"What are the benefits of regular exercise?\"\ncompletions = pipe(prompt)[\"sample\"][0]\n\nprint(completions)",
        "instruction": "Write a Python program that utilizes the 'diffusers' API to generate a list of possible completions for the provided prompt, 'What are the benefits of regular exercise?'",
        "prediction": ":import torch\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-3-diffusers\",\n    use_auth_token=True,\n    revision=\"fp16\",\n    torch_dtype=torch.float16\n)\npipe = pipe.to(\"cpu\")\n\nprompt = \"What are the benefits of regular exercise?\"\ncompletions = pipe(prompt)[\"sample\"][0]\n\nprint(completions)",
        "source": "This is some documentation of diffusers API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n \nText-to-image\n\n\nThe Stable Diffusion model was created by researchers and engineers from CompVis, Stability AI, Runway, and LAION. The StableDiffusionPipeline is capable of generating photorealistic images given any text input. It\u2019s trained on 512x512 images from a subset of the LAION-5B dataset. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on consumer GPUs. Latent diffusion is the research on top of which Stable Diffusion was built. It was proposed in High-Resolution Image Synthesis with Latent Diffusion Models by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer.\n\n\n*By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion.*\n\n\nMake sure to check out the Stable Diffusion Tips section to learn how to explore the tradeoff between scheduler speed and quality\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nWrite a Python program that utilizes the 'diffusers' API to generate a list of possible completions for the provided prompt, 'What are the benefits of regular exercise?'\n\n### Code:"
    },
    {
        "api": "emoji",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install emoji\nimport emoji\n\ndef has_emoji_code(txt, codes):\n  for char in txt:\n    for code in codes:\n      if emoji.demojize(char) == code:\n        return True\n  return False\n\ntxts = [\"I am so happy \ud83d\ude00\", \n          \"Python is :thumbs_up:\", \n          \"Funny \ud83d\ude02\", \n          \"I liked it :red_heart:\",\n          \"I am so happy\",\n          \"This is a normal text\"]\n\ncodes_to_check = [\":thumbs_up:\", \":red_heart:\", \":smile:\"]\n\nfor txt in txts:\n  print(has_emoji_code(txt, codes_to_check))\n",
        "instruction": "Create a Python program that uses the 'emoji' API to check if a given text contains any specific emoji codes. The program should take a text and a list of emoji codes as input and return True if the text contains any emojis that match the specified codes, and False otherwise. Run the program on a list of sample texts containing emojis and non-emojis, and specify a list of emoji codes to check for each text.",
        "prediction": ":import emoji\nimport emoji\n\ndef has_emoji_code(txt, codes):\n  for char in txt:\n    for code in codes:\n      if emoji.demojize(char).lower().find(code.lower())!= -1:\n        return True\n  return False\n\ntxts = [\"I am so happy \ud83d\ude00\", \n          \"Python is :thumbs_up:\", \n          \"Funny \ud83d\ude02\", \n          \"I liked it :red_heart:\",\n          \"I am so happy\",\n          \"This is a normal text\"]\n\ncodes_to_check = [\"\ud83d\ude00\", \":thumbs_up:\", \":red_heart\", \"red_heart\"]\n\nfor txt in txts:\n  print(has_emoji_code(txt, codes_to_check))\n",
        "source": "This is some documentation of emoji API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n API Reference\u00b6\n\n\n| Table of Contents |  |\n| --- | --- |\n| **Functions:** |  |\n| `emojize()` | Replace emoji names with Unicode codes |\n| `demojize()` | Replace Unicode emoji with emoji shortcodes |\n| `analyze()` | Find Unicode emoji in a string |\n| `replace\\_emoji()` | Replace Unicode emoji with a customizable string |\n| `emoji\\_list()` | Location of all emoji in a string |\n| `distinct\\_emoji\\_list()` | Distinct list of emojis in the string |\n| `emoji\\_count()` | Number of emojis in a string |\n| `is\\_emoji()` | Check if a string/character is a single emoji |\n| `purely\\_emoji()` | Check if a string contains only emojis |\n| `version()` | Find Unicode/Emoji version of an emoji |\n| **Module variables:** |  |\n| `EMOJI\\_DATA` | Dict of all emoji |\n| `STATUS` | Dict of Unicode/Emoji status |\n| `config` | Module wide configuration |\n| **Classes:** |  |\n| `EmojiMatch` |  |\n| `EmojiMatchZWJ` |  |\n| `EmojiMatchZWJNonRGI` |  |\n| `Token` |  |\n## emoji for Python\u00b6\n\n\nemoji terminal output for Python.\n\n```\n>>> import emoji\n>>> print(emoji.emojize('Python is :thumbsup:', language='alias'))\nPython is \ud83d\udc4d\n>>> print(emoji.emojize('Python is :thumbs\\_up:'))\nPython is \ud83d\udc4d\n\n```\n\n*class* emoji.EmojiMatch(*emoji: str*, *start: int*, *end: int*, *data: dict | None*)[source]\u00b6\nRepresents a match of a \u201crecommended for general interchange\u201d (RGI)\nemoji\n\n==================\n Document 1 \n----------------\n\n# API Reference\u00b6\n\n\n| Table of\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that uses the 'emoji' API to check if a given text contains any specific emoji codes. The program should take a text and a list of emoji codes as input and return True if the text contains any emojis that match the specified codes, and False otherwise. Run the program on a list of sample texts containing emojis and non-emojis, and specify a list of emoji codes to check for each text.\n\n### Code:"
    },
    {
        "api": "emoji",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install emoji\nimport emoji\n\ndef has_emoji_name(txt, names):\n  for char in txt:\n    for name in names:\n      if emoji.demojize(char).lower().find(name.lower()) != -1:\n        return True\n  return False\n\ntxts = [\"I am so happy \ud83d\ude00\", \n          \"Python is :thumbs_up:\", \n          \"Funny \ud83d\ude02\", \n          \"I liked it :red_heart:\",\n          \"I am so happy\",\n          \"This is a normal text\"]\n\nnames_to_check = [\"happy\", \"thumbs\", \"red\"]\n\nfor txt in txts:\n  print(has_emoji_name(txt, names_to_check))\n",
        "instruction": "Create a Python program that uses the 'emoji' API to check if a given text contains any specific emoji names. The program should take a text and a list of emoji names as input and return True if the text contains any emojis that match the specified names, and False otherwise. Run the program on a list of sample texts containing emojis and non-emojis, and specify a list of emoji names to check for each text.",
        "prediction": ":import emoji\nimport emoji\n\ndef has_emoji_names(txt, names):\n  for char in txt:\n    for name in names:\n      if emoji.demojize(char).lower().find(name.lower())!= -1:\n        return True\n  return False\n\ntxts = [\"I am so happy \ud83d\ude00\", \n          \"Python is :thumbs_up:\", \n          \"Funny \ud83d\ude02\", \n          \"I liked it :red_heart:\",\n          \"I am so happy\",\n          \"This is a normal text\"]\n\nnames_to_check = [\"happy\", \"thumbs\", \"red\"]\n\nfor txt in txts:\n  print(has_emoji_names(txt, names_to_check))\n",
        "source": "This is some documentation of emoji API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n API Reference\u00b6\n\n\n| Table of Contents |  |\n| --- | --- |\n| **Functions:** |  |\n| `emojize()` | Replace emoji names with Unicode codes |\n| `demojize()` | Replace Unicode emoji with emoji shortcodes |\n| `analyze()` | Find Unicode emoji in a string |\n| `replace\\_emoji()` | Replace Unicode emoji with a customizable string |\n| `emoji\\_list()` | Location of all emoji in a string |\n| `distinct\\_emoji\\_list()` | Distinct list of emojis in the string |\n| `emoji\\_count()` | Number of emojis in a string |\n| `is\\_emoji()` | Check if a string/character is a single emoji |\n| `purely\\_emoji()` | Check if a string contains only emojis |\n| `version()` | Find Unicode/Emoji version of an emoji |\n| **Module variables:** |  |\n| `EMOJI\\_DATA` | Dict of all emoji |\n| `STATUS` | Dict of Unicode/Emoji status |\n| `config` | Module wide configuration |\n| **Classes:** |  |\n| `EmojiMatch` |  |\n| `EmojiMatchZWJ` |  |\n| `EmojiMatchZWJNonRGI` |  |\n| `Token` |  |\n## emoji for Python\u00b6\n\n\nemoji terminal output for Python.\n\n```\n>>> import emoji\n>>> print(emoji.emojize('Python is :thumbsup:', language='alias'))\nPython is \ud83d\udc4d\n>>> print(emoji.emojize('Python is :thumbs\\_up:'))\nPython is \ud83d\udc4d\n\n```\n\n*class* emoji.EmojiMatch(*emoji: str*, *start: int*, *end: int*, *data: dict | None*)[source]\u00b6\nRepresents a match of a \u201crecommended for general interchange\u201d (RGI)\nemoji\n\n==================\n Document 1 \n----------------\n\n# API Reference\u00b6\n\n\n| Table of\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that uses the 'emoji' API to check if a given text contains any specific emoji names. The program should take a text and a list of emoji names as input and return True if the text contains any emojis that match the specified names, and False otherwise. Run the program on a list of sample texts containing emojis and non-emojis, and specify a list of emoji names to check for each text.\n\n### Code:"
    },
    {
        "api": "evaluate",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install datasets\n#!pip install evaluate\nfrom datasets import load_dataset\nfrom evaluate import evaluator\n\ndata = load_dataset(\"imdb\", split=\"test\").shuffle(seed=42).select(range(1000))\ntask_evaluator = evaluator(\"text-classification\")\n\n# Pass a model name or path\neval_results = task_evaluator.compute(\n    model_or_pipeline=\"lvwerra/distilbert-imdb\",\n    data=data,\n    label_mapping={\"NEGATIVE\": 0, \"POSITIVE\": 1}\n)\n\nprint(eval_results)",
        "instruction": "Create a Python program that uses the 'evaluate' API to perform sentiment analysis on a dataset. The program should load the IMDb dataset, shuffle it, select the first 1000 examples, and then use the 'lvwerra/distilbert-imdb' model to classify the text as either 'NEGATIVE' or 'POSITIVE.' The results should be printed.",
        "prediction": ":import evaluate\nfrom evaluate import evaluator\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nfrom transformers import pipeline\n\n# Load the IMDb dataset\ndataset_ = \"mozilla-foundation/common_voice\\_voice\\_11\\_0\"\nbatch_size = 100\n\n# Shuffle the dataset\ndataset = load_dataset(\"evaluate\", \"weird-tiny.arrow.gz\")\n\n# Select the first 1000 examples\ndataset\n\n# Get the first 1000 rows\ndataset\n\n# Get the first 100 rows\ndataset\n\n# Load the model\n",
        "source": "This is some documentation of evaluate API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n## class evaluate.AutomaticSpeechRecognitionEvaluator\n\n<\nsource\n>\n(\ntask = 'automatic-speech-recognition'\ndefault\\_metric\\_name = None\n\nAutomatic speech recognition evaluator.\nThis automatic speech recognition evaluator can currently be loaded from evaluator() using the default task name\n`automatic-speech-recognition`.\nMethods in this class assume a data format compatible with the `AutomaticSpeechRecognitionPipeline`.\n\n<\nsource\n>\n(\nmodel\\_or\\_pipeline: typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None\ndata: typing.Union[str, datasets.arrow\\_dataset.Dataset] = None\nsubset: typing.Optional[str] = None\nsplit: typing.Optional[str] = None\nmetric: typing.Union[str, evaluate.module.EvaluationModule] = None\ntokenizer: typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None\nstrategy: typing.Literal['simple', 'bootstrap'] ='simple'\nconfidence\\_level: float = 0.95\nn\\_resamples: int = 9999\ndevice: int = None\nrandom\\_state: typing.Optional[int] = None\ninput\\_column: str = 'path'\nlabel\\_column: str ='sentence'\ngeneration\\_kwargs: dict = None\n\n```\n>>> from evaluate import evaluator\n>>> from datasets import load_dataset\n>>> task_evaluator = evaluator(\"automatic-speech-recognition\")\n>>> data = load_dataset(\"mozilla-foundation/common\\_voice\\_11\\_0\", \"en\", split=\"validation[:40]\")\n>>> results = task_evaluator.compute(\n>>>     model_or_pipeline=\"https://huggingface.co/openai/whisper-tiny.en\",\n>>>     data=data,\n>>>     input_column=\"path\",\n>>>     label_\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that uses the 'evaluate' API to perform sentiment analysis on a dataset. The program should load the IMDb dataset, shuffle it, select the first 1000 examples, and then use the 'lvwerra/distilbert-imdb' model to classify the text as either 'NEGATIVE' or 'POSITIVE.' The results should be printed.\n\n### Code:"
    },
    {
        "api": "evaluate",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install evaluate\nfrom evaluate import evaluator\n\n# Load the text summarization model\nsummarization_model = \"facebook/bart-large-cnn\"\n\ndef perform_summarization(text):\n    summarization_evaluator = evaluator(\"text-summarization\")\n\n    # Use the model for text summarization\n    summary = summarization_evaluator.compute(\n        model_or_pipeline=summarization_model,\n        data=text\n    )\n\n    return summary\n\n# User input\nuser_text = input(\"Enter the text for summarization: \")\nresult = perform_summarization(user_text)\n\nprint(\"Summarized Text:\")\nprint(result[0][\"summary_text\"])\n",
        "instruction": "Create a Python program that uses the 'evaluate' API to perform text summarization on a given text. The program should take a text as input, use the 'facebook/bart-large-cnn' model for text summarization, and print the summarized text.",
        "prediction": ":pip install evaluate\nfrom evaluate import evaluator\n\n# Load the text\n\n# Use the model for text summarization\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\n\n# Print the summarized text\nDocument,,, Install,\nfromevaluate, evaluate, text2text-generation, evaluator\n\n# Load the text\n\n# Create the evaluation evaluator\n\n# Create a dictionary\nmodel_for_summarization = {\n    \"facebook/bart-large-cnn\": \"Evaluator\"\n}\n\ndef perform_summarization(text):\n    summarization_evaluator = model_for_summarization[ \"facebook/bart-large-cnn\"](model_for_summarization)\n\n\n# User input\nuser_text =\n\n# Perform summarization\nresult = summarization_evaluator.compute(\n    model_for_summarization,\n    user_input\n)\n\nreturn result\n\n# User input\nuser_text\n\nsummarization_evaluator = model_for_summarization[ \"facebook/bart-large-cnn\"](model_for_summarization)\n\nresult = summarization_evaluator.compute(\n    model_for_summarization,\n    user_input\n)\n\nreturn result\n\nsummarization_evaluator = evaluator(\"text-summarization\")\n\nuser_input =\n\nresult = summarization_evaluator.compute(\n    model_for",
        "source": "This is some documentation of evaluate API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n## class evaluate.Text2TextGenerationEvaluator\n\n<\nsource\n>\n(\ntask = 'text2text-generation'\ndefault\\_metric\\_name = None\n\nText2Text generation evaluator.\nThis Text2Text generation evaluator can currently be loaded from evaluator() using the default task name\n`text2text-generation`.\nMethods in this class assume a data format compatible with the `Text2TextGenerationPipeline`.\n\n<\nsource\n>\n(\nmodel\\_or\\_pipeline: typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None\ndata: typing.Union[str, datasets.arrow\\_dataset.Dataset] = None\nsubset: typing.Optional[str] = None\nsplit: typing.Optional[str] = None\nmetric: typing.Union[str, evaluate.module.EvaluationModule] = None\ntokenizer: typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None\nstrategy: typing.Literal['simple', 'bootstrap'] ='simple'\nconfidence\\_level: float = 0.95\nn\\_resamples: int = 9999\ndevice: int = None\nrandom\\_state: typing.Optional[int] = None\ninput\\_column: str = 'text'\nlabel\\_column: str = 'label'\ngeneration\\_kwargs: dict = None\n\n\n* **model\\_or\\_pipeline** (`str` or `Pipeline` or `Callable` or `PreTrainedModel` or `TFPreTrainedModel`, defaults to `None`) \u2014\nIf the argument in not specified, we initialize the default pipeline for the task (in this case\n`text-classification` or its alias - `sentiment-analysis`). If the argument is of the type `str` or\nis a model instance, we use it to initialize a new `Pipeline` with the given model. Otherwise we assume the\nargument specifies a pre-initialized pipeline.\n* **data** (`str` or `Datas\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that uses the 'evaluate' API to perform text summarization on a given text. The program should take a text as input, use the 'facebook/bart-large-cnn' model for text summarization, and print the summarized text.\n\n### Code:"
    },
    {
        "api": "evaluate",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install evaluate\nfrom evaluate import evaluator\n\n# Load the text summarization model\nsummarization_model = \"facebook/bart-large-cnn\"\n\ndef perform_summarization(text):\n    summarization_evaluator = evaluator(\"text-summarization\")\n\n    # Use the model for text summarization\n    summary = summarization_evaluator.compute(\n        model_or_pipeline=summarization_model,\n        data=text\n    )\n\n    return summary\n\n# User input\nuser_text = input(\"Enter the text for summarization: \")\nresult = perform_summarization(user_text)\n\nprint(\"Summarized Text:\")\nprint(result[0][\"summary_text\"])\n",
        "instruction": "Create a Python program that uses the 'evaluate' API to perform text summarization on a given text. The program should allow the user to input a text, then use the 'facebook/bart-large-cnn' model for text summarization and print the summarized text.",
        "prediction": ":pip install evaluate\nfrom evaluate import evaluator\n\n# Load the 'facebook/bart-large-cnn' model\nmodel_name = \"facebook/bart-large-cnn\"\n\n# Use the model for text summarization\ntext_summarization_evaluator = evaluator(\"text-summarization\")\n\n# User input\nuser_text = input(\"Enter the text for summarization: \")\nresult = text_summarization_evaluator.compute(\n    model_or_pipeline=model_name,\n    data=user_text\n)\n\nprint(\"Summarized Text:\")\nprint(result[0][\"generated_summary\"])\n",
        "source": "This is some documentation of evaluate API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n## class evaluate.Text2TextGenerationEvaluator\n\n<\nsource\n>\n(\ntask = 'text2text-generation'\ndefault\\_metric\\_name = None\n\nText2Text generation evaluator.\nThis Text2Text generation evaluator can currently be loaded from evaluator() using the default task name\n`text2text-generation`.\nMethods in this class assume a data format compatible with the `Text2TextGenerationPipeline`.\n\n<\nsource\n>\n(\nmodel\\_or\\_pipeline: typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None\ndata: typing.Union[str, datasets.arrow\\_dataset.Dataset] = None\nsubset: typing.Optional[str] = None\nsplit: typing.Optional[str] = None\nmetric: typing.Union[str, evaluate.module.EvaluationModule] = None\ntokenizer: typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None\nstrategy: typing.Literal['simple', 'bootstrap'] ='simple'\nconfidence\\_level: float = 0.95\nn\\_resamples: int = 9999\ndevice: int = None\nrandom\\_state: typing.Optional[int] = None\ninput\\_column: str = 'text'\nlabel\\_column: str = 'label'\ngeneration\\_kwargs: dict = None\n\n\n* **model\\_or\\_pipeline** (`str` or `Pipeline` or `Callable` or `PreTrainedModel` or `TFPreTrainedModel`, defaults to `None`) \u2014\nIf the argument in not specified, we initialize the default pipeline for the task (in this case\n`text-classification` or its alias - `sentiment-analysis`). If the argument is of the type `str` or\nis a model instance, we use it to initialize a new `Pipeline` with the given model. Otherwise we assume the\nargument specifies a pre-initialized pipeline.\n* **data** (`str` or `Datas\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that uses the 'evaluate' API to perform text summarization on a given text. The program should allow the user to input a text, then use the 'facebook/bart-large-cnn' model for text summarization and print the summarized text.\n\n### Code:"
    },
    {
        "api": "gymnasium",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install ale_py gymnasium\n#!pip install swig\n#!pip install gymnasium[box2d]\n#!pip install gymnasium[classic-control]\nimport gymnasium as gym\nfrom gymnasium.utils import play\n\nenv = gym.make(\"Taxi-v3\")\n\nmapping = {\"w\": 0,\n            \"a\": 3,\n            \"s\": 2,\n            \"d\": 1}\n\ndefault_action = 0\nplay.play(env, keys_to_action=mapping, noop=default_action)",
        "instruction": "Develop a Python program using the 'gymnasium' API to play the 'Taxi-v3' environment. The program should control the taxi within the environment using keyboard inputs (W, A, S, D) to move up, left, down, and right, respectively.",
        "prediction": ":import gymnasium as gym\nfrom gymnasium.utils import play\nimport gymnasium as gym\nfrom gymnasium.utils import play\n\nenv = gym.make(\"Taxi-v3\")\n\nmapping = {\"w\": 2,\n            \"a\": 3,\n            \"s\": 0,\n            \"d\": 1}\n\ndefault_action = 0\nplay.play(env, keys_to_action=mapping, noop=default_action)",
        "source": "This is some documentation of gymnasium API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# Env#\n\n## gymnasium.Env#\n\n\n*class* gymnasium.Env[source]#\nThe main Gymnasium class for implementing Reinforcement Learning Agents environments.\n\n\nThe class encapsulates an environment with arbitrary behind-the-scenes dynamics through the `step()` and `reset()` functions.\nAn environment can be partially or fully observed by single agents. For multi-agent environments, see\n\n==================\n Document 1 \n----------------\n## Attributes#\n\n\nEnv.action\\_space*: spaces.Space[ActType]*#\nThe Space object corresponding to valid actions, all valid actions should be contained with the space. For example, if the action space is of type Discrete and gives the value Discrete(2), this means there are two valid discrete actions: 0 & 1.\n\n```\n>>> env.action\\_space\nDiscrete(2)\n>>> env.observation\\_space\nBox(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n\n```\n\nEnv.observation\\_space*: spaces.Space[ObsType]*#\nThe Space object corresponding to valid observations, all valid observations should be contained with the space. For example, if the observation space is of type `Box` and the shape of the object is `(4,)`, this denotes a valid observation will be an array of 4 numbers. We can check the box bounds as well with attributes.\n\n```\n>>> env.observation\\_space.high\narray([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38], dtype=float32)\n>>> env.observation\\_space.low\narray([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38], dtype=float32)\n\nEnv.metadata*: dict[str, Any]* *= {'render\\_modes': []}*#\nThe metadata of the environment containing rendering modes, rendering fps, etc\n\n\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program using the 'gymnasium' API to play the 'Taxi-v3' environment. The program should control the taxi within the environment using keyboard inputs (W, A, S, D) to move up, left, down, and right, respectively.\n\n### Code:"
    },
    {
        "api": "gymnasium",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install ale_py gymnasium\n#!pip install swig\n#!pip install gymnasium[box2d]\n#!pip install gymnasium[classic-control]\nimport gymnasium as gym\n\nenv = gym.make(\"RoboschoolInvertedPendulum-v1\")\n\ndone = False\nwhile not done:\n    action = env.action_space.sample()\n    observation, reward, done, info = env.step(action)\n    print(\"Action:\", action)\n    print(\"Observation:\", observation)\n    print(\"Reward:\", reward)\n    print(\"Done:\", done)\n    print(\"Info:\", info)",
        "instruction": "Develop a Python program using the 'gymnasium' API to play the 'RoboschoolInvertedPendulum-v1' environment. The program should control the inverted pendulum within the environment using continuous actions to apply torque to the joint.",
        "prediction": ":import gymnasium\n#!pip install ale_py gymnasium\n#!pip install swig\n#!pip install gymnasium[box2d]\n#!pip install gymnasium[classic-control]\nimport gymnasium as gym\n\nenv = gym.make(\"RoboschoolInvertedPendulum-v1\")\n\ndone = False\nwhile not done:\n    action = env.action_space.sample()\n    observation, reward, done, info = env.step(action)\n    print(\"Action:\", action)\n    print(\"Observation:\", observation)\n    print(\"Reward:\", reward)\n    print(\"Done:\", done)\n    print(\"Info:\", info)",
        "source": "This is some documentation of gymnasium API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# Env#\n\n## gymnasium.Env#\n\n\n*class* gymnasium.Env[source]#\nThe main Gymnasium class for implementing Reinforcement Learning Agents environments.\n\n\nThe class encapsulates an environment with arbitrary behind-the-scenes dynamics through the `step()` and `reset()` functions.\nAn environment can be partially or fully observed by single agents. For multi-agent environments, see\n\n==================\n Document 1 \n----------------\n## Attributes#\n\n\nEnv.action\\_space*: spaces.Space[ActType]*#\nThe Space object corresponding to valid actions, all valid actions should be contained with the space. For example, if the action space is of type Discrete and gives the value Discrete(2), this means there are two valid discrete actions: 0 & 1.\n\n```\n>>> env.action\\_space\nDiscrete(2)\n>>> env.observation\\_space\nBox(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n\n```\n\nEnv.observation\\_space*: spaces.Space[ObsType]*#\nThe Space object corresponding to valid observations, all valid observations should be contained with the space. For example, if the observation space is of type `Box` and the shape of the object is `(4,)`, this denotes a valid observation will be an array of 4 numbers. We can check the box bounds as well with attributes.\n\n```\n>>> env.observation\\_space.high\narray([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38], dtype=float32)\n>>> env.observation\\_space.low\narray([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38], dtype=float32)\n\nEnv.metadata*: dict[str, Any]* *= {'render\\_modes': []}*#\nThe metadata of the environment containing rendering modes, rendering fps, etc\n\n\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program using the 'gymnasium' API to play the 'RoboschoolInvertedPendulum-v1' environment. The program should control the inverted pendulum within the environment using continuous actions to apply torque to the joint.\n\n### Code:"
    },
    {
        "api": "gymnasium",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install ale_py gymnasium\n#!pip install swig\n#!pip install gymnasium[box2d]\n#!pip install gymnasium[classic-control]\nimport gymnasium as gym\nfrom gymnasium.utils import play\n\nenv = gym.make(\"FrozenLake-v0\")\n\nmapping = {\"w\": 0,\n            \"a\": 3,\n            \"s\": 2,\n            \"d\": 1}\n\ndefault_action = 0\nplay.play(env, keys_to_action=mapping, noop=default_action)",
        "instruction": "Develop a Python program using the 'gymnasium' API to play the 'FrozenLake-v0' environment. The program should control the agent within the environment using keyboard inputs (W, A, S, D) to move up, left, down, and right, respectively.",
        "prediction": ":import gymnasium\n#!pip install swig\n#!pip install gymnasium[box2d]\n#!pip install gymnasium[classic-control]\nimport gymnasium as gym\nfrom gymnasium.utils import play\n\nenv = gym.make(\"FrozenLake-v0\")\n\nmapping = {\"w\": 0,\n            \"a\": 3,\n            \"s\": 2,\n            \"d\": 1}\n\ndefault_action = 0\nplay.play(env, keys_to_action=mapping, noop=default_action)",
        "source": "This is some documentation of gymnasium API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# Env#\n\n## gymnasium.Env#\n\n\n*class* gymnasium.Env[source]#\nThe main Gymnasium class for implementing Reinforcement Learning Agents environments.\n\n\nThe class encapsulates an environment with arbitrary behind-the-scenes dynamics through the `step()` and `reset()` functions.\nAn environment can be partially or fully observed by single agents. For multi-agent environments, see\n\n==================\n Document 1 \n----------------\n## Attributes#\n\n\nEnv.action\\_space*: spaces.Space[ActType]*#\nThe Space object corresponding to valid actions, all valid actions should be contained with the space. For example, if the action space is of type Discrete and gives the value Discrete(2), this means there are two valid discrete actions: 0 & 1.\n\n```\n>>> env.action\\_space\nDiscrete(2)\n>>> env.observation\\_space\nBox(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n\n```\n\nEnv.observation\\_space*: spaces.Space[ObsType]*#\nThe Space object corresponding to valid observations, all valid observations should be contained with the space. For example, if the observation space is of type `Box` and the shape of the object is `(4,)`, this denotes a valid observation will be an array of 4 numbers. We can check the box bounds as well with attributes.\n\n```\n>>> env.observation\\_space.high\narray([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38], dtype=float32)\n>>> env.observation\\_space.low\narray([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38], dtype=float32)\n\nEnv.metadata*: dict[str, Any]* *= {'render\\_modes': []}*#\nThe metadata of the environment containing rendering modes, rendering fps, etc\n\n\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program using the 'gymnasium' API to play the 'FrozenLake-v0' environment. The program should control the agent within the environment using keyboard inputs (W, A, S, D) to move up, left, down, and right, respectively.\n\n### Code:"
    },
    {
        "api": "holidays",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install holidays\nimport holidays\n\n# Define the date to check\ndate_to_check = \"2023-07-04\" \n\n# Specify the country or region (e.g., 'US' for the United States)\ncountry_code = 'US'\n\n# Create a dictionary of holidays for the specified country or region\nholiday_dict = holidays.country_holidays(country_code)\n\n# Check if the date is a public holiday\nis_holiday = date_to_check in holiday_dict\n\n# Display the result\nif is_holiday:\n    print(f\"{date_to_check} is a public holiday in {country_code}.\")\nelse:\n    print(f\"{date_to_check} is not a public holiday in {country_code}.\")",
        "instruction": "Create a Python program using the 'holidays' API to check if a specified date is a public holiday in a given country or region.",
        "prediction": ":import holidays\nimport holidays\n\n# Define the date to check\ndate_to_check = \"2023-07-04\" \n\n# Specify the country or region (e.g., 'US' for the United States)\ncountry_code = 'US'\n\n# Create a dictionary of holidays for the specified country\nholiday_dict = holidays.CountryHoliday(country_code, observed=True, years=2023)\n\n# Check if the date is a public holiday in the specified country or region\nis_holiday = date_to_check in holiday_dict\n\n# Display the result\nif is_holiday:\n    print(f\"{date_to_check} is a public holiday in {country_code}.\")\nelse:\n    print(f\"{date_to_check} is not a public holiday in {country_code}.\")",
        "source": "This is some documentation of holidays API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n For a specific subdivision (e.g. state or province):\n>>> calif\\_holidays = country\\_holidays('US', subdiv='CA')\n\n```\n\n\nThe below will cause 2015 holidays to be calculated on the fly:\n\n```\n>>> from datetime import date\n>>> assert date(2015, 1, 1) in us\\_holidays\n\n\nThis will be faster because 2015 holidays are already calculated:\n\n```\n>>> assert date(2015, 1, 2) not in us\\_holidays\n\n\nThe `HolidayBase` class also recognizes strings of many formats\nand numbers representing a POSIX timestamp:\n\n```\n>>> assert '2014-01-01' in us\\_holidays\n>>> assert '1/1/2014' in us\\_holidays\n>>> assert 1388597445 in us\\_holidays\n\n\nShow the holiday\u2019s name:\n\n```\n>>> us\\_holidays.get('2014-01-01')\n\"New Year's Day\"\n\n\nCheck a range:\n\n```\n>>> us\\_holidays['2014-01-01': '2014-01-03']\n[datetime.date(2014, 1, 1)]\n\n\nList all 2020 holidays:\n\n```\n>>> us\\_holidays = country\\_holidays('US', years=2020)\n>>> for day in us\\_holidays.items():\n...     print(day)\n(datetime.date(2020, 1, 1), \"New Year's Day\")\n(datetime.date(2020, 1, 20), 'Martin Luther King Jr. Day')\n(datetime.date(2020, 2, 17), \"Washington's Birthday\")\n(datetime.date(2020, 5, 25), 'Memorial Day')\n(datetime.date(2020, 7, 4), 'Independence Day')\n(datetime.date(2020, 7, 3), 'Independence Day (Observed)')\n(datetime.date(2020, 9, 7), 'Labor Day')\n(datetime.date(2020, 10, 12), 'Columbus Day')\n(datetime.date(2020, 11, 11), 'Veterans Day')\n(datetime.date(2020, 11\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'holidays' API to check if a specified date is a public holiday in a given country or region.\n\n### Code:"
    },
    {
        "api": "holidays",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install holidays\nimport holidays\n\n# Specify the country or region (e.g., 'US' for the United States)\ncountry_code = 'US'\n\n# Specify the state or province code (e.g., 'CA' for California)\nstate_code = 'CA'\n\n# Specify the region (e.g., 'LA' for Los Angeles)\nregion = 'LA'\n\n# Specify the year for which you want to list holidays\nyear = 2023\n\n# Create a dictionary of holidays for the specified region and year\nholiday_dict = holidays.RegionHoliday(country_code, region, observed=True, years=year)\n\n# List all the public holidays\nholiday_list = sorted(holiday_dict.keys())\n\n# Display the list of holidays\nprint(f\"Public holidays in {region}, {state_code}, {country_code} for the year {year}:\")\nfor holiday in holiday_list:\n    print(holiday)",
        "instruction": "Create a Python program using the 'holidays' API to list all the public holidays in a specific region (e.g., a city or district) within a country and state for a given year.",
        "prediction": ":import holidays\nfrom datetime import date\n\n# Specify the country or region (e.g., 'US' for the United States)\ncountry_code = 'US'\n\n# Specify the state or province code (e.g., 'CA' for California)\nstate_code = 'CA'\n\n# Specify the year for which you want to list holidays\nyear = 2023\n\n# Create a dictionary of holidays for the specified region and year\nholiday_dict = holidays.RegionHoliday(country_code, region=state_code, years=year)\n\n# List all the public holidays\nholiday_list = sorted(holiday_dict.keys())\n\n# Display the list of holidays\nprint(f\"Public holidays in {state_code}, {country_code} for the year {year}:\")\nfor holiday in holiday_list:\n    print(holiday)",
        "source": "This is some documentation of holidays API:\n\n=====\n\n\n==================\n Document 0 \n----------------\nFor a specific subdivisions (e.g. state or province):\n>>> california\\_holidays = country\\_holidays('US', subdiv='CA')\n\n\nFor special (one-off) country-wide holidays handling use\n`special\\_holidays`:\n\n```\nspecial\\_holidays = {\n    1977: ((JUN, 7, \"Silver Jubilee of Elizabeth II\"),),\n    1981: ((JUL, 29, \"Wedding of Charles and Diana\"),),\n    1999: ((DEC, 31, \"Millennium Celebrations\"),),\n    2002: ((JUN, 3, \"Golden Jubilee of Elizabeth II\"),),\n    2011: ((APR, 29, \"Wedding of William and Catherine\"),),\n    2012: ((JUN, 5, \"Diamond Jubilee of Elizabeth II\"),),\n    2022: (\n        (JUN, 3, \"Platinum Jubilee of Elizabeth II\"),\n        (SEP, 19, \"State Funeral of Queen Elizabeth II\"),\n    ),\n}\n\ndef \\_populate(self, year):\n    super().\\_populate(year)\n\n   ...\n\nParameters:\n* **years** (*Set**[**int**]*) \u2013 The year(s) to pre-calculate public holidays for at instantiation.\n* **expand** (*bool*) \u2013 Whether the entire year is calculated when one date from that year\nis requested.\n* **observed** (*bool*) \u2013 Whether to include the dates when public holiday are observed\n(e.g. a holiday falling on a Sunday being observed the\nfollowing Monday). This doesn\u2019t work for all countries.\n* **subdiv** (*str* *|* *None*) \u2013 The subdivision (e.g. state or province); not implemented for all\ncountries (see documentation).\n* **prov** (*str* *|* *None*) \u2013 *deprecated* use subdiv instead.\n* **state** (*str* *|* *None*) \u2013 *deprecated* use subdiv instead.\n* **language** (*str* *|* *None*) \u2013 The language which the returned holiday names will be translated\ninto. It must be an ISO 639-1 (2-letter) language code. If the\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'holidays' API to list all the public holidays in a specific region (e.g., a city or district) within a country and state for a given year.\n\n### Code:"
    },
    {
        "api": "hypothesis",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install hypothesis\nimport re\nimport unittest\nfrom hypothesis import given, strategies as st\n\ndef is_valid_date(date):\n    pattern = r\"^\\d{4}-\\d{2}-\\d{2}$\"\n    return re.match(pattern, date) is not None\n\ndef has_valid_dates(lst):\n    return any(is_valid_date(s) for s in lst)\n\nclass HasValidDatesTest(unittest.TestCase):\n    @given(st.lists(st.text(min_size=10, max_size=10, alphabet=st.characters(whitelist_categories=('Nd', 'Pd')))))\n    def test_has_valid_dates(self, lst):\n        result = has_valid_dates(lst)\n        self.assertEqual(result, any(is_valid_date(s) for s in lst))\n\nif __name__ == \"__main__\":\n    unittest.main()",
        "instruction": "Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any valid dates in the format 'YYYY-MM-DD'. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the date checking function.",
        "prediction": ":import unittest\nimport unittest\nfrom hypothesis import given, strategies as st\n\ndef has_valid_dates(lst):\n    return any(is_valid_format('YYYY-MM-DD', s) for s in lst)\n\nclass HasValidDatesTest(unittest.TestCase):\n    @given(st.lists(st.text()))\n    def test_has_valid_dates(self, lst):\n        result = has_valid_dates(lst)\n        self.assertEqual(result, any(is_valid_format('YYYY-MM-DD', s) for s in lst))\n\nif __name__ == \"__main__\":\n    unittest.main()",
        "source": "This is some documentation of hypothesis API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n Welcome to Hypothesis!\u00b6\n\n\nHypothesis is a Python library for\ncreating unit tests which are simpler to write and more powerful when run,\nfinding edge cases in your code you wouldn\u2019t have thought to look for. It is\nstable, powerful and easy to add to any existing test suite.\n\n\nIt works by letting you write tests that assert that something should be true\nfor every case, not just the ones you happen to think of.\n\n\nThink of a normal unit test as being something like the following:\n\n\n1. Set up some data.\n2. Perform some operations on the data.\n3. Assert something about the result.\n\n\nHypothesis lets you write tests which instead look like this:\n\n\n1. For all data matching some specification.\n2. Perform some operations on the data.\n3. Assert something about the result.\n\n\nThis is often called property-based testing, and was popularised by the\nHaskell library Quickcheck.\n\n\nIt works by generating arbitrary data matching your specification and checking\nthat your guarantee still holds in that case. If it finds an example where it doesn\u2019t,\nit takes that example and cuts it down to size, simplifying it until it finds a\nmuch smaller example that still causes the problem. It then saves that example\nfor later, so that once it has found a problem with your code it will not forget\nit in the future.\n\n\nWriting tests of this form usually consists of deciding on guarantees that\nyour code should make - properties that should always hold true,\nregardless of what the world throws at you. Examples of such guarantees\nmight be:\n\n\n* Your code shouldn\u2019t throw an exception, or should only throw a particular\ntype of exception (this works particularly well if you have a lot of internal\nassertions).\n* If you delete an object, it is no longer visible.\n* If you serialize and then deserialize a value, then you get the same value back.\n\n\nNow you know the basics of what Hypothesis does, the rest of this\ndocumentation will take you through how and why. It\u2019s divided into a\nnumber of sections, which you can see in the sidebar (or the\nmenu at the top if you\u2019re on mobile), but you probably want to begin with\nthe Quick\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any valid dates in the format 'YYYY-MM-DD'. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the date checking function.\n\n### Code:"
    },
    {
        "api": "hypothesis",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install hypothesis\nimport math\nimport unittest\nfrom hypothesis import given, strategies as st\n\ndef is_prime(n):\n    if n < 2:\n        return False\n    for i in range(2, int(math.sqrt(n))+1):\n        if n % i == 0:\n            return False\n    return True\n\nclass PrimeNumberTest(unittest.TestCase):\n    @given(st.integers(min_value=0, max_value=100))\n    def test_is_prime(self, n):\n        result = is_prime(n)\n        self.assertEqual(result, all(n % i != 0 for i in range(2, int(math.sqrt(n))+1)))\n\nif __name__ == \"__main__\":\n    unittest.main()",
        "instruction": "Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given number is a prime number. The 'hypothesis' library will be used to generate various input numbers and verify the correctness of the prime number checking function.",
        "prediction": ":import unittest\nimport unittest\nfrom hypothesis import given, strategies as st\n\ndef is_prime(n):\n    if n < 2:\n        return False\n    for i in range(2, n):\n        if n % i == 0:\n            return False\n    return True\n\nclass IsPrimeTest(unittest.TestCase):\n    @given(st.integers(min_value=0, max_value=100))\n    def test_is_prime(self, n):\n        result = is_prime(n)\n        self.assertEqual(result, n > 1 and all(n % i > 0 for i in range(2, n-1)))\n\nif __name__ == \"__main__\":\n    unittest.main()",
        "source": "This is some documentation of hypothesis API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# Welcome to Hypothesis!\u00b6\n\n\nHypothesis is a Python library for\ncreating unit tests which are simpler to write and more powerful when run,\nfinding edge cases in your code you wouldn\u2019t have thought to look for. It is\nstable, powerful and easy to add\n\n==================\n Document 1 \n----------------\n Welcome to Hypothesis!\u00b6\n\n\nHypothesis is a Python library for\ncreating unit tests which are simpler to write and more powerful when run,\nfinding edge cases in your code you wouldn\u2019t have thought to look for. It is\nstable, powerful and easy to add to any existing test suite.\n\n\nIt works by letting you write tests that assert that something should be true\nfor every case, not just the ones you happen to think of.\n\n\nThink of a normal unit test as being something like the following:\n\n\n1. Set up some data.\n2. Perform some operations on the data.\n3. Assert something about the result.\n\n\nHypothesis lets you write tests which instead look like this:\n\n\n1. For all data matching some specification.\n2. Perform some operations on the data.\n3. Assert something about the result.\n\n\nThis is often called property-based testing, and was popularised by the\nHaskell library Quickcheck.\n\n\nIt works by generating arbitrary data matching your specification and checking\nthat your guarantee still holds in that case. If it finds an example where it doesn\u2019t,\nit takes that example and cuts it down to size, simplifying it until it finds a\nmuch smaller example that still causes the problem. It then saves that example\nfor later, so that once it has found a problem with your code it will not forget\nit in the future.\n\n\nWriting tests of this form usually consists of deciding on guarantees that\nyour code should make - properties that should always hold true,\nregardless of what the world throws at you. Examples of such guarantees\nmight be:\n\n\n* Your code shouldn\u2019t throw an exception, or should only throw a particular\ntype of exception (this works particularly well if you have a lot of internal\nassertions).\n* If you delete an object, it is no longer visible.\n* If you serialize and then deserialize a value, then you get the same value back.\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given number is a prime number. The 'hypothesis' library will be used to generate various input numbers and verify the correctness of the prime number checking function.\n\n### Code:"
    },
    {
        "api": "hypothesis",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install hypothesis\nimport re\nimport unittest\nfrom hypothesis import given, strategies as st\n\ndef is_valid_ipv4_address(ip_address):\n    pattern = r\"^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$\"\n    return re.match(pattern, ip_address) is not None\n\nclass ValidIPv4AddressTest(unittest.TestCase):\n    @given(st.text(min_size=7, max_size=15, alphabet=st.characters(whitelist_categories=('Nd', 'Pd'))))\n    def test_is_valid_ipv4_address(self, ip_address):\n        result = is_valid_ipv4_address(ip_address)\n        self.assertEqual(result, re.match(r\"^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$\", ip_address) is not None)\n\nif __name__ == \"__main__\":\n    unittest.main()",
        "instruction": "Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given string is a valid IPv4 address. The 'hypothesis' library will be used to generate various input strings and verify the correctness of the IPv4 address validation function.",
        "prediction": ":!pip install hypothesis\nimport re\nimport unittest\nfrom hypothesis import given, strategies as st\n\ndef is_valid_ipv4_address(ip_address):\n    pattern = r\"^\\d{1,3}(\\.\\d{1,3}){3}$\"\n    return re.match(pattern, ip_address) is not None\n\nclass ValidIpv4AddressTest(unittest.TestCase):\n    @given(st.ipvXAddress())\n    def test_is_valid_ipv4_address(self, ip_address):\n        result = is_valid_ipv4_address(ip_address)\n        self.assertEqual(result, re.match(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\", ip_address) is not None)\n\nif __name__ == \"__main__\":\n    unittest.main()",
        "source": "This is some documentation of hypothesis API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# Welcome to Hypothesis!\u00b6\n\n\nHypothesis is a Python library for\ncreating unit tests which are simpler to write and more powerful when run,\nfinding edge cases in your code you wouldn\u2019t have thought to look for. It is\nstable, powerful and easy to add\n\n==================\n Document 1 \n----------------\n Welcome to Hypothesis!\u00b6\n\n\nHypothesis is a Python library for\ncreating unit tests which are simpler to write and more powerful when run,\nfinding edge cases in your code you wouldn\u2019t have thought to look for. It is\nstable, powerful and easy to add to any existing test suite.\n\n\nIt works by letting you write tests that assert that something should be true\nfor every case, not just the ones you happen to think of.\n\n\nThink of a normal unit test as being something like the following:\n\n\n1. Set up some data.\n2. Perform some operations on the data.\n3. Assert something about the result.\n\n\nHypothesis lets you write tests which instead look like this:\n\n\n1. For all data matching some specification.\n2. Perform some operations on the data.\n3. Assert something about the result.\n\n\nThis is often called property-based testing, and was popularised by the\nHaskell library Quickcheck.\n\n\nIt works by generating arbitrary data matching your specification and checking\nthat your guarantee still holds in that case. If it finds an example where it doesn\u2019t,\nit takes that example and cuts it down to size, simplifying it until it finds a\nmuch smaller example that still causes the problem. It then saves that example\nfor later, so that once it has found a problem with your code it will not forget\nit in the future.\n\n\nWriting tests of this form usually consists of deciding on guarantees that\nyour code should make - properties that should always hold true,\nregardless of what the world throws at you. Examples of such guarantees\nmight be:\n\n\n* Your code shouldn\u2019t throw an exception, or should only throw a particular\ntype of exception (this works particularly well if you have a lot of internal\nassertions).\n* If you delete an object, it is no longer visible.\n* If you serialize and then deserialize a value, then you get the same value back.\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given string is a valid IPv4 address. The 'hypothesis' library will be used to generate various input strings and verify the correctness of the IPv4 address validation function.\n\n### Code:"
    },
    {
        "api": "hypothesis",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install hypothesis\nimport re\nimport unittest\nfrom hypothesis import given, strategies as st\n\ndef has_email_addresses(lst):\n    pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\"\n    return any(re.match(pattern, s) for s in lst)\n\nclass HasEmailAddressesTest(unittest.TestCase):\n    @given(st.lists(st.text()))\n    def test_has_email_addresses(self, lst):\n        result = has_email_addresses(lst)\n        self.assertEqual(result, any(re.match(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", s) for s in lst))\n\nif __name__ == \"__main__\":\n    unittest.main()",
        "instruction": "Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any email addresses. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the email address checking function.",
        "prediction": ":import unittest\nimport re\nimport unittest\nfrom hypothesis import given, strategies as st\n\ndef is_haskell_email(email):\n    pattern = r\"^[a-zA-Z0-9_-]{1,64}@[a-zA-Z0-9-]{1,64}.com$\"\n    return re.match(pattern, email) is not None\n\nclass Haskell_email_address_checker_test(unittest.TestCase):\n    @given(st.lists(st.text(min_size=5, max_size=50, alphabet=st.characters(whitelist_categories=('Ll', 'Lu', 'Nd', 'Pd')))))\n    def test_haskell_email(self, emails):\n        return unittest.is_instantiation(emails) and all(is_haskell_email(s) for s in emails)\n\nif __name__ == \"__main__\":\n    unittest.main()",
        "source": "This is some documentation of hypothesis API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# Welcome to Hypothesis!\u00b6\n\n\nHypothesis is a Python library for\ncreating unit tests which are simpler to write and more powerful when run,\nfinding edge cases in your code you wouldn\u2019t have thought to look for. It is\nstable, powerful and easy to add\n\n==================\n Document 1 \n----------------\n Welcome to Hypothesis!\u00b6\n\n\nHypothesis is a Python library for\ncreating unit tests which are simpler to write and more powerful when run,\nfinding edge cases in your code you wouldn\u2019t have thought to look for. It is\nstable, powerful and easy to add to any existing test suite.\n\n\nIt works by letting you write tests that assert that something should be true\nfor every case, not just the ones you happen to think of.\n\n\nThink of a normal unit test as being something like the following:\n\n\n1. Set up some data.\n2. Perform some operations on the data.\n3. Assert something about the result.\n\n\nHypothesis lets you write tests which instead look like this:\n\n\n1. For all data matching some specification.\n2. Perform some operations on the data.\n3. Assert something about the result.\n\n\nThis is often called property-based testing, and was popularised by the\nHaskell library Quickcheck.\n\n\nIt works by generating arbitrary data matching your specification and checking\nthat your guarantee still holds in that case. If it finds an example where it doesn\u2019t,\nit takes that example and cuts it down to size, simplifying it until it finds a\nmuch smaller example that still causes the problem. It then saves that example\nfor later, so that once it has found a problem with your code it will not forget\nit in the future.\n\n\nWriting tests of this form usually consists of deciding on guarantees that\nyour code should make - properties that should always hold true,\nregardless of what the world throws at you. Examples of such guarantees\nmight be:\n\n\n* Your code shouldn\u2019t throw an exception, or should only throw a particular\ntype of exception (this works particularly well if you have a lot of internal\nassertions).\n* If you delete an object, it is no longer visible.\n* If you serialize and then deserialize a value, then you get the same value back.\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any email addresses. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the email address checking function.\n\n### Code:"
    },
    {
        "api": "ibis-framework",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install 'ibis-framework[duckdb]'\nimport ibis\nimport pandas as pd\n\nibis.options.interactive = True\n\ndf = pd.DataFrame(\n    [[\"a\", 1, 2], [\"b\", 3, 4], [\"c\", 5, 6]],\n    columns=[\"one\", \"two\", \"three\"],\n    index=[5, 6, 7],\n)\nt = ibis.memtable(df, name=\"t\")\n\n# Add a new column to the table\ntransformed_table = t.mutate(four=t.two + t.three)\n\n# Rename existing columns\ntransformed_table = transformed_table.rename({\"one\": \"new_one\", \"two\": \"new_two\", \"three\": \"new_three\", \"four\": \"new_four\"})\n\nprint(\"Transformed table: \")\nprint(transformed_table)",
        "instruction": "Create a Python program using the 'ibis-framework' API to perform data transformation operations on a data table. The program should define a sample data table using a Pandas DataFrame and convert it to an 'ibis' table. You need to interact with the 'ibis-framework' library to load the data, transform the table by adding a new column, and rename existing columns.",
        "prediction": ":import ibis\nimport ibis\nimport pandas as pd\n\nibis.options.interactive = True\n\ndf = pd.DataFrame(\n    [[\"a\", 1, 2], [\"b\", 3, 4], [\"c\", 5, 6]],\n    columns=[\"one\", \"two\", \"three\"],\n    index=[5, 6, 7],\n)\nt = ibis.memtable(df, name=\"t\")\n\n# Add a new column to the table\ntransformed_table = t.mutate(four=t.two + t.three)\n\n# Rename existing columns\ntransformed_table = transformed_table.mutate(one=transformed_table.one.cast(\"int\"))\n\nprint(\"Transformed table: \")\nprint(transformed_table)",
        "source": "This is some documentation of ibis-framework API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n\n1. Expression API\n2. Table expressions\n\n\n# Table expressions\n\n\nTables are one of the core data structures in Ibis.\n\n\n# Table\n\n\n`Table()`\n\n\n## Attributes\n\n\n| Name | Description |\n| --- | --- |\n| columns | The list of columns in this table. |\n\n\n## Methods\n\n\n| Name | Description |\n| --- | --- |\n| aggregate | Aggregate a table with a given set of reductions grouping by `by`. |\n| alias | Create a table expression with a specific name `alias`. |\n| as\\_table | Promote\n\n==================\n Document 1 \n----------------\n table\n\n\n`ibis.table(schema=None, name=None)`\n\n\nCreate a table literal or an abstract table without data.\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `schema` | `SupportsSchema` | None | A schema for the table | `None` |\n| `name` | str | None | Name for the table. One is generated if this value is `None`. | `None` |\n\n\n| Type | Description |\n| --- | --- |\n| `Table` | A table expression |\n\n\nCreate a table with no data backing it\n\n\n```\n>>> import ibis\n>>> ibis.options.interactive\n```\n\n```\n>>> t = ibis.table(schema=dict(a=\"int\", b=\"string\"), name=\"t\")\n>>> t\n```\n\n```\nIbisError: Expression contains unbound tables and therefore cannot be executed. Use ibis.<backend>.execute(expr) or assign a backend instance to `ibis.options.default_backend`.\n```\n\n\n# difference\n\n\n`ibis.difference(table, *rest, distinct=True)`\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `table` | `ir`.`Table` | A table expression | *required* |\n| `*rest` | `ir`.`Table` | Additional table expressions | `()` |\n| `distinct` | bool | Only diff distinct rows not occurring in the calling table |\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'ibis-framework' API to perform data transformation operations on a data table. The program should define a sample data table using a Pandas DataFrame and convert it to an 'ibis' table. You need to interact with the 'ibis-framework' library to load the data, transform the table by adding a new column, and rename existing columns.\n\n### Code:"
    },
    {
        "api": "ibis-framework",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install 'ibis-framework[duckdb]'\nimport ibis\nimport pandas as pd\n\nibis.options.interactive = True\n\ndf = pd.DataFrame(\n    [[\"a\", 1, 2], [\"b\", 3, 4]],\n    columns=[\"one\", \"two\", \"three\"],\n    index=[5, 6],\n)\nt = ibis.memtable(df, name=\"t\")\nprint(\"Ibis data table: \")\nprint(t)\nprint(\"Ibis data schema: \")\nprint(t.schema())",
        "instruction": "Create a Python program using the 'ibis-framework' API to work with data tables and schemas. The program should define a sample data table using a Pandas DataFrame and convert it to an 'ibis' table. You need to interact with the 'ibis-framework' library to load the data, define a schema, and display the table and its schema.",
        "prediction": ":import ibis\nimport ibis\nimport pandas as pd\n\nibis.options.interactive = True\n\ndf = pd.DataFrame(\n    [[\"a\", 1, 2], [\"b\", 3, 4], [\"c\", 5, 6]],\n    columns=[\"one\", \"two\", \"three\"],\n    index=[5, 6, 7],\n)\nt = ibis.memtable(df, name=\"t\")\n\nprint(\"Schema: \")\nprint(t.schema)\nprint(\"Table: \")\nprint(t)",
        "source": "This is some documentation of ibis-framework API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n\n1. Expression API\n2. Table expressions\n\n\n# Table expressions\n\n\nTables are one of the core data structures in Ibis.\n\n\n# Table\n\n\n`Table()`\n\n\n## Attributes\n\n\n| Name | Description |\n| --- | --- |\n| columns | The list of columns in this table. |\n\n\n## Methods\n\n\n| Name | Description |\n| --- | --- |\n| aggregate | Aggregate a table with a given set of reductions grouping by `by`. |\n| alias | Create a table expression with a specific name `alias`. |\n| as\\_table | Promote\n\n==================\n Document 1 \n----------------\n table\n\n\n`ibis.table(schema=None, name=None)`\n\n\nCreate a table literal or an abstract table without data.\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `schema` | `SupportsSchema` | None | A schema for the table | `None` |\n| `name` | str | None | Name for the table. One is generated if this value is `None`. | `None` |\n\n\n| Type | Description |\n| --- | --- |\n| `Table` | A table expression |\n\n\nCreate a table with no data backing it\n\n\n```\n>>> import ibis\n>>> ibis.options.interactive\n```\n\n```\n>>> t = ibis.table(schema=dict(a=\"int\", b=\"string\"), name=\"t\")\n>>> t\n```\n\n```\nIbisError: Expression contains unbound tables and therefore cannot be executed. Use ibis.<backend>.execute(expr) or assign a backend instance to `ibis.options.default_backend`.\n```\n\n\n# difference\n\n\n`ibis.difference(table, *rest, distinct=True)`\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `table` | `ir`.`Table` | A table expression | *required* |\n| `*rest` | `ir`.`Table` | Additional table expressions | `()` |\n| `distinct` | bool | Only diff distinct rows not occurring in the calling table |\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'ibis-framework' API to work with data tables and schemas. The program should define a sample data table using a Pandas DataFrame and convert it to an 'ibis' table. You need to interact with the 'ibis-framework' library to load the data, define a schema, and display the table and its schema.\n\n### Code:"
    },
    {
        "api": "json-tricks",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install json_tricks\nfrom json_tricks import dumps\nimport math\n\ndata = {\n    'pi': math.pi,\n    'nan_value': math.nan,\n    'infinity_value': math.inf\n}\n\njson_str = dumps(data, special_values={'NaN': 'NaN', 'Infinity': 'Infinity'}, indent=4)\n\nprint(json_str)",
        "instruction": "Create a Python program that uses 'json-tricks' to handle JSON data with custom serialization of NaN and Infinity. Define a dictionary with floating-point values that include NaN and Infinity. Serialize the dictionary into a JSON string using 'dumps' and ensure that the custom serialization of NaN and Infinity is applied.",
        "prediction": ":pip install json_tricks\nfrom json_tricks import dumps\n\ndata = {\n    'nan': float('nan'),\n    'inf': float('inf')\n}\n\njson_str = dumps(data, special=True, indent=4)\n\nprint(json_str)",
        "source": "This is some documentation of json-tricks API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n JSON tricks (python)\u00b6\n\n\nThe pyjson-tricks package brings several pieces of functionality to python handling of json files:\n\n\n1. **Store and load numpy arrays** in human-readable format.\n2. **Store and load class instances** both generic and customized.\n3. **Store and load date/times** as a dictionary (including timezone).\n4. **Preserve map order** `{}` using `OrderedDict`.\n5. **Allow for comments** in json files by starting lines with `#`.\n6. Sets, complex numbers, Decimal, Fraction, enums, compression, duplicate keys, \u2026\n\n\nAs well as compression and disallowing duplicate keys.\n\n\n* Code: https://github.com/mverleg/pyjson\\_tricks\n* Documentation: http://json-tricks.readthedocs.org/en/latest/\n* PIP: https://pypi.python.org/pypi/json\\_tricks\n\n\nThe 2.0 series added some of the above features and broke backward compatibility. The version 3.0 series is a more readable rewrite that also makes it easier to combine encoders, again not fully backward compatible.\n\n\nSeveral keys of the format `\\_\\_keyname\\_\\_` have special meanings, and more might be added in future releases.\n\n\nIf you\u2019re considering JSON-but-with-comments as a config file format, have a look at HJSON, it might be more appropriate. For other purposes, keep reading!\n\n\nThanks for all the Github stars!\n\n# Installation and use\u00b6\n\n\nYou can install using\n\n```\npip install json-tricks  # or e.g. 'json-tricks<3.0' for older versions\n\n```\n\n\nDecoding of some data types needs the corresponding package to be installed, e.g. `numpy` for arrays, `pandas` for dataframes and `pytz` for timezone-aware datetimes.\n\n\nYou\n\n==================\n Document 1 \n----------------\n\n# JSON tricks (python)\u00b6\n\n\nThe pyjson-tricks package brings several pieces of functionality to python handling of json files:\n\n\n1. **Store and load numpy arrays\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that uses 'json-tricks' to handle JSON data with custom serialization of NaN and Infinity. Define a dictionary with floating-point values that include NaN and Infinity. Serialize the dictionary into a JSON string using 'dumps' and ensure that the custom serialization of NaN and Infinity is applied.\n\n### Code:"
    },
    {
        "api": "json-tricks",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install json_tricks\nfrom json_tricks import dumps\n\nnested_data = {\n    'name': 'John',\n    'info': {\n        'age': 30,\n        'address': {\n            'street': '123 Main St',\n            'city': 'Cityville'\n        },\n        'hobbies': ['Reading', 'Traveling']\n    }\n}\n\njson_str = dumps(nested_data, indent=4)\n\nprint(json_str)",
        "instruction": "Develop a Python program that showcases the use of 'json-tricks' to handle JSON data with nested objects. Create a dictionary containing nested dictionaries and lists. Serialize the dictionary into a JSON string using 'dumps' and ensure that the nested structure is preserved.",
        "prediction": ":import json_tricks\nfrom json_tricks import dumps\n\ndata = {\n    'name': 'Bob',\n    'age': 35,\n    'hobbies': ['Traveling', 'Reading']\n}\n\njson_str = dumps(data, use_decimal=True, indent=4)\n\nprint(json_str)",
        "source": "This is some documentation of json-tricks API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n# JSON tricks (python)\u00b6\n\n\nThe pyjson-tricks package brings several pieces of functionality to python handling of json files:\n\n\n1. **Store and load numpy arrays** in human-readable format.\n2. **Store and load class instances** both generic and customized.\n3. **Store and load date/times** as a\n\n==================\n Document 1 \n----------------\n JSON tricks (python)\u00b6\n\n\nThe pyjson-tricks package brings several pieces of functionality to python handling of json files:\n\n\n1. **Store and load numpy arrays** in human-readable format.\n2. **Store and load class instances** both generic and customized.\n3. **Store and load date/times** as a dictionary (including timezone).\n4. **Preserve map order** `{}` using `OrderedDict`.\n5. **Allow for comments** in json files by starting lines with `#`.\n6. Sets, complex numbers, Decimal, Fraction, enums, compression, duplicate keys, \u2026\n\n\nAs well as compression and disallowing duplicate keys.\n\n\n* Code: https://github.com/mverleg/pyjson\\_tricks\n* Documentation: http://json-tricks.readthedocs.org/en/latest/\n* PIP: https://pypi.python.org/pypi/json\\_tricks\n\n\nThe 2.0 series added some of the above features and broke backward compatibility. The version 3.0 series is a more readable rewrite that also makes it easier to combine encoders, again not fully backward compatible.\n\n\nSeveral keys of the format `\\_\\_keyname\\_\\_` have special meanings, and more might be added in future releases.\n\n\nIf you\u2019re considering JSON-but-with-comments as a config file format, have a look at HJSON, it might be more appropriate. For other purposes, keep reading!\n\n\nThanks for all the Github stars!\n\n# Installation and use\u00b6\n\n\nYou can install using\n\n```\npip install json-tricks  # or e.g. 'json-tricks<3.0' for older versions\n\n```\n\n\nDecoding of some data types needs the corresponding package to be installed,\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program that showcases the use of 'json-tricks' to handle JSON data with nested objects. Create a dictionary containing nested dictionaries and lists. Serialize the dictionary into a JSON string using 'dumps' and ensure that the nested structure is preserved.\n\n### Code:"
    },
    {
        "api": "json-tricks",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install json_tricks\nfrom json_tricks import dumps, loads\n\ndef custom_encoder(obj):\n    if isinstance(obj, complex):\n        return {'__complex__': True, 'real': obj.real, 'imag': obj.imag}\n    raise TypeError(repr(obj) + ' is not JSON serializable')\n\ndef custom_decoder(dct):\n    if '__complex__' in dct:\n        return complex(dct['real'], dct['imag'])\n    return dct\n\ndata = {\n    'complex_number': 2 + 3j,\n    'regular_number': 42\n}\n\njson_str = dumps(data, default=custom_encoder, indent=4)\nparsed_data = loads(json_str, object_hook=custom_decoder)\n\nprint(parsed_data['complex_number'])\nprint(parsed_data['regular_number'])",
        "instruction": "Create a Python program that showcases the use of 'json-tricks' for handling JSON data with custom encoder and decoder functions. Define custom functions for encoding and decoding data. Use these functions to serialize and deserialize a dictionary containing special data types. Ensure that the custom encoding and decoding functions are applied to the data.",
        "prediction": ":import os\nfrom json_tricks import encoder, decoder\n\n# Define custom encoder/decoder functions\ndef encode_custom_data(data):\n    if isinstance(data, special_data_types):\n        return data.__class__.__name__ + '_' + data.__str__()\n    else:\n        return data\n\ndef decode_custom_data(value):\n    if isinstance(value, str):\n        return special_data_types(value)\n\n# Define the custom encoder and decoder functions\ndef custom_encoder(data):\n    encoded_data = encoder(data)\n    encoded_data['_keyname1'] = encode_custom_data(data['_keyname1'])\n    return encoded_data\n\ndef custom_decoder(data):\n    data['_keyname1'] = decode_custom_data(data['_keyname1'])\n    return decoder(data)\n\n# Define the special data type\nspecial_data_types = np.special_data_types\n\n# Define the data\ndata = {\n    '_keyname1': special_data_types('Barry Block'),\n    '_keyname2': 42\n}\n\n# Define the configuration\nconfig_file_format = '_keyname1_ `\\_\\_keyname1_\\_` = encode_custom_data(data[\"_keyname1\"])'\n\n# Define the configuration\nencoder_function = custom_encoder\ndecoder_function = custom_decoder\n\n# Encodes the data\ndata_tricks = encoder(data, config_file_format=config_file_format, encoder_function=encoder_function, decoder_function=decoder_function)\n\n# Store the data\njson.dump(data_tricks, open('json_tricks_dumping.txt', 'w'))\n\n# Store and load class instances\nwith open('json_tricks_tricks.txt', 'w') as outfile:\n    json.dump(special_data_types(), outfile, cls=json_tricks.encoder(extension='.json'), indent=4, encoding='utf-8')\n\nwith open('json_tricks_tricks.txt', 'r') as infile:\n    data = json.load(infile, cls=json_tricks.decoder(extension='.json'), encoding='utf-8')\n\nprint(data)",
        "source": "This is some documentation of json-tricks API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n# JSON tricks (python)\u00b6\n\n\nThe pyjson-tricks package brings several pieces of functionality to python handling of json files:\n\n\n1. **Store and load numpy arrays** in human-readable format.\n2. **Store and load class instances** both generic and customized.\n3. **Store and load date/times** as a\n\n==================\n Document 1 \n----------------\n JSON tricks (python)\u00b6\n\n\nThe pyjson-tricks package brings several pieces of functionality to python handling of json files:\n\n\n1. **Store and load numpy arrays** in human-readable format.\n2. **Store and load class instances** both generic and customized.\n3. **Store and load date/times** as a dictionary (including timezone).\n4. **Preserve map order** `{}` using `OrderedDict`.\n5. **Allow for comments** in json files by starting lines with `#`.\n6. Sets, complex numbers, Decimal, Fraction, enums, compression, duplicate keys, \u2026\n\n\nAs well as compression and disallowing duplicate keys.\n\n\n* Code: https://github.com/mverleg/pyjson\\_tricks\n* Documentation: http://json-tricks.readthedocs.org/en/latest/\n* PIP: https://pypi.python.org/pypi/json\\_tricks\n\n\nThe 2.0 series added some of the above features and broke backward compatibility. The version 3.0 series is a more readable rewrite that also makes it easier to combine encoders, again not fully backward compatible.\n\n\nSeveral keys of the format `\\_\\_keyname\\_\\_` have special meanings, and more might be added in future releases.\n\n\nIf you\u2019re considering JSON-but-with-comments as a config file format, have a look at HJSON, it might be more appropriate. For other purposes, keep reading!\n\n\nThanks for all the Github stars!\n\n# Installation and use\u00b6\n\n\nYou can install using\n\n```\npip install json-tricks  # or e.g. 'json-tricks<3.0' for older versions\n\n```\n\n\nDecoding of some data types needs the corresponding package to be installed,\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that showcases the use of 'json-tricks' for handling JSON data with custom encoder and decoder functions. Define custom functions for encoding and decoding data. Use these functions to serialize and deserialize a dictionary containing special data types. Ensure that the custom encoding and decoding functions are applied to the data.\n\n### Code:"
    },
    {
        "api": "json-tricks",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install json_tricks\nfrom json_tricks import dumps\n\nclass CustomObject:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\ndata = {\n    'person': CustomObject('Frank', 45)\n}\n\njson_str = dumps(data, object_serialization={\n    'person': lambda obj: {'name': obj.name, 'age': obj.age}\n}, indent=4)\n\nprint(json_str)",
        "instruction": "Develop a Python program that demonstrates the use of 'json-tricks' for handling JSON data with custom object serialization. Define a dictionary that includes custom Python objects and serialize it into a JSON string using 'dumps'. Specify custom serialization options for the objects.",
        "prediction": ":pip install json_tricks\nfrom json_tricks import dumps\n\ndata = {\n    'integer_as_str': 42,\n    'float_with_precision': 3.141592653589793,\n    'ignore_key': 'ignore_value',\n    'in_np-array': np.arange(10),\n    'in_ordered-dict': OrderedDict([('a', 1), ('b', 2)]),\n    'in_custom': [1, 2, 3],\n    'in_date/time': 'The date/time is Unix: Unix Epoch: 1970-01-01 00:00:00',\n    'in_complex': 42 + 1j,\n    'in_duplicate': ['Eve', 'Bob', 'Tim']\n}\n\njson_str = dumps(data, object_hook= lambda d: data, indent=4)\n\nprint(json_str)",
        "source": "This is some documentation of json-tricks API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n# JSON tricks (python)\u00b6\n\n\nThe pyjson-tricks package brings several pieces of functionality to python handling of json files:\n\n\n1. **Store and load numpy arrays** in human-readable format.\n2. **Store and load class instances** both generic and customized.\n3. **Store and load date/times** as a\n\n==================\n Document 1 \n----------------\n JSON tricks (python)\u00b6\n\n\nThe pyjson-tricks package brings several pieces of functionality to python handling of json files:\n\n\n1. **Store and load numpy arrays** in human-readable format.\n2. **Store and load class instances** both generic and customized.\n3. **Store and load date/times** as a dictionary (including timezone).\n4. **Preserve map order** `{}` using `OrderedDict`.\n5. **Allow for comments** in json files by starting lines with `#`.\n6. Sets, complex numbers, Decimal, Fraction, enums, compression, duplicate keys, \u2026\n\n\nAs well as compression and disallowing duplicate keys.\n\n\n* Code: https://github.com/mverleg/pyjson\\_tricks\n* Documentation: http://json-tricks.readthedocs.org/en/latest/\n* PIP: https://pypi.python.org/pypi/json\\_tricks\n\n\nThe 2.0 series added some of the above features and broke backward compatibility. The version 3.0 series is a more readable rewrite that also makes it easier to combine encoders, again not fully backward compatible.\n\n\nSeveral keys of the format `\\_\\_keyname\\_\\_` have special meanings, and more might be added in future releases.\n\n\nIf you\u2019re considering JSON-but-with-comments as a config file format, have a look at HJSON, it might be more appropriate. For other purposes, keep reading!\n\n\nThanks for all the Github stars!\n\n# Installation and use\u00b6\n\n\nYou can install using\n\n```\npip install json-tricks  # or e.g. 'json-tricks<3.0' for older versions\n\n```\n\n\nDecoding of some data types needs the corresponding package to be installed,\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program that demonstrates the use of 'json-tricks' for handling JSON data with custom object serialization. Define a dictionary that includes custom Python objects and serialize it into a JSON string using 'dumps'. Specify custom serialization options for the objects.\n\n### Code:"
    },
    {
        "api": "json-tricks",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install json_tricks\nfrom json_tricks import dumps, loads\n\npeople = [\n    {'name': 'Alice', 'age': 25, 'address': '123 Main St'},\n    {'name': 'Bob', 'age': 30, 'address': '456 Elm St'},\n    {'name': 'Charlie', 'age': 35, 'address': '789 Oak St'}\n]\n\njson_str = dumps(people, indent=4)\nparsed_people = loads(json_str)\n\nprint(parsed_people)\nprint(parsed_people[0]['name'])  # Should print 'Alice'.",
        "instruction": "Create a Python program that demonstrates the use of 'json-tricks' for handling JSON arrays. Define a list of dictionaries, each representing a person's information, such as name, age, and address. Serialize this list of dictionaries into a JSON string using the 'dumps' function. Then, deserialize the JSON string back into a list of dictionaries using the 'loads' function.",
        "prediction": ":importjson_tricks\nfrom json_tricks import dumps, loads\nfrom json_tricks import exceptions\n\ndata = [\n    {\n        'name': 'Alice',\n        'age': 28,\n        'addresses': [\n            {'street': 'Street Address 1', 'city': 'City Address 1','state': 'State Address 1', 'zip': 'Zip Address 1', 'country': 'Country Address 1', 'house': np.arange(10), 'office': np.arange(10)},\n            {'street': 'Street Address 2', 'city': 'City Address 2','state': 'State Address 2', 'zip': 'Zip Address 2', 'country': 'Country Address 2', 'house': np.arange(10), 'office': np.arange(10)},\n        ],\n    },\n    {\n        'name': 'Alice',\n        'age': 28,\n        'addresses': [\n            {'street': 'Street Address 1', 'city': 'City Address 1','state': 'State Address 1', 'zip': 'Zip Address 1', 'country': 'Country Address 1', 'house': np.arange(10), 'office': np.arange(10)},\n            {'street': 'Street Address 2', 'city': 'City Address 2','state': 'State Address 2', 'zip': 'Zip Address 2', 'country': 'Country Address 2', 'house': np.arange(10), 'office': np.arange(10)},\n        ],\n    },\n    {\n        'name': 'Alice',\n        'age': 28,\n        'addresses': [\n            {'street': 'Street Address 1', 'city': 'City Address 1','state': 'State Address 1', 'zip': 'Zip Address 1', 'country': 'Country Address 1', 'house': np.arange(10), 'office': np.arange(10)},\n            {'street': 'Street Address 2', 'city': 'City Address 2','state': 'State Address 2', 'zip': 'Zip Address 2', 'country': 'Country Address 2', 'house': np.arange(10), 'office': np.arange(10)},\n        ],\n    },\n    {\n        'name': 'Alice',\n        'age': 28,\n        'addresses': [\n            {'street': 'Street Address 1', 'city': 'City Address 1','state': 'State Address 1', 'zip': 'Zip Address 1', 'country': 'Country Address 1', 'house': np.arange(10), 'office': np.arange(10)},\n            {'street': 'Street Address 2', 'city': 'City Address 2','state': 'State Address 2', 'zip': 'Zip Address 2', 'country': 'Country Address 2', 'house': np.arange(10), 'office': np.arange(10)},\n        ],\n    },\n    {\n        'name': 'Alice',\n        'age': 28,\n        'addresses': [\n            {'street': 'Street Address 1', 'city': 'City Address 1','state': 'State Address 1', 'zip': 'Zip Address 1', 'country': 'Country Address 1', 'house': np.arange(10), 'office': np.arange(10)},\n            {'street': 'Street Address 2', 'city': 'City Address 2','state': 'State Address 2', 'zip': 'Zip Address 2', 'country': 'Country Address 2', 'house': np.arange(10), 'office': np.arange(10)},\n        ],\n    },\n    {\n        'name': 'Alice',\n        'age': 28,\n        'addresses': [\n            {'street': 'Street Address 1', 'city': 'City Address 1','state': 'State Address 1', 'zip': 'Zip Address 1', 'country': 'Country Address 1', 'house': np.arange(10), 'office': np.arange(10)},\n            {'street': 'Street Address 2', 'city': 'City Address 2','state': 'State Address 2', 'zip': 'Zip Address 2', 'country': 'Country Address 2', 'house': np.arange(10), 'office': np.arange(10)},\n        ],\n    },\n]\n\njson_str = dumps(data, indent=4)\n\ndata_rec = loads(json_str)\n\nprint(data_rec)",
        "source": "This is some documentation of json-tricks API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n# JSON tricks (python)\u00b6\n\n\nThe pyjson-tricks package brings several pieces of functionality to python handling of json files:\n\n\n1. **Store and load numpy arrays** in human-readable format.\n2. **Store and load class instances** both generic and customized.\n3. **Store and load date/times** as a\n\n==================\n Document 1 \n----------------\n JSON tricks (python)\u00b6\n\n\nThe pyjson-tricks package brings several pieces of functionality to python handling of json files:\n\n\n1. **Store and load numpy arrays** in human-readable format.\n2. **Store and load class instances** both generic and customized.\n3. **Store and load date/times** as a dictionary (including timezone).\n4. **Preserve map order** `{}` using `OrderedDict`.\n5. **Allow for comments** in json files by starting lines with `#`.\n6. Sets, complex numbers, Decimal, Fraction, enums, compression, duplicate keys, \u2026\n\n\nAs well as compression and disallowing duplicate keys.\n\n\n* Code: https://github.com/mverleg/pyjson\\_tricks\n* Documentation: http://json-tricks.readthedocs.org/en/latest/\n* PIP: https://pypi.python.org/pypi/json\\_tricks\n\n\nThe 2.0 series added some of the above features and broke backward compatibility. The version 3.0 series is a more readable rewrite that also makes it easier to combine encoders, again not fully backward compatible.\n\n\nSeveral keys of the format `\\_\\_keyname\\_\\_` have special meanings, and more might be added in future releases.\n\n\nIf you\u2019re considering JSON-but-with-comments as a config file format, have a look at HJSON, it might be more appropriate. For other purposes, keep reading!\n\n\nThanks for all the Github stars!\n\n# Installation and use\u00b6\n\n\nYou can install using\n\n```\npip install json-tricks  # or e.g. 'json-tricks<3.0' for older versions\n\n```\n\n\nDecoding of some data types needs the corresponding package to be installed,\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that demonstrates the use of 'json-tricks' for handling JSON arrays. Define a list of dictionaries, each representing a person's information, such as name, age, and address. Serialize this list of dictionaries into a JSON string using the 'dumps' function. Then, deserialize the JSON string back into a list of dictionaries using the 'loads' function.\n\n### Code:"
    },
    {
        "api": "jsonschema",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install jsonschema\nimport jsonschema\n\n# Define a JSON schema with conditional validation based on properties\nschema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"isAdult\": {\"type\": \"boolean\"},\n        \"age\": {\"type\": \"integer\"}\n    },\n    \"if\": {\"properties\": {\"isAdult\": {\"const\": True}}, \"then\": {\"required\": [\"age\"]}}\n}\n\n# Create a JSON object to validate\njson_data = {\n    \"isAdult\": True,\n    \"age\": 25\n}\n\n# Validate the JSON data against the schema with conditional validation\ntry:\n    jsonschema.validate(instance=json_data, schema=schema)\n    print(\"JSON data is valid according to the schema with conditional validation.\")\nexcept jsonschema.exceptions.ValidationError as e:\n    print(f\"JSON data is not valid according to the schema with conditional validation. Error: {e}\")\n",
        "instruction": "Create a Python program that uses 'jsonschema' to validate a JSON object against a schema with conditional validation based on properties.",
        "prediction": ":pip install jsonschema\nimport jsonschema\n\n# Define a JSON schema with conditional validation based on properties list\nschema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"isMember\": {\"type\": \"boolean\"},\n        \"membershipType\": {\"type\": \"string\"},\n        \"discountCode\": {\"type\": \"string\"}\n    },\n    \"properties.membershipType\": {\"const\": \"normal\"},\n    \"required\": [\"isMember\"]\n}\n\n# Create a JSON object to validate\njson_data = {\n    \"isMember\": True,\n    \"membershipType\": \"normal\",\n    \"discountCode\": \"VALIDDISCOUNTcode\"\n}\n\n# Custom format checker for formats with conditional validation based on properties\nformat_validate = jsonschema.FormatChecker(checkers={\"properties.membershipType\": isinstance})\n\n# Validate the JSON data against the schema with conditional validation and custom format checker\ntry:\n    jsonschema.validate(instance=json_data, schema=schema, format_checker=format_validate)\n    print(\"JSON data is valid according to the schema with conditional validation and custom format checker.\")\nexcept jsonschema.exceptions.ValidationError as e:\n    print(f\"JSON data is not valid according to the schema. Error: {e}\")\n",
        "source": "This is some documentation of jsonschema API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# API Reference#\n\n\n## Submodules#\n\n* `jsonschema.validators`\n* `jsonschema.exceptions`\n* `jsonschema.protocols`\n\n## `jsonschema`#\n\n\nAn implementation of JSON Schema for Python.\n\n\nThe main functionality is provided by the validator classes for each of the\nsupported JSON Schema versions.\n\n\nMost commonly, `jsonschema.validators.validate` is the quickest way to simply\nvalidate a given instance under a schema, and will create\n\n==================\n Document 1 \n----------------\n# `jsonschema`#\n\n\nAn implementation of JSON Schema for Python.\n\n\nThe main functionality is provided by the validator classes for each of the\nsupported JSON Schema versions.\n\n\nMost commonly, `jsonschema.validators.validate` is the quickest way to simply\nvalidate a given instance under a schema, and will create a validator\nfor you.\n\n\n*class* jsonschema.FormatChecker(*formats: Iterable[str] | None = None*)[source]#\nA `format` property checker.\n\n\nJSON Schema does not mandate that the `format` property actually do any\nvalidation. If validation is desired however, instances of this class can\nbe hooked into validators to enable format validation.\n\n\n`FormatChecker` objects always return `True` when asked about\nformats that they do not know how to validate.\n\n\nTo add a check for a custom format use the `FormatChecker.checks`\ndecorator.\n\nParameters:\n**formats** \u2013 The known formats to validate. This argument can be used to\nlimit which formats will be used during validation.\n\n\ncheck(*instance: object*, *format: str*) \u2192 None[source]#\nCheck whether the instance conforms to the given format.\n\nParameters:\n* **instance** (*any primitive type*, i.e. str, number, bool) \u2013 The instance to check\n* **format** \u2013 The format that instance should conform to\n\nRaises:\n**FormatError** \u2013 if the instance does not conform to `format`\n\nchecks(*format: str*, *raises: Type[Exception] | T\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that uses 'jsonschema' to validate a JSON object against a schema with conditional validation based on properties.\n\n### Code:"
    },
    {
        "api": "jsonschema",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install jsonschema\nimport jsonschema\n\n# Define a JSON schema with format-based date validation for a property, custom keyword validation, conditional validation based on property values, and pattern-based validation for another property\nschema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"birthDate\": {\"type\": \"string\", \"format\": \"date\"},\n        \"isStudent\": {\"type\": \"boolean\"},\n        \"studentID\": {\"type\": \"string\"},\n        \"email\": {\"type\": \"string\", \"pattern\": \"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\"}\n    },\n    \"custom_keywords\": {\n        \"student_required\": {\n            \"if\": {\"properties\": {\"isStudent\": {\"const\": True}}, \"then\": {\"required\": [\"studentID\"]}}\n        },\n        \"student_email_required\": {\n            \"if\": {\n                \"properties\": {\"isStudent\": {\"const\": True}, \"studentID\": {\"pattern\": \"^[A-Z]{3}[0-9]{3}$\"}},\n                \"then\": {\"required\": [\"email\"]}\n            }\n        }\n    }\n}\n\n# Create a JSON object to validate\njson_data = {\n    \"birthDate\": \"1995-01-15\",\n    \"isStudent\": True,\n    \"studentID\": \"ABC123\",\n    \"email\": \"student@example.com\"\n}\n\n# Custom keyword validation function\ndef validate_custom_keywords(validator, custom_keywords, instance, schema):\n    if instance[\"isStudent\"] and \"studentID\" in instance and not instance[\"studentID\"].startswith(\"ABC\"):\n        yield jsonschema.exceptions.ValidationError(\"Student ID must start with 'ABC' for students.\")\n    if instance[\"isStudent\"] and \"studentID\" in instance and not instance[\"email\"].endswith(\"student.com\"):\n        yield jsonschema.exceptions.ValidationError(\"Invalid email format for student with ID starting with 'ABC'.\")\n\n# Add the custom keyword validator to the validator registry\njsonschema.validators.validator_for(schema).VALIDATORS[\"custom_keywords\"] = validate_custom_keywords\n\n# Validate the JSON data against the schema with format-based date validation, custom keywords, conditional validation, and pattern-based validation\ntry:\n    jsonschema.validate(instance=json_data, schema=schema, format_checker=jsonschema.FormatChecker())\n    print(\"JSON data is valid according to the schema with format-based date validation, custom keywords, conditional validation, and pattern-based validation.\")\nexcept jsonschema.exceptions.ValidationError as e:\n    print(f\"JSON data is not valid according to the schema. Error: {e}\")\n",
        "instruction": "Create a Python program that utilizes 'jsonschema' to validate a JSON object against a schema with format-based date validation for a property, custom keyword validation, conditional validation based on property values, and pattern-based validation for another property.",
        "prediction": ":import jsonschema\nfrom jsonschema import FormatChecker\nfrom jsonschema.exceptions import ValidationError\n\n# Define a JSON schema with format-based date validation for a property, custom keyword validation, conditional validation based on property values, and pattern-based validation for another property\nschema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"dateOfBirth\": {\"type\": \"string\", \"format\": \"date\"},\n        \"isEmployee\": {\"type\": \"boolean\"},\n        \"employeeType\": {\"type\": \"string\"},\n        \"employeeID\": {\"type\": \"string\"},\n        \"email\": {\"type\": \"string\", \"pattern\": \"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\"}\n    },\n    \"custom_keywords\": {\n        \"validate_employee_type\": {\n            \"if\": {\n                \"properties\": {\"isEmployee\": {\"const\": True}, \"employeeType\": {\"pattern\": \"^(Manager|Employee)$\"}},\n                \"then\": {\"required\": [\"employeeID\"]}\n            }\n        },\n        \"pattern-based\": {\n            \"passport\": {\n                \"or\": {\n                    \"required\": [\"serialNumber\", \"isEmployee\"]\n                }\n            }\n        }\n    }\n}\n\n# Create a JSON object to validate\njson_data = {\n    \"dateOfBirth\": \"1990-05-20\",\n    \"isEmployee\": True,\n    \"employeeType\": \"Manager\",\n    \"employeeID\": \"M123\",\n    \"email\": \"manager@example.com\"\n}\n\n# Custom keyword validation function\ndef validate_custom_keywords(validator, custom_keywords, instance, schema):\n    if instance[\"isEmployee\"] and instance[\"employeeType\"] == \"Manager\" and not instance[\"employeeID\"].startswith(\"M\"):\n        yield jsonschema.exceptions.ValidationError(\"Invalid employee ID for Manager employees.\")\n\n# Add the custom keyword validator to the validator registry\njsonschema.validators.validator_for(schema).VALIDATORS[\"custom_keywords\"] = validate_custom_keywords\n\n# Validate the JSON data against the schema with format-based date validation, custom keywords, conditional validation, and pattern-based validation\ntry:\n    jsonschema.validate(instance=json_data, schema=schema, format_checker=jsonschema.FormatChecker())\n    print(\"JSON data is valid according to the schema with format-based date validation, custom keywords, conditional validation, and pattern-based validation.\")\nexcept jsonschema.exceptions.ValidationError as e:\n    print(f\"JSON data is not valid according to the schema. Error: {e}\")\n",
        "source": "This is some documentation of jsonschema API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# API Reference#\n\n\n## Submodules#\n\n* `jsonschema.validators`\n* `jsonschema.exceptions`\n* `jsonschema.protocols`\n\n## `jsonschema`#\n\n\nAn implementation of JSON Schema for Python.\n\n\nThe main functionality is provided by the validator classes for each of the\nsupported JSON Schema versions.\n\n\nMost commonly, `jsonschema.validators.validate` is the quickest way to simply\nvalidate a given instance under a schema, and will create\n\n==================\n Document 1 \n----------------\n# `jsonschema`#\n\n\nAn implementation of JSON Schema for Python.\n\n\nThe main functionality is provided by the validator classes for each of the\nsupported JSON Schema versions.\n\n\nMost commonly, `jsonschema.validators.validate` is the quickest way to simply\nvalidate a given instance under a schema, and will create a validator\nfor you.\n\n\n*class* jsonschema.FormatChecker(*formats: Iterable[str] | None = None*)[source]#\nA `format` property checker.\n\n\nJSON Schema does not mandate that the `format` property actually do any\nvalidation. If validation is desired however, instances of this class can\nbe hooked into validators to enable format validation.\n\n\n`FormatChecker` objects always return `True` when asked about\nformats that they do not know how to validate.\n\n\nTo add a check for a custom format use the `FormatChecker.checks`\ndecorator.\n\nParameters:\n**formats** \u2013 The known formats to validate. This argument can be used to\nlimit which formats will be used during validation.\n\n\ncheck(*instance: object*, *format: str*) \u2192 None[source]#\nCheck whether the instance conforms to the given format.\n\nParameters:\n* **instance** (*any primitive type*, i.e. str, number, bool) \u2013 The instance to check\n* **format** \u2013 The format that instance should conform to\n\nRaises:\n**FormatError** \u2013 if the instance does not conform to `format`\n\nchecks(*format: str*, *raises: Type[Exception] | T\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that utilizes 'jsonschema' to validate a JSON object against a schema with format-based date validation for a property, custom keyword validation, conditional validation based on property values, and pattern-based validation for another property.\n\n### Code:"
    },
    {
        "api": "jsonschema",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install jsonschema\nimport jsonschema\n\n# Define a JSON schema with custom keyword validation\nschema = {\n    \"type\": \"object\",\n    \"custom_keyword\": \"my_custom_value\"\n}\n\n# Create a JSON object to validate\njson_data = {\n    \"custom_keyword\": \"my_custom_value\"\n}\n\n# Custom keyword validation function\ndef validate_custom_keyword(validator, custom_keyword, instance, schema):\n    if instance != custom_keyword:\n        yield jsonschema.exceptions.ValidationError(f\"Value does not match the custom keyword: {custom_keyword}\")\n\n# Add the custom keyword validator to the validator registry\njsonschema.validators.validator_for(schema).VALIDATORS[\"custom_keyword\"] = validate_custom_keyword\n\n# Validate the JSON data against the schema with custom keyword validation\ntry:\n    jsonschema.validate(instance=json_data, schema=schema)\n    print(\"JSON data is valid according to the schema with custom keyword validation.\")\nexcept jsonschema.exceptions.ValidationError as e:\n    print(f\"JSON data is not valid according to the schema with custom keyword validation. Error: {e}\")\n",
        "instruction": "Create a Python program that utilizes 'jsonschema' to validate a JSON object against a schema that includes custom keyword validation.",
        "prediction": ":import jsonschema\nfrom jsonschema import FormatChecker\nfrom jsonschema.exceptions import ValidationError\nimport json\n\n# Define a JSON schema with custom keyword validation\nschema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\"type\": \"string\", \"minLength\": 3}\n    },\n    \"custom_keywords\": {\n        \"type\": \"array\",\n        \"items\": {\"type\": \"string\"}\n    }\n}\n\n# Create a JSON data object\njson_data = {\n    \"name\": \"John Doe\",\n    \"age\": 25\n}\n\n# Validate the JSON data against the schema with custom keyword validation\ntry:\n    jsonschema.validate(instance=json_data, schema=schema, format_checker=FormatChecker())\n    print(\"JSON data is valid according to the schema with custom keyword validation.\")\nexcept ValidationError as e:\n    print(f\"JSON data is not valid according to the schema with custom keyword validation. Error: {e}\")\n",
        "source": "This is some documentation of jsonschema API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# API Reference#\n\n\n## Submodules#\n\n* `jsonschema.validators`\n* `jsonschema.exceptions`\n* `jsonschema.protocols`\n\n## `jsonschema`#\n\n\nAn implementation of JSON Schema for Python.\n\n\nThe main functionality is provided by the validator classes for each of the\nsupported JSON Schema versions.\n\n\nMost commonly, `jsonschema.validators.validate` is the quickest way to simply\nvalidate a given instance under a schema, and will create\n\n==================\n Document 1 \n----------------\n# `jsonschema`#\n\n\nAn implementation of JSON Schema for Python.\n\n\nThe main functionality is provided by the validator classes for each of the\nsupported JSON Schema versions.\n\n\nMost commonly, `jsonschema.validators.validate` is the quickest way to simply\nvalidate a given instance under a schema, and will create a validator\nfor you.\n\n\n*class* jsonschema.FormatChecker(*formats: Iterable[str] | None = None*)[source]#\nA `format` property checker.\n\n\nJSON Schema does not mandate that the `format` property actually do any\nvalidation. If validation is desired however, instances of this class can\nbe hooked into validators to enable format validation.\n\n\n`FormatChecker` objects always return `True` when asked about\nformats that they do not know how to validate.\n\n\nTo add a check for a custom format use the `FormatChecker.checks`\ndecorator.\n\nParameters:\n**formats** \u2013 The known formats to validate. This argument can be used to\nlimit which formats will be used during validation.\n\n\ncheck(*instance: object*, *format: str*) \u2192 None[source]#\nCheck whether the instance conforms to the given format.\n\nParameters:\n* **instance** (*any primitive type*, i.e. str, number, bool) \u2013 The instance to check\n* **format** \u2013 The format that instance should conform to\n\nRaises:\n**FormatError** \u2013 if the instance does not conform to `format`\n\nchecks(*format: str*, *raises: Type[Exception] | T\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that utilizes 'jsonschema' to validate a JSON object against a schema that includes custom keyword validation.\n\n### Code:"
    },
    {
        "api": "langchain",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install langchain\n# Import necessary libraries\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.chains import LLMChain\n\n# Define the GPT-3.5 Turbo model\nllm_model = \"gpt-3.5-turbo\"\n\n# Initialize a ChatOpenAI instance with the specified model and temperature\nllm = ChatOpenAI(temperature=0.5, model=llm_model)\n\n# Define a template for chat prompts, where {country} will be replaced with the actual country name\nprompt = ChatPromptTemplate.from_template(\"What is the capital of {country}?\")\n\n# Initialize an LLMChain with the ChatOpenAI instance and the prompt template\nchain = LLMChain(llm=llm, prompt=prompt)\n\n# Specify the country you want to inquire about\ncountry = \"Germany\"\n\n# Run the chain to get a response about the capital of the specified country\nchain.run(country)",
        "instruction": "Create a Python program that utilizes the \"langchain\" API to interact with the GPT-3.5 Turbo model. Define a chat prompt template that includes a variable {country}. Use this template to inquire about the capital of a specific country, e.g., \"What is the capital of {country}?\" Replace {country} with \"Germany\" and retrieve the model's response. Save the response.",
        "prediction": ":import langchain as h\n\n# Define the chat prompt template\nchat_template = \"What is the capital of {country}?\"\n\n# Define the model\nmodel = h.InteractiveModel.load_from_torch(\n    \"https://s3.eu-central-1.angchain.io/turbo-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.5-3.",
        "source": "This is some documentation of langchain API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n Retrieval\n\nMany LLM applications require user-specific data that is not part of the model's training set.\nThe primary way of accomplishing this is through Retrieval Augmented Generation (RAG).\nIn this process, external data is *retrieved* and then passed to the LLM when doing the *generation* step.\n\nLangChain provides all the building blocks for RAG applications - from simple to complex.\nThis section of the documentation covers everything related to the *retrieval* step - e.g. the fetching of the data.\nAlthough this sounds simple, it can be subtly complex.\nThis encompasses several key modules.\n\n\n\n**Document loaders**\n\nLoad documents from many different sources.\nLangChain provides over 100 different document loaders as well as integrations with other major providers in the space,\nlike AirByte and Unstructured.\nWe provide integrations to load all types of documents (HTML, PDF, code) from all types of locations (private s3 buckets, public websites).\n\n**Document transformers**\n\nA key part of retrieval is fetching only the relevant parts of documents.\nThis involves several transformation steps in order to best prepare the documents for retrieval.\nOne of the primary ones here is splitting (or chunking) a large document into smaller chunks.\nLangChain provides several different algorithms for doing this, as well as logic optimized for specific document types (code, markdown, etc).\n\n**Text embedding models**\n\nAnother key part of retrieval has become creating embeddings for documents.\nEmbeddings capture the semantic meaning of the text, allowing you to quickly and\nefficiently find other pieces of text that are similar.\nLangChain provides integrations with over 25 different embedding providers and methods,\nfrom open-source to proprietary API,\nallowing you to choose the one best suited for your needs.\nLangChain provides a standard interface, allowing you to easily swap between models.\n\n**Vector stores**\n\nWith the rise of embeddings, there has emerged a need for databases to support efficient storage and searching of these embeddings.\nLangChain provides integrations with over 50 different vectorstores, from open-source local ones to cloud-hosted proprietary ones,\nallowing you to choose the one best suited for your needs.\nLang\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that utilizes the \"langchain\" API to interact with the GPT-3.5 Turbo model. Define a chat prompt template that includes a variable {country}. Use this template to inquire about the capital of a specific country, e.g., \"What is the capital of {country}?\" Replace {country} with \"Germany\" and retrieve the model's response. Save the response.\n\n### Code:"
    },
    {
        "api": "linear-operator",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install linear-operator\nimport torch\nfrom linear_operator import to_linear_operator\nfrom scipy.linalg import eigh\n\n# Define a square matrix M\nM = torch.Tensor([[4, 2], [2, 5]])\n\n# Compute the eigenvalues and eigenvectors\nM_op = to_linear_operator(M)\neigenvalues, eigenvectors = eigh(M_op.to_dense())\n\n# Print the eigenvalues and eigenvectors\nprint(\"Eigenvalues:\", eigenvalues)\nprint(\"Eigenvectors:\", eigenvectors)",
        "instruction": "Create a Python program that utilizes the 'linear-operator' API to compute the eigenvalues and eigenvectors of a given square matrix. The program should print the computed eigenvalues and eigenvectors.",
        "prediction": ":import linear-operator\nimport torch\n\n# Define a square matrix M\nM = torch.Tensor([[1, 2], [3, 4]])\n\n# Compute the eigenvalues and eigenvectors of M\neigenvalues, eigenvectors = M.symeig(eigenvectors=True)\n\n# Print the computed eigenvalues and eigenvectors\nprint(\"Eigenvalues:\", eigenvalues)\nprint(\"Eigenvectors:\", eigenvectors)",
        "source": "This is some documentation of linear-operator API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n What is a Linear Operator?\uf0c1\n\n\nA linear operator is a generalization of a matrix.\nIt is a linear function that is defined in by its application to a vector.\nThe most common linear operators are (potentially structured) matrices,\nwhere the function applying them to a vector are (potentially efficient)\nmatrix-vector multiplication routines.\n\n\nIn code, a `LinearOperator` is a class that\n\n\n1. specifies the tensor(s) needed to define the LinearOperator,\n2. specifies a `\\_matmul` function (how the LinearOperator is applied to a vector),\n3. specifies a `\\_size` function (how big is the LinearOperator if it is represented as a matrix, or batch of matrices), and\n4. specifies a `\\_transpose\\_nonbatch` function (the adjoint of the LinearOperator).\n5. (optionally) defines other functions (e.g. `logdet`, `eigh`, etc.) to accelerate computations for which efficient sturcture-exploiting routines exist.\n\n\nFor example:\n\n```\nclass DiagLinearOperator(linear\\_operator.LinearOperator):\n r\"\"\"\n A LinearOperator representing a diagonal matrix.\n \"\"\"\n    def \\_\\_init\\_\\_(self, diag):\n        # diag: the vector that defines the diagonal of the matrix\n        self.diag = diag\n\n    def \\_matmul(self, v):\n        return self.diag.unsqueeze(-1) \\* v\n\n    def \\_size(self):\n        return torch.Size([\\*self.diag.shape, self.diag.size(-1)])\n\n    def \\_transpose\\_nonbatch(self):\n        return self  # Diagonal matrices are symmetric\n\n    # this function is optional, but it will accelerate computation\n    def logdet(self):\n        return self.diag.log().sum(dim=-1)\n#...\n\nD = DiagLinearOperator(torch.tensor([1., 2., 3.])\n\n# Represents the matrix\n\n# [[1., 0., 0.],\n\n#\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that utilizes the 'linear-operator' API to compute the eigenvalues and eigenvectors of a given square matrix. The program should print the computed eigenvalues and eigenvectors.\n\n### Code:"
    },
    {
        "api": "linear-operator",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install linear-operator\nimport torch\nfrom linear_operator import to_linear_operator\nfrom scipy.linalg import sinm\n\n# Define a matrix M\nM = torch.Tensor([[1, 2], [3, 4]])\n\n# Compute the matrix sine\nM_op = to_linear_operator(M)\nmatrix_sine = sinm(M_op.to_dense())\n\n# Print the matrix sine\nprint(\"Matrix Sine:\", matrix_sine)",
        "instruction": "Create a Python program that utilizes the 'linear-operator' API to compute the matrix sine of a given matrix. The program should print the computed matrix sine.",
        "prediction": ":import linear-operator as linear_operator\n# Define a matrix M\nM = torch.Tensor([[1, 2], [3, 4]])\n\n# Compute the matrix sine of M\nM_op = DiagLinearOperator(M.diag())\nmatrix_sine = linear_operator.to_for(M_op.logdet())\n\n# Print the matrix sine\nprint(\"Matrix Sine:\", matrix_sine)",
        "source": "This is some documentation of linear-operator API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n What is a Linear Operator?\uf0c1\n\n\nA linear operator is a generalization of a matrix.\nIt is a linear function that is defined in by its application to a vector.\nThe most common linear operators are (potentially structured) matrices,\nwhere the function applying them to a vector are (potentially efficient)\nmatrix-vector multiplication routines.\n\n\nIn code, a `LinearOperator` is a class that\n\n\n1. specifies the tensor(s) needed to define the LinearOperator,\n2. specifies a `\\_matmul` function (how the LinearOperator is applied to a vector),\n3. specifies a `\\_size` function (how big is the LinearOperator if it is represented as a matrix, or batch of matrices), and\n4. specifies a `\\_transpose\\_nonbatch` function (the adjoint of the LinearOperator).\n5. (optionally) defines other functions (e.g. `logdet`, `eigh`, etc.) to accelerate computations for which efficient sturcture-exploiting routines exist.\n\n\nFor example:\n\n```\nclass DiagLinearOperator(linear\\_operator.LinearOperator):\n r\"\"\"\n A LinearOperator representing a diagonal matrix.\n \"\"\"\n    def \\_\\_init\\_\\_(self, diag):\n        # diag: the vector that defines the diagonal of the matrix\n        self.diag = diag\n\n    def \\_matmul(self, v):\n        return self.diag.unsqueeze(-1) \\* v\n\n    def \\_size(self):\n        return torch.Size([\\*self.diag.shape, self.diag.size(-1)])\n\n    def \\_transpose\\_nonbatch(self):\n        return self  # Diagonal matrices are symmetric\n\n    # this function is optional, but it will accelerate computation\n    def logdet(self):\n        return self.diag.log().sum(dim=-1)\n#...\n\nD = DiagLinearOperator(torch.tensor([1., 2., 3.])\n\n# Represents the matrix\n\n# [[1., 0., 0.],\n\n#\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that utilizes the 'linear-operator' API to compute the matrix sine of a given matrix. The program should print the computed matrix sine.\n\n### Code:"
    },
    {
        "api": "linear-operator",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install linear-operator\nimport torch\nfrom linear_operator import to_linear_operator\nfrom scipy.linalg import sqrtm\n\n# Define a matrix M\nM = torch.Tensor([[1, 2], [3, 4]])\n\n# Compute the matrix square root\nM_op = to_linear_operator(M)\nmatrix_sqrt = sqrtm(M_op.to_dense())\n\n# Print the matrix square root\nprint(\"Matrix Square Root:\", matrix_sqrt)",
        "instruction": "Create a Python program that utilizes the 'linear-operator' API to compute the matrix square root of a given matrix. The program should print the computed matrix square root.",
        "prediction": ":import linear-operator\nimport torch\nfrom linear_operator import to_linear_operator\n\n# Define a matrix M\nM = torch.Tensor([[1, 2], [3, 4]])\n\n# Compute the matrix square root of M\nM_op = to_linear_operator(M)\nmatrix_square_root = matrix_operator_to_torch_sqrt(M_op.to_dense())\n\n# Print the matrix square root of M\nprint(\"Matrix Square Root:\", matrix_square_root)",
        "source": "This is some documentation of linear-operator API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n linear\\_operator\n \n\n latest\n \n\n\nGetting Started\n\n\n* LinearOperator\n* Installation\n\n\nBasic Concepts\n\n\n* What is a Linear Operator?\n* Using LinearOperator Objects\n* Converting Between LinearOperators and torch.Tensor\n* Writing Your Own LinearOpeators\n\n\nLinear Operator Objects\n\n\n* LinearOperator\n* Data-Sparse LinearOperators\n* Composition/Decoration LinearOperators\n* Structured LinearOperators\n\n\nAdvanced Package Reference\n\n\n* Settings\n* Functions\n* Utilities\n\nlinear\\_operator\n\n\n* \n* LinearOperator\n* Edit on GitHub\n\n\n---\n\n\n# LinearOperator\uf0c1\n\n\nLinearOperator is a PyTorch package for abstracting away the linear algebra routines needed for structured matrices (or operators).\n\n\n**This package is in beta.**\nCurrently, most of the functionality only supports positive semi-definite and triangular matrices.\nPackage development TODOs:\n\n\n* Support PSD operators\n* Support triangular operators\n* Interface to specify structure (i.e. symmetric, triangular, PSD, etc.)\n* Add algebraic routines for symmetric operators\n* Add algebraic routines for generic square operators\n* Add algebraic routines for generic rectangular operators\n* Add sparse operators\n\n\n## Why LinearOperator\uf0c1\n\n\n### Why LinearOperator\uf0c1\n\n\nBefore describing what linear operators are and why they make a useful abstraction, it\u2019s easiest to see an example.\nLet\u2019s say you wanted to compute a matrix solve:\n\n\\[\\boldsymbol A^{-1} \\boldsymbol b.\\]\nIf you didn\u2019t know anything about the matrix \\(\\boldsymbol A\\), the simplest (and best) way to accomplish this in code is:\n\n```\n\n# A = torch.randn(1000, 1000)\n# b = torch.randn(1000)\ntorch.linalg.solve(A, b)  # computes A^{-1} b\n\n```\n\n\nWhile this is easy, the `solve` routine is \\(\\mathcal O(N^3)\\), which gets very slow as \\(N\\) grows large.\n\n\nHowever, let\u2019s imagine that we\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that utilizes the 'linear-operator' API to compute the matrix square root of a given matrix. The program should print the computed matrix square root.\n\n### Code:"
    },
    {
        "api": "llama-index",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install llama-cpp-python\n#!pip install llama-index html2text trafilatura\n\nfrom llama_index import GPTListIndex\nfrom llama_index import TrafilaturaWebReader\n\ndef scrape_and_index_robotics_info(url, *topics):\n    documents = TrafilaturaWebReader().load_data([url])\n    index = GPTListIndex.from_documents(documents)\n    engine = index.as_query_engine()\n\n    results = {}\n    for topic in topics:\n        results[topic] = engine.query(topic)\n\n    return results\n\nif __name__ == \"__main__\":\n    url = \"https://en.wikipedia.org/wiki/Robotics\"\n    topics = [\"History of robotics\", \"Types of robotics\"]\n    extracted_data = scrape_and_index_robotics_info(url, *topics)\n    \n    for topic, data in extracted_data.items():\n        print(f\"{topic}:\\n{data}\")",
        "instruction": "Develop a Python program using the 'llama-index' API to scrape and index data from 'https://en.wikipedia.org/wiki/Robotics.' Extract information about the history and types of robotics. Print the extracted information.",
        "prediction": ":import llama_index as llama\n\ndef scrape_and_index_info(url, *topics):\n    # Get history and types of robotics\n    indices.indices.tree = Playground(topics=topics)\n    documents = loader.load_data(pages=['Berlin'])\n    index = VectorStoreIndex.from_documents(documents)\n    index.compare(\"History and types of robotics\")\n    return \"Scraped and indexed information.\"\n\nif __name__ == \"__main__\":\n    if not scraper:\n        print(scrape_and_index_info)",
        "source": "This is some documentation of llama-index API:\n\n=====\n\n\n==================\n Document 0 \n----------------\nPlayground\uf0c1\n\n\nThe Playground module in LlamaIndex is a way to automatically test your data (i.e. documents) across a diverse combination of indices, models, embeddings, modes, etc. to decide which ones are best for your purposes. More options will continue to be added.\n\n\nFor each combination, you\u2019ll be able to compare the results for any query and compare the answers, latency, tokens used, and so on.\n\n\nYou may initialize a Playground with a list of pre-built indices, or initialize one from a list of Documents using the preset indices.\n\n\nA sample usage is given below.\n\n```\nfrom llama\\_index import download\\_loader\nfrom llama\\_index.indices.vector\\_store import VectorStoreIndex\nfrom llama\\_index.indices.tree.base import TreeIndex\nfrom llama\\_index.playground import Playground\n\n# load data \nWikipediaReader = download\\_loader(\"WikipediaReader\")\nloader = WikipediaReader()\ndocuments = loader.load\\_data(pages=['Berlin'])\n\n\n# define multiple index data structures (vector index, summary index)\nindices = [VectorStoreIndex(documents), TreeIndex(documents)]\n\n\n# initialize playground\nplayground = Playground(indices=indices)\n\n\n# playground compare\nplayground.compare(\"What is the population of Berlin?\")\n\n* Playground\n\n\n# Token Counting - Migration Guide\uf0c1\n\n\n\n# Cost Analysis\uf0c1\n\n\n# Playground\uf0c1\n\n==================\n Document 1 \n----------------\n Query and print response\nquery\\_engine = index.as\\_query\\_engine()\nresponse = query\\_engine.query(\"<query\\_text>\")\nprint(response)\n\n\nUsing this method, you can use any LLM. Maybe you have one running locally, or running on your own server. As long as the class is implemented and the generated tokens are returned, it should work out. Note that we need to use the prompt helper to customize the prompt sizes, since every model has a slightly different context length.\n\n\nThe decorator is optional, but provides observability via callbacks on the LLM calls.\n\n\nNote that you may have\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program using the 'llama-index' API to scrape and index data from 'https://en.wikipedia.org/wiki/Robotics.' Extract information about the history and types of robotics. Print the extracted information.\n\n### Code:"
    },
    {
        "api": "llama-index",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install llama-cpp-python\n#!pip install llama-index html2text trafilatura\n\nfrom llama_index import GPTListIndex\nfrom llama_index import TrafilaturaWebReader\n\ndef scrape_and_index_machine_vision_info(url, *topics):\n    documents = TrafilaturaWebReader().load_data([url])\n    index = GPTListIndex.from_documents(documents)\n    engine = index.as_query_engine()\n\n    results = {}\n    for topic in topics:\n        results[topic] = engine.query(topic)\n\n    return results\n\nif __name__ == \"__main__\":\n    url = \"https://en.wikipedia.org/wiki/Machine_vision\"\n    topics = [\"History of machine vision\", \"Applications of machine vision\"]\n    extracted_data = scrape_and_index_machine_vision_info(url, *topics)\n\n    for topic, data in extracted_data.items():\n        print(f\"{topic}:\\n{data}\")",
        "instruction": "Write a Python program using the 'llama-index' API to scrape and index data from 'https://en.wikipedia.org/wiki/Machine_vision.' Extract information about the history and applications of machine vision. Print the extracted information.",
        "prediction": ":import llama-index\nfrom llama_index import GPTListIndex\nfrom llama_index import TraverseGPT\n\ndef scrape_and_index_machine_vision_info(url, *topics):\n    documents = TraverseGPT(\"Machine Vision\")[url]\n    index = GPTListIndex.from_documents(documents)\n    engine = GPTListQueryEngine[index]\n    results = engine.query(*topics)\n    return results\n\nif __name__ == \"__main__\":\n    url = \"https://en.wikipedia.org/wiki/Machine_vision\"\n    topics = [\"History of machine vision\", \"Applications of machine vision\"]\n    extracted_data = scrape_and_index_machine_vision_info(url, *topics)\n\n    for data in extracted_data:\n        print(data)",
        "source": "This is some documentation of llama-index API:\n\n=====\n\n\n==================\n Document 0 \n----------------\nPlayground\uf0c1\n\n\nThe Playground module in LlamaIndex is a way to automatically test your data (i.e. documents) across a diverse combination of indices, models, embeddings, modes, etc. to decide which ones are best for your purposes. More options will continue to be added.\n\n\nFor each combination, you\u2019ll be able to compare the results for any query and compare the answers, latency, tokens used, and so on.\n\n\nYou may initialize a Playground with a list of pre-built indices, or initialize one from a list of Documents using the preset indices.\n\n\nA sample usage is given below.\n\n```\nfrom llama\\_index import download\\_loader\nfrom llama\\_index.indices.vector\\_store import VectorStoreIndex\nfrom llama\\_index.indices.tree.base import TreeIndex\nfrom llama\\_index.playground import Playground\n\n# load data \nWikipediaReader = download\\_loader(\"WikipediaReader\")\nloader = WikipediaReader()\ndocuments = loader.load\\_data(pages=['Berlin'])\n\n\n# define multiple index data structures (vector index, summary index)\nindices = [VectorStoreIndex(documents), TreeIndex(documents)]\n\n\n# initialize playground\nplayground = Playground(indices=indices)\n\n\n# playground compare\nplayground.compare(\"What is the population of Berlin?\")\n\n* Playground\n\n\n# Token Counting - Migration Guide\uf0c1\n\n\n\n# Cost Analysis\uf0c1\n\n\n# Playground\uf0c1\n\n==================\n Document 1 \n----------------\n LLM\uf0c1\n\n\nPicking the proper Large Language Model (LLM) is one of the first steps you need to consider when building any LLM application over your data.\n\n\nLLMs are a core component of LlamaIndex. They can be used as standalone modules or plugged into other core LlamaIndex modules (indices, retrievers, query engines). They are always used during the response synthesis step (e.g. after retrieval). Depending on the type of index being used, LLMs may also be used during index construction, insertion, and query traversal.\n\n\nLlamaIndex provides a unified interface for defining LLM modules, whether it\u2019s\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nWrite a Python program using the 'llama-index' API to scrape and index data from 'https://en.wikipedia.org/wiki/Machine_vision.' Extract information about the history and applications of machine vision. Print the extracted information.\n\n### Code:"
    },
    {
        "api": "mlflow",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install mlflow\nimport mlflow\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\n# Generate a synthetic dataset\nX = np.random.rand(300, 2)\n\n# Start an MLflow run\nwith mlflow.start_run() as run:\n    # Train a K-means clustering model\n    kmeans = KMeans(n_clusters=3, random_state=42)\n    kmeans.fit(X)\n    \n    # Log hyperparameters\n    mlflow.log_params(kmeans.get_params())\n    \n    # Log the trained model\n    mlflow.sklearn.log_model(kmeans, \"kmeans_model\")\n",
        "instruction": "Develop a program that uses the MLflow API to train a K-means clustering model on a synthetic dataset. Log the model and its hyperparameters.",
        "prediction": ":import mlflow\nimport mlflow\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import accuracy_score\n\n# Generate a synthetic dataset\nX, labels = make_blobs(n_samples=300, centers=4, random_state=42)\n\n# Start an MLflow run\nwith mlflow.start_run() as run:\n    # Train a K-means clustering model\n    kmeans = KMeans(n_clusters=4, random_state=42)\n    kmeans.fit(X)\n    \n    # Log hyperparameters\n    mlflow.log_params(kmeans.get_params())\n    \n    # Make predictions on the dataset\n    y_pred = kmeans.predict(X)\n    \n    # Calculate and log accuracy as a metric\n    accuracy = accuracy_score(labels, y_pred)\n    mlflow.log_metric(\"accuracy\", accuracy)\n    \n    # Log the trained model\n    mlflow.sklearn.log_model(kmeans, \"kmeans_model\")",
        "source": "This is some documentation of mlflow API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n## Workflows\n\n\n`save\\_model()` and `log\\_model()` support the following workflows:\n\n\n1. Programmatically defining a new MLflow model, including its attributes and artifacts.\n\n\nGiven a set of artifact URIs, `save\\_model()` and `log\\_model()` can\nautomatically download artifacts from their URIs and create an MLflow model directory.\n\n\nIn this case, you must define a Python class which inherits from `PythonModel`,\ndefining `predict()` and, optionally, `load\\_context()`. An instance of this class is\nspecified via the `python\\_model` parameter; it is automatically serialized and deserialized\nas a Python class, including all of its attributes.\n2. Interpreting pre-existing data as an MLflow model.\n\n\nIf you already have a directory containing model data, `save\\_model()` and\n`log\\_model()` can import the data as an MLflow model. The `data\\_path` parameter\nspecifies the local filesystem path to the directory containing model data.\n\n\nIn this case, you must provide a Python module, called a loader module. The\nloader module defines a `\\_load\\_pyfunc()` method that performs the following tasks:\n\n\n\t* Load data from the specified `data\\_path`. For example, this process may include\n\tdeserializing pickled Python objects or models or parsing CSV files.\n\t* Construct and return a pyfunc-compatible model wrapper. As in the first\n\tuse case, this wrapper must define a `predict()` method that is used to evaluate\n\tqueries. `predict()` must adhere to the Inference API.The `loader\\_module` parameter specifies the name of your loader module.\n\n\nFor an example loader module implementation, refer to the loader module\nimplementation in mlflow.sklearn.\n\n### Which workflow is right for my use case?\n\n\nWe consider the first workflow to be more user-friendly and generally recommend it for the\nfollowing reasons:\n\n\n* It automatically resolves and collects specified model artifacts.\n* It automatically serializes and deserializes the `python\\_model` instance\n\n==================\n Document 1 \n----------------\n mlflow.\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a program that uses the MLflow API to train a K-means clustering model on a synthetic dataset. Log the model and its hyperparameters.\n\n### Code:"
    },
    {
        "api": "mlflow",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install mlflow\nimport mlflow\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n# Generate a synthetic binary classification dataset\nX = np.random.rand(100, 10)\ny = np.random.randint(2, size=100)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Start an MLflow run\nwith mlflow.start_run() as run:\n    # Train a logistic regression classifier\n    logreg = LogisticRegression(random_state=42)\n    logreg.fit(X_train, y_train)\n    \n    # Log hyperparameters\n    mlflow.log_params(logreg.get_params())\n    \n    # Log the trained model\n    mlflow.sklearn.log_model(logreg, \"logistic_regression_model\")\n",
        "instruction": "Develop a program that uses the MLflow API to train a logistic regression classifier on a synthetic binary classification dataset. Log the model and its hyperparameters.",
        "prediction": ":import mlflow\nimport mlflow\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Generate a synthetic binary classification dataset\nX = np.random.rand(100, 10)\ny = np.random.randint(2, size=100)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Start an MLflow run\nwith mlflow.start_run() as run:\n    # Train a logistic regression classifier\n    logreg = LogisticRegression(random_state=42)\n    logreg.fit(X_train, y_train)\n    \n    # Log hyperparameters\n    mlflow.log_params(logreg.get_params())\n    \n    # Make predictions on the test set\n    y_pred = logreg.predict(X_test)\n    \n    # Calculate and log accuracy as a metric\n    accuracy = accuracy_score(y_test, y_pred)\n    mlflow.log_metric(\"accuracy\", accuracy)\n    \n    # Log the trained model\n    mlflow.sklearn.log_model(logreg, \"logistic_regression_model\")\n",
        "source": "This is some documentation of mlflow API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n## Workflows\n\n\n`save\\_model()` and `log\\_model()` support the following workflows:\n\n\n1. Programmatically defining a new MLflow model, including its attributes and artifacts.\n\n\nGiven a set of artifact URIs, `save\\_model()` and `log\\_model()` can\nautomatically download artifacts from their URIs and create an MLflow model directory.\n\n\nIn this case, you must define a Python class which inherits from `PythonModel`,\ndefining `predict()` and, optionally, `load\\_context()`. An instance of this class is\nspecified via the `python\\_model` parameter; it is automatically serialized and deserialized\nas a Python class, including all of its attributes.\n2. Interpreting pre-existing data as an MLflow model.\n\n\nIf you already have a directory containing model data, `save\\_model()` and\n`log\\_model()` can import the data as an MLflow model. The `data\\_path` parameter\nspecifies the local filesystem path to the directory containing model data.\n\n\nIn this case, you must provide a Python module, called a loader module. The\nloader module defines a `\\_load\\_pyfunc()` method that performs the following tasks:\n\n\n\t* Load data from the specified `data\\_path`. For example, this process may include\n\tdeserializing pickled Python objects or models or parsing CSV files.\n\t* Construct and return a pyfunc-compatible model wrapper. As in the first\n\tuse case, this wrapper must define a `predict()` method that is used to evaluate\n\tqueries. `predict()` must adhere to the Inference API.The `loader\\_module` parameter specifies the name of your loader module.\n\n\nFor an example loader module implementation, refer to the loader module\nimplementation in mlflow.sklearn.\n\n### Which workflow is right for my use case?\n\n\nWe consider the first workflow to be more user-friendly and generally recommend it for the\nfollowing reasons:\n\n\n* It automatically resolves and collects specified model artifacts.\n* It automatically serializes and deserializes the `python\\_model` instance\n\n==================\n Document 1 \n----------------\n mlflow.\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a program that uses the MLflow API to train a logistic regression classifier on a synthetic binary classification dataset. Log the model and its hyperparameters.\n\n### Code:"
    },
    {
        "api": "mlflow",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install mlflow\nimport mlflow\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load the Iris dataset\ndata = load_iris()\nX, y = data.data, data.target\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Start an MLflow run\nwith mlflow.start_run() as run:\n    # Train a Decision Tree classifier\n    clf = DecisionTreeClassifier()\n    clf.fit(X_train, y_train)\n    \n    # Log hyperparameters\n    mlflow.log_param(\"criterion\", clf.criterion)\n    mlflow.log_param(\"max_depth\", clf.max_depth)\n    \n    # Make predictions on the test set\n    y_pred = clf.predict(X_test)\n    \n    # Calculate and log accuracy as a metric\n    accuracy = accuracy_score(y_test, y_pred)\n    mlflow.log_metric(\"accuracy\", accuracy)\n    \n    # Save the model as an artifact\n    mlflow.sklearn.log_model(clf, \"decision_tree_model\")",
        "instruction": "Create a program that uses the MLflow API to train a simple decision tree classifier on the Iris dataset. Log the model, its hyperparameters, and accuracy as a metric. Save the model as an artifact.",
        "prediction": ":mlflow.autolog()\nimport mlflow\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\n\n# Load the Iris dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Start an MLflow run\nwith mlflow.start_run() as run:\n    # Split the dataset into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Start an MLflow run\n    with mlflow.start_run() as run:\n    \n        # Train a Decision Tree classifier\n        clf = DecisionTreeClassifier()\n        clf.fit(X_train, y_train)\n        \n        # Log hyperparameters\n        mlflow.log_params(clf.get_params())\n        \n        # Make predictions on the test set\n        y_pred = clf.predict(X_test)\n        \n        # Calculate and log accuracy as a metric\n        acc = accuracy_score(y_test, y_pred)\n        mlflow.log_metric(\"accuracy\", acc)\n        \n        # Log the trained model\n        mlflow.sklearn.save_model(clf, \"decision_tree_classifier_model\")\n",
        "source": "This is some documentation of mlflow API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n mlflow.sklearn\n\n\nThe `mlflow.sklearn` module provides an API for logging and loading scikit-learn models. This\nmodule exports scikit-learn models with the following flavors:\n\nPython (native) pickle formatThis is the main flavor that can be loaded back into scikit-learn.\n\n`mlflow.pyfunc`Produced for use by generic pyfunc-based deployment tools and batch inference.\nNOTE: The mlflow.pyfunc flavor is only added for scikit-learn models that define predict(),\nsince predict() is required for pyfunc model inference.\n\n\n`mlflow.sklearn.``autolog`(*log\\_input\\_examples=False*, *log\\_model\\_signatures=True*, *log\\_models=True*, *log\\_datasets=True*, *disable=False*, *exclusive=False*, *disable\\_for\\_unsupported\\_versions=False*, *silent=False*, *max\\_tuning\\_runs=5*, *log\\_post\\_training\\_metrics=True*, *serialization\\_format='cloudpickle'*, *registered\\_model\\_name=None*, *pos\\_label=None*, *extra\\_tags=None*)[source] \n\n\nAutologging is known to be compatible with the following package versions: `0.22.1` <= `scikit-learn` <= `1.3.0`. Autologging may not succeed when used with package versions outside of this range.\n\nEnables (or disables) and configures autologging for scikit-learn estimators.\n\n**When is autologging performed?**Autologging is performed when you call:\n\n\n* `estimator.fit()`\n* `estimator.fit\\_predict()`\n* `estimator.fit\\_transform()`\n\n**Logged information**\n**Parameters*** Parameters obtained by `estimator.get\\_params(deep=True)`. Note that `get\\_params`\nis called with `deep=True`. This means when you fit a meta estimator that chains\na series of estimators, the parameters of these child estimators are\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a program that uses the MLflow API to train a simple decision tree classifier on the Iris dataset. Log the model, its hyperparameters, and accuracy as a metric. Save the model as an artifact.\n\n### Code:"
    },
    {
        "api": "more-itertools",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install more-itertools\nfrom more_itertools import consecutive_groups\n\nstrings = [\"123\", \"456\", \"789\", \"abc\", \"def\", \"ghi\", \"123456789\"]\npartitioned_sublists = list(consecutive_groups(strings))\nfiltered_sublists = [sublist for sublist in partitioned_sublists if all(word.isdigit() for word in sublist)]\nlengths = [len(sublist) for sublist in filtered_sublists]\n\nprint(\"Filtered Sublists:\")\nfor sublist in filtered_sublists:\n    print(list(sublist))\n\nprint(\"Lengths:\")\nprint(lengths)",
        "instruction": "Create a Python program using the 'more-itertools' API to demonstrate the grouping and filtering of elements in a list based on a condition. The program should take a list of strings, group them into consecutive sublists using the 'consecutive_groups' function from 'more-itertools', and filter out sublists that contain only strings with all numeric characters. Finally, print the filtered sublists and calculate the length of each sublist and print the lengths.",
        "prediction": ":import more_itertools\nfrom more_itertools import consecutive_groups\n\nstrings = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\", \"fig\", \"grape\"]\npartitioned_sublists = list(consecutive_groups(strings))\nfiltered_sublists = [sublist for sublist in partitioned_sublists if all(word.isdigit for word in sublist)]\nlengths = [len(sublist) for sublist in filtered_sublists]\n\nprint(\"Filtered Sublists:\")\nfor sublist in filtered_sublists:\n    print(list(sublist))\n\nprint(\"Lengths:\")\nprint(lengths)",
        "source": "This is some documentation of more-itertools API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n# Summarizing\u00b6\n\n\nThese tools return summarized or aggregated data from an iterable.\n\n\n`more_itertools.``ilen`(*iterable*)[source]\u00b6\nReturn the number of items in *iterable*.\n\n```\n>>> ilen(x for x in range(1000000) if x % 3 == 0)\n333334\n\n\nThis consumes the iterable, so handle with care.\n\n`more_itertools.``unique_to_each`(*\\*iterables*)[source]\u00b6\nReturn the elements from each of the input iterables that aren\u2019t in the\nother input iterables.\n\n\nFor example, suppose you have a set of packages, each with a set of\ndependencies:\n\n```\n{'pkg\\_1': {'A', 'B'}, 'pkg\\_2': {'B', 'C'}, 'pkg\\_3': {'B', 'D'}}\n\n\nIf you remove one package, which dependencies can also be removed?\n\n\nIf `pkg\\_1` is removed, then `A` is no longer necessary - it is not\nassociated with `pkg\\_2` or `pkg\\_3`. Similarly, `C` is only needed for\n`pkg\\_2`, and `D` is only needed for `pkg\\_3`:\n\n```\n>>> unique\\_to\\_each({'A', 'B'}, {'B', 'C'}, {'B', 'D'})\n[['A'], ['C'], ['D']]\n\n\nIf there are duplicates in one input iterable that aren\u2019t in the others\nthey will be duplicated in the output. Input order is preserved:\n\n```\n>>> unique\\_to\\_each(\"mississippi\", \"missouri\")\n[['p', 'p'], ['o', 'u', 'r']]\n\n\nIt is assumed that the elements of each iterable are hashable.\n\n`more_itertools.``sample`(*iterable*, *k=1*, *weights=None*)[source]\u00b6\nReturn a *k*-length list of elements chosen (without replacement)\nfrom the *iterable*. Like `random.sample()\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'more-itertools' API to demonstrate the grouping and filtering of elements in a list based on a condition. The program should take a list of strings, group them into consecutive sublists using the 'consecutive_groups' function from 'more-itertools', and filter out sublists that contain only strings with all numeric characters. Finally, print the filtered sublists and calculate the length of each sublist and print the lengths.\n\n### Code:"
    },
    {
        "api": "more-itertools",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install more-itertools\nfrom more_itertools import consecutive_groups\n\nstrings = [\"APPLE\", \"BANANA\", \"CHERRY\", \"DATE\", \"ELDERBERRY\", \"FIG\", \"GRAPE\"]\npartitioned_sublists = list(consecutive_groups(strings))\nfiltered_sublists = [sublist for sublist in partitioned_sublists if all(word.isupper() for word in sublist)]\nlengths = [len(sublist) for sublist in filtered_sublists]\n\nprint(\"Filtered Sublists:\")\nfor sublist in filtered_sublists:\n    print(list(sublist))\n\nprint(\"Lengths:\")\nprint(lengths)",
        "instruction": "Create a Python program using the 'more-itertools' API to demonstrate the grouping and filtering of elements in a list based on a condition. The program should take a list of strings, group them into consecutive sublists using the 'consecutive_groups' function from 'more-itertools', and filter out sublists that contain only strings with all uppercase letters. Finally, print the filtered sublists and calculate the length of each sublist and print the lengths.",
        "prediction": ":import more_itertools\nfrom more_itertools import consecutive_groups\n\nstrings = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\", \"fig\", \"grape\"]\npartitioned_sublists = list(consecutive_groups(strings))\nfiltered_sublists = [sublist for sublist in partitioned_sublists if all(word.isupper() for word in sublist)]\nlengths = [len(sublist) for sublist in filtered_sublists]\n\nprint(\"Filtered Sublists:\")\nfor sublist in filtered_sublists:\n    print(list(sublist))\n\nprint(\"Lengths:\")\nprint(lengths)",
        "source": "This is some documentation of more-itertools API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n# Summarizing\u00b6\n\n\nThese tools return summarized or aggregated data from an iterable.\n\n\n`more_itertools.``ilen`(*iterable*)[source]\u00b6\nReturn the number of items in *iterable*.\n\n```\n>>> ilen(x for x in range(1000000) if x % 3 == 0)\n333334\n\n\nThis consumes the iterable, so handle with care.\n\n`more_itertools.``unique_to_each`(*\\*iterables*)[source]\u00b6\nReturn the elements from each of the input iterables that aren\u2019t in the\nother input iterables.\n\n\nFor example, suppose you have a set of packages, each with a set of\ndependencies:\n\n```\n{'pkg\\_1': {'A', 'B'}, 'pkg\\_2': {'B', 'C'}, 'pkg\\_3': {'B', 'D'}}\n\n\nIf you remove one package, which dependencies can also be removed?\n\n\nIf `pkg\\_1` is removed, then `A` is no longer necessary - it is not\nassociated with `pkg\\_2` or `pkg\\_3`. Similarly, `C` is only needed for\n`pkg\\_2`, and `D` is only needed for `pkg\\_3`:\n\n```\n>>> unique\\_to\\_each({'A', 'B'}, {'B', 'C'}, {'B', 'D'})\n[['A'], ['C'], ['D']]\n\n\nIf there are duplicates in one input iterable that aren\u2019t in the others\nthey will be duplicated in the output. Input order is preserved:\n\n```\n>>> unique\\_to\\_each(\"mississippi\", \"missouri\")\n[['p', 'p'], ['o', 'u', 'r']]\n\n\nIt is assumed that the elements of each iterable are hashable.\n\n`more_itertools.``sample`(*iterable*, *k=1*, *weights=None*)[source]\u00b6\nReturn a *k*-length list of elements chosen (without replacement)\nfrom the *iterable*. Like `random.sample()\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'more-itertools' API to demonstrate the grouping and filtering of elements in a list based on a condition. The program should take a list of strings, group them into consecutive sublists using the 'consecutive_groups' function from 'more-itertools', and filter out sublists that contain only strings with all uppercase letters. Finally, print the filtered sublists and calculate the length of each sublist and print the lengths.\n\n### Code:"
    },
    {
        "api": "numpy",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install numpy\nimport numpy as np\n\n# Generate a random matrix of size (3x3)\noriginal_matrix = np.random.rand(3, 3)\nprint(\"Original Matrix: \", original_matrix)\n\n# Compute the matrix's transpose\ntransposed_matrix = np.transpose(original_matrix)\nprint(\"Transposed Matrix: \", transposed_matrix)\n\n# Calculate the matrix product\nmatrix_product = np.dot(original_matrix, transposed_matrix)\nprint(\"Matrix Product: \", matrix_product)",
        "instruction": "Create a Python program that uses the 'numpy' API to perform matrix operations, including generating a random matrix, computing its transpose, and calculating the matrix product.",
        "prediction": ":import numpy as np\n\n# Generate a random matrix of size (3x3)\noriginal_matrix = np.random.rand(3, 3)\nprint(\"Original Matrix: \", original_matrix)\n\n# Compute the matrix product of the matrix 'A' and 'B'\ntranspose = np.transpose(original_matrix)\nmatrix_product = np.matmul(original_matrix, transpose)\nprint(\"Transpose: \", transpose)\nprint(\"Matrix Product: \", matrix_product)",
        "source": "This is some documentation of numpy API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n Linear algebra (`numpy.linalg`)#\n\n\nThe NumPy linear algebra functions rely on BLAS and LAPACK to provide efficient\nlow level implementations of standard linear algebra algorithms. Those\nlibraries may be provided by NumPy itself using C versions of a subset of their\nreference implementations but, when possible, highly optimized libraries that\ntake advantage of specialized processor functionality are preferred. Examples\nof such libraries are OpenBLAS, MKL (TM), and ATLAS. Because those libraries\nare multithreaded and processor dependent, environmental variables and external\npackages such as threadpoolctl may be needed to control the number of threads\nor specify the processor architecture.\n\n\nThe SciPy library also contains a `linalg` submodule, and there is\noverlap in the functionality provided by the SciPy and NumPy submodules. SciPy\ncontains functions not found in `numpy.linalg`, such as functions related to\nLU decomposition and the Schur decomposition, multiple ways of calculating the\npseudoinverse, and matrix transcendentals such as the matrix logarithm. Some\nfunctions that exist in both have augmented functionality in `scipy.linalg`.\nFor example, `scipy.linalg.eig` can take a second matrix argument for solving\ngeneralized eigenvalue problems. Some functions in NumPy, however, have more\nflexible broadcasting options. For example, `numpy.linalg.solve` can handle\n\u201cstacked\u201d arrays, while `scipy.linalg.solve` accepts only a single square\narray as its first argument.\n\n\nThe term *matrix* as it is used on this page indicates a 2d `numpy.array`\nobject, and *not* a `numpy.matrix` object. The latter is no longer\nrecommended, even for linear algebra. See\nthe matrix object documentation for\nmore information.\n\n\n## The `@` operator#\n\n\nIntroduced in NumPy 1.10.0, the `@` operator is preferable to\nother methods when computing the matrix product between 2d arrays. The\n`numpy.matmul` function implements the `@` operator.\n\n\n## Matrix and vector products#\n\n\n|  |  |\n| ---\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that uses the 'numpy' API to perform matrix operations, including generating a random matrix, computing its transpose, and calculating the matrix product.\n\n### Code:"
    },
    {
        "api": "numpy",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install numpy\nimport numpy as np\nimport csv\n\n# Read the CSV file\ndata = []\nwith open('data.csv', 'r') as file:\n    reader = csv.reader(file)\n    for row in reader:\n        data.append(row)\n\n# Convert the data to a numpy array\ndata_array = np.array(data, dtype=float)\n\n# Calculate the mean of each column\nmean = np.mean(data_array, axis=0)\nprint(\"Mean: \", mean)\n\n# Calculate the median of each column\nmedian = np.median(data_array, axis=0)\nprint(\"Median: \", median)\n\n# Calculate the standard deviation of each column\nstd_dev = np.std(data_array, axis=0)\nprint(\"Standard Deviation: \", std_dev)",
        "instruction": "Create a Python program that uses the 'numpy' API to perform statistical operations on a given dataset. The program should read a CSV file containing numerical data, calculate the mean, median, and standard deviation of each column, and display the results.",
        "prediction": ":import numpy as np\n\n# Read the CSV file\ndata = np.genfromtxt('numerical data.csv', delimiter=',')\n\n# Calculate the mean, median, and standard deviation\nmeans = np.mean(data, axis=0)\nmedians = np.median(data, axis=0)\nstddevs = np.std(data, axis=0)\n\n# Display the results\nprint(\"Means:\", means)\nprint(\"Medians:\", medians)\nprint(\"Standard Deviations:\", stddevs)",
        "source": "This is some documentation of numpy API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n# Background#\n\n\nThe API exposed by NumPy for third-party extensions has grown over\nyears of releases, and has allowed programmers to directly access\nNumPy functionality from C. This API can be best described as\n\u201corganic\u201d. It has emerged from multiple competing desires and from\nmultiple points of view over the years, strongly influenced by the\ndesire to make it easy for users to move to NumPy from Numeric and\nNumarray. The core API originated with Numeric in 1995 and there are\npatterns such as the heavy use of macros written to mimic Python\u2019s\nC-API as well as account for compiler technology of the late 90\u2019s.\nThere is also only a small group of volunteers who have had very little\ntime to spend on improving this API.\n\n\nThere is an ongoing effort to improve the API.\nIt is important in this effort\nto ensure that code that compiles for NumPy 1.X continues to\ncompile for NumPy 1.X. At the same time, certain API\u2019s will be marked\nas deprecated so that future-looking code can avoid these API\u2019s and\nfollow better practices.\n\n\nAnother important role played by deprecation markings in the C API is to move\ntowards hiding internal details of the NumPy implementation. For those\nneeding direct, easy, access to the data of ndarrays, this will not\nremove this ability. Rather, there are many potential performance\noptimizations which require changing the implementation details, and\nNumPy developers have been unable to try them because of the high\nvalue of preserving ABI compatibility. By deprecating this direct\naccess, we will in the future be able to improve NumPy\u2019s performance\nin ways we cannot presently.\n\n## Deprecation Mechanism NPY\\_NO\\_DEPRECATED\\_API#\n\n\nIn C, there is no equivalent to the deprecation warnings that Python\nsupports. One way to do deprecations is to flag them in the\ndocumentation and release notes, then remove or change the deprecated\nfeatures in a future major version\n\n==================\n Document 1 \n----------------\n# Advice for using NumPy on untrusted data#\n\n\nA user who can freely execute NumPy (or Python) functions must be considered\nto have the same privilege as\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that uses the 'numpy' API to perform statistical operations on a given dataset. The program should read a CSV file containing numerical data, calculate the mean, median, and standard deviation of each column, and display the results.\n\n### Code:"
    },
    {
        "api": "optuna",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install optuna scikit-learn\nimport optuna\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import f1_score\n\n# Generate a custom dataset\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n\n# Define the machine learning task: Random Forest Classifier\ndef objective(trial):\n    # Define hyperparameters to search\n    n_estimators = trial.suggest_int(\"n_estimators\", 50, 200)\n    max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n\n    # Create and evaluate the random forest classifier with the chosen hyperparameters\n    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n    f1_scores = cross_val_score(clf, X, y, cv=5, scoring='f1')\n\n    return np.mean(f1_scores)\n\n# Create a study and optimize hyperparameters\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=50)\n\n    # Get the best hyperparameters\n    best_params = study.best_params\n    n_estimators = best_params[\"n_estimators\"]\n    max_depth = best_params[\"max_depth\"]\n\n    print(\"Best Hyperparameters:\", best_params)\n\n    # Create and evaluate the random forest classifier with the best hyperparameters\n    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n    f1_scores = cross_val_score(clf, X, y, cv=5, scoring='f1')\n    mean_f1_score = np.mean(f1_scores)\n\n    print(\"Cross-Validated F1 Score with Best Hyperparameters:\", mean_f1_score)\n",
        "instruction": "Create a Python program using the 'optuna' API to optimize the hyperparameters of a random forest classifier. The program should search for the best combination of 'n_estimators' and 'max_depth' hyperparameters for a random forest classifier on a custom dataset and print the cross-validated F1 score.",
        "prediction": ":import optuna\nimport optuna\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\n\n# Generate a custom dataset\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n\n# Define the machine learning task: Random Forest Classifier\ndef objective(trial):\n    # Define hyperparameters to search\n    n_estimators = trial.suggest_int(\"n_estimators\", 50, 200)\n    max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n\n    # Create and evaluate the random forest classifier with the chosen hyperparameters\n    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n    f1_scores = cross_val_score(clf, X, y, cv=5, scoring='f1')\n\n    return np.mean(f1_scores)\n\n# Create a study and optimize hyperparameters\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=50)\n\n    # Get the best hyperparameters\n    best_params = study.best_params\n    n_estimators = best_params[\"n_estimators\"]\n    max_depth = best_params[\"max_depth\"]\n\n    print(\"Best Hyperparameters:\", best_params)\n\n    # Create and evaluate the random forest classifier with the best hyperparameters\n    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n    f1_scores = cross_val_score(clf, X, y, cv=5, scoring='f1')\n    mean_f1_score = np.mean(f1_scores)\n\n    print(\"Cross-Validated F1 Score with Best Hyperparameters:\", mean_f1_score)\n",
        "source": "This is some documentation of optuna API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n\n# API Reference\uf0c1\n\n* optuna\n* optuna.artifacts\n* optuna.cli\n* optuna.distributions\n* optuna.exceptions\n* optuna.importance\n* optuna.integration\n* optuna.logging\n* optuna.pruners\n* optuna.samplers\n* optuna.search\\_space\n* optuna.storages\n* optuna.study\n* optuna.terminator\n* optuna.trial\n* optuna.visualization\n\n\n# optuna\uf0c1\n\n\nThe `optuna` module is primarily used as an alias for basic Optuna functionality coded in other modules. Currently, two modules are aliased: (1) from `optuna.study`, functions regarding the Study lifecycle, and (2) from `optuna.exceptions`, the TrialPruned Exception raised when\n\n==================\n Document 1 \n----------------\n optuna.integration\uf0c1\n\n\nThe `integration` module contains classes used to integrate Optuna with external machine learning frameworks.\n\nNote\n\n\nOptuna\u2019s integration modules for third-party libraries have started migrating from Optuna itself to a package called\noptuna-integration. Please check the repository and\nthe documentation.\n\nFor most of the ML frameworks supported by Optuna, the corresponding Optuna integration class serves only to implement a callback object and functions, compliant with the framework\u2019s specific callback API, to be called with each intermediate step in the model training. The functionality implemented in these callbacks across the different ML frameworks includes:\n\n\n1. Reporting intermediate model scores back to the Optuna trial using `optuna.trial.Trial.report()`,\n2. According to the results of `optuna.trial.Trial.should\\_prune()`, pruning the current model by raising `optuna.TrialPruned()`, and\n3. Reporting intermediate Optuna data such as the current trial number back to the framework, as done in `MLflowCallback`.\n\n\nFor scikit-learn, an integrated `OptunaSearchCV` estimator is available that combines scikit-learn BaseEstimator functionality with access to a class-level `Study` object.\n\n## BoTorch\uf0c1\n\n\n|\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'optuna' API to optimize the hyperparameters of a random forest classifier. The program should search for the best combination of 'n_estimators' and 'max_depth' hyperparameters for a random forest classifier on a custom dataset and print the cross-validated F1 score.\n\n### Code:"
    },
    {
        "api": "optuna",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install optuna scikit-learn\nimport optuna\nimport numpy as np\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.datasets import make_classification\n\n# Generate a custom dataset\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n\n# Define the machine learning task: Linear Discriminant Analysis (LDA)\ndef objective(trial):\n    # Define hyperparameters to search\n    n_components = trial.suggest_int(\"n_components\", 1, 20)\n    solver = trial.suggest_categorical(\"solver\", [\"svd\", \"lsqr\", \"eigen\"])\n\n    # Create and evaluate the LDA model with the chosen hyperparameters\n    lda = LinearDiscriminantAnalysis(n_components=n_components, solver=solver)\n    lda.fit(X, y)\n    explained_variance = np.sum(lda.explained_variance_ratio_)\n\n    return explained_variance\n\n# Create a study and optimize hyperparameters\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=50)\n\n    # Get the best hyperparameters\n    best_params = study.best_params\n    n_components = best_params[\"n_components\"]\n    solver = best_params[\"solver\"]\n\n    print(\"Best Hyperparameters:\", best_params)\n\n    # Create and evaluate the LDA model with the best hyperparameters\n    lda = LinearDiscriminantAnalysis(n_components=n_components, solver=solver)\n    lda.fit(X, y)\n    explained_variance = np.sum(lda.explained_variance_ratio_\n\n)\n\n    print(\"Explained Variance Ratio with Best Hyperparameters:\", explained_variance)\n",
        "instruction": "Create a Python program using the 'optuna' API to optimize the hyperparameters of a linear discriminant analysis (LDA) model. The program should search for the best combination of 'n_components' and 'solver' hyperparameters for an LDA model on a custom dataset and print the explained variance ratio.",
        "prediction": ":import optuna\nimport optuna\nimport numpy as np\nimport skopt\nimport skopt.space\nfrom skopt import gp",
        "source": "This is some documentation of optuna API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n\n# API Reference\uf0c1\n\n* optuna\n* optuna.artifacts\n* optuna.cli\n* optuna.distributions\n* optuna.exceptions\n* optuna.importance\n* optuna.integration\n* optuna.logging\n* optuna.pruners\n* optuna.samplers\n* optuna.search\\_space\n* optuna.storages\n* optuna.study\n* optuna.terminator\n* optuna.trial\n* optuna.visualization\n\n\n# optuna\uf0c1\n\n\nThe `optuna` module is primarily used as an alias for basic Optuna functionality coded in other modules. Currently, two modules are aliased: (1) from `optuna.study`, functions regarding the Study lifecycle, and (2) from `optuna.exceptions`, the TrialPruned Exception raised when\n\n==================\n Document 1 \n----------------\n optuna\uf0c1\n\n\nThe `optuna` module is primarily used as an alias for basic Optuna functionality coded in other modules. Currently, two modules are aliased: (1) from `optuna.study`, functions regarding the Study lifecycle, and (2) from `optuna.exceptions`, the TrialPruned Exception raised when a trial is pruned.\n\n\n|  |  |\n| --- | --- |\n| `optuna.create\\_study` | Create a new `Study`. |\n| `optuna.load\\_study` | Load the existing `Study` that has the specified name. |\n| `optuna.delete\\_study` | Delete a `Study` object. |\n| `optuna.copy\\_study` | Copy study from one storage to another. |\n| `optuna.get\\_all\\_study\\_summaries` | Get all history of studies stored in a specified storage. |\n| `optuna.TrialPruned` | Exception for pruned trials. |\n\n# optuna.artifacts\uf0c1\n\n\nThe `artifacts` module provides the way to manage artifacts (output files) in Optuna.\n\n\n|  |  |\n| --- | --- |\n| `optuna.artifacts.FileSystemArtifactStore` | An artifact backend for\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'optuna' API to optimize the hyperparameters of a linear discriminant analysis (LDA) model. The program should search for the best combination of 'n_components' and 'solver' hyperparameters for an LDA model on a custom dataset and print the explained variance ratio.\n\n### Code:"
    },
    {
        "api": "pandas",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install pandas\nimport pandas as pd\n\n# Read the JSON file into a DataFrame\ndf = pd.read_json('sales_data.json')\n\n# Calculate total sales revenue for each product\nproduct_sales = df.groupby('Product')['Quantity'].sum() * df.groupby('Product')['Price'].sum()\nproduct_sales = product_sales.reset_index()\nproduct_sales.columns = ['Product', 'Total Revenue']\n\n# Identify the product with the highest revenue\nhighest_revenue_product = product_sales[product_sales['Total Revenue'] == product_sales['Total Revenue'].max()]\n\n# Create a summary DataFrame\nsummary_df = pd.concat([highest_revenue_product, product_sales], axis=1)\n\nprint(\"Summary Dataframe: \")\nprint(summary_df)",
        "instruction": "Create a Python program that uses the 'pandas' API to read a JSON file containing sales data, calculate total sales revenue for each product, identify the product with the highest revenue, and create a summary DataFrame.",
        "prediction": ":import pandas as pd\n  import pandas as pd\n\n# Read the JSON file into a JSON table\ndata = pd.read_json('sales_data.json')\n\n# Calculate total sales revenue for each product\ntotal_sales_revenue = data.groupby('Product')['Revenue'].sum()\ntotal_sales_revenue = total_sales_revenue.reset_index()\ntotal_sales_revenue.columns = ['Product', 'Total Revenue']\n\n# Identify the product with the highest revenue\nhighest_revenue_product = total_sales_revenue[total_sales_revenue['Total Revenue'] == total_sales_revenue['Total Revenue'].max()]\n\n# Create a summary DataFrame\nsummary_df = pd.concat([highest_revenue_product, total_sales_revenue], axis=1)\n\nprint(\"Summary Dataframe: \")\nprint(summary_df)",
        "source": "This is some documentation of pandas API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n# Excel#\n\n\n|  |  |\n| --- | --- |\n| `read\\_excel`(io[,\u00c2\u00a0sheet\\_name,\u00c2\u00a0header,\u00c2\u00a0names,\u00c2\u00a0...]) | Read an Excel file into a pandas DataFrame. |\n| `DataFrame.to\\_excel`(excel\\_writer[,\u00c2\u00a0...]) | Write object to an Excel sheet. |\n| `ExcelFile`(path\\_or\\_buffer[,\u00c2\u00a0engine,\u00c2\u00a0...]) | Class for parsing tabular Excel sheets into DataFrame objects. |\n| `ExcelFile.book` |  |\n| `ExcelFile.sheet\\_names` |  |\n| `ExcelFile.parse`([sheet\\_name,\u00c2\u00a0header,\u00c2\u00a0names,\u00c2\u00a0...]) | Parse specified sheet(s) into a DataFrame. |\n\n\n|  |  |\n| --- | --- |\n| `Styler.to\\_excel`(excel\\_writer[,\u00c2\u00a0sheet\\_name,\u00c2\u00a0...]) | Write Styler to an Excel sheet. |\n\n\n|  |  |\n| --- | --- |\n| `ExcelWriter`(path[,\u00c2\u00a0engine,\u00c2\u00a0date\\_format,\u00c2\u00a0...]) | Class for writing DataFrame objects into excel sheets. |\n\n\n## JSON#\n\n\n|  |  |\n| --- | --- |\n| `read\\_json`(path\\_or\\_buf,\u00c2\u00a0\\*[,\u00c2\u00a0orient,\u00c2\u00a0typ,\u00c2\u00a0...]) | Convert a JSON string to pandas object. |\n| `json\\_normalize`(data[,\u00c2\u00a0record\\_path,\u00c2\u00a0meta,\u00c2\u00a0...]) | Normalize semi-structured JSON data into a flat table. |\n| `DataFrame.to\\_json`([path\\_or\\_buf,\u00c2\u00a0orient,\u00c2\u00a0...]) | Convert the object to a JSON string. |\n\n\n|  |  |\n| --- | --- |\n| `build\\_table\\_schema`(data[,\u00c2\u00a0index,\u00c2\u00a0...]) | Create a Table schema from `data`. |\n\n\n\n\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that uses the 'pandas' API to read a JSON file containing sales data, calculate total sales revenue for each product, identify the product with the highest revenue, and create a summary DataFrame.\n\n### Code:"
    },
    {
        "api": "pandas",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install pandas\nimport pandas as pd\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv('sales_data.csv')\n\n# Calculate the total sales revenue for each customer\ncustomer_total_revenue = df.groupby('Customer')['Revenue'].sum()\ncustomer_total_revenue = customer_total_revenue.reset_index()\ncustomer_total_revenue.columns = ['Customer', 'Total Revenue']\n\n# Identify the customer with the highest sales revenue\nhighest_total_revenue_customer = customer_total_revenue[customer_total_revenue['Total Revenue'] == customer_total_revenue['Total Revenue'].max()]\n\n# Create a summary DataFrame\nsummary_df = pd.concat([highest_total_revenue_customer, customer_total_revenue], axis=1)\n\nprint(\"Summary Dataframe: \")\nprint(summary_df)",
        "instruction": "Create a Python program that uses the 'pandas' API to read a CSV file containing sales data, calculate the total sales revenue for each customer, identify the customer with the highest sales revenue, and create a summary DataFrame.",
        "prediction": ":import pandas as pd\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv('sales_data.csv')\n\n# Calculate the total sales revenue for each customer\ncustomer_total_revenue = df.groupby('Customer')['Revenue'].sum()\ncustomer_total_revenue = customer_total_revenue.reset_index()\ncustomer_total_revenue.columns = ['Customer', 'Total Revenue']\n\n# Identify the customer with the highest sales revenue\nhighest_total_revenue_customer = customer_total_revenue[customer_total_revenue['Total Revenue'] == customer_total_revenue['Total Revenue'].max()]\n\n# Create a summary DataFrame\nsummary_df = pd.concat([highest_total_revenue_customer, customer_total_revenue], axis=1)\n\nprint(\"Summary Dataframe: \")\nprint(summary_df)",
        "source": "This is some documentation of pandas API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n# Excel#\n\n\n|  |  |\n| --- | --- |\n| `read\\_excel`(io[,\u00c2\u00a0sheet\\_name,\u00c2\u00a0header,\u00c2\u00a0names,\u00c2\u00a0...]) | Read an Excel file into a pandas DataFrame. |\n| `DataFrame.to\\_excel`(excel\\_writer[,\u00c2\u00a0...]) | Write object to an Excel sheet. |\n| `ExcelFile`(path\\_or\\_buffer[,\u00c2\u00a0engine,\u00c2\u00a0...]) | Class for parsing tabular Excel sheets into DataFrame objects. |\n| `ExcelFile.book` |  |\n| `ExcelFile.sheet\\_names` |  |\n| `ExcelFile.parse`([sheet\\_name,\u00c2\u00a0header,\u00c2\u00a0names,\u00c2\u00a0...]) | Parse specified sheet(s) into a DataFrame. |\n\n\n|  |  |\n| --- | --- |\n| `Styler.to\\_excel`(excel\\_writer[,\u00c2\u00a0sheet\\_name,\u00c2\u00a0...]) | Write Styler to an Excel sheet. |\n\n\n|  |  |\n| --- | --- |\n| `ExcelWriter`(path[,\u00c2\u00a0engine,\u00c2\u00a0date\\_format,\u00c2\u00a0...]) | Class for writing DataFrame objects into excel sheets. |\n\n\n## JSON#\n\n\n|  |  |\n| --- | --- |\n| `read\\_json`(path\\_or\\_buf,\u00c2\u00a0\\*[,\u00c2\u00a0orient,\u00c2\u00a0typ,\u00c2\u00a0...]) | Convert a JSON string to pandas object. |\n| `json\\_normalize`(data[,\u00c2\u00a0record\\_path,\u00c2\u00a0meta,\u00c2\u00a0...]) | Normalize semi-structured JSON data into a flat table. |\n| `DataFrame.to\\_json`([path\\_or\\_buf,\u00c2\u00a0orient,\u00c2\u00a0...]) | Convert the object to a JSON string. |\n\n\n|  |  |\n| --- | --- |\n| `build\\_table\\_schema`(data[,\u00c2\u00a0index,\u00c2\u00a0...]) | Create a Table schema from `data`. |\n\n\n\n\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that uses the 'pandas' API to read a CSV file containing sales data, calculate the total sales revenue for each customer, identify the customer with the highest sales revenue, and create a summary DataFrame.\n\n### Code:"
    },
    {
        "api": "pandas",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install pandas\nimport pandas as pd\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv('customer_data.csv')\n\n# Calculate the average age for each occupation\noccupation_avg_age = df.groupby('Occupation')['Age'].mean()\noccupation_avg_age = occupation_avg_age.reset_index()\noccupation_avg_age.columns = ['Occupation', 'Average Age']\n\n# Identify the occupation with the highest average age\nhighest_avg_age_occupation = occupation_avg_age[occupation_avg_age['Average Age'] == occupation_avg_age['Average Age'].max()]\n\n# Create a summary DataFrame\nsummary_df = pd.concat([highest_avg_age_occupation, occupation_avg_age], axis=1)\n\nprint(\"Summary Dataframe: \")\nprint(summary_df)",
        "instruction": "Create a Python program that uses the 'pandas' API to read a CSV file containing customer data, calculate the average age for each occupation, identify the occupation with the highest average age, and create a summary DataFrame.",
        "prediction": ":import pandas as pd\n  import pandas as pd\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv('data.csv')\n\n# Calculate the average age for each occupation\noccupation_avg_age = df.groupby('Occupation')['Age']\n                                                     \noccupation_avg_age.mean()\n\n# Identify the occupation with the highest average age\nhighest_avg_age = occupation_avg_age.mean().max()\n\n# Create a summary DataFrame\nsummary_df = pd.concat([occupation_avg_age.mean(), highest_avg_age], axis=1)\n\nprint(\"Summary Dataframe: \")\nprint(summary_df)",
        "source": "This is some documentation of pandas API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n# Excel#\n\n\n|  |  |\n| --- | --- |\n| `read\\_excel`(io[,\u00c2\u00a0sheet\\_name,\u00c2\u00a0header,\u00c2\u00a0names,\u00c2\u00a0...]) | Read an Excel file into a pandas DataFrame. |\n| `DataFrame.to\\_excel`(excel\\_writer[,\u00c2\u00a0...]) | Write object to an Excel sheet. |\n| `ExcelFile`(path\\_or\\_buffer[,\u00c2\u00a0engine,\u00c2\u00a0...]) | Class for parsing tabular Excel sheets into DataFrame objects. |\n| `ExcelFile.book` |  |\n| `ExcelFile.sheet\\_names` |  |\n| `ExcelFile.parse`([sheet\\_name,\u00c2\u00a0header,\u00c2\u00a0names,\u00c2\u00a0...]) | Parse specified sheet(s) into a DataFrame. |\n\n\n|  |  |\n| --- | --- |\n| `Styler.to\\_excel`(excel\\_writer[,\u00c2\u00a0sheet\\_name,\u00c2\u00a0...]) | Write Styler to an Excel sheet. |\n\n\n|  |  |\n| --- | --- |\n| `ExcelWriter`(path[,\u00c2\u00a0engine,\u00c2\u00a0date\\_format,\u00c2\u00a0...]) | Class for writing DataFrame objects into excel sheets. |\n\n\n## JSON#\n\n\n|  |  |\n| --- | --- |\n| `read\\_json`(path\\_or\\_buf,\u00c2\u00a0\\*[,\u00c2\u00a0orient,\u00c2\u00a0typ,\u00c2\u00a0...]) | Convert a JSON string to pandas object. |\n| `json\\_normalize`(data[,\u00c2\u00a0record\\_path,\u00c2\u00a0meta,\u00c2\u00a0...]) | Normalize semi-structured JSON data into a flat table. |\n| `DataFrame.to\\_json`([path\\_or\\_buf,\u00c2\u00a0orient,\u00c2\u00a0...]) | Convert the object to a JSON string. |\n\n\n|  |  |\n| --- | --- |\n| `build\\_table\\_schema`(data[,\u00c2\u00a0index,\u00c2\u00a0...]) | Create a Table schema from `data`. |\n\n\n\n\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that uses the 'pandas' API to read a CSV file containing customer data, calculate the average age for each occupation, identify the occupation with the highest average age, and create a summary DataFrame.\n\n### Code:"
    },
    {
        "api": "peft",
        "decoder_input_ids": true,
        "ground-truth": "# Install necessary libraries\n#!pip install transformers\n#!pip install peft\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Load Models\nbase_model = \"gpt-3.5-turbo\"\npeft_model = \"gpt-3.5-turbo\"\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(base_model)\n\n# Load peft\nmodel = PeftModel.from_pretrained(model, peft_model)\nmodel = model.eval()\n\n# Generate code for a language translation application\ntranslation_app_code = model.generate_translation_app_code(max_length=500)\n\n# Print generated language translation application code\nprint(translation_app_code)",
        "instruction": "Create a Python program that utilizes the 'peft' API to generate code for a basic language translation application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a language translation application that can translate text between different languages.",
        "prediction": ":import transformers\n# Load Models\nbase_model = \"gpt-3.5-turbo\"\npeft_model = \"gpt-3.5-turbo\"\n\n# Load peft\nmodel = PeftModel.from_pretrained(base_model, peft_model)\n\n# Generate code for a language translation application\nprint(model.generate_language_translation_application(\"\", \"arabic\", \"german\"))",
        "source": "This is some documentation of peft API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n \n\n# \nModels\n\n\nPeftModel is the base model class for specifying the base Transformer model and configuration to apply a PEFT method to. The base `PeftModel` contains methods for loading and saving models from the Hub, and supports the PromptEncoder for prompt learning.\n\n\n\n## \nPeftModel\n\n### class peft.PeftModel\n\n<\nsource\n>\n(\nmodel: PreTrainedModel\npeft\\_config: PeftConfig\nadapter\\_name: str = 'default'\n\n)\n\n\nParameters \n\n\n* **model** (PreTrainedModel) \u2014 The base transformer model used for Peft.\n* **peft\\_config** (PeftConfig) \u2014 The configuration of the Peft model.\n* **adapter\\_name** (`str`) \u2014 The name of the adapter, defaults to `\"default\"`.\n\nBase model\n\n==================\n Document 1 \n----------------\n### from\\_json\\_file\n\n<\nsource\n>\n(\npath\\_json\\_file\n\\*\\*kwargs\n\n\n* **path\\_json\\_file** (`str`) \u2014\nThe path to the json file.\n\nLoads a configuration file from a json file.\n\n<\nsource\n>\n(\npretrained\\_model\\_name\\_or\\_path\nsubfolder = None\n\\*\\*kwargs\n\n\n* **pretrained\\_model\\_name\\_or\\_path** (`str`) \u2014\nThe directory or the Hub repository id where the configuration is saved.\n* **kwargs** (additional keyword arguments, *optional*) \u2014\nAdditional keyword arguments passed along to the child class initialization.\n\nThis method loads the configuration of your adapter model from a directory.\n\n<\nsource\n>\n(\nsave\\_directory\n\\*\\*kwargs\n\n\n* **save\\_directory** (`str`) \u2014\nThe directory where the configuration will be saved.\n* **kwargs** (additional keyword arguments, *optional*) \u2014\nAdditional keyword arguments passed along to the push\\_to\\_hub\nmethod.\n\nThis method saves the configuration of your adapter model in a directory.\n\n\n\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that utilizes the 'peft' API to generate code for a basic language translation application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a language translation application that can translate text between different languages.\n\n### Code:"
    },
    {
        "api": "peft",
        "decoder_input_ids": true,
        "ground-truth": "# Install necessary libraries\n#!pip install transformers\n#!pip install peft\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Load Models\nbase_model = \"gpt-3.5-turbo\"\npeft_model = \"gpt-3.5-turbo\"\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(base_model)\n\n# Load peft\nmodel = PeftModel.from_pretrained(model, peft_model)\nmodel = model.eval()\n\n# Generate code for a basic e-commerce website\necommerce_website_code = model.generate_ecommerce_website_code(max_length=500)\n\n# Print generated e-commerce website code\nprint(ecommerce_website_code)",
        "instruction": "Develop a Python program using the 'peft' API to generate code for a basic e-commerce website. Utilize the 'gpt-3.5-turbo' model for website code generation. The program should generate code for a simple e-commerce website with product listings, a shopping cart, and checkout functionality.",
        "prediction": ":pip install transformers\n#!pip install peft\n#!pip install peft\n#!pip install transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Load Models\nbase_model = \"gpt-3.5-turbo\"\npeft_model = \"gpt-3.5-turbo\"\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(base_model)\n\n# Load peft\nmodel = PeftModel.from_pretrained(model, peft_model)\nmodel = model.eval()\n\n# Generate code for a simple e-commerce website\ne_commerce_code = model.generate_ecommerce_code(max_length=500)\n\n# Print generated e-commerce website code\nprint(e_commerce_code)",
        "source": "This is some documentation of peft API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n### from\\_json\\_file\n\n<\nsource\n>\n(\npath\\_json\\_file\n\\*\\*kwargs\n\n\n* **path\\_json\\_file** (`str`) \u2014\nThe path to the json file.\n\nLoads a configuration file from a json file.\n\n<\nsource\n>\n(\npretrained\\_model\\_name\\_or\\_path\nsubfolder = None\n\\*\\*kwargs\n\n\n* **pretrained\\_model\\_name\\_or\\_path** (`str`) \u2014\nThe directory or the Hub repository id where the configuration is saved.\n* **kwargs** (additional keyword arguments, *optional*) \u2014\nAdditional keyword arguments passed along to the child class initialization.\n\nThis method loads the configuration of your adapter model from a directory.\n\n<\nsource\n>\n(\nsave\\_directory\n\\*\\*kwargs\n\n\n* **save\\_directory** (`str`) \u2014\nThe directory where the configuration will be saved.\n* **kwargs** (additional keyword arguments, *optional*) \u2014\nAdditional keyword arguments passed along to the push\\_to\\_hub\nmethod.\n\nThis method saves the configuration of your adapter model in a directory.\n\n\n## \nPeftConfig\n\n\n### class peft.PeftConfig\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\n\n\n* **peft\\_type** (Union[`~peft.utils.config.PeftType`, `str`]) \u2014 The type of Peft method to use.\n* **task\\_type** (Union[`~peft.utils.config.TaskType`, `str`]) \u2014 The type of task to perform.\n* **inference\\_mode\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program using the 'peft' API to generate code for a basic e-commerce website. Utilize the 'gpt-3.5-turbo' model for website code generation. The program should generate code for a simple e-commerce website with product listings, a shopping cart, and checkout functionality.\n\n### Code:"
    },
    {
        "api": "peft",
        "decoder_input_ids": true,
        "ground-truth": "# Install necessary libraries\n#!pip install transformers\n#!pip install peft\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Load Models\nbase_model = \"gpt2\"\npeft_model = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(base_model)\n\n# Load peft\nmodel = PeftModel.from_pretrained(model, peft_model)\nmodel = model.eval()\n\n# Query description\nquery_description = \"Retrieve all orders placed by customer 'John Smith' in the year 2022.\"\n\n# Generate SQL query\ngenerated_query = model.generate_sql_query(query_description, max_length=200)\n\n# Print generated SQL query\nprint(generated_query)",
        "instruction": "Create a Python program that utilizes the 'peft' API to generate SQL queries based on a given description of a database query. Use the 'gpt2' model for SQL query generation. The program should accept high-level descriptions of queries and produce SQL queries that retrieve the requested data from a database.",
        "prediction": ":pip install transformers\nimport transformers\n\n# Load Models\nbase_model = \"gpt2\"\npeft_model = \"gpt2\"\ntokenizer = transformers.AutoTokenizer.from_pretrained(base_model)\nmodel = transformers.AutoModelForSeq2SeqLM.from_pretrained(base_model)\n\n# Load peft\nmodel = transformers.AutoModelForSeq2SeqLM.from_pretrained(model, trust_remote_code=True)\nmodel = transformers.AutoModelForSeq2SeqLM.from_config(model.get_config())\n\n# Load peft\nfrom peft import PeftModel\nfrom transformers import AutoModelForSeq2SeqLM\n\n# Load peft\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# Load peft\nfrom peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# Load model\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# Load model\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nfrom peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nfrom transformers.adapter_ex todas:\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nfrom peft import PeftModel\nfrom peft import PeftModel\nfrom peft import PeftModel\nfrom peft import PeftModel\nfrom peft import PeftModel\nfrom peft import PeftModel\nfrom peft import PeftModel\nfrom peft import PeftModel\nfrom peft import PeftModel\nfrom peft import PeftModel\nfrom peft import PeftModel\nfrom peft import PeftModel\nfrom peft import PeftModel\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom peft import PeftModel\nfrom peft import PeftModel\nfrom peft import PeftModel\nfrom peft import PeftModel\nfrom peft import PeftModel\nfrom peft import PeftModel\nfrom peft import PeftModel\n\nfrom peft import PeftModel\nfrom peft import PeftModel\nfrom peft import PeftModel\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom peft import PeftModel\nfrom peft import PeftModel\nfrom peft import PeftModel\nfrom peft import PeftModel\n\nfrom peft import PeftModel\nfrom peft import PeftModel\nfrom peft import PeftModel\n\nfrom peft import PeftModel\nfrom peft import PeftModel\n\nfrom peft import PeftModel\nfrom peft import PeftModel\n\nfrom peft import PeftModel\n\nfrom peft import PeftModel\n\nfrom peft import PeftModel\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom peft import PeftModel\nfrom peft import PeftModel\nfrom peft import PeftModel\nfrom peft import PeftModel\nfrom peft import PeftModel\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom peft import PeftModel\nfrom peft import PeftModel\n\nfrom transformers import AutoTokenizer\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\nfrom transform",
        "source": "This is some documentation of peft API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n### from\\_json\\_file\n\n<\nsource\n>\n(\npath\\_json\\_file\n\\*\\*kwargs\n\n\n* **path\\_json\\_file** (`str`) \u2014\nThe path to the json file.\n\nLoads a configuration file from a json file.\n\n<\nsource\n>\n(\npretrained\\_model\\_name\\_or\\_path\nsubfolder = None\n\\*\\*kwargs\n\n\n* **pretrained\\_model\\_name\\_or\\_path** (`str`) \u2014\nThe directory or the Hub repository id where the configuration is saved.\n* **kwargs** (additional keyword arguments, *optional*) \u2014\nAdditional keyword arguments passed along to the child class initialization.\n\nThis method loads the configuration of your adapter model from a directory.\n\n<\nsource\n>\n(\nsave\\_directory\n\\*\\*kwargs\n\n\n* **save\\_directory** (`str`) \u2014\nThe directory where the configuration will be saved.\n* **kwargs** (additional keyword arguments, *optional*) \u2014\nAdditional keyword arguments passed along to the push\\_to\\_hub\nmethod.\n\nThis method saves the configuration of your adapter model in a directory.\n\n\n## \nPeftConfig\n\n\n### class peft.PeftConfig\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\n\n\n* **peft\\_type** (Union[`~peft.utils.config.PeftType`, `str`]) \u2014 The type of Peft method to use.\n* **task\\_type** (Union[`~peft.utils.config.TaskType`, `str`]) \u2014 The type of task to perform.\n* **inference\\_mode\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that utilizes the 'peft' API to generate SQL queries based on a given description of a database query. Use the 'gpt2' model for SQL query generation. The program should accept high-level descriptions of queries and produce SQL queries that retrieve the requested data from a database.\n\n### Code:"
    },
    {
        "api": "peft",
        "decoder_input_ids": true,
        "ground-truth": "# Install necessary libraries\n#!pip install transformers\n#!pip install peft\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Load Models\nbase_model = \"gpt-3.5-turbo\"\npeft_model = \"gpt-3.5-turbo\"\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(base_model)\n\n# Load peft\nmodel = PeftModel.from_pretrained(model, peft_model)\nmodel = model.eval()\n\n# Generate code for a note-taking application\nnote_app_code = model.generate_note_app_code(max_length=500)\n\n# Print generated note-taking application code\nprint(note_app_code)",
        "instruction": "Develop a Python program using the 'peft' API to generate code for a basic note-taking application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a note-taking application that allows users to create, edit, and organize notes.",
        "prediction": ":pip install transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Load Models\nbase_model = \"gpt-3.5-turbo\"\npeft_model = \"gpt-3.5-turbo\"\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(base_model)\n\n# Load peft\nmodel = PeftModel.from_pretrained(model, peft_model)\nmodel = model.eval()\n\n# Generate code for a note-taking application\nnote_app_code = model.generate_note_app_code(max_length=500)\n\n# Print generated note-taking application code\nprint(note_app_code)",
        "source": "This is some documentation of peft API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n### from\\_json\\_file\n\n<\nsource\n>\n(\npath\\_json\\_file\n\\*\\*kwargs\n\n\n* **path\\_json\\_file** (`str`) \u2014\nThe path to the json file.\n\nLoads a configuration file from a json file.\n\n<\nsource\n>\n(\npretrained\\_model\\_name\\_or\\_path\nsubfolder = None\n\\*\\*kwargs\n\n\n* **pretrained\\_model\\_name\\_or\\_path** (`str`) \u2014\nThe directory or the Hub repository id where the configuration is saved.\n* **kwargs** (additional keyword arguments, *optional*) \u2014\nAdditional keyword arguments passed along to the child class initialization.\n\nThis method loads the configuration of your adapter model from a directory.\n\n<\nsource\n>\n(\nsave\\_directory\n\\*\\*kwargs\n\n\n* **save\\_directory** (`str`) \u2014\nThe directory where the configuration will be saved.\n* **kwargs** (additional keyword arguments, *optional*) \u2014\nAdditional keyword arguments passed along to the push\\_to\\_hub\nmethod.\n\nThis method saves the configuration of your adapter model in a directory.\n\n\n## \nPeftConfig\n\n\n### class peft.PeftConfig\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\n\n\n* **peft\\_type** (Union[`~peft.utils.config.PeftType`, `str`]) \u2014 The type of Peft method to use.\n* **task\\_type** (Union[`~peft.utils.config.TaskType`, `str`]) \u2014 The type of task to perform.\n* **inference\\_mode\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program using the 'peft' API to generate code for a basic note-taking application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a note-taking application that allows users to create, edit, and organize notes.\n\n### Code:"
    },
    {
        "api": "peft",
        "decoder_input_ids": true,
        "ground-truth": "# Install necessary libraries\n#!pip install transformers\n#!pip install peft\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Load Models\nbase_model = \"gpt2\"\npeft_model = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(base_model)\n\n# Load peft\nmodel = PeftModel.from_pretrained(model, peft_model)\nmodel = model.eval()\n\n# Starting phrase\nstarting_phrase = \"To be or not to be, that is the\"\n\n# Generate text\ngenerated_text = model.generate_text(starting_phrase, max_length=200)\n\n# Print generated text\nprint(generated_text)",
        "instruction": "Create a Python program using the 'peft' API to generate text in the style of William Shakespeare. Utilize the 'gpt2' model for text generation. The program should take a starting phrase and generate a coherent and Shakespearean-style text.",
        "prediction": ":import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Load Models\ngpt2_model = \"gpt2\"\npeft_model = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(gpt2_model)\nmodel = AutoModelForCausalLM.from_pretrained(pretrained_model\\_name_or_path)\n\n# Generate text\nstarting_phrase = \"William Shakespeare was\"\n\n# Generate the style of William Shakespeare Shakespearean-style text\n\ufffd__code__\n.method for phrase\n    \nreturned_text = f\"I think, or like, I think, in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in\n\n\n# Print generated text\nprint(returned_text)",
        "source": "This is some documentation of peft API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n \n\n# \nModels\n\n\nPeftModel is the base model class for specifying the base Transformer model and configuration to apply a PEFT method to. The base `PeftModel` contains methods for loading and saving models from the Hub, and supports the PromptEncoder for prompt learning.\n\n\n\n## \nPeftModel\n\n### class peft.PeftModel\n\n<\nsource\n>\n(\nmodel: PreTrainedModel\npeft\\_config: PeftConfig\nadapter\\_name: str = 'default'\n\n)\n\n\nParameters \n\n\n* **model** (PreTrainedModel) \u2014 The base transformer model used for Peft.\n* **peft\\_config** (PeftConfig) \u2014 The configuration of the Peft model.\n* **adapter\\_name** (`str`) \u2014 The name of the adapter, defaults to `\"default\"`.\n\nBase model\n\n==================\n Document 1 \n----------------\n### from\\_json\\_file\n\n<\nsource\n>\n(\npath\\_json\\_file\n\\*\\*kwargs\n\n\n* **path\\_json\\_file** (`str`) \u2014\nThe path to the json file.\n\nLoads a configuration file from a json file.\n\n<\nsource\n>\n(\npretrained\\_model\\_name\\_or\\_path\nsubfolder = None\n\\*\\*kwargs\n\n\n* **pretrained\\_model\\_name\\_or\\_path** (`str`) \u2014\nThe directory or the Hub repository id where the configuration is saved.\n* **kwargs** (additional keyword arguments, *optional*) \u2014\nAdditional keyword arguments passed along to the child class initialization.\n\nThis method loads the configuration of your adapter model from a directory.\n\n<\nsource\n>\n(\nsave\\_directory\n\\*\\*kwargs\n\n\n* **save\\_directory** (`str`) \u2014\nThe directory where the configuration will be saved.\n* **kwargs** (additional keyword arguments, *optional*) \u2014\nAdditional keyword arguments passed along to the push\\_to\\_hub\nmethod.\n\nThis method saves the configuration of your adapter model in a directory.\n\n\n\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'peft' API to generate text in the style of William Shakespeare. Utilize the 'gpt2' model for text generation. The program should take a starting phrase and generate a coherent and Shakespearean-style text.\n\n### Code:"
    },
    {
        "api": "plotly",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install plotly\nimport plotly.graph_objects as go\n\n# Define data points for the x and y axes\nx = [1, 2, 3, 4, 5]\ny = [10, 16, 5, 11, 8]\n\n# Define color values for each data point\ncolor = [1, 2, 3, 4, 5]\n\n# Create a scatter plot with a color scale using Plotly Graph Objects\nfig = go.Figure(data=go.Scatter(x=x, y=y, mode='markers', marker=dict(color=color, colorscale='Viridis', showscale=True)))\nfig.update_layout(title=\"Scatter Plot with Color Scale\", xaxis_title=\"X-axis\", yaxis_title=\"Y-axis\")\n\n# Save the chart as a PDF file\nfig.write_image(\"scatter_plot_with_color_scale.pdf\")\n\n# Display the chart on the screen\nfig.show()\n",
        "instruction": "Create a Python program using the 'plotly' API to generate a scatter plot with a color scale. The program should define data points for the x and y axes, as well as the color values for each data point. The program should create the scatter plot with a color scale, customize the chart title, X-axis label, and Y-axis label, and display the color scale on the chart. Finally, save the chart as a PDF file and display it on the screen.",
        "prediction": ":import plotly Graph\nimport plotly Graph\n\n# Define data points for the x and y axes\nx = [1, 2, 3, 4, 5]\ny = [10, 16, 5, 11, 8]\n\n# Define color values for each data point\ncolor = y\n\n# Create a scatter plot with a color scale using Plotly Graph\nfig = Graph.Scatter(x=x, y=y, mode='markers', marker=dict(color=color, colorscale='Viridis', showscale=True))\nfig.update_layout(title=\"Scatter Plot with Color Scale\", xaxis_title=\"X-axis\", yaxis_title=\"Y-axis\")\n\n# Save the chart as a PDF file\nfig.write_image(\"scatter_plot_with_color_scale.pdf\")\n\n# Display the chart on the screen\nfig.show()\n",
        "source": "This is some documentation of plotly API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n Plotly Python Open Source Graphing Library Scientific Charts\n\n\nPlotly's Python graphing library makes interactive, publication-quality graphs online. Examples of how to make scientific charts such as contour plots, heatmaps, dendrograms, polar charts, and ternary plots.\n\n\n* Contour Plots\n\nView Tutorial\n\n* Heatmaps\n\nView Tutorial\n\n* Imshow\n\nView Tutorial\n\n* Ternary Plots\n\nView Tutorial\n\n* Log Plots\n\nView Tutorial\n\n* Dendrograms\n\nView Tutorial\n\n* Annotated Heatmaps\n\nView Tutorial\n\n* Ternary Overlay\n\nView Tutorial\n\n* Parallel Coordinates Plot\n\nView Tutorial\n\n* Quiver Plots\n\nView Tutorial\n\n* Streamline Plots\n\nView Tutorial\n\n* Network Graphs\n\nView Tutorial\n\n* Carpet Plots\n\nView Tutorial\n\n* Carpet Contour Plot\n\nView Tutorial\n\n* Carpet Scatter Plot\n\nView Tutorial\n\n* Polar Charts\n\nView Tutorial\n\n* Radar Charts\n\nView Tutorial\n\n* Ternary contours\n\nView Tutorial\n\n* Wind Rose and Polar Bar Charts\n\nView Tutorial\n\n* Plotly and Datashader\n\nView Tutorial\n\n* Smith Charts\n\nView Tutorial\n\n\n \n \n > Scientific Charts\n \n \n > Contour Plots\n\n\n# \n Contour Plots\n \n in \n Python\n\n\nHow to make Contour plots in Python with Plotly. \n\n\n\n### Basic Contour Plot\u00b6\n\nA 2D contour plot shows the contour lines of a 2D numerical array `z`, i.e. interpolated lines of isovalues of `z`.\n\nfig = go.Figure(data =\n    go.Contour(\n        z=[[10, 10.625, 12.5, 15.625, 20],\n           [5.625, 6.25, 8.125, 11.25, 15.625],\n           [2.5, 3.125, 5., 8.125, 12.5],\n           [0.625, 1.25, 3.125, 6.25, 10.625],\n           [0,\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'plotly' API to generate a scatter plot with a color scale. The program should define data points for the x and y axes, as well as the color values for each data point. The program should create the scatter plot with a color scale, customize the chart title, X-axis label, and Y-axis label, and display the color scale on the chart. Finally, save the chart as a PDF file and display it on the screen.\n\n### Code:"
    },
    {
        "api": "plotly",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install plotly\nimport plotly.graph_objects as go\n\n# Define data points for the x, y, and z axes\nx = [1, 2, 3, 4, 5]\ny = [10, 16, 5, 11, 8]\nz = [7, 3, 9, 5, 2]\n\n# Define the size of the bubbles\nsize = [30, 50, 20, 40, 10]\n\n# Create a bubble chart using Plotly Graph Objects\nfig = go.Figure(data=go.Scatter(x=x, y=y, mode='markers', marker=dict(size=size)))\nfig.update_layout(title=\"Bubble Chart\", xaxis_title=\"X-axis\", yaxis_title=\"Y-axis\")\n\n# Save the chart as a PNG image file\nfig.write_image(\"bubble_chart.png\")\n\n# Display the chart on the screen\nfig.show()\n",
        "instruction": "Create a Python program using the 'plotly' API to generate a bubble chart. The program should define data points for the x, y, and z axes, create the chart, customize the chart title, X-axis label, Y-axis label, and size of the bubbles. Finally, save the chart as a PNG image file and display it on the screen.",
        "prediction": ":import plotly.graph_objects as go\n\n# Define data points for the x, y, and z axes\nx = [1, 2, 3, 4]\ny = [10, 11, 12, 13]\nz = [miles * 2 for miles in y]\n\n# Create a bubble chart using Plotly Graph Objects\nfig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z, mode='markers', marker=dict(size=z, colors=z, colorscale='Viridis', showscale=True))])\n\n# Customize the chart title, X-axis label, Y-axis label, and size of the bubbles\nfig.update_layout(title=\"My Bubble chart\", scene=dict(xaxis_title=\"X-axis\", yaxis_title=\"Y-axis\", zaxis_title=\"Z-axis\"), scene_aspectmode='manual', scene_xaxis_label0='X-axis', scene_yaxis_label0='Y-axis', scene_zaxis_label0='Z-axis')\n\n# Save the chart as a PNG image file\nfig.write_image(\"bubble_chart.png\")\n\n# Display the chart on the screen\nfig.show()\n",
        "source": "This is some documentation of plotly API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n# Bubble chart with plotly.express\u00b6\n\nA bubble chart is a scatter plot in which a third dimension of the data is shown through the size of markers. For other types of scatter plot, see the scatter plot documentation.\n\n\nWe first show a bubble chart example using Plotly Express. Plotly Express is the easy-to-use, high-level interface to Plotly, which operates on a variety of types of data and produces easy-to-style figures. The size of markers is set from the dataframe column given as the `size` parameter.\n\n```\nimport plotly.express as px\ndf = px.data.gapminder()\n\nfig = px.scatter(df.query(\"year==2007\"), x=\"gdpPercap\", y=\"lifeExp\",\n\t         size=\"pop\", color=\"continent\",\n                 hover\\_name=\"country\", log\\_x=True, size\\_max=60)\nfig.show()\n\n\n## Bubble Chart with plotly.graph\\_objects\u00b6\n\nIf Plotly Express does not provide a good starting point, it is also possible to use the more generic `go.Scatter` class from `plotly.graph_objects`, and define the size of markers to create a bubble chart. All of the available options are described in the scatter section of the reference page: https://plotly.com/python/reference#scatter.\n\n\n\n### Simple Bubble Chart\u00b6\n\nfig = go.Figure(data=[go.Scatter(\n    x=[1, 2, 3, 4], y=[10, 11, 12, 13],\n    mode='markers',\n    marker\\_size=[40, 60, 80, 100])\n])\n\n\n\n### Setting Marker Size and Color\u00b6\n\nfig = go.Figure(data=[go.Scatter(\n    x=[1, 2, 3, 4], y=[10, 11, 12, 13],\n    mode='markers',\n    marker=dict(\n        color=['rgb(93, 164, 214)', 'rgb(255, 144, 14)',\n               'rgb(44, 160, 101)', 'rgb(255, 65, 54)'],\n        opacity=[1, 0.8,\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'plotly' API to generate a bubble chart. The program should define data points for the x, y, and z axes, create the chart, customize the chart title, X-axis label, Y-axis label, and size of the bubbles. Finally, save the chart as a PNG image file and display it on the screen.\n\n### Code:"
    },
    {
        "api": "plotly",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install plotly\nimport plotly.graph_objects as go\n\n# Define data points for the x and y axes\nx = [1, 2, 3, 4, 5]\ny = [10, 16, 5, 11, 8]\n\n# Define error values for each data point\nerror = [1, 2, 0.5, 1.5, 1]\n\n# Create a scatter plot with error bars using Plotly Graph Objects\nfig = go.Figure(data=go.Scatter(x=x, y=y, mode='markers', error_y=dict(type='data', array=error, visible=True)))\nfig.update_layout(title=\"Scatter Plot with Error Bars\", xaxis_title=\"X-axis\", yaxis_title=\"Y-axis\")\n\n# Save the chart as a PDF file\nfig.write_image(\"scatter_plot_with_error_bars.pdf\")\n\n# Display the chart on the screen\nfig.show()\n",
        "instruction": "Create a Python program using the 'plotly' API to generate a scatter plot with error bars. The program should define data points for the x and y axes, as well as the error values for each data point. The program should create the scatter plot with error bars, customize the chart title, X-axis label, and Y-axis label, and display the error bars on the chart. Finally, save the chart as a PDF file and display it on the screen.",
        "prediction": ":import plotly.graph_objects as go\nimport plotly.graph_objects as go\n\n# Define data points for the x and y axes\nx = [1, 2, 3, 4, 5]\ny = [10, 16, 5, 11, 8]\n\n# Define error values for each data point\nerror = [1, 2, 0.5, 1.5, 1]\n\n# Create a scatter plot with error bars using Plotly Graph Objects\nfig = go.Figure(data=go.Scatter(x=x, y=y, mode='markers', error_y=dict(type='data', array=error, visible=True)))\nfig.update_layout(title=\"Scatter Plot with Error Bars\", xaxis_title=\"X-axis\", yaxis_title=\"Y-axis\")\n\n# Save the chart as a PDF file\nfig.write_image(\"scatter_plot_with_error_bars.pdf\")\n\n# Display the chart on the screen\nfig.show()\n",
        "source": "This is some documentation of plotly API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n  Plotly Python Open Source Graphing Library Statistical Charts\n\n\nPlotly's Python graphing library makes interactive, publication-quality graphs online. Examples of how to make statistical charts such as box plots, histograms, and distrubution plots.\n\n\n* Error Bars\n\nView Tutorial\n\n* Box Plots\n\nView Tutorial\n\n* Histograms\n\nView Tutorial\n\n* Distplots\n\nView Tutorial\n\n* 2D Histograms\n\nView Tutorial\n\n* Scatterplot Matrix\n\nView Tutorial\n\n* Facet and Trellis Plots\n\nView Tutorial\n\n* Parallel Categories Diagram\n\nView Tutorial\n\n* Tree-plots\n\nView Tutorial\n\n* Violin Plots\n\nView Tutorial\n\n* 2D Histogram Contour\n\nView Tutorial\n\n* Linear and Non-Linear Trendlines\n\nView Tutorial\n\n* Marginal Distribution Plots\n\nView Tutorial\n\n* Strip Charts\n\nView Tutorial\n\n* Continuous Error Bands\n\nView Tutorial\n\n* Empirical Cumulative Distribution Plots\n\nView Tutorial\n\n\n\n### Statistical charts in Dash\n\n \n \n > Statistical Charts\n \n \n > Error Bars\n\n\n\n# \n Error Bars\n \n in \n Python\n\n\nHow to add error-bars to charts in Python with Plotly. \n\n\n\n### Error Bars with Plotly Express\u00b6\n\nPlotly Express is the easy-to-use, high-level interface to Plotly, which operates on a variety of types of data and produces easy-to-style figures. For functions representing 2D data points such as `px.scatter`, `px.line`, `px.bar` etc., error bars are given as a column name which is the value of the `error_x` (for the error on x position) and `error_y` (for the error on y position).\n\n\n\n#### Asymmetric Error Bars with Plotly Express\u00b6\n\n```\nimport plotly.express as px\ndf = px.data.iris()\ndf[\"e\\_plus\"] = df[\"sepal\\_width\"]/100\ndf[\"e\\_minus\"] = df[\"sepal\\_width\"]/40\nfig = px.scatter(df, x=\"sepal\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'plotly' API to generate a scatter plot with error bars. The program should define data points for the x and y axes, as well as the error values for each data point. The program should create the scatter plot with error bars, customize the chart title, X-axis label, and Y-axis label, and display the error bars on the chart. Finally, save the chart as a PDF file and display it on the screen.\n\n### Code:"
    },
    {
        "api": "plotly",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install plotly\nimport plotly.graph_objects as go\n\n# Define data points for the locations and values\nlocations = ['USA', 'Canada', 'Mexico']\nvalues = [10, 5, 7]\n\n# Create a choropleth map using Plotly Graph Objects\nfig = go.Figure(data=go.Choropleth(locations=locations, z=values, locationmode='country names'))\nfig.update_layout(title=\"Choropleth Map\")\n\n# Save the map as an HTML file\nfig.write_html(\"choropleth_map.html\")\n\n# Display the map on the screen\nfig.show()\n",
        "instruction": "Create a Python program using the 'plotly' API to generate a choropleth map. The program should define data points for the locations and values, create the map, customize the map title, and display the data points on the map. Finally, save the map as an HTML file and display it on the screen.",
        "prediction": ":import plotly.graph_objects as go\nfrom plotly.offline import download_clibge\nimport plotly.graph_objects as go\n\n# Define data points for the locations and values\nus_counties = [\n    \"New York\",\n    \"\ufffdWashington\",\n    \"Los Angeles\",\n    \"San Francisco\",\n]\n\nus_population = [\n    \"865,167\", \n    \"858,778\", \n    \"749,241\", \n    \"7,832,774\",\n]\n\n# Create a choropleth map using Plotly Graph Objects\nfig = go.Figure(\n    data= [\n        go.Choroplethmapbox(\n            locations=us_counties,\n            z=us_population,\n            text=us_counties,\n            marker=go.choroplethmapbox.Marker(\n                colorscale=plotly.colors.sequential.Viridis,\n            ),\n        )\n    ]\n)\nfig.update_layout(\n    title=\"Population Counties for United States\",\n    geo=go.Geo(projection=go.crs.PlateCarree()),\n)\n\n# Save the map as an HTML file\nfig.write_html(\"choropleth_map.html\")\n\n# Display the map\nfig.show()\n",
        "source": "This is some documentation of plotly API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n Plotly Python Open Source Graphing Library Maps\n\n\nPlotly's Python graphing library makes interactive, publication-quality maps online. Examples of how to make maps with Plotly and Mapbox.\n\n\n* Mapbox Choropleth Maps\n\nView Tutorial\n\n* Lines on Mapbox\n\nView Tutorial\n\n* Filled Area on Maps\n\nView Tutorial\n\n* Bubble Maps\n\nView Tutorial\n\n* Mapbox Density Heatmap\n\nView Tutorial\n\n* Lines on Maps\n\nView Tutorial\n\n* Choropleth Maps\n\nView Tutorial\n\n* Mapbox Map Layers\n\nView Tutorial\n\n* Scatter Plots on Mapbox\n\nView Tutorial\n\n* USA County Choropleth Maps\n\nView Tutorial\n\n* Scatter Plots on Maps\n\nView Tutorial\n\n* Map Configuration and Styling\n\nView Tutorial\n\n* Hexbin Mapbox\n\nView Tutorial\n\n\n\n### Maps in Dash\n\n \n \n > Maps\n \n \n > Mapbox Choropleth Maps\n\n\n\n# \n Mapbox Choropleth Maps\n \n in \n Python\n\n\nHow to make a Mapbox Choropleth Map of US Counties in Python with Plotly. \n\n\nA Choropleth Map is a map composed of colored polygons. It is used to represent spatial variations of a quantity. This page documents how to build **tile-map** choropleth maps, but you can also build **outline** choropleth maps using our non-Mapbox trace types.\n\n\nBelow we show how to create Choropleth Maps using either Plotly Express' `px.choropleth_mapbox` function or the lower-level `go.Choroplethmapbox` graph object.\n\n\n\n#### Mapbox Access Tokens and Base Map Configuration\u00b6\n\nTo plot on Mapbox maps with Plotly you *may* need a Mapbox account and a public Mapbox Access Token. See our Mapbox Map Layers documentation for more information.\n\n\n### Introduction: main parameters for choropleth tile maps\u00b6\n\nMaking choropleth Mapbox maps requires two main types of input:\n\n\n1. GeoJSON-formatted geometry information where each feature has either an `id` field or some identifying value in `properties`.\n\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'plotly' API to generate a choropleth map. The program should define data points for the locations and values, create the map, customize the map title, and display the data points on the map. Finally, save the map as an HTML file and display it on the screen.\n\n### Code:"
    },
    {
        "api": "polars",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install polars\nimport polars as pl\n\n# Load a sample CSV file into a Polars DataFrame\ndata = pl.read_csv(\"sample_data.csv\")\n\n# Filter rows\nfiltered_data = data.filter(data['country'] == 'Canada')\n\n# Select specific columns\nselected_columns = filtered_data.select(['name', 'age', 'country'])\n\n# Calculate the maximum age\nmax_age = selected_columns['age'].max()\n\n# Display the resulting maximum age\nprint(max_age)\n",
        "instruction": "Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'country' column is 'Canada', select specific columns ('name', 'age', 'country'), and calculate the maximum age. Finally, display the resulting maximum age.",
        "prediction": ":pip install polars\nimport polars as pl\n\n# Load a sample CSV file into a Polars DataFrame\ndata = pl.read_csv(\"sample_data.csv\")\n\n# Filter rows\nfiltered_data = data.filter(data['country'] == 'Canada')\n\n# Select specific columns\nselected_columns = filtered_data.select(['name', 'age', 'country'])\n\n# Calculate the maximum age\nmax_age = selected_columns['age'].max()\n\n# Display the resulting maximum age\nprint(max_age)\n",
        "source": "This is some documentation of polars API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n# polars.read\\_csv#\n\n\npolars.read\\_csv(\n\n*source: str | TextIO | BytesIO | Path | BinaryIO | bytes*,\n*\\**,\n*has\\_header: bool = True*,\n*columns: Sequence[int] | Sequence[str] | None = None*,\n*new\\_columns: Sequence[str] | None = None*,\n*separator: str = ','*,\n*comment\\_char: str | None = None*,\n*quote\\_char: str | None =\n\n==================\n Document 1 \n----------------\n polars.scan\\_csv#\n\n\npolars.scan\\_csv(\n\n*source: str | Path*,\n*\\**,\n*has\\_header: bool = True*,\n*separator: str = ','*,\n*comment\\_char: str | None = None*,\n*quote\\_char: str | None = '\"'*,\n*skip\\_rows: int = 0*,\n*dtypes: SchemaDict | Sequence[PolarsDataType] | None = None*,\n*schema: SchemaDict | None = None*,\n*null\\_values: str | Sequence[str] | dict[str, str] | None = None*,\n*missing\\_utf8\\_is\\_empty\\_string: bool = False*,\n*ignore\\_errors: bool = False*,\n*cache: bool = True*,\n*with\\_column\\_names: Callable[[list[str]], list[str]] | None = None*,\n*infer\\_schema\\_length: int | None = 100*,\n*n\\_rows: int | None = None*,\n*encoding: CsvEncoding = 'utf8'*,\n*low\\_memory: bool = False*,\n*rechunk: bool = True*,\n*skip\\_rows\\_after\\_header: int = 0*,\n*row\\_count\\_name: str | None = None*,\n*row\\_count\\_offset: int = 0*,\n*try\\_parse\\_dates: bool = False*,\n*eol\\_char: str = '\\n'*,\n\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'country' column is 'Canada', select specific columns ('name', 'age', 'country'), and calculate the maximum age. Finally, display the resulting maximum age.\n\n### Code:"
    },
    {
        "api": "polars",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install polars\nimport polars as pl\n\n# Load a sample CSV file into a Polars DataFrame\ndata = pl.read_csv(\"sample_data.csv\")\n\n# Filter rows\nfiltered_data = data.filter(data['country'] == 'USA')\n\n# Select specific columns\nselected_columns = filtered_data.select(['name', 'age', 'country'])\n\n# Calculate the average age\naverage_age = selected_columns['age'].mean()\n\n# Display the resulting average age\nprint(average_age)\n",
        "instruction": "Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'country' column is 'USA', select specific columns ('name', 'age', 'country'), and calculate the average age. Finally, display the resulting average age.",
        "prediction": ":pip install polars\nimport polars as pl\n\n# Load a sample CSV file into a Polars DataFrame\ndata = pl.read_csv(\"sample_data.csv\")\n\n# Filter rows\nfiltered_data = data.filter(data['country'] == 'USA')\n\n# Select specific columns\nselected_columns = filtered_data.select(['name', 'age', 'country'])\n\n# Calculate the average age\naverage_age = selected_columns['age'].mean()\n\n# Display the resulting average age\nprint(average_age)\n",
        "source": "This is some documentation of polars API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n# polars.read\\_csv#\n\n\npolars.read\\_csv(\n\n*source: str | TextIO | BytesIO | Path | BinaryIO | bytes*,\n*\\**,\n*has\\_header: bool = True*,\n*columns: Sequence[int] | Sequence[str] | None = None*,\n*new\\_columns: Sequence[str] | None = None*,\n*separator: str = ','*,\n*comment\\_char: str | None = None*,\n*quote\\_char: str | None =\n\n==================\n Document 1 \n----------------\n polars.scan\\_csv#\n\n\npolars.scan\\_csv(\n\n*source: str | Path*,\n*\\**,\n*has\\_header: bool = True*,\n*separator: str = ','*,\n*comment\\_char: str | None = None*,\n*quote\\_char: str | None = '\"'*,\n*skip\\_rows: int = 0*,\n*dtypes: SchemaDict | Sequence[PolarsDataType] | None = None*,\n*schema: SchemaDict | None = None*,\n*null\\_values: str | Sequence[str] | dict[str, str] | None = None*,\n*missing\\_utf8\\_is\\_empty\\_string: bool = False*,\n*ignore\\_errors: bool = False*,\n*cache: bool = True*,\n*with\\_column\\_names: Callable[[list[str]], list[str]] | None = None*,\n*infer\\_schema\\_length: int | None = 100*,\n*n\\_rows: int | None = None*,\n*encoding: CsvEncoding = 'utf8'*,\n*low\\_memory: bool = False*,\n*rechunk: bool = True*,\n*skip\\_rows\\_after\\_header: int = 0*,\n*row\\_count\\_name: str | None = None*,\n*row\\_count\\_offset: int = 0*,\n*try\\_parse\\_dates: bool = False*,\n*eol\\_char: str = '\\n'*,\n\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'country' column is 'USA', select specific columns ('name', 'age', 'country'), and calculate the average age. Finally, display the resulting average age.\n\n### Code:"
    },
    {
        "api": "polars",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install polars\nimport polars as pl\n\n# Load a sample CSV file into a Polars DataFrame\ndata = pl.read_csv(\"sample_data.csv\")\n\n# Filter rows\nfiltered_data = data.filter(data['score'] > 80)\n\n# Select specific columns\nselected_columns = filtered_data.select(['name', 'score', 'grade'])\n\n# Calculate the average score\naverage_score = selected_columns['score'].mean()\n\n# Display the resulting average score\nprint(average_score)\n",
        "instruction": "Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'score' column is greater than 80, select specific columns ('name', 'score', 'grade'), and calculate the average score. Finally, display the resulting average score.",
        "prediction": ":pip install polars\nimport polars as pl\n\n# Load a sample CSV file into a Polars DataFrame\ndata = pl.read_csv(\"sample_data.csv\")\n\n# Filter rows\nfiltered_data = data.filter(data['score'] > 80)\n\n# Select specific columns\nselected_columns = filtered_data.select(['name','score', 'grade'])\n\n# Calculate the average score\naverage_score = selected_columns['score'].mean()\n\n# Display the resulting average score\nprint(average_score)\n",
        "source": "This is some documentation of polars API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n# polars.read\\_csv#\n\n\npolars.read\\_csv(\n\n*source: str | TextIO | BytesIO | Path | BinaryIO | bytes*,\n*\\**,\n*has\\_header: bool = True*,\n*columns: Sequence[int] | Sequence[str] | None = None*,\n*new\\_columns: Sequence[str] | None = None*,\n*separator: str = ','*,\n*comment\\_char: str | None = None*,\n*quote\\_char: str | None =\n\n==================\n Document 1 \n----------------\n polars.scan\\_csv#\n\n\npolars.scan\\_csv(\n\n*source: str | Path*,\n*\\**,\n*has\\_header: bool = True*,\n*separator: str = ','*,\n*comment\\_char: str | None = None*,\n*quote\\_char: str | None = '\"'*,\n*skip\\_rows: int = 0*,\n*dtypes: SchemaDict | Sequence[PolarsDataType] | None = None*,\n*schema: SchemaDict | None = None*,\n*null\\_values: str | Sequence[str] | dict[str, str] | None = None*,\n*missing\\_utf8\\_is\\_empty\\_string: bool = False*,\n*ignore\\_errors: bool = False*,\n*cache: bool = True*,\n*with\\_column\\_names: Callable[[list[str]], list[str]] | None = None*,\n*infer\\_schema\\_length: int | None = 100*,\n*n\\_rows: int | None = None*,\n*encoding: CsvEncoding = 'utf8'*,\n*low\\_memory: bool = False*,\n*rechunk: bool = True*,\n*skip\\_rows\\_after\\_header: int = 0*,\n*row\\_count\\_name: str | None = None*,\n*row\\_count\\_offset: int = 0*,\n*try\\_parse\\_dates: bool = False*,\n*eol\\_char: str = '\\n'*,\n\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'score' column is greater than 80, select specific columns ('name', 'score', 'grade'), and calculate the average score. Finally, display the resulting average score.\n\n### Code:"
    },
    {
        "api": "polars",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install polars\nimport polars as pl\n\n# Load a sample CSV file into a Polars DataFrame\ndata = pl.read_csv(\"sample_data.csv\")\n\n# Filter rows\nfiltered_data = data.filter(data['city'] == 'Toronto')\n\n# Select specific columns\nselected_columns = filtered_data.select(['name', 'age', 'city'])\n\n# Calculate the minimum age\nmin_age = selected_columns['age'].min()\n\n# Display the resulting minimum age\nprint(min_age)\n",
        "instruction": "Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'city' column is 'Toronto', select specific columns ('name', 'age', 'city'), and calculate the minimum age. Finally, display the resulting minimum age.",
        "prediction": ":pip install polars\nimport polars as pl\n\n# Load a sample CSV file into a Polars DataFrame\ndata = pl.read_csv(\"sample_data.csv\")\n\n# Filter rows\nfiltered_data = data.filter(data['city'] == 'Toronto')\n\n# Select specific columns\nselected_columns = filtered_data.select(['name', 'age', 'city'])\n\n# Calculate the minimum age\nmin_age = selected_columns['age'].min()\n\n# Display the resulting minimum age\nprint(min_age)\n",
        "source": "This is some documentation of polars API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n# polars.read\\_csv#\n\n\npolars.read\\_csv(\n\n*source: str | TextIO | BytesIO | Path | BinaryIO | bytes*,\n*\\**,\n*has\\_header: bool = True*,\n*columns: Sequence[int] | Sequence[str] | None = None*,\n*new\\_columns: Sequence[str] | None = None*,\n*separator: str = ','*,\n*comment\\_char: str | None = None*,\n*quote\\_char: str | None =\n\n==================\n Document 1 \n----------------\n polars.testing.parametric.dataframes#\n\n\npolars.testing.parametric.dataframes(\n\n*cols: int | column | Sequence[column] | None = None*,\n*\\**,\n*lazy: Literal[False] = False*,\n*min\\_cols: int | None = 0*,\n*max\\_cols: int | None = MAX\\_COLS*,\n*size: int | None = None*,\n*min\\_size: int | None = 0*,\n*max\\_size: int | None = MAX\\_DATA\\_SIZE*,\n*chunked: bool | None = None*,\n*include\\_cols: Sequence[column] | column | None = None*,\n*null\\_probability: float | dict[str, float] = 0.0*,\n*allow\\_infinities: bool = True*,\n*allowed\\_dtypes: Collection[PolarsDataType] | PolarsDataType | None = None*,\n*excluded\\_dtypes: Collection[PolarsDataType] | PolarsDataType | None = None*,\n\n) \u2192 SearchStrategy[DataFrame][source]#\n\npolars.testing.parametric.dataframes(\n\n*cols: int | column | Sequence[column] | None = None*,\n*\\**,\n*lazy: Literal[True]*,\n*min\\_cols: int | None = 0*,\n*max\\_cols: int | None = MAX\\_COLS*,\n*size: int | None = None*,\n*min\\_size:\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'city' column is 'Toronto', select specific columns ('name', 'age', 'city'), and calculate the minimum age. Finally, display the resulting minimum age.\n\n### Code:"
    },
    {
        "api": "pyglove",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install pyglove\nimport pyglove as pg\nimport networkx as nx\n\ndef load_and_analyze_network():\n  # Replace this with your code to load and analyze a network\n  # The example below creates a simple social network and computes network statistics\n  G = nx.Graph()\n  G.add_edges_from([(1, 2), (2, 3), (3, 4), (4, 5), (5, 1)])\n\n  num_nodes = G.number_of_nodes()\n  num_edges = G.number_of_edges()\n  average_degree = sum(dict(G.degree()).values()) / num_nodes\n  network_density = nx.density(G)\n\n  return num_nodes, num_edges, average_degree, network_density\n\nif __name__ == '__main__':\n  num_nodes, num_edges, average_degree, network_density = load_and_analyze_network()\n\n  print(f'Number of Nodes: {num_nodes}')\n  print(f'Number of Edges: {num_edges}')\n  print(f'Average Degree: {average_degree:.2f}')\n  print(f'Network Density: {network_density:.2f}')\n",
        "instruction": "Create a Python program using the 'pyglove' API to perform network analysis. The program should analyze a network (e.g., social network or computer network) by loading a dataset, applying network analysis techniques, and outputting relevant statistics or insights. Additionally, provide a unit test that verifies the accuracy of the network analysis results.",
        "prediction": ":import pyglove\nimport network analysis techniques\nimport numpy as np\nimport sklearn\n\nif import test\n\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'dataset.dat'\n# Load the document\ndataset = 'dataset.dat'\n# Load the dataset\ndataset = 'datas",
        "source": "This is some documentation of pyglove API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# Public API: pyglove#\n\n\n## Modules#\n\n\n### core#\n\n* pg.detouring\n* pg.geno\n* pg.hyper\n* pg.object\\_utils\n* pg.patching\n* pg.symbolic\n* pg.tuning\n* pg.typing\n\n\n### ext#\n\n* pg.early\\_stopping\n* pg.evolution\n* pg.mutfun\n* pg.scalars\n\n\n\n## Top-level shortcurts#\n\n\n### Objects#\n\n\n* pg.MISSING\\_VALUE\n\n\n\n### Classes#\n\n\n* pg.ClassWrapper\n* pg.CustomTyping\n* pg.DNA\n* pg.DNAGenerator\n* pg.DNASpec\n* pg.Dict\n* pg.Diff\n* pg.DocStr\n* pg.Field\n* pg.FieldUpdate\n* pg.Formattable\n* pg.Functor\n* pg.Inferentiable\n* pg.InferredValue\n* pg.Insertion\n* pg.JSONConvertible\n* pg.KeyPath\n* pg.KeySpec\n* pg.List\n* pg.MaybePartial\n* pg.Object\n* pg.ObjectFactory\n* pg.Origin\n* pg.PureSymbolic\n* pg.Ref\n* pg.Schema\n* pg.Symbolic\n* pg.TraverseAction\n* pg.ValueSpec\n* pg.WritePermissionError\n* pg.dict\n* pg.list\n\n\n### Functions#\n\n\n* pg.allow\\_empty\\_field\\_description\n* pg.allow\\_partial\n* pg.allow\\_repeated\\_class\\_registration\n* pg.allow\\_writable\\_accessors\n* pg.apply\\_wrappers\n* pg.as\\_sealed\n* pg.auto\\_call\\_functors\n* pg.boilerplate\\_class\n* pg.catch\\_errors\n* pg.clone\n* pg.compound\n* pg.compound\\_class\n* pg.contains\n* pg.detour\n* pg.diff\n* pg.dna\\_spec\n* pg.docstr\n* pg.enable\\\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'pyglove' API to perform network analysis. The program should analyze a network (e.g., social network or computer network) by loading a dataset, applying network analysis techniques, and outputting relevant statistics or insights. Additionally, provide a unit test that verifies the accuracy of the network analysis results.\n\n### Code:"
    },
    {
        "api": "pyglove",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install pyglove\nimport pyglove as pg\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport numpy as np\n\ndef download_and_preprocess_documents():\n  # Replace this with your code to download and preprocess a collection of text documents\n  documents = [\n    \"This is the first document.\",\n    \"This document is the second document.\",\n    \"And this is the third one.\",\n    \"Is this the first document?\"\n  ]\n  return documents\n\ndef apply_lda(documents, num_topics=5):\n  # Convert documents to document-term matrix\n  vectorizer = CountVectorizer()\n  dtm = vectorizer.fit_transform(documents)\n\n  # Apply Latent Dirichlet Allocation\n  lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)\n  lda.fit(dtm)\n\n  return vectorizer, lda\n\ndef extract_topics_and_words(vectorizer, lda, num_words=10):\n  feature_names = vectorizer.get_feature_names_out()\n  topics = []\n  for topic_idx, topic in enumerate(lda.components_):\n    top_words_idx = topic.argsort()[:-num_words - 1:-1]\n    top_words = [feature_names[i] for i in top_words_idx]\n    topics.append({'topic': topic_idx, 'words': top_words})\n  return topics\n\nif __name__ == '__main__':\n  documents = download_and_preprocess_documents()\n  num_topics = 3\n  vectorizer, lda = apply_lda(documents, num_topics)\n  topics = extract_topics_and_words(vectorizer, lda)\n\n  for topic in topics:\n    print(f'Topic {topic[\"topic\"]}: {\", \".join(topic[\"words\"])}')\n",
        "instruction": "Create a Python program using the 'pyglove' API to perform topic modeling on a collection of text documents. The program should download and preprocess a collection of text documents, apply Latent Dirichlet Allocation (LDA) to the documents, and output the topics and their associated words. Additionally, provide a unit test that verifies the presence of topics and associated words in the output.",
        "prediction": ":import numpy as np\nfrom pyglove import WordEmbedding, gst, lda\n\n# Download and preprocess the documents\ndef download_and_preprocess_documents():\n  # Replace this with your code to download and preprocess the documents\n  # The example below downloads and preprocess a collection of text documents\n  text_docs = [\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n    \"banana is a fruit.\",\n    \"cherry is a fruit.\",\n    \"apple is a fruit.\",\n",
        "source": "This is some documentation of pyglove API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# Public API: pyglove#\n\n\n## Modules#\n\n\n### core#\n\n* pg.detouring\n* pg.geno\n* pg.hyper\n* pg.object\\_utils\n* pg.patching\n* pg.symbolic\n* pg.tuning\n* pg.typing\n\n\n### ext#\n\n* pg.early\\_stopping\n* pg.evolution\n* pg.mutfun\n* pg.scalars\n\n\n\n## Top-level shortcurts#\n\n\n### Objects#\n\n\n* pg.MISSING\\_VALUE\n\n\n\n### Classes#\n\n\n* pg.ClassWrapper\n* pg.CustomTyping\n* pg.DNA\n* pg.DNAGenerator\n* pg.DNASpec\n* pg.Dict\n* pg.Diff\n* pg.DocStr\n* pg.Field\n* pg.FieldUpdate\n* pg.Formattable\n* pg.Functor\n* pg.Inferentiable\n* pg.InferredValue\n* pg.Insertion\n* pg.JSONConvertible\n* pg.KeyPath\n* pg.KeySpec\n* pg.List\n* pg.MaybePartial\n* pg.Object\n* pg.ObjectFactory\n* pg.Origin\n* pg.PureSymbolic\n* pg.Ref\n* pg.Schema\n* pg.Symbolic\n* pg.TraverseAction\n* pg.ValueSpec\n* pg.WritePermissionError\n* pg.dict\n* pg.list\n\n\n### Functions#\n\n\n* pg.allow\\_empty\\_field\\_description\n* pg.allow\\_partial\n* pg.allow\\_repeated\\_class\\_registration\n* pg.allow\\_writable\\_accessors\n* pg.apply\\_wrappers\n* pg.as\\_sealed\n* pg.auto\\_call\\_functors\n* pg.boilerplate\\_class\n* pg.catch\\_errors\n* pg.clone\n* pg.compound\n* pg.compound\\_class\n* pg.contains\n* pg.detour\n* pg.diff\n* pg.dna\\_spec\n* pg.docstr\n* pg.enable\\\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'pyglove' API to perform topic modeling on a collection of text documents. The program should download and preprocess a collection of text documents, apply Latent Dirichlet Allocation (LDA) to the documents, and output the topics and their associated words. Additionally, provide a unit test that verifies the presence of topics and associated words in the output.\n\n### Code:"
    },
    {
        "api": "pymc",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install pymc arviz\nfrom pymc import HalfCauchy, Model, Normal, sample\nimport arviz as az\nimport numpy as np\n\nrng = np.random.default_rng(8927)\nsize = 200\ntrue_intercept = 1\ntrue_slope = 2\n\nx = np.linspace(0, 1, size)\n# y = a + b*x\ntrue_regression_line = true_intercept + true_slope * x\n# add noise\ny = true_regression_line + rng.normal(scale=0.5, size=size)\n\n\nwith Model() as model:  \n    # Define priors\n    sigma = HalfCauchy(\"sigma\", beta=10)\n    intercept = Normal(\"Intercept\", 0, sigma=20)\n    slope = Normal(\"slope\", 0, sigma=20)\n\n    # Define likelihood\n    likelihood = Normal(\"y\", mu=intercept + slope * x, sigma=sigma, observed=y)\n\n    # Inference!\n    # draw 30 posterior samples using NUTS sampling\n    idata = sample(30)\n\nprint(\"30 posterior samples using NUTS sampling: \")\nprint(az.summary(idata, round_to=2))",
        "instruction": "Create a Python program using the 'pymc' API to perform Bayesian linear regression on a dataset. The program should define prior distributions for the model's parameters, including the intercept and slope, and use a likelihood function to model the observed data. After setting up the model, perform Bayesian inference to draw posterior samples using NUTS sampling.",
        "prediction": ":import pymc as pymc\nfrom pymc import HalfCauchy, Normal, Model, Normal, sample\nimport numpy as np\n\nrng = np.random.default_rng(8927)\nsize = 100\ntrue_intercept = 0\ntrue_slope = 1\n\n# Simulate a dataset\n# Simulate data\n# use rng\ntrue_true_slope = 1\ntrue_true_intercept = 0\nno_samples = 1000\ntrue_true_slope = rng.normal(true_true_slope, 10)\ntrue_true_intercept = rng.normal(true_true_intercept, 10)\n\n# Simulate data\na = true_true_intercept\nb = true_true_slope\nno_data = rng.normal(a, b, no_samples)\n\nprint(\"Simulated data: \")\nprint(no_data)\n\nwith Model() as model:\n    # Define prior\n    intercept = Normal(\"intercept\", mu=0, sigma=10)\n    slope = Normal(\"slope\", mu=0, sigma=10)\n\n    # Define likelihood\n    likelihood = Normal(\"likelihood\", mu=intercept, sigma=1, observed=no_data)\n\n    # Inference!\n    # draw 1000 posterior samples using NUTS sampling\n    idata = sample(1000, tune=500, step=nuto_arviz.NUTS(), progressbar=False)\n",
        "source": "This is some documentation of pymc API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# API#\n\n* Distributions\n* Gaussian Processes\n* Model\n* Samplers\n* Variational Inference\n* Sequential Monte Carlo\n* Data\n* Ordinary differential equations (ODEs)\n* Probability\n* Tuning\n* Math\n* PyTensor utils\n* shape\\_utils\n* Storage backends\n* Other utils\n\n\n\n## Dimensionality#\n\n\nPyMC provides numerous methods, and syntactic sugar, to easily specify the dimensionality of\nRandom Variables in modeling. Refer to Distribution Dimensionality notebook to see examples\ndemonstrating the functionality.\n\n\n\n## API extensions#\n\n\n### Plots, stats and diagnostics#\n\n\nPlots, stats and diagnostics are delegated to the\nArviZ.\nlibrary, a general purpose library for\n\u201cexploratory analysis of Bayesian models\u201d.\n\n\n* Functions from the arviz.plots module are available through `pymc.<function>` or `pymc.plots.<function>`,\nbut for their API documentation please refer to the ArviZ documentation.\n* Functions from the arviz.stats module are available through `pymc.<function>` or `pymc.stats.<function>`,\nbut for their API documentation please refer to the ArviZ documentation.\n\n\nArviZ is a dependency of PyMC and so, in addition to the locations described above,\nimporting ArviZ and using `arviz.<function>` will also work without any extra installation.\n\n\n\n### Generalized Linear Models (GLMs)#\n\n\nGeneralized Linear Models are delegated to the\nBambi.\nlibrary, a high-level Bayesian model-building\ninterface built on top of PyMC.\n\n\nBambi is not a dependency of PyMC and should be installed in addition to PyMC\nto use it to generate PyMC models via formula syntax.\n\n\n# Learn PyMC & Bayesian modeling#\n\n* Installation\n* Notebooks on core features\n* Books\n* Videos and Podcasts\n* Consulting\n* Glossary\n\n\n\n## At a glance#\n\n\n### Beginner#\n\n\n* Book: Bayesian Methods for Hackers\n* Book: Bayesian Analysis with Python\n\n\n\n### Intermediate#\n\n\n* Introductory\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'pymc' API to perform Bayesian linear regression on a dataset. The program should define prior distributions for the model's parameters, including the intercept and slope, and use a likelihood function to model the observed data. After setting up the model, perform Bayesian inference to draw posterior samples using NUTS sampling.\n\n### Code:"
    },
    {
        "api": "pymc",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install pymc arviz\nfrom pymc import Model, Normal, sample\nimport arviz as az\nimport numpy as np\n\nrng = np.random.default_rng(8927)\nsize = 100\ntrue_mean = 0\ntrue_std = 1\n\n# Simulate observed continuous outcomes\noutcomes = rng.normal(true_mean, true_std, size=size)\n\nwith Model() as model:\n    # Define prior\n    mean = Normal(\"mean\", mu=0, sigma=10)\n    std = Normal(\"std\", mu=1, sigma=10)\n\n    # Define likelihood\n    likelihood = Normal(\"likelihood\", mu=mean, sigma=std, observed=outcomes)\n\n    # Inference!\n    # draw 1000 posterior samples using Hamiltonian Monte Carlo (HMC) sampling\n    idata = sample(1000, tune=500)\n\nprint(\"1000 posterior samples using Hamiltonian Monte Carlo (HMC) sampling: \")\nprint(az.summary(idata, round_to=2))",
        "instruction": "Create a Python program using the 'pymc' API to perform Bayesian inference on a dataset of observed continuous outcomes. The program should define a prior distribution for the mean and standard deviation, and use a likelihood function to model the observed outcomes. After setting up the model, perform Bayesian inference to draw posterior samples using Hamiltonian Monte Carlo (HMC) sampling.",
        "prediction": ":import pymc as pymc\nfrom pymc import HalfCauchy, Model, Normal, sample\nimport numpy as np\n\nrng = np.random.default_rng(8927)\nsize = 100\ntrue_mean = 0\ntrue_std = 1\n\n# Simulate observed continuous outcomes\noutcomes = rng.normal(true_mean, true_std, size=size)\n\nwith Model() as model:\n    # Define prior\n    mean = Normal(\"mean\", mu=0, sigma=10)\n    std = HalfCauchy(\"std\", beta=10)\n\n    # Define likelihood\n    likelihood = Normal(\"likelihood\", mu=mean, sigma=std, observed=outcomes)\n\n    # Inference!\n    # draw 1000 posterior samples using Hamiltonian Monte Carlo (HMC) sampling\n    idata = sample(1000, tune=500)\n\nprint(\"1000 posterior samples using Hamiltonian Monte Carlo (HMC) sampling: \")\nprint(idata)",
        "source": "This is some documentation of pymc API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# API#\n\n* Distributions\n* Gaussian Processes\n* Model\n* Samplers\n* Variational Inference\n* Sequential Monte Carlo\n* Data\n* Ordinary differential equations (ODEs)\n* Probability\n* Tuning\n* Math\n* PyTensor utils\n* shape\\_utils\n* Storage backends\n* Other utils\n\n\n\n## Dimensionality#\n\n\nPyMC provides numerous methods, and syntactic sugar, to easily specify the dimensionality of\nRandom Variables in modeling. Refer to Distribution Dimensionality notebook to see examples\ndemonstrating the functionality.\n\n\n\n## API extensions#\n\n\n### Plots, stats and diagnostics#\n\n\nPlots, stats and diagnostics are delegated to the\nArviZ.\nlibrary, a general purpose library for\n\u201cexploratory analysis of Bayesian models\u201d.\n\n\n* Functions from the arviz.plots module are available through `pymc.<function>` or `pymc.plots.<function>`,\nbut for their API documentation please refer to the ArviZ documentation.\n* Functions from the arviz.stats module are available through `pymc.<function>` or `pymc.stats.<function>`,\nbut for their API documentation please refer to the ArviZ documentation.\n\n\nArviZ is a dependency of PyMC and so, in addition to the locations described above,\nimporting ArviZ and using `arviz.<function>` will also work without any extra installation.\n\n\n\n### Generalized Linear Models (GLMs)#\n\n\nGeneralized Linear Models are delegated to the\nBambi.\nlibrary, a high-level Bayesian model-building\ninterface built on top of PyMC.\n\n\nBambi is not a dependency of PyMC and should be installed in addition to PyMC\nto use it to generate PyMC models via formula syntax.\n\n\n# Learn PyMC & Bayesian modeling#\n\n* Installation\n* Notebooks on core features\n* Books\n* Videos and Podcasts\n* Consulting\n* Glossary\n\n\n\n## At a glance#\n\n\n### Beginner#\n\n\n* Book: Bayesian Methods for Hackers\n* Book: Bayesian Analysis with Python\n\n\n\n### Intermediate#\n\n\n* Introductory\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'pymc' API to perform Bayesian inference on a dataset of observed continuous outcomes. The program should define a prior distribution for the mean and standard deviation, and use a likelihood function to model the observed outcomes. After setting up the model, perform Bayesian inference to draw posterior samples using Hamiltonian Monte Carlo (HMC) sampling.\n\n### Code:"
    },
    {
        "api": "pypdf",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install pypdf\nimport pypdf\n\n# Specify the path of the PDF file with embedded files\npdf_file_path = \"document_with_attachments.pdf\"\n\n# Open the PDF file in read-binary mode\nwith open(pdf_file_path, \"rb\") as pdf_file:\n    # Create a PDF reader object\n    pdf_reader = pypdf.PdfReader(pdf_file)\n\n    # Initialize a list to store the names of embedded files\n    embedded_files = []\n\n    # Check if the PDF has file attachments\n    if '/EmbeddedFiles' in pdf_reader.Info:\n        embedded_file_dict = pdf_reader.Info.EmbeddedFiles\n        for key, value in embedded_file_dict.items():\n            embedded_files.append(key)\n\n    # Display the list of embedded files\n    for file_name in embedded_files:\n        print(file_name)\n",
        "instruction": "Create a Python program using the 'pypdf' API to extract and display the list of all embedded files (attachments) in a PDF file.",
        "prediction": ":import pypdf\nimport pypdf\n\n# Specify the path of the PDF file with embedded files\npdf_file_path = \"document_with_embedded_files.pdf\"\n\n# Open the PDF file in read-binary mode\nwith open(pdf_file_path, \"rb\") as pdf_file:\n    # Create a PDF reader object\n    pdf_reader = pypdf.PdfReader(pdf_file)\n\n    # Extract embedded files from the PDF\n    embedded_files = pdf_reader.Page.EmbeddedFiles\n\n    # Display the list of embedded files\n    for embedded_file in embedded_files:\n        print(embedded_file)\n",
        "source": "This is some documentation of pypdf API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n Example: This will launch the print window when the PDF is opened.\n\n```\n\naddJS(*javascript: str*) \u2192 None[source]\uf0c1\nUse `add\\_js()` instead.\n\n\nadd\\_attachment(*filename: str*, *data: Union[str, bytes]*) \u2192 None[source]\uf0c1\nEmbed a file inside the PDF.\n\n\nReference:\nhttps://www.adobe.com/content/dam/Adobe/en/devnet/acrobat/pdfs/PDF32000\\_2008.pdf\nSection 7.11.3\n\nParameters\n* **filename** \u2013 The filename to display.\n* **data** \u2013 The data in the file.\n\naddAttachment(*fname: str*, *fdata: Union[str, bytes]*) \u2192 None[source]\uf0c1\nUse `add\\_attachment()` instead.\n\n\nappend\\_pages\\_from\\_reader(*reader: PdfReader*, *after\\_page\\_append: Optional[Callable[[PageObject], None]] = None*) \u2192 None[source]\uf0c1\nCopy pages from reader to writer. Includes an optional callback\nparameter which is invoked after pages are appended to the writer.\n\n\n`append` should be prefered.\n\nParameters\n* **reader** \u2013 a PdfReader object from which to copy page\nannotations to this writer object. The writer\u2019s annots\nwill then be updated\n* **after\\_page\\_append** \u2013 Callback function that is invoked after each page is appended to\nthe writer. Signature includes a reference to the appended page\n(delegates to append\\_pages\\_from\\_reader). The single parameter of\nthe callback is a reference to the page just appended to the\ndocument.\n\nappendPagesFromReader(*reader: PdfReader*, *after\\_page\\_append: Optional[Callable[[PageObject], None]] = None*) \u2192 None[source]\uf0c1\nUse `append\\_pages\\_from\\_reader()` instead.\n\n\nupdate\\_page\\_form\\_field\\_values(*page: ~pypdf.\\_page.PageObject, fields: ~typing.Dict[str, ~\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'pypdf' API to extract and display the list of all embedded files (attachments) in a PDF file.\n\n### Code:"
    },
    {
        "api": "pypdf",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install pypdf\nimport pypdf\n\n# Specify the path of the PDF file to analyze page dimensions\npdf_file_path = \"document.pdf\"\n\n# Open the PDF file in read-binary mode\nwith open(pdf_file_path, \"rb\") as pdf_file:\n    # Create a PDF reader object\n    pdf_reader = pypdf.PdfReader(pdf_file)\n\n    # Initialize a list to store page dimensions and orientation\n    page_info = []\n\n    # Iterate through the pages and extract page dimensions and orientation\n    for page in pdf_reader.pages:\n        page_width = page.MediaBox[2] - page.MediaBox[0]\n        page_height = page.MediaBox[3] - page.MediaBox[1]\n        orientation = \"Portrait\" if page_width <= page_height else \"Landscape\"\n        page_info.append(f\"Page {page.page_number}: {page_width} x {page_height} ({orientation})\")\n\n    # Display the extracted page information\n    for info in page_info:\n        print(info)\n",
        "instruction": "Create a Python program using the 'pypdf' API to extract and display the page dimensions and orientation (portrait or landscape) of each page in a PDF file.",
        "prediction": ":import pypdf\n\n# Load the PDF file in a PdfReader object\npdf_file_path = \"document_with_page_dimensions.pdf\"\nwith open(pdf_file_path, \"rb\") as pdf_file:\n    pdf_reader = pypdf.PdfReader(pdf_file)\n\n    # Extract page dimensions and orientation (portrait or landscape) for each page\n    for page in pdf_reader.pages:\n        print(f\"Page {page.page_number}: Page dimensions: {page.page_dimension}\")\n        print(f\"Page {page.page_number}: Portrait or landscape: {page.orientation}\")\n",
        "source": "This is some documentation of pypdf API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n The annotations module\uf0c1\n\n\nPDF specifies several annotation types which pypdf makes available here.\n\n\nThe names of the annotations and their attributes do not reflect the names in\nthe specification in all cases. For example, the PDF standard defines a\n\u2018Square\u2019 annotation that does not actually need to be square. For this reason,\npypdf calls it \u2018Rectangle\u2019.\n\n\nAt their core, all annotation types are DictionaryObjects. That means if pypdf\ndoes not implement a feature, users can easily extend the given functionality.\n\n\n*class* pypdf.annotations.AnnotationDictionary[source]\uf0c1\nBases: `DictionaryObject`, `ABC`\n\n\n*property* flags*: pypdf.constants.AnnotationFlag*\uf0c1\n\n\n*class* pypdf.annotations.MarkupAnnotation(*\\**, *title\\_bar: Optional[str] = None*)[source]\uf0c1\nBases: `AnnotationDictionary`, `ABC`\n\n\nBase class for all markup annotations.\n\nParameters\n**title\\_bar** \u2013 Text to be displayed in the title bar of the annotation;\nby convention this is the name of the author\n\n*class* pypdf.annotations.Ellipse(*rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *\\**, *interiour\\_color: Optional[str] = None*, *\\*\\*kwargs: Any*)[source]\uf0c1\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.FreeText(*\\**, *text: str*, *rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *font: str = 'Helvetica'*, *bold: bool = False*, *italic: bool = False*, *font\\_size: str = '14pt'*, *font\\_color: str = '000000'*, *border\\_color: Optional[str] = '000000'*, *background\\_color: Optional[str] = 'ffffff'*, *\\*\\*kwargs: Any*)[source]\uf0c1\nBases: `MarkupAnnotation`\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'pypdf' API to extract and display the page dimensions and orientation (portrait or landscape) of each page in a PDF file.\n\n### Code:"
    },
    {
        "api": "pypdf",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install pypdf\nimport pypdf\n\n# Specify the path of the PDF file\npdf_file_path = \"document.pdf\"\n\n# Specify the keyword or title to search for\nsearch_keyword = \"Chapter 3\"\n\n# Open the PDF file in read-binary mode\nwith open(pdf_file_path, \"rb\") as pdf_file:\n    # Create a PDF reader object\n    pdf_reader = pypdf.PdfReader(pdf_file)\n\n    # Initialize variables to track the start and end of the section\n    section_start = None\n    section_end = None\n\n    # Iterate through the pages and find the section based on the keyword\n    for page_num, page in enumerate(pdf_reader.pages, start=1):\n        page_text = page.extract_text()\n        if search_keyword in page_text:\n            if section_start is None:\n                section_start = page_num\n            section_end = page_num\n\n    # Check if the section is found\n    if section_start is not None:\n        # Extract and display the content of the section\n        section_text = \"\"\n        for page_num in range(section_start, section_end + 1):\n            page = pdf_reader.pages[page_num - 1]\n            section_text += page.extract_text()\n        print(section_text)\n    else:\n        print(f\"Section '{search_keyword}' not found.\")\n",
        "instruction": "Create a Python program using the 'pypdf' API to extract and display the content of a specific section (e.g., chapter) from a PDF file based on a keyword or title.",
        "prediction": ":pip install pypdf\nimport pypdf\n\n# Specify the path of the PDF file with book pages\npdf_file_path = \"document_with_section.pdf\"\n\n# Specify a section in the PDF file based on a keyword or title\nkeyword = \"Introduction\"\n\n# Open the PDF file in read-binary mode\nwith open(pdf_file_path, \"rb\") as pdf_file:\n    # Create a PDF reader object\n    pdf_reader = pypdf.PdfReader(pdf_file)\n\n    # Initialize a list to store the content of the pages\n    content = []\n\n    # Iterate through the pages and extract content based on the keyword\n    for page in pdf_reader.pages:\n        content.append(page.extract_text(x_t=keyword))\n\n    # Display the content\n    for page in content:\n        print(page)\n",
        "source": "This is some documentation of pypdf API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n The annotations module\uf0c1\n\n\nPDF specifies several annotation types which pypdf makes available here.\n\n\nThe names of the annotations and their attributes do not reflect the names in\nthe specification in all cases. For example, the PDF standard defines a\n\u2018Square\u2019 annotation that does not actually need to be square. For this reason,\npypdf calls it \u2018Rectangle\u2019.\n\n\nAt their core, all annotation types are DictionaryObjects. That means if pypdf\ndoes not implement a feature, users can easily extend the given functionality.\n\n\n*class* pypdf.annotations.AnnotationDictionary[source]\uf0c1\nBases: `DictionaryObject`, `ABC`\n\n\n*property* flags*: pypdf.constants.AnnotationFlag*\uf0c1\n\n\n*class* pypdf.annotations.MarkupAnnotation(*\\**, *title\\_bar: Optional[str] = None*)[source]\uf0c1\nBases: `AnnotationDictionary`, `ABC`\n\n\nBase class for all markup annotations.\n\nParameters\n**title\\_bar** \u2013 Text to be displayed in the title bar of the annotation;\nby convention this is the name of the author\n\n*class* pypdf.annotations.Ellipse(*rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *\\**, *interiour\\_color: Optional[str] = None*, *\\*\\*kwargs: Any*)[source]\uf0c1\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.FreeText(*\\**, *text: str*, *rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *font: str = 'Helvetica'*, *bold: bool = False*, *italic: bool = False*, *font\\_size: str = '14pt'*, *font\\_color: str = '000000'*, *border\\_color: Optional[str] = '000000'*, *background\\_color: Optional[str] = 'ffffff'*, *\\*\\*kwargs: Any*)[source]\uf0c1\nBases: `MarkupAnnotation`\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'pypdf' API to extract and display the content of a specific section (e.g., chapter) from a PDF file based on a keyword or title.\n\n### Code:"
    },
    {
        "api": "pypdf",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install pypdf\nimport pypdf\n\n# Specify the path of the PDF file\npdf_file_path = \"document.pdf\"\n\n# Specify the page number to extract (e.g., page 3)\npage_number_to_extract = 3\n\n# Open the PDF file in read-binary mode\nwith open(pdf_file_path, \"rb\") as pdf_file:\n    # Create a PDF reader object\n    pdf_reader = pypdf.PdfReader(pdf_file)\n\n    # Check if the specified page number is within the valid range\n    if 1 <= page_number_to_extract <= len(pdf_reader.pages):\n        # Extract and display the content of the specified page\n        page = pdf_reader.pages[page_number_to_extract - 1]\n        page_text = page.extract_text()\n        print(page_text)\n    else:\n        print(\"Page number is out of range.\")\n",
        "instruction": "Create a Python program using the 'pypdf' API to extract and display the content of a specific page (e.g., page 3) from a PDF file.",
        "prediction": ":import pypdf\nimport pypdf\n\n# Specify the path of the PDF file with page 3 (e.g., 3) as page_num\npdf_file_path = \"sample.pdf\"\n\n# Open the PDF file in read-binary mode\nwith open(pdf_file_path, \"rb\") as pdf_file:\n    # Create a PDF reader object\n    pdf_reader = pypdf.PdfReader(pdf_file)\n\n    # Extract and display the content of the page\n    display_extracted_content(pdf_reader, 3)\n",
        "source": "This is some documentation of pypdf API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n Example: This will launch the print window when the PDF is opened.\n\n```\n\naddJS(*javascript: str*) \u2192 None[source]\uf0c1\nUse `add\\_js()` instead.\n\n\nadd\\_attachment(*filename: str*, *data: Union[str, bytes]*) \u2192 None[source]\uf0c1\nEmbed a file inside the PDF.\n\n\nReference:\nhttps://www.adobe.com/content/dam/Adobe/en/devnet/acrobat/pdfs/PDF32000\\_2008.pdf\nSection 7.11.3\n\nParameters\n* **filename** \u2013 The filename to display.\n* **data** \u2013 The data in the file.\n\naddAttachment(*fname: str*, *fdata: Union[str, bytes]*) \u2192 None[source]\uf0c1\nUse `add\\_attachment()` instead.\n\n\nappend\\_pages\\_from\\_reader(*reader: PdfReader*, *after\\_page\\_append: Optional[Callable[[PageObject], None]] = None*) \u2192 None[source]\uf0c1\nCopy pages from reader to writer. Includes an optional callback\nparameter which is invoked after pages are appended to the writer.\n\n\n`append` should be prefered.\n\nParameters\n* **reader** \u2013 a PdfReader object from which to copy page\nannotations to this writer object. The writer\u2019s annots\nwill then be updated\n* **after\\_page\\_append** \u2013 Callback function that is invoked after each page is appended to\nthe writer. Signature includes a reference to the appended page\n(delegates to append\\_pages\\_from\\_reader). The single parameter of\nthe callback is a reference to the page just appended to the\ndocument.\n\nappendPagesFromReader(*reader: PdfReader*, *after\\_page\\_append: Optional[Callable[[PageObject], None]] = None*) \u2192 None[source]\uf0c1\nUse `append\\_pages\\_from\\_reader()` instead.\n\n\nupdate\\_page\\_form\\_field\\_values(*page: ~pypdf.\\_page.PageObject, fields: ~typing.Dict[str, ~\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'pypdf' API to extract and display the content of a specific page (e.g., page 3) from a PDF file.\n\n### Code:"
    },
    {
        "api": "pytorch-lightning",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install pytorch-lightning\nimport torch\nimport pytorch_lightning as pl\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\n\nclass DataModule(pl.LightningDataModule):\n    def __init__(\n        self,\n        model_name=\"google/bert_uncased_L-2_H-128_A-2\",\n        batch_size=64,\n        max_length=128,\n    ):\n        super().__init__()\n\n        self.batch_size = batch_size\n        self.max_length = max_length\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    def prepare_data(self):\n        cola_dataset = load_dataset(\"glue\", \"cola\")\n        self.train_data = cola_dataset[\"train\"]\n        self.val_data = cola_dataset[\"validation\"]\n\n    def tokenize_data(self, example):\n        return self.tokenizer(\n            example[\"sentence\"],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_length,\n        )\n\n    def setup(self, stage=None):\n        if stage == \"fit\" or stage is None:\n            self.train_data = self.train_data.map(self.tokenize_data, batched=True)\n            self.train_data.set_format(\n                type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n            )\n\n            self.val_data = self.val_data.map(self.tokenize_data, batched=True)\n            self.val_data.set_format(\n                type=\"torch\",\n                columns=[\"input_ids\", \"attention_mask\", \"label\"],\n                output_all_columns=True,\n            )\n\n    def train_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.train_data, batch_size=self.batch_size, shuffle=True\n        )\n\n    def val_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.val_data, batch_size=self.batch_size, shuffle=False\n        )\n\n\nif __name__ == \"__main__\":\n    data_model = DataModule()\n    data_model.prepare_data()\n    data_model.setup()\n    print(next(iter(data_model.train_dataloader()))[\"input_ids\"].shape)",
        "instruction": "Create a Python program using the 'pytorch-lightning' API to define a 'DataModule' class that prepares and tokenizes data for a natural language processing task. The 'DataModule' should load a dataset using the 'datasets' and 'transformers' libraries, tokenize the data with a specific model tokenizer, and set up data loaders. The program should demonstrate the use of 'pytorch-lightning' to create a 'DataModule' instance, prepare data, set it up, and print the shape of a batch from the data loader.",
        "prediction": ":import\nfrom pytorch_lightning import LightningDataModule\nimport tokenizers\nfrom transformers import AutoTokenizer\n\n# Define a DataModule for preparing and tokenizing data for a natural language processing task\nclass DataModule(LightningDataModule):\n    def __init__(self):\n        super().__init__()\n        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n        self.dataset = datasets\n        \n    def prepare_data(self):\n        pass\n    \n    def setup(self, stage=None):\n        if stage == \"fit\" or stage is None:\n            self.data = datasets\n            \n    def tokenize_data(self, batch):\n        return tokenizer(batch[\"X\"], padding=\"max_length\", truncation=True, max_length=512)\n    \n    def train_dataloader(self):\n        return data. combined_loader()\n    \n    def test_dataloader(self):\n        return data. combined_loader()\n    \n    def val_dataloader(self):\n        return data. combined_loader()\n    \n    def get_shape(self):\n        return data_loader\n    \n    def to_torch_dataset():\n        return datasets\n\n#\ufffd Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library\n# Install the library",
        "source": "This is some documentation of pytorch-lightning API:\n\n=====\n\n\n==================\n Document 0 \n----------------\nstrategies\u00b6\n\n\n|  |  |\n| --- | --- |\n| `DDPStrategy` | Strategy for multi-process single-device training on one or multiple nodes. |\n| `DeepSpeedStrategy` | Provides capabilities to run training using the DeepSpeed library, with training optimizations for large billion parameter models. |\n| `FSDPStrategy` | Strategy for Fully Sharded Data Parallel provided by torch.distributed. |\n| `ParallelStrategy` | Strategy for training with multiple processes in parallel. |\n| `SingleDeviceStrategy` | Strategy that handles communication on a single device. |\n| `SingleDeviceXLAStrategy` | Strategy for training on a single XLA device. |\n| `Strategy` | Base class for all strategies that change the behaviour of the training, validation and test- loop. |\n| `XLAStrategy` | Strategy for training multiple TPU devices using the `torch\\_xla.distributed.xla\\_multiprocessing.spawn()` method. |\n\n\n# tuner\u00b6\n\n\n|  |  |\n| --- | --- |\n| `Tuner` | Tuner class to tune your model. |\n\n\n\n# utilities\u00b6\n\n\n|  |  |\n| --- | --- |\n| `combined\\_loader` |  |\n| `data` |  |\n| `deepspeed` | Utilities that can be used with Deepspeed. |\n| `memory` | Utilities related to memory. |\n| `model\\_summary` |  |\n| `parsing` | Utilities used for parameter parsing. |\n| `rank\\_zero` | Utilities that can be used for calling functions on a particular rank. |\n| `seed` | Utilities to help with reproducibility of models. |\n| `warnings` | Warning-related utilities. |\n\n==================\n Document 1 \n----------------\n\n\n# accelerators\u00b6\n\n\n|  |  |\n| --- | --- |\n| `Accelerator` | The Accelerator base class for Lightning PyTorch. |\n| `CPUAccelerator` | Accelerator for CPU devices. |\n| `CUDAAccelerator` | Accelerator for NVIDIA CUDA devices. |\n| `XLAAc\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'pytorch-lightning' API to define a 'DataModule' class that prepares and tokenizes data for a natural language processing task. The 'DataModule' should load a dataset using the 'datasets' and 'transformers' libraries, tokenize the data with a specific model tokenizer, and set up data loaders. The program should demonstrate the use of 'pytorch-lightning' to create a 'DataModule' instance, prepare data, set it up, and print the shape of a batch from the data loader.\n\n### Code:"
    },
    {
        "api": "pytorch-lightning",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install pytorch-lightning\nimport pytorch_lightning as pl\nimport torch\n\n# Define a custom callback for logging losses\nclass LossLoggingCallback(pl.Callback):\n    def on_epoch_end(self, trainer, pl_module):\n        train_loss = trainer.callback_metrics[\"train_loss_step\"]\n        val_loss = trainer.callback_metrics[\"val_loss_step\"]\n        print(f\"Epoch {trainer.current_epoch}: Train Loss - {train_loss:.4f}, Val Loss - {val_loss:.4f}\")\n\n# Create a LightningModule for demonstration\nclass ExampleModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(10, 1)\n\n    def forward(self, x):\n        return self.l1(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = torch.nn.functional.mse_loss(y_hat, y)\n        self.log(\"train_loss_step\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = torch.nn.functional.mse_loss(y_hat, y)\n        self.log(\"val_loss_step\", loss)\n\nif __name__ == \"__main__\":\n    model = ExampleModel()\n\n    trainer = pl.Trainer(\n        gpus=0,\n        max_epochs=5,\n        callbacks=[LossLoggingCallback()],\n    )\n\n    trainer.fit(model)",
        "instruction": "Create a Python program using the 'pytorch-lightning' API to define a custom callback that logs the training loss and validation loss during training. The program should demonstrate the use of this custom callback to monitor and log losses during model training.",
        "prediction": ":import pytorch_lightning as pl\nfrom pytorch_lightning import LightningModule\nfrom pytorch_lightning import loggers\n\n# Define a LightningModule for demonstration\nclass ExampleModel(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(10, name=\"l1\")\n        self.loss = torch.nn.L1Loss()\n\n    def forward(self, x):\n        return self.l1(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.loss(y_hat, y)\n        self.log(\"train_loss_step\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.loss(y_hat, y)\n        self.log(\"val_loss_step\", loss)\n\nif __name__ == \"__main__\":\n    model = ExampleModel()\n\n    log_every_n_steps = 1\n    trainer = pl.Trainer(max_steps=10, callbacks=[ModelSummary(\"10_step_epoch\", log_every_n_steps=log_every_n_steps)])\n\n)",
        "source": "This is some documentation of pytorch-lightning API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n callbacks\u00b6\n\n\n|  |  |\n| --- | --- |\n| `BackboneFinetuning` | Finetune a backbone model based on a learning rate user-defined scheduling. |\n| `BaseFinetuning` | This class implements the base logic for writing your own Finetuning Callback. |\n| `BasePredictionWriter` | Base class to implement how the predictions should be stored. |\n| `BatchSizeFinder` | The `BatchSizeFinder` callback tries to find the largest batch size for a given model that does not give an out of memory (OOM) error. |\n| `Callback` | Abstract base class used to build new callbacks. |\n| `DeviceStatsMonitor` | Automatically monitors and logs device stats during training, validation and testing stage. |\n| `EarlyStopping` | Monitor a metric and stop training when it stops improving. |\n| `GradientAccumulationScheduler` | Change gradient accumulation factor according to scheduling. |\n| `LambdaCallback` | Create a simple callback on the fly using lambda functions. |\n| `LearningRateFinder` | The `LearningRateFinder` callback enables the user to do a range test of good initial learning rates, to reduce the amount of guesswork in picking a good starting learning rate. |\n| `LearningRateMonitor` | Automatically monitor and logs learning rate for learning rate schedulers during training. |\n| `ModelCheckpoint` | Save the model periodically by monitoring a quantity. |\n| `ModelPruning` | Model pruning Callback, using PyTorch's prune utilities. |\n| `ModelSummary` | Generates a summary of all layers in a `LightningModule`. |\n| `OnExceptionCheckpoint` | Used to save a checkpoint on exception. |\n| `ProgressBar` | The base class for progress bars in Lightning. |\n| `RichModelSummary` | Generates a summary of all layers in a `LightningModule` with rich text formatting. |\n| `RichProgressBar` | Create a progress bar with rich text formatting.\n |\n| `StochasticWeightAveraging` | Implements the Stochastic Weight Averaging (SWA) Callback to average a model. |\n| `Timer` | The Tim\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'pytorch-lightning' API to define a custom callback that logs the training loss and validation loss during training. The program should demonstrate the use of this custom callback to monitor and log losses during model training.\n\n### Code:"
    },
    {
        "api": "rapidfuzz",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install rapidfuzz\nfrom rapidfuzz import fuzz, process\n\ntarget_strings = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\", \"fig\", \"grape\"]\n\nquery_string = \"cranberry\"\n\nprint(\"Query: \", query_string)\n\nresults = process.extract(query_string, target_strings, scorer=fuzz.WRatio, limit=len(target_strings))\n\nfor result in results:\n    target, score, _ = result\n    token_ratio_score = fuzz.token_ratio(query_string, target)\n    print(f\"{target}: Weighted Ratio Score {score}, Token Ratio Score {token_ratio_score}\")",
        "instruction": "Create a Python program using the 'rapidfuzz' API to demonstrate fuzzy string matching. The program should take a query string and compare it to a list of target strings using the 'fuzz.WRatio' scorer from 'rapidfuzz'. Print the query string and the similarity score for each target string. Additionally, the program should calculate and print the token ratio similarity score for each target string. The goal is to find similar strings in the list based on both the weighted ratio and token ratio similarity scores.",
        "prediction": ":import rapidfuzz import fuzzwuzzy from rapidfuzz\nimport rapidfuzz\n\ntarget_strings = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\", \"fig\", \"grape\"]\n\nquery_string = \"cranberry\"\n\nprint(\"Query: \", query_string)\n\nresults = rapidfuzz.process.autocomplete(query_string, target_strings, scorer=rapidfuzz.fuzz.WRatio, score_cutoff=90)\n\nprint(\"String and Score for each target string:\")\n\nfor result in results:\n    target, score, _ = result\n    print(f\"{target}: Weighted Ratio Score {score}\")\n\nprint()\n\nprint(\"Token Ratio Score for each target string:\")\n\nfor result in results:\n    target, score, _ = result\n    token_ratio_score = rapidfuzz.token_ratio_score(query_string, target)\n    print(f\"{target}: Token Ratio Score {token_ratio_score}\")",
        "source": "This is some documentation of rapidfuzz API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n\nBack to top\n\n\nToggle Light / Dark / Auto color theme\n\n\nToggle table of contents sidebar\n\n# header-image#\n\n\nRapidFuzz performs fuzzy string matching between two or more sequences using various string metrics.\n\n\n## Fundamental algorithms#\n\n\nRapidFuzz provides a wide range of fundamental algorithms for string matching.\nIn addition it provides an API compatible implementation of all algorithms in `thefuzz` / `fuzzywuzzy`.\n\n\n\n## Performant#\n\n\nRapidfuzz wraps highly-optimized implementations written in C++.\nEnjoy the flexibility of Python with the speed of compiled code.\n\n\n\n## Easy to use#\n\n\nRapidFuzz\u2019s high level syntax makes it accessible and productive for programmers\nfrom any background or experience level.\n\n\n\n## Compatible#\n\n\nRapidFuzz provides a pure python fallback for every algorithm. This ensures that the library works on platforms\nwhere compiling a C extension is not possible.\n\n\n\n## Open source#\n\n\nDistributed under a MIT License, RapidFuzz is developed and maintained publicly on GitHub.\n\n\n# Installation#\n\n\nWhile there are several ways of install RapidFuzz, the recommended methods\nare either by using `pip` (the Python package manager) or\n`conda` (an open-source, cross-platform, package manager)\n\n\n## using pip#\n\n\nRapidFuzz can be installed with `pip`:\n\n```\npip install rapidfuzz\n\n```\n\n\nThere are pre-built binaries (wheels) of RapidFuzz for MacOS (10.9 and later), Linux x86\\_64 and Windows.\n\nfailure \"ImportError: DLL load failed\"\n\n If you run into this error on Windows the reason is most likely, that the\n Visual C++ 2019 redistributable is not installed, which is required to\n find C++ Libraries (The C++ 2019 version includes the 2015, 2017 and 2019 version).\n\n\n\n## using conda#\n\n\nRapidFuzz can be installed with `conda`:\n\n```\nconda install -c conda-forge rapidfuzz\n\n\n\n## from git#\n\n\nRapidFuzz can be directly used from GitHub by cloning the\nrepository. This\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'rapidfuzz' API to demonstrate fuzzy string matching. The program should take a query string and compare it to a list of target strings using the 'fuzz.WRatio' scorer from 'rapidfuzz'. Print the query string and the similarity score for each target string. Additionally, the program should calculate and print the token ratio similarity score for each target string. The goal is to find similar strings in the list based on both the weighted ratio and token ratio similarity scores.\n\n### Code:"
    },
    {
        "api": "rapidfuzz",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install rapidfuzz\nfrom rapidfuzz import fuzz, process\n\ntarget_strings = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\", \"fig\", \"grape\"]\n\nquery_string = \"cranberry\"\n\nprint(\"Query: \", query_string)\n\nresults = process.extract(query_string, target_strings, scorer=fuzz.WRatio, limit=len(target_strings))\n\nfor result in results:\n    target, score, _ = result\n    token_set_ratio_score = fuzz.token_set_ratio(query_string, target)\n    print(f\"{target}: Weighted Ratio Score {score}, Token Set Ratio Score {token_set_ratio_score}\")",
        "instruction": "Create a Python program using the 'rapidfuzz' API to demonstrate fuzzy string matching. The program should take a query string and compare it to a list of target strings using the 'fuzz.WRatio' scorer from 'rapidfuzz'. Print the query string and the similarity score for each target string. Additionally, the program should calculate and print the token set ratio similarity score for each target string. The goal is to find similar strings in the list based on both the weighted ratio and token set ratio similarity scores.",
        "prediction": ":import rapidfuzz import fuzzwuzzy from rapidfuzz\nimport rapidfuzz\n\ntarget_strings = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\", \"fig\", \"grape\"]\n\nquery_string = \"cranberry\"\n\nprint(\"Query: \", query_string)\n\nresults = rapidfuzz.process.ext.automate.autoping(query_string)\n\nprint(\"Query Weight Ratio: \", results[0])\n\nprint(\"Token Set Ratio: \", results[1])",
        "source": "This is some documentation of rapidfuzz API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n\nBack to top\n\n\nToggle Light / Dark / Auto color theme\n\n\nToggle table of contents sidebar\n\n# header-image#\n\n\nRapidFuzz performs fuzzy string matching between two or more sequences using various string metrics.\n\n\n## Fundamental algorithms#\n\n\nRapidFuzz provides a wide range of fundamental algorithms for string matching.\nIn addition it provides an API compatible implementation of all algorithms in `thefuzz` / `fuzzywuzzy`.\n\n\n\n## Performant#\n\n\nRapidfuzz wraps highly-optimized implementations written in C++.\nEnjoy the flexibility of Python with the speed of compiled code.\n\n\n\n## Easy to use#\n\n\nRapidFuzz\u2019s high level syntax makes it accessible and productive for programmers\nfrom any background or experience level.\n\n\n\n## Compatible#\n\n\nRapidFuzz provides a pure python fallback for every algorithm. This ensures that the library works on platforms\nwhere compiling a C extension is not possible.\n\n\n\n## Open source#\n\n\nDistributed under a MIT License, RapidFuzz is developed and maintained publicly on GitHub.\n\n\n# Installation#\n\n\nWhile there are several ways of install RapidFuzz, the recommended methods\nare either by using `pip` (the Python package manager) or\n`conda` (an open-source, cross-platform, package manager)\n\n\n## using pip#\n\n\nRapidFuzz can be installed with `pip`:\n\n```\npip install rapidfuzz\n\n```\n\n\nThere are pre-built binaries (wheels) of RapidFuzz for MacOS (10.9 and later), Linux x86\\_64 and Windows.\n\nfailure \"ImportError: DLL load failed\"\n\n If you run into this error on Windows the reason is most likely, that the\n Visual C++ 2019 redistributable is not installed, which is required to\n find C++ Libraries (The C++ 2019 version includes the 2015, 2017 and 2019 version).\n\n\n\n## using conda#\n\n\nRapidFuzz can be installed with `conda`:\n\n```\nconda install -c conda-forge rapidfuzz\n\n\n\n## from git#\n\n\nRapidFuzz can be directly used from GitHub by cloning the\nrepository. This\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'rapidfuzz' API to demonstrate fuzzy string matching. The program should take a query string and compare it to a list of target strings using the 'fuzz.WRatio' scorer from 'rapidfuzz'. Print the query string and the similarity score for each target string. Additionally, the program should calculate and print the token set ratio similarity score for each target string. The goal is to find similar strings in the list based on both the weighted ratio and token set ratio similarity scores.\n\n### Code:"
    },
    {
        "api": "rich",
        "decoder_input_ids": true,
        "ground-truth": "# Install rich\n#!pip install rich\nfrom rich.console import Console\nfrom rich.table import Table\n\n# Define a list of famous historical quotes\nquotes = [\n    {\n        \"text\": \"Those who cannot remember the past are condemned to repeat it.\",\n        \"author\": \"George Santayana\",\n        \"source\": \"The Life of Reason\",\n    },\n    {\n        \"text\": \"In the end, we will remember not the words of our enemies, but the silence of our friends.\",\n        \"author\": \"Martin Luther King Jr.\",\n        \"source\": \"Strength to Love\",\n    },\n    {\n        \"text\": \"Never interrupt your enemy when he is making a mistake.\",\n        \"author\": \"Napoleon Bonaparte\",\n        \"source\": \"Letter to Charles-Maurice Talleyrand-Perigord\",\n    },\n    {\n        \"text\": \"We must, indeed, all hang together or, most assuredly, we shall all hang separately.\",\n        \"author\": \"Benjamin Franklin\",\n        \"source\": \"In the Continental Congress just before signing the Declaration of Independence\",\n    },\n]\n\n# Create a rich table for famous historical quotes\nconsole = Console()\ntable = Table(title=\"Famous Historical Quotes\", style=\"italic\")\ntable.add_column(\"Quote\", style=\"bold\")\ntable.add_column(\"Author\", style=\"bold\")\ntable.add_column(\"Source\", style=\"bold\")\n\n# Populate the table with quote data\nfor quote in quotes:\n    table.add_row(quote[\"text\"], quote[\"author\"], quote[\"source\"])\n\n# Display the famous historical quotes\nconsole.print(table)\n\n",
        "instruction": "Create a Python program using the 'rich' API to display a colorful and styled list of famous historical quotes. The program should include the quote text, the author, and the source of the quote.",
        "prediction": ":pip install rich\nimport rich as rConsole\n\n# Define a list of famous historical quotes\nquotes = [\n    {\n        \"text\": \"In the beginning, the Universe was created. This has made a great many things.\",\n        \"author\": \"Carlsig\",\n        \"source\": \"Source of the quote\",\n    },\n    {\n        \"text\": \"The history of rich is the history of famous.\",\n        \"author\": \"Steve Jobs\",\n        \"source\": \"Source of the quote\",\n    },\n    {\n        \"text\": \"In the beginning, the Universe was created. This in the beginning was done.\",\n        \"author\": \"Neil deGrasse Tyson\",\n        \"source\": \"Source of the quote\",\n    },\n    {\n        \"text\": \"In the beginning, the Universe was created. This in the beginning was done.\",\n        \"author\": \"Neil deGrasse Tyson\",\n        \"source\": \"Source of the quote\",\n    },\n]\n\n# Create a rich table for famous historical quotes\nconsole = rConsole.Console()\ntable = console.table(title=\"Famous Historical Quotes\", style=\"italic\")\ntable.add_column(\"Quote\", style=\"bold\")\ntable.add_column(\"Author\", style=\"bold\")\ntable.add_column(\"Source\", style=\"bold\")\n\n# Populate the table with quote data\nfor quote in quotes:\n    table.add_row(quote[\"text\"], quote[\"author\"], quote[\"source\"])\n\n# Display the famous historical quotes\nconsole.print(table)\n\n",
        "source": "This is some documentation of rich API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n rich\u00b6\n\n\nRich text and beautiful formatting in the terminal.\n\n\nrich.get\\_console()[source]\u00b6\nGet a global `Console` instance. This function is used when Rich requires a Console,\nand hasn\u2019t been explicitly given one.\n\nReturns\nA console instance.\n\nReturn type\nConsole\n\nrich.inspect(*obj*, *\\**, *console=None*, *title=None*, *help=False*, *methods=False*, *docs=True*, *private=False*, *dunder=False*, *sort=True*, *all=False*, *value=True*)[source]\u00b6\nInspect any Python object.\n\n\n* inspect(<OBJECT>) to see summarized info.\n* inspect(<OBJECT>, methods=True) to see methods.\n* inspect(<OBJECT>, help=True) to see full (non-abbreviated) help.\n* inspect(<OBJECT>, private=True) to see private attributes (single underscore).\n* inspect(<OBJECT>, dunder=True) to see attributes beginning with double underscore.\n* inspect(<OBJECT>, all=True) to see all attributes.\n\nParameters\n* **obj** (*Any*) \u2013 An object to inspect.\n* **title** (*str**,* *optional*) \u2013 Title to display over inspect result, or None use type. Defaults to None.\n* **help** (*bool**,* *optional*) \u2013 Show full help text rather than just first paragraph. Defaults to False.\n* **methods** (*bool**,* *optional*) \u2013 Enable inspection of callables. Defaults to False.\n* **docs** (*bool**,* *optional*) \u2013 Also render doc strings. Defaults to True.\n* **private** (*bool**,* *optional*) \u2013 Show private attributes (beginning with underscore). Defaults to False.\n* **dunder** (*bool**,* *optional*) \u2013 Show attributes starting with double underscore. Defaults to False.\n* **sort** (*bool**,* *optional*) \u2013 Sort attributes alphabetically. Defaults to True.\n* **all** (*bool**,* *optional*) \u2013 Show all attributes. Defaults to False.\n* **value** (*bool**,* *optional*)\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'rich' API to display a colorful and styled list of famous historical quotes. The program should include the quote text, the author, and the source of the quote.\n\n### Code:"
    },
    {
        "api": "rich",
        "decoder_input_ids": true,
        "ground-truth": "# Install rich\n#!pip install rich\nimport time\nfrom rich.console import Console\n\n# Create a real-time digital clock\nconsole = Console()\nwhile True:\n    console.clear()\n    current_time = time.strftime(\"%H:%M:%S\")\n    console.print(current_time, style=\"bold yellow on green\")\n    time.sleep(1)\n\n",
        "instruction": "Generate a Python program using the 'rich' API to display a stylish and colored digital clock that shows the current time. The program should update the time every second.",
        "prediction": ":import rich\nimport time\nfrom rich.console import Console\nfrom rich.panel import Panel\n\n# Create a real-time digital clock in rich\nconsole = Console()\n\nwhile True:\n    console.clear()\n    current_time = time.strftime(\"%H:%M:%S\")\n    panel = Panel(current_time, style=\"bold green on black\")\n    console.print(panel)\n    time.sleep(1)\n\n",
        "source": "This is some documentation of rich API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n rich.console\u00b6\n\n\n*class* rich.console.Capture(*console*)[source]\u00b6\nContext manager to capture the result of printing to the console.\nSee `capture()` for how to use.\n\nParameters\n**console** (*Console*) \u2013 A console instance to capture output.\n\n\nget()[source]\u00b6\nGet the result of the capture.\n\n\n*exception* rich.console.CaptureError[source]\u00b6\nAn error in the Capture context manager.\n\n*class* rich.console.Console(*\\**, *color\\_system='auto'*, *force\\_terminal=None*, *force\\_jupyter=None*, *force\\_interactive=None*, *soft\\_wrap=False*, *theme=None*, *stderr=False*, *file=None*, *quiet=False*, *width=None*, *height=None*, *style=None*, *no\\_color=None*, *tab\\_size=8*, *record=False*, *markup=True*, *emoji=True*, *emoji\\_variant=None*, *highlight=True*, *log\\_time=True*, *log\\_path=True*, *log\\_time\\_format='[%X]'*, *highlighter=<rich.highlighter.ReprHighlighter object>*, *legacy\\_windows=None*, *safe\\_box=True*, *get\\_datetime=None*, *get\\_time=None*, *\\_environ=None*)[source]\u00b6\nA high level console interface.\n\nParameters\n* **color\\_system** (*str**,* *optional*) \u2013 The color system supported by your terminal,\neither `\"standard\"`, `\"256\"` or `\"truecolor\"`. Leave as `\"auto\"` to autodetect.\n* **force\\_terminal** (*Optional**[**bool**]**,* *optional*) \u2013 Enable/disable terminal control codes, or None to auto-detect terminal. Defaults to None.\n* **force\\_jupyter** (*Optional**[**bool**]**,* *optional*) \u2013 Enable/disable Jupyter rendering, or None to auto-detect\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nGenerate a Python program using the 'rich' API to display a stylish and colored digital clock that shows the current time. The program should update the time every second.\n\n### Code:"
    },
    {
        "api": "rich",
        "decoder_input_ids": true,
        "ground-truth": "# Install rich\n#!pip install rich\nfrom rich.console import Console\nfrom rich.table import Table\n\n# Define a list of inspirational quotes\nquotes = [\n    {\n        \"text\": \"The future belongs to those who believe in the beauty of their dreams.\",\n        \"author\": \"Eleanor Roosevelt\",\n        \"source\": \"You Learn by Living: Eleven Keys for a More Fulfilling Life\",\n    },\n    {\n        \"text\": \"The only limit to our realization of tomorrow will be our doubts of today.\",\n        \"author\": \"Franklin D. Roosevelt\",\n        \"source\": \"Great Speeches\",\n    },\n    {\n        \"text\": \"Believe you can and you're halfway there.\",\n        \"author\": \"Theodore Roosevelt\",\n        \"source\": \"Quoted in Half-Hearted Enemies: Nova Scotia Texts\",\n    },\n    {\n        \"text\": \"The greatest glory in living lies not in never falling, but in rising every time we fall.\",\n        \"author\": \"Nelson Mandela\",\n        \"source\": \"Long Walk to Freedom: The Autobiography of Nelson Mandela\",\n    },\n]\n\n# Create a rich table for inspirational quotes\nconsole = Console()\ntable = Table(title=\"Inspirational Quotes\", style=\"italic\")\ntable.add_column(\"Quote\", style=\"bold\")\ntable.add_column(\"Author\", style=\"bold\")\ntable.add_column(\"Source\", style=\"bold\")\n\n# Populate the table with quote data\nfor quote in quotes:\n    table.add_row(quote[\"text\"], quote[\"author\"], quote[\"source\"])\n\n# Display the inspirational quotes\nconsole.print(table)\n\n",
        "instruction": "Generate a Python program using the 'rich' API to display a styled and colorful list of inspirational quotes. The program should include the quote text, the author, and the source of the quote.",
        "prediction": ":pip install rich\nimport rich\nfrom rich.table import Table\n\n# Define a list of inspirational quotes\nquotes = [\n    {\n        \"text\": \"The only way to do great work is to love what you do.\",\n        \"author\": \"Steve Jobs\",\n        \"source\": \"Stanford University commencement speech\",\n    },\n    {\n        \"text\": \"The only way to do great work is to love what you do.\",\n        \"author\": \"Steve Jobs\",\n        \"source\": \"Stanford University commencement speech\",\n    },\n    {\n        \"text\": \"The only way to do great work is to love what you do.\",\n        \"author\": \"Steve Jobs\",\n        \"source\": \"Stanford University commencement speech\",\n    },\n    {\n        \"text\": \"The only way to do great work is to love what you do.\",\n        \"author\": \"Steve Jobs\",\n        \"source\": \"Stanford University commencement speech\",\n    },\n    {\n        \"text\": \"The only way to do great work is to love what you do.\",\n        \"author\": \"Steve Jobs\",\n        \"source\": \"Stanford University commencement speech\",\n    },\n]\n\n# Create a rich table for insight\nconsole = rich.Console()\ntable = Table(title=\"Insight\", style=\"bold under\ufffddefault\")\ntable.add_column(\"Quote\", style=\"bold\")\ntable.add_column(\"Author\", style=\"bold\")\ntable.add_column(\"Source\", style=\"bold\")\n\n# Populate the table with quote data\nfor quote in quotes:\n    table.add_row(quote[\"text\"], quote[\"author\"], quote[\"source\"])\n\n# Display the table\nconsole.print(table)\n\n",
        "source": "This is some documentation of rich API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n rich\u00b6\n\n\nRich text and beautiful formatting in the terminal.\n\n\nrich.get\\_console()[source]\u00b6\nGet a global `Console` instance. This function is used when Rich requires a Console,\nand hasn\u2019t been explicitly given one.\n\nReturns\nA console instance.\n\nReturn type\nConsole\n\nrich.inspect(*obj*, *\\**, *console=None*, *title=None*, *help=False*, *methods=False*, *docs=True*, *private=False*, *dunder=False*, *sort=True*, *all=False*, *value=True*)[source]\u00b6\nInspect any Python object.\n\n\n* inspect(<OBJECT>) to see summarized info.\n* inspect(<OBJECT>, methods=True) to see methods.\n* inspect(<OBJECT>, help=True) to see full (non-abbreviated) help.\n* inspect(<OBJECT>, private=True) to see private attributes (single underscore).\n* inspect(<OBJECT>, dunder=True) to see attributes beginning with double underscore.\n* inspect(<OBJECT>, all=True) to see all attributes.\n\nParameters\n* **obj** (*Any*) \u2013 An object to inspect.\n* **title** (*str**,* *optional*) \u2013 Title to display over inspect result, or None use type. Defaults to None.\n* **help** (*bool**,* *optional*) \u2013 Show full help text rather than just first paragraph. Defaults to False.\n* **methods** (*bool**,* *optional*) \u2013 Enable inspection of callables. Defaults to False.\n* **docs** (*bool**,* *optional*) \u2013 Also render doc strings. Defaults to True.\n* **private** (*bool**,* *optional*) \u2013 Show private attributes (beginning with underscore). Defaults to False.\n* **dunder** (*bool**,* *optional*) \u2013 Show attributes starting with double underscore. Defaults to False.\n* **sort** (*bool**,* *optional*) \u2013 Sort attributes alphabetically. Defaults to True.\n* **all** (*bool**,* *optional*) \u2013 Show all attributes. Defaults to False.\n* **value** (*bool**,* *optional*)\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nGenerate a Python program using the 'rich' API to display a styled and colorful list of inspirational quotes. The program should include the quote text, the author, and the source of the quote.\n\n### Code:"
    },
    {
        "api": "rich",
        "decoder_input_ids": true,
        "ground-truth": "# Install rich\n#!pip install rich\nimport time\nfrom rich.progress import Progress\n\n# Simulate video rendering process\ndef render_video():\n    for _ in range(100):\n        time.sleep(0.1)\n\n# Create a styled progress bar for video rendering\nwith Progress() as progress:\n    task = progress.add_task(\"[cyan]Rendering video...\", total=100)\n    progress.console.show_cursor(False)\n    \n    # Simulate video rendering and update the progress\n    for _ in range(100):\n        time.sleep(0.1)\n        progress.update(task, completed=1)\n        progress.refresh()\n\n# Display completion message\nprogress.console.show_cursor(True)\nprint(\"[green]Video rendering completed!\")\n\n",
        "instruction": "Generate a Python program using the 'rich' API to display a colorful and styled progress bar that shows the progress of a video rendering process. The program should update the progress as the rendering continues.",
        "prediction": ":import rich\nfrom rich.progress import Progress\n\n# Simulate a video rendering process\ndef render_task():\n    for _ in range(100):\n        yield \"*rendered*\"\n\n# Create a styled progress bar for video rendering\nwith Progress() as progress:\n    task = progress.add_task(\"[cyan]Rendering video...\", total=100)\n    progress.console.show_cursor(False)\n    \n    # Simulate video rendering\n    for _ in render_task():\n        pass\n        progress.update(task, completed=1)\n        progress.refresh()\n        time.sleep(0.1)\n        progress.refresh()\n\n# Display completion message\nprogress.console.show_cursor(True)\nprint(\"[green]Rendering completed!\")\n\n",
        "source": "This is some documentation of rich API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n rich.progress\u00b6\n\n\n*class* rich.progress.BarColumn(*bar\\_width=40*, *style='bar.back'*, *complete\\_style='bar.complete'*, *finished\\_style='bar.finished'*, *pulse\\_style='bar.pulse'*, *table\\_column=None*)[source]\u00b6\nRenders a visual progress bar.\n\nParameters\n* **bar\\_width** (*Optional**[**int**]**,* *optional*) \u2013 Width of bar or None for full width. Defaults to 40.\n* **style** (*StyleType**,* *optional*) \u2013 Style for the bar background. Defaults to \u201cbar.back\u201d.\n* **complete\\_style** (*StyleType**,* *optional*) \u2013 Style for the completed bar. Defaults to \u201cbar.complete\u201d.\n* **finished\\_style** (*StyleType**,* *optional*) \u2013 Style for a finished bar. Defaults to \u201cbar.finished\u201d.\n* **pulse\\_style** (*StyleType**,* *optional*) \u2013 Style for pulsing bars. Defaults to \u201cbar.pulse\u201d.\n* **table\\_column** (*Optional**[**Column**]*) \u2013\n\n\nrender(*task*)[source]\u00b6\nGets a progress bar widget for a task.\n\nParameters\n**task** (*Task*) \u2013 \n\nReturn type\n*ProgressBar*\n\n\n*class* rich.progress.DownloadColumn(*binary\\_units=False*, *table\\_column=None*)[source]\u00b6\nRenders file size downloaded and total, e.g. \u20180.5/2.3 GB\u2019.\n\nParameters\n* **binary\\_units** (*bool**,* *optional*) \u2013 Use binary units, KiB, MiB etc. Defaults to False.\n* **table\\_column** (*Optional**[**Column**]*) \u2013\n\n\nrender(*task*)[source]\u00b6\nCalculate common unit for completed and total.\n\nReturn type\n*Text*\n\n\n*class* rich.progress.FileSizeColumn(*table\\_column=None*)[source]\u00b6\nRenders completed filesize.\n\nParameters\n**\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nGenerate a Python program using the 'rich' API to display a colorful and styled progress bar that shows the progress of a video rendering process. The program should update the progress as the rendering continues.\n\n### Code:"
    },
    {
        "api": "rustworkx",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install rustworkx\nimport rustworkx as rx\n\ngraph = rx.PyGraph()\n\n# Each time add node is called, it returns a new node index\na = graph.add_node(\"A\")\nb = graph.add_node(\"B\")\nc = graph.add_node(\"C\")\nd = graph.add_node(\"D\")\n\n# add_edges_from takes tuples of node indices and weights,\n# and returns edge indices\nedge_indices = graph.add_edges_from([(a, b, 1.5), (a, d, 2.0), (b, c, 2.5),(d, c, 1.5)])\nprint(edge_indices)\n\n# Returns the path \npath_mapping = rx.dijkstra_shortest_paths(graph, a, c, weight_fn=float)\nprint(path_mapping)",
        "instruction": "Create a Python program using the 'rustworkx' API to define a directed graph and find the shortest path between two nodes. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish weighted edges between these nodes and print the indices of the added edges. Then, use Dijkstra's shortest path algorithm to find and print the shortest path from node 'A' to node 'C'",
        "prediction": ":import rustworkx as rx\n\n# Create a directed graph\ngraph = rx.PyDiGraph()\n\n# 'A', 'B', 'C', 'D' are node identifiers\na = graph.add_node(\"A\")\nb = graph.add_node(\"B\")\nc = graph.add_node(\"C\")\nd = graph.add_node(\"D\")\n\n# weighted edges of the directed graph\nedge_weight = {a: 3, b: 2, c: 1}\n\n# add weighted edges of 'A' and 'B' to node and store edge indices\nedge_indices = graph.add_edges(weight=3, node_1=a, node_2=b)\nedge_indices = graph.add_edges(weight=2, node_1=b, node_2=c)\n\n# Find shortest path from node 'A' to node 'C'\ndist = rx.shortest_path(graph, 'A', 'C')\nprint(dist)",
        "source": "This is some documentation of rustworkx API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n\n  \n  \n  \n\n# Rustworkx API Reference#\n\n* Graph Classes\n\t+ PyGraph\n\t+ PyDiGraph\n\t+ PyDAG\n* Algorithm Functions\n\t+ Centrality\n\t+ Connectivity and Cycles\n\t+ DAG Algorithms\n\t+ Graph Operations\n\t+ Isomorphism\n\t+ Link Analysis\n\t+ Matching\n\t+ Other Algorithm Functions\n\t+ Shortest Paths\n\t+ Traversal\n\t+ Tree\n\n\n* Generators\n* Random Graph Generator Functions\n* Layout Functions\n* Serialization\n* Converters\n* API functions for PyDigraph\n* API functions for PyGraph\n* Exceptions\n* Custom Return Types\n\n  \n  \n  \n\n\n# Graph Classes#\n\n|  |  |\n| --- | --- |\n| `rustworkx.PyGraph`([multigraph, attrs]) | A class for creating undirected graphs |\n| `rustworkx.PyDiGraph`([check\\_cycle,...]) | A class for creating directed graphs |\n| `rustworkx.PyDAG`([check\\_cycle, multigraph, attrs]) | A class for creating direct acyclic graphs. |\n\n# Algorithm Functions#\n\n* Centrality\n\t+ rustworkx.betweenness\\_centrality\n\t+ rustworkx.edge\\_betweenness\\_centrality\n\t+ rustworkx.eigenvector\\_centrality\n\t+ rustworkx.katz\\_centrality\n\t+ rustworkx.closeness\\_centrality\n* Connectivity and Cycles\n\t+ rustworkx.number\\_connected\\_components\n\t+ rustworkx.connected\\_components\n\t+ rustworkx.node\\_connected\\_component\n\t+ rustworkx.is\\_connected\n\t+ rustworkx.strongly\\_connected\\_components\n\t+ rustworkx.number\\_weakly\\_connected\\_components\n\t+ rustworkx.weakly\\_connected\\_components\n\t+ rustworkx.is\\_weakly\\_connected\n\t+ rustworkx.cycle\\_basis\n\t+ rustworkx.simple\\_cycles\n\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'rustworkx' API to define a directed graph and find the shortest path between two nodes. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish weighted edges between these nodes and print the indices of the added edges. Then, use Dijkstra's shortest path algorithm to find and print the shortest path from node 'A' to node 'C'\n\n### Code:"
    },
    {
        "api": "rustworkx",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install rustworkx\nimport rustworkx as rx\n\ngraph = rx.PyGraph()\n\n# Each time add node is called, it returns a new node index\na = graph.add_node(\"A\")\nb = graph.add_node(\"B\")\nc = graph.add_node(\"C\")\nd = graph.add_node(\"D\")\n\n# add_edges_from takes tuples of node indices and weights,\n# and returns edge indices\nedge_indices = graph.add_edges_from([(a, b, 1.5), (a, d, 2.0), (b, c, 2.5),(d, c, 1.5)])\nprint(edge_indices)\n\n# Returns the minimum spanning tree\nmst = rx.minimum_spanning_tree(graph, weight_fn=float)\nprint(mst)",
        "instruction": "Create a Python program using the 'rustworkx' API to define an undirected graph and find the minimum spanning tree. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish weighted edges between these nodes and print the indices of the added edges. Then, use Kruskal's algorithm to find and print the minimum spanning tree of the graph.",
        "prediction": ":import rustworkx as rx\n\ngraph = rx.PyGraph()\n\n# Each time add node is called, it returns a new node index\na = graph.add_node(\"A\")\nb = graph.add_node(\"B\")\nc = graph.add_node(\"C\")\nd = graph.add_node(\"D\")\n\n# add_edges_from takes tuples of node indices and weights,\n# and returns edge indices\nedge_indices = graph.add_edges_from([(a, b), (b, c), (c, d)])\nprint(edge_indices)\n\n# Returns the minimum spanning tree of the graph\n# Kruskal's algorithm works on connected graphs\nmin_span_tree = rx.min_spanning_tree(graph)\nprint(min_span_tree)",
        "source": "This is some documentation of rustworkx API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n\n  \n  \n  \n\n# Rustworkx API Reference#\n\n* Graph Classes\n\t+ PyGraph\n\t+ PyDiGraph\n\t+ PyDAG\n* Algorithm Functions\n\t+ Centrality\n\t+ Connectivity and Cycles\n\t+ DAG Algorithms\n\t+ Graph Operations\n\t+ Isomorphism\n\t+ Link Analysis\n\t+ Matching\n\t+ Other Algorithm Functions\n\t+ Shortest Paths\n\t+ Traversal\n\t+ Tree\n\n\n* Generators\n* Random Graph Generator Functions\n* Layout Functions\n* Serialization\n* Converters\n* API functions for PyDigraph\n* API functions for PyGraph\n* Exceptions\n* Custom Return Types\n\n  \n  \n  \n\n\n# Graph Classes#\n\n|  |  |\n| --- | --- |\n| `rustworkx.PyGraph`([multigraph, attrs]) | A class for creating undirected graphs |\n| `rustworkx.PyDiGraph`([check\\_cycle,...]) | A class for creating directed graphs |\n| `rustworkx.PyDAG`([check\\_cycle, multigraph, attrs]) | A class for creating direct acyclic graphs. |\n\n# Algorithm Functions#\n\n* Centrality\n\t+ rustworkx.betweenness\\_centrality\n\t+ rustworkx.edge\\_betweenness\\_centrality\n\t+ rustworkx.eigenvector\\_centrality\n\t+ rustworkx.katz\\_centrality\n\t+ rustworkx.closeness\\_centrality\n* Connectivity and Cycles\n\t+ rustworkx.number\\_connected\\_components\n\t+ rustworkx.connected\\_components\n\t+ rustworkx.node\\_connected\\_component\n\t+ rustworkx.is\\_connected\n\t+ rustworkx.strongly\\_connected\\_components\n\t+ rustworkx.number\\_weakly\\_connected\\_components\n\t+ rustworkx.weakly\\_connected\\_components\n\t+ rustworkx.is\\_weakly\\_connected\n\t+ rustworkx.cycle\\_basis\n\t+ rustworkx.simple\\_cycles\n\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'rustworkx' API to define an undirected graph and find the minimum spanning tree. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish weighted edges between these nodes and print the indices of the added edges. Then, use Kruskal's algorithm to find and print the minimum spanning tree of the graph.\n\n### Code:"
    },
    {
        "api": "scikit-learn",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install scikit-learn\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.tree import DecisionTreeClassifier\n\n#Load the LFW dataset\nlfw = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\nX, y = lfw.data, lfw.target\n\n#Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n#Initialize the Decision Tree classifier\ndt_classifier = DecisionTreeClassifier()\n\n#Train the classifier\ndt_classifier.fit(X_train, y_train)\n\n#Make predictions\ny_pred = dt_classifier.predict(X_test)\n\n#Calculate accuracy\naccuracy = (y_pred == y_test).mean()\nprint(f'Accuracy: {accuracy:.2f}')\n\n#Serialize and save the model\njoblib.dump(dt_classifier, 'lfw_classifier.pkl')\n",
        "instruction": "Create a Python program that uses the 'scikit-learn' API to load the LFW (Labeled Faces in the Wild) dataset, split it into training and testing sets, train a Decision Tree classifier, make predictions, calculate accuracy, and save the trained model.",
        "prediction": ":import joblib\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.tree import DecisionTreeClassifier\n\n#Load the LFW dataset\nlfw = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\nX, y = lfw.data, lfw.target\n\n#Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n#Initialize the Decision Tree classifier\ndt_classifier = DecisionTreeClassifier()\n\n#Train the classifier\ndt_classifier.fit(X_train, y_train)\n\n#Make predictions\ny_pred = dt_classifier.predict(X_test)\n\n#Calculate accuracy\naccuracy = (y_pred == y_test).mean()\nprint(f'Accuracy: {accuracy:.2f}')\n\n#Serialize and save the model\njoblib.dump(dt_classifier, 'lfw_classifier.pkl')\n",
        "source": "This is some documentation of scikit-learn API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n# Examples using `sklearn.datasets.load\\_iris`\u00b6\n\n\nRelease Highlights for scikit-learn 1.2\n\nRelease Highlights for scikit-learn 0.24\n\n\nRelease Highlights for scikit-learn 0.24\n\nRelease Highlights for scikit-learn 0.22\n\n\nRelease Highlights for scikit-learn 0.22\n\nPlot classification probability\n\n\nPlot classification probability\n\nK-means Clustering\n\n\nK-means Clustering\n\nPlot Hierarchical Clustering Dendrogram\n\n\nPlot Hierarchical Clustering Dendrogram\n\nThe Iris Dataset\n\n\nThe Iris Dataset\n\nPlot the decision surface of decision trees trained on the iris dataset\n\n\nPlot the decision surface of decision trees trained on the iris dataset\n\nUnderstanding the decision tree structure\n\n\nUnderstanding the decision tree structure\n\nComparison of LDA and PCA 2D projection of Iris dataset\n\n\nComparison of LDA and PCA 2D projection of Iris dataset\n\nFactor Analysis (with rotation) to visualize patterns\n\n\nFactor Analysis (with rotation) to visualize patterns\n\nIncremental PCA\n\n\nIncremental PCA\n\nPCA example with Iris Data-set\n\n\nPCA example with Iris Data-set\n\nEarly stopping of Gradient Boosting\n\n\nEarly stopping of Gradient Boosting\n\nPlot the decision boundaries of a VotingClassifier\n\n\nPlot the decision boundaries of a VotingClassifier\n\nPlot the decision surfaces of ensembles of trees on the iris dataset\n\n\nPlot the decision surfaces of ensembles of trees on the iris dataset\n\nUnivariate Feature Selection\n\n\nUnivariate Feature Selection\n\nGMM covariances\n\n\nGMM covariances\n\nGaussian process classification (GPC) on iris dataset\n\n\nGaussian process classification (GPC) on iris dataset\n\nLogistic Regression 3-class Classifier\n\n\nLogistic Regression 3-class Classifier\n\nPlot multi-class SGD on the iris dataset\n\n\nPlot multi-class SGD on the iris dataset\n\nRegularization path of L1- Logistic Regression\n\n\nRegularization path of L1- Logistic Regression\n\nIntroducing the set\\_output API\n\n\nIntroducing the set\\_output API\n\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that uses the 'scikit-learn' API to load the LFW (Labeled Faces in the Wild) dataset, split it into training and testing sets, train a Decision Tree classifier, make predictions, calculate accuracy, and save the trained model.\n\n### Code:"
    },
    {
        "api": "scikit-learn",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install scikit-learn\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Load the LFW dataset\nlfw = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\nX, y = lfw.data, lfw.target\n\n#Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n#Initialize the Random Forest classifier\nrf_classifier = RandomForestClassifier(n_estimators=100)\n\n#Train the classifier\nrf_classifier.fit(X_train, y_train)\n\n#Make predictions\ny_pred = rf_classifier.predict(X_test)\n\n#Calculate accuracy\naccuracy = (y_pred == y_test).mean()\nprint(f'Accuracy: {accuracy:.2f}')\n\n#Serialize and save the model\njoblib.dump(rf_classifier, 'lfw_classifier.pkl')\n",
        "instruction": "Create a Python program that uses the 'scikit-learn' API to load the LFW (Labeled Faces in the Wild) dataset, split it into training and testing sets, train a Random Forest classifier, make predictions, calculate accuracy, and save the trained model.",
        "prediction": ":import joblib\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Load the LFW dataset\nlfw_dataset = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n\n#Split the dataset\nX, Y = lfw_dataset.data, lfw_dataset.target\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n#Initialize the Random Forest classifier\nrf_classifier = RandomForestClassifier(n_estimators=10)\n\n#Train the classifier\nrf_classifier.fit(X_train, Y_train)\n\n#Make predictions\ny_pred = rf_classifier.predict(X_test)\n\n#Calculate accuracy\naccuracy = (y_pred == Y_test).mean()\nprint(accuracy)\n\n#Serialize and save the model\njoblib.dump(rf_classifier, 'lfw_classifier.pkl')\n",
        "source": "This is some documentation of scikit-learn API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n# 1.11.2. Random forests and other randomized tree ensembles\u00b6\n\n\nThe `sklearn.ensemble` module includes two averaging algorithms based\non randomized decision trees: the RandomForest algorithm\nand the Extra-Trees method. Both algorithms are perturb-and-combine\ntechniques [B1998] specifically designed for trees. This means a diverse\nset of classifiers is created by introducing randomness in the classifier\nconstruction. The prediction of the ensemble is given as the averaged\nprediction of the individual classifiers.\n\n\nAs other classifiers, forest classifiers have to be fitted with two\narrays: a sparse or dense array X of shape `(n\\_samples, n\\_features)`\nholding the training samples, and an array Y of shape `(n\\_samples,)`\nholding the target values (class labels) for the training samples:\n\n```\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> X = [[0, 0], [1, 1]]\n>>> Y = [0, 1]\n>>> clf = RandomForestClassifier(n\\_estimators=10)\n>>> clf = clf.fit(X, Y)\n\n\nLike decision trees, forests of trees also extend to\nmulti-output problems (if Y is an array\nof shape `(n\\_samples, n\\_outputs)`).\n### 1.11.2.1. Random Forests\u00b6\n\n\nIn random forests (see `RandomForestClassifier` and\n`RandomForestRegressor` classes), each tree in the ensemble is built\nfrom a sample drawn with replacement (i.e., a bootstrap sample) from the\ntraining set.\n\n\nFurthermore, when splitting each node during the construction of a tree, the\nbest\n\n==================\n Document 1 \n----------------\n## 1.11.2.1. Random Forests\u00b6\n\n\nIn random forests (see `RandomForestClassifier` and\n`RandomForestRegressor` classes), each tree in the ensemble is built\nfrom a sample drawn with replacement (i.e., a bootstrap sample) from the\ntraining set.\n\n\nFurthermore, when splitting each node during the construction of a tree, the\nbest split is found either from all input features or a random subset of\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that uses the 'scikit-learn' API to load the LFW (Labeled Faces in the Wild) dataset, split it into training and testing sets, train a Random Forest classifier, make predictions, calculate accuracy, and save the trained model.\n\n### Code:"
    },
    {
        "api": "scikit-learn",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install scikit-learn\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Load the Breast Cancer dataset\nbreast_cancer = load_breast_cancer()\nX, y = breast_cancer.data, breast_cancer.target\n\n#Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n#Initialize the K-Nearest Neighbors classifier\nknn_classifier = KNeighborsClassifier(n_neighbors=3)\n\n#Train the classifier\nknn_classifier.fit(X_train, y_train)\n\n#Make predictions\ny_pred = knn_classifier.predict(X_test)\n\n#Calculate accuracy\naccuracy = (y_pred == y_test).mean()\nprint(f'Accuracy: {accuracy:.2f}')\n\n#Serialize and save the model\njoblib.dump(knn_classifier, 'breast_cancer_classifier.pkl')\n",
        "instruction": "Create a Python program that uses the 'scikit-learn' API to load the Breast Cancer dataset, split it into training and testing sets, train a K-Nearest Neighbors classifier, make predictions, calculate accuracy, and save the trained model.",
        "prediction": ":import joblib\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Load the Breast Cancer dataset\nbreast_cancer = load_breast_cancer()\nX, y = breast_cancer.data, breast_cancer.target\n\n#Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n#Initialize the K-Nearest Neighbors classifier\nkde = KNeighborsClassifier()\n\n#Train the classifier\nkde.fit(X_train, y_train)\n\n#Make predictions\ny_pred = kde.predict(X_test)\n\n#Calculate accuracy\naccuracy = (y_pred == y_test).mean()\nprint(f'Accuracy: {accuracy:.2f}')\n\n#Serialize and save the model\njoblib.dump(kde, 'breast_cancer_kde.pkl')\n",
        "source": "This is some documentation of scikit-learn API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n# 2.8.2. Kernel Density Estimation\u00b6\n\n\nKernel density estimation in scikit-learn is implemented in the\n`KernelDensity` estimator, which uses the\nBall Tree or KD Tree for efficient queries (see Nearest Neighbors for\na discussion of these). Though the above example\nuses a 1D data set for simplicity, kernel density estimation can be\nperformed in any number of dimensions, though in practice the curse of\ndimensionality causes its performance to degrade in high dimensions.\n\n\nIn the following figure, 100 points are drawn from a bimodal distribution,\nand the kernel density estimates are shown for three choices of kernels:\n\n****\n\nIt\u2019s clear how the kernel shape affects the smoothness of the resulting\ndistribution. The scikit-learn kernel density estimator can be used as\nfollows:\n\n```\n>>> from sklearn.neighbors import KernelDensity\n>>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(X)\n>>> kde.score\\_samples(X)\narray([-0.41075698, -0.41075698, -0.41076071, -0.41075698, -0.41075698,\n -0.41076071])\n\n\nHere we have used `kernel='gaussian'`, as seen above.\nMathematically, a kernel is a positive function \\(K(x;h)\\)\nwhich is controlled by the bandwidth parameter \\(h\\).\nGiven this kernel form, the density estimate at a point \\(y\\) within\na group of points \\(x\\_i; i=1\\cdots N\\) is given by:\n\n\\[\\rho\\_K(y) = \\sum\\_{i=1}^{N} K(y - x\\_i; h)\\]\nThe bandwidth here acts as a smoothing parameter, controlling the tradeoff\nbetween bias and variance in the result. A large bandwidth leads to a very\nsmooth (i.e. high\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that uses the 'scikit-learn' API to load the Breast Cancer dataset, split it into training and testing sets, train a K-Nearest Neighbors classifier, make predictions, calculate accuracy, and save the trained model.\n\n### Code:"
    },
    {
        "api": "scikit-learn",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install scikit-learn\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.cluster import KMeans\n\n#Load the Breast Cancer dataset\nbreast_cancer = load_breast_cancer()\nX, y = breast_cancer.data, breast_cancer.target\n\n#Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n#Initialize the K-Means clustering model\nkmeans = KMeans(n_clusters=2)\n\n#Train the model\nkmeans.fit(X_train)\n\n#Make predictions\ny_pred = kmeans.predict(X_test)\n\n#Serialize and save the model\njoblib.dump(kmeans, 'breast_cancer_kmeans_model.pkl')\n",
        "instruction": "Create a Python program that uses the 'scikit-learn' API to load the Breast Cancer dataset, split it into training and testing sets, train a K-Means clustering model, make predictions, and save the trained model.",
        "prediction": ":import joblib\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.cluster import KMeans\n\n#Load the Breast Cancer dataset\nbreast_cancer = load_breast_cancer()\nX, y = breast_cancer.data, breast_cancer.feature_names\n\n#Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n#Initialize the K-Means clustering model\nkmeans = KMeans(n_clusters=2)\n\n#Train the model\nkmeans.fit(X_train)\n\n#Make predictions\ny_pred = kmeans.predict(X_test)\n\n#Serialize and save the model\njoblib.dump(kmeans, 'breast_cancer_kmeans_model.pkl')\n",
        "source": "This is some documentation of scikit-learn API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n# 2.3.1. Overview of clustering methods\u00b6\n\n\n\nA comparison of the clustering algorithms in scikit-learn\u00b6\n\n\n| Method name | Parameters | Scalability | Usecase | Geometry (metric used) |\n| --- | --- | --- | --- | --- |\n| K-Means | number of clusters | Very large `n\\_samples`, medium `n\\_clusters` with\nMiniBatch code | General-purpose, even cluster size, flat geometry,\nnot too many clusters, inductive | Distances between points |\n| Affinity propagation | damping, sample preference | Not scalable with n\\_samples | Many clusters, uneven cluster size, non-flat geometry, inductive | Graph distance (e.g. nearest-neighbor graph) |\n| Mean-shift | bandwidth | Not scalable with `n\\_samples` | Many clusters, uneven cluster size, non-flat geometry, inductive | Distances between points |\n| Spectral clustering | number of clusters | Medium `n\\_samples`, small `n\\_clusters` | Few clusters, even cluster size, non-flat geometry, transductive | Graph distance (e.g. nearest-neighbor graph) |\n| Ward hierarchical clustering | number of clusters or distance threshold | Large `n\\_samples` and `n\\_clusters` | Many clusters, possibly connectivity constraints, transductive | Distances between points |\n| Agglomerative clustering | number of clusters or distance threshold, linkage type, distance | Large `n\\_samples` and `n\\_clusters` | Many clusters, possibly connectivity constraints, non Euclidean\ndistances, transductive | Any pairwise distance |\n| DBSCAN | neighborhood size | Very large `n\\_samples`, medium `n\\_clusters` | Non-flat geometry, uneven cluster sizes, outlier removal,\ntransductive | Distances between nearest points |\n| HDBSCAN | minimum cluster membership, minimum point neighbors | large `n\\_samples`, medium `n\\_clusters` | Non-flat geometry, uneven cluster sizes, outlier removal,\ntransductive, hierarchical, variable cluster density | Distances between nearest points |\n| OPTICS | minimum\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that uses the 'scikit-learn' API to load the Breast Cancer dataset, split it into training and testing sets, train a K-Means clustering model, make predictions, and save the trained model.\n\n### Code:"
    },
    {
        "api": "scipy",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install scipy matplotlib\nimport scipy as sp\nimport matplotlib.pyplot as plt\n\nface = sp.datasets.face(gray=True)\n\nshifted_face = sp.ndimage.shift(face, (50, 50))\nrotated_face = sp.ndimage.rotate(face, 30)\n\nplt.figure(figsize=(15, 3))\nplt.subplot(151)\nplt.imshow(face, cmap=plt.cm.gray)\nplt.axis(\"off\")\n\nplt.subplot(152)\nplt.imshow(shifted_face, cmap=plt.cm.gray)\nplt.axis(\"off\")\n\nplt.subplot(153)\nplt.imshow(rotated_face, cmap=plt.cm.gray)\nplt.axis(\"off\")\n\nplt.subplots_adjust(wspace=0.05, left=0.01, bottom=0.01, right=0.99, top=0.99)\n\nplt.show()",
        "instruction": "Create a Python program using the 'scipy' API to apply various image transformations to a sample image. The program should perform operations like shifting and rotating the image and display the original and transformed images.",
        "prediction": ":import scipy as sp\nimport scipy as sp\n\n# Read the sample image\nsample_image = sp.misc.imread('sampleImage.jpg')\n\n# Shifting and Display the original and transformed images\nimport scipy as sp\nimport scipy.misc\nfrom scipy.misc.pil_' import Image\nfrom scipy.misc.pil_' import ImageFilter\nfrom scipy.misc.pil_' import ImageEnhance\n\n# Display the original and transformed images\nimport scipy as sp\nimport scipy.misc\nfrom scipy.misc.pil_' import Image\nfrom scipy.misc.pil_' import ImageFilter\nfrom scipy.misc.pil_' import ImageEnhance\n\n# Display the original and transformed images\nimport scipy as sp\nimport scipy.misc\nfrom scipy.misc.pil_' import Image\nfrom scipy.misc.pil_' import ImageFilter\nfrom scipy.misc.pil_' import ImageEnhance\n\n# Display the original and transformed images\nimport scipy as sp\nimport scipy.misc\nfrom scipy.misc.pil_' import Image\nfrom scipy.misc.pil_' import ImageFilter\nfrom scipy.misc.pil_' import ImageEnhance\n\n# Shifting and Display the original and transformed images\nimport scipy as sp\nimport scipy.misc\nfrom scipy.misc.pil_' import Image\nfrom scipy.misc.pil_' import ImageFilter\nfrom scipy.misc.pil_' import ImageEnhance\n\n# Display the original and transformed images\nimport scipy as sp\nimport scipy.misc\nfrom scipy.misc.pil_' import Image\nfrom scipy.misc.pil_' import ImageFilter\nfrom scipy.misc.pil_' import ImageEnhance\n\n# Display the original and transformed images\nimport scipy as sp\nimport scipy.misc\nfrom scipy.misc.pil_' import Image\nfrom scipy.misc.pil_' import ImageFilter\nfrom scipy.misc.pil_' import ImageEnhance\n\n# Shifting and Display the original and transformed images\nimport scipy as sp\nimport scipy.misc\nfrom scipy.misc.pil_' import Image\nfrom scipy.misc.pil_' import ImageFilter\nfrom scipy.misc.pil_' import ImageEnhance\n\n# Display the original and transformed images\nimport scipy as sp\nimport scipy.misc\nfrom scipy.misc.pil_' import Image\nfrom scipy.misc.pil_' import ImageFilter\nfrom scipy.misc.pil_' import ImageEnhance\n\n# Shifting and Display the original and transformed images\nimport scipy as sp\nimport scipy.misc\nfrom scipy.misc.pil_' import Image\nfrom scipy.misc.pil_' import ImageFilter\nfrom scipy.misc.pil_' import ImageEnhance\n\n# Display the original and transformed images\nimport scipy as sp\nimport scipy.misc\nfrom scipy.misc.pil_' import Image\nfrom scipy.misc.pil_' import ImageFilter\nfrom scipy.misc.pil_' import ImageEnhance\n\n# Display the original and transformed images\nimport scipy as sp\nimport scipy.misc\nfrom scipy.misc.pil_' import Image\nfrom scipy.misc.pil_' import ImageFilter\nfrom scipy.misc.pil_' import ImageEnhance\n  )\n\n# Shifting and Display the original and transformed images\nimport scipy as sp\nimport scipy.misc\nfrom scipy.misc.pil_' import Image\nfrom scipy.misc.pil_' import ImageFilter\nfrom scipy.misc.pil_' import ImageEnhance\n\n# Display the original and transformed images\nimport scipy as sp\nimport scipy.misc\nfrom scipy.misc.pil_' import Image\nfrom scipy.misc.pil_' import ImageFilter\nfrom scipy.misc.pil_' import ImageEnhance\n\n# Shifting and Display the original and transformed images\nimport scipy",
        "source": "This is some documentation of scipy API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# SciPy API#\n\n## Importing from SciPy#\n\n\nIn Python, the distinction between what is the public API of a library and what\nare private implementation details is not always clear. Unlike in other\nlanguages like Java, it is possible in Python to access \u00e2\u0080\u009cprivate\u00e2\u0080\u009d functions or\nobjects.\n\n==================\n Document 1 \n----------------\n# Usage information#\n\n\nThere are seven available sparse matrix types:\n\n> \n> 1. csc\\_matrix: Compressed Sparse Column format\n> 2. csr\\_matrix: Compressed Sparse Row format\n> 3. bsr\\_matrix: Block Sparse Row format\n> 4. lil\\_matrix: List of Lists format\n> 5. dok\\_matrix: Dictionary of Keys format\n> 6. coo\\_matrix: COOrdinate format (aka IJV, triplet format)\n> 7. dia\\_matrix: DIAgonal format\n> \n> \n> \n\n\nTo construct a matrix efficiently, use either dok\\_matrix or lil\\_matrix.\nThe lil\\_matrix class supports basic slicing and fancy indexing with a\nsimilar syntax to NumPy arrays. As illustrated below, the COO format\nmay also be used to efficiently construct matrices. Despite their\nsimilarity to NumPy arrays, it is **strongly discouraged** to use NumPy\nfunctions directly on these matrices because NumPy may not properly convert\nthem for computations, leading to unexpected (and incorrect) results. If you\ndo want to apply a NumPy function to these matrices, first check if SciPy has\nits own implementation for the given sparse matrix class, or **convert the\nsparse matrix to a NumPy array** (e.g., using the *toarray()* method of the\nclass) first before applying the method.\n\n\nTo perform manipulations such as multiplication or inversion, first\nconvert the matrix to either CSC or CSR format. The lil\\_matrix format is\nrow-based, so conversion to CSR is efficient, whereas conversion to CSC\nis less so.\n\n\nAll conversions among the CSR, CSC, and COO formats are efficient\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'scipy' API to apply various image transformations to a sample image. The program should perform operations like shifting and rotating the image and display the original and transformed images.\n\n### Code:"
    },
    {
        "api": "scipy",
        "decoder_input_ids": true,
        "ground-truth": "import scipy as sp\nfrom scipy.optimize import minimize\n\n# Define the objective function\ndef objective(x):\n    return x[0]**2 + x[1]**2\n\n# Define the nonlinear inequality constraints\ndef constraint(x):\n    return x[0]**2 + x[1]**2 - 1\n\n# Specify the optimization method\nmethod = 'SLSQP'\n\n# Specify the initial guess for the variables\nx0 = [0, 0]\n\n# Specify the constraints\nconstraints = [{'type': 'ineq', 'fun': constraint}]\n\n# Find the optimal solution with nonlinear inequality constraints\nresult = minimize(objective, x0, method=method, constraints=constraints)\n\n# Display the optimal solution\nprint(\"Optimal solution:\", result.x)",
        "instruction": "Create a Python program using the 'scipy' API to perform numerical optimization with nonlinear inequality constraints. The program should define an objective function to be minimized or maximized, specify the nonlinear inequality constraints, and use the 'scipy.optimize.minimize' function with the 'constraints' parameter to find the optimal solution.",
        "prediction": ":import scipy as sp\nfrom scipy.optimize import minimize\n\n# Define the objective function\ndef objective(x):\n    return x[0]**2 + x[1]**2\n\n# Define the nonlinear inequality constraints\ndef constraint(x):\n    return x[0]**2 + x[1]**2 - 1\n\n# Specify the optimization method\nmethod = 'SLSQP'\n\n# Specify the initial guess for the variables\nx0 = [0, 0]\n\n# Specify the constraints\nconstraints = [{'type': 'eq', 'fun': constraint}]\n\n# Find the optimal solution with nonlinear inequality constraints\nresult = minimize(objective, x0, method=method, constraints=constraints)\n\n# Display the optimal solution\nprint(\"Optimal solution:\", result.x)",
        "source": "This is some documentation of scipy API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n# SciPy API#\n\n## Importing from SciPy#\n\n\nIn Python, the distinction between what is the public API of a library and what\nare private implementation details is not always clear. Unlike in other\nlanguages like Java, it is possible in Python to access \u00e2\u0080\u009cprivate\u00e2\u0080\u009d functions or\nobjects.\n\n==================\n Document 1 \n----------------\n## Old API#\n\n\nThese are the routines developed earlier for SciPy. They wrap older solvers\nimplemented in Fortran (mostly ODEPACK). While the interface to them is not\nparticularly convenient and certain features are missing compared to the new\nAPI, the solvers themselves are of good quality and work fast as compiled\nFortran code. In some cases, it might be worth using this old API.\n\n\n|  |  |\n| --- | --- |\n| `odeint`(func,\u00c2\u00a0y0,\u00c2\u00a0t[,\u00c2\u00a0args,\u00c2\u00a0Dfun,\u00c2\u00a0col\\_deriv,\u00c2\u00a0...]) | Integrate a system of ordinary differential equations. |\n| `ode`(f[,\u00c2\u00a0jac]) | A generic interface class to numeric integrators. |\n| `complex\\_ode`(f[,\u00c2\u00a0jac]) | A wrapper of ode for complex systems. |\n\n## Solving boundary value problems for ODE systems#\n\n\n|  |  |\n| --- | --- |\n| `solve\\_bvp`(fun,\u00c2\u00a0bc,\u00c2\u00a0x,\u00c2\u00a0y[,\u00c2\u00a0p,\u00c2\u00a0S,\u00c2\u00a0fun\\_jac,\u00c2\u00a0...]) | Solve a boundary value problem for a system of ODEs. |\n\n\n\n# Interpolation (`scipy.interpolate`)#\n\n\nSub-package for objects used in interpolation.\n\n\nAs listed below, this sub-package contains spline functions and classes,\n1-D and multidimensional (univariate and multivariate)\ninterpolation classes, Lagrange and Taylor polynomial interpolators, and\nwrappers for FITPACK\nand DFITPACK functions.\n\n## Univariate interpolation#\n\n\n|  |  |\n| --- | --- |\n| `interp1d`\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'scipy' API to perform numerical optimization with nonlinear inequality constraints. The program should define an objective function to be minimized or maximized, specify the nonlinear inequality constraints, and use the 'scipy.optimize.minimize' function with the 'constraints' parameter to find the optimal solution.\n\n### Code:"
    },
    {
        "api": "spacy",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install spacy\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef extract_named_entities(text):\n    doc = nlp(text)\n    entities = []\n    for ent in doc.ents:\n        entities.append((ent.text, ent.label_))\n    return entities\n\ntext = \"Apple Inc. is a technology company headquartered in Cupertino, California. Tim Cook is the CEO of Apple.\"\nentities = extract_named_entities(text)\nfor entity in entities:\n    print(entity)",
        "instruction": "Create a Python program using the 'spacy' API to analyze text and extract named entities. The program should identify and label different types of named entities such as persons, organizations, and locations.",
        "prediction": ":import spacy\nfrom spacy.lang.en import English\n\nnlp = English()\n\ndef extract_named_entities(text):\n    doc = nlp(text)\n    entities = []\n    for ent in doc.ents:\n        entities.append((ent.text, ent.label_))\n    return entities\n\ntext = \"Apple Inc. is planning to open a new store in New York City. Tim Cook, the CEO of Apple, announced the news yesterday.\"\nentities = extract_named_entities(text)\nfor entity in entities:\n    print(entity)",
        "source": "This is some documentation of spacy API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n# Library Architecture\n\nThe central data structures in spaCy are the `Language` class,\nthe `Vocab` and the `Doc` object. The `Language` class\nis used to process a text and turn it into a `Doc` object. It\u2019s typically stored\nas a variable called `nlp`. The\n\n==================\n Document 1 \n----------------\n Library Architecture\n\nThe central data structures in spaCy are the `Language` class,\nthe `Vocab` and the `Doc` object. The `Language` class\nis used to process a text and turn it into a `Doc` object. It\u2019s typically stored\nas a variable called `nlp`. The `Doc` object owns the **sequence of tokens** and\nall their annotations. By centralizing strings, word vectors and lexical\nattributes in the `Vocab`, we avoid storing multiple copies of this data. This\nsaves memory, and ensures there\u2019s a **single source of truth**.\n\n\nText annotations are also designed to allow a single source of truth: the `Doc`\nobject owns the data, and `Span` and `Token` are\n**views that point into it**. The `Doc` object is constructed by the\n`Tokenizer`, and then **modified in place** by the components\nof the pipeline. The `Language` object coordinates these components. It takes\nraw text and sends it through the pipeline, returning an **annotated document**.\nIt also orchestrates training and serialization.\n\n\n### Container objects\n\n\n| Name | Description |\n| --- | --- |\n| `Doc` | A container for accessing linguistic annotations. |\n| `DocBin` | A collection of `Doc` objects for efficient binary serialization. Also used for training data. |\n| `Example` | A\n\n==================\n Document 2 \n----------------\n## Calling the training function from Python  v3.2\n\nThe training CLI exposes a `train` helper function that lets you run the\ntraining just like `spacy train`. Usually it\u2019s easier to use the command line\ndirectly, but if you need to kick off training from code this is how to do it.\n\n| Name | Description |\n| --- | --- |\n| `config_path` | Path\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'spacy' API to analyze text and extract named entities. The program should identify and label different types of named entities such as persons, organizations, and locations.\n\n### Code:"
    },
    {
        "api": "spacy",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install spacy\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef sentiment_analysis(text):\n    doc = nlp(text)\n    sentiment = \"Neutral\"\n    # Perform sentiment analysis based on the content of the text\n    # Add your sentiment analysis logic here\n    return sentiment\n\ntext = \"I love this movie!\"\nsentiment = sentiment_analysis(text)\nprint(sentiment)",
        "instruction": "Create a Python program using the 'spacy' API to perform sentiment analysis on a given text. The program should analyze the sentiment of the text and classify it as positive, negative, or neutral.",
        "prediction": ":import spacy\nfrom spacy.lang.en import English\nfrom spacy.matcher import Matcher\n\nnlp = English()\n\ndoc = nlp(\"The text is positive Neutral Negative neutral Neutral neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral Negative neutral",
        "source": "This is some documentation of spacy API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n# Library Architecture\n\nThe central data structures in spaCy are the `Language` class,\nthe `Vocab` and the `Doc` object. The `Language` class\nis used to process a text and turn it into a `Doc` object. It\u2019s typically stored\nas a variable called `nlp`. The\n\n==================\n Document 1 \n----------------\n Library Architecture\n\nThe central data structures in spaCy are the `Language` class,\nthe `Vocab` and the `Doc` object. The `Language` class\nis used to process a text and turn it into a `Doc` object. It\u2019s typically stored\nas a variable called `nlp`. The `Doc` object owns the **sequence of tokens** and\nall their annotations. By centralizing strings, word vectors and lexical\nattributes in the `Vocab`, we avoid storing multiple copies of this data. This\nsaves memory, and ensures there\u2019s a **single source of truth**.\n\n\nText annotations are also designed to allow a single source of truth: the `Doc`\nobject owns the data, and `Span` and `Token` are\n**views that point into it**. The `Doc` object is constructed by the\n`Tokenizer`, and then **modified in place** by the components\nof the pipeline. The `Language` object coordinates these components. It takes\nraw text and sends it through the pipeline, returning an **annotated document**.\nIt also orchestrates training and serialization.\n\n\n### Container objects\n\n\n| Name | Description |\n| --- | --- |\n| `Doc` | A container for accessing linguistic annotations. |\n| `DocBin` | A collection of `Doc` objects for efficient binary serialization. Also used for training data. |\n| `Example` | A\n\n==================\n Document 2 \n----------------\n## Annotation format for creating training examples\n\nAn `Example` object holds the information for one training\ninstance. It stores two `Doc` objects: one for holding the\ngold-standard reference data, and one for holding the predictions of the\npipeline. Examples can be created using the\n`Example.from_dict` method with a reference `Doc` and\na dictionary of gold-standard annotations.\n\n| Name |\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'spacy' API to perform sentiment analysis on a given text. The program should analyze the sentiment of the text and classify it as positive, negative, or neutral.\n\n### Code:"
    },
    {
        "api": "stumpy",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install stumpy\nimport stumpy\nimport numpy as np\n\n# Generate a random time series\nyour_time_series = np.random.rand(1000)\n\n# Specify the motif length\nmotif_length = 30\n\n# Perform motif discovery\nmotif_indices = stumpy.motif(your_time_series, motif_length)\n\n# Print the indices of identified motifs\nprint(f\"Motif Indices: {motif_indices}\")",
        "instruction": "Develop a Python program using the 'stumpy' API to perform motif discovery in a given time series. The program should allow the user to specify the desired motif length and find the motifs within the time series. It should return the indices of all identified motifs.",
        "prediction": ":import stumpy\nimport stumpy\nimport numpy as np\n\n# Generate a random time series\nyour_time_series = np.random.rand(1000)\n\n# Specify the motif length\nmotif_length = 50\n\n# Perform motif discovery\nmotif_indices = stumpy.motif(your_time_series, motif_length)\n\n# Print the indices of identified motifs\nprint(f\"Motif Indices: {motif_indices}\")",
        "source": "This is some documentation of stumpy API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n# motifs#\n\n\nstumpy.motifs(*T*, *P*, *min\\_neighbors=1*, *max\\_distance=None*, *cutoff=None*, *max\\_matches=10*, *max\\_motifs=1*, *atol=1e-08*, *normalize=True*, *p=2.0*, *T\\_subseq\\_isconstant=None*)[source]#\nDiscover the top motifs for time series T\n\n\nA subsequence, Q, becomes a candidate motif if there are at least min\\_neighbor\nnumber of other subsequence matches in T (outside the exclusion zone) with a\ndistance less or equal to max\\_distance.\n\n\nNote that, in the best case scenario, the returned arrays would have shape\n(max\\_motifs, max\\_matches) and contain all finite values. However, in reality,\nmany conditions (see below) need to be satisfied in order for this to be true. Any\ntruncation in the number of rows (i.e., motifs) may be the result of insufficient\ncandidate motifs with matches greater than or equal to min\\_neighbors or that the\nmatrix profile value for the candidate motif was larger than cutoff. Similarly,\nany truncation in the number of columns (i.e., matches) may be the result of\ninsufficient matches being found with distances (to their corresponding candidate\nmotif) that are equal to or less than max\\_distance. Only motifs and matches that\nsatisfy all of these constraints will be returned.\n\n\nIf you must return a shape of (max\\_motifs, max\\_matches), then you may consider\nspecifying a smaller min\\_neighbors, a larger max\\_distance, and/or a larger\ncutoff. For example, while it is ill advised, setting min\\_neighbors=1,\nmax\\_distance=np.inf, and cutoff=np.inf will ensure that the shape of the output\narrays will be (max\\_motifs, max\\_matches). However, given the lack of constraints,\nthe quality of each motif and the quality of each match may be drastically\ndifferent. Setting appropriate conditions will help\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program using the 'stumpy' API to perform motif discovery in a given time series. The program should allow the user to specify the desired motif length and find the motifs within the time series. It should return the indices of all identified motifs.\n\n### Code:"
    },
    {
        "api": "stumpy",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install stumpy\nimport stumpy\nimport numpy as np\n\n# Generate a random time series\nyour_time_series = np.random.rand(10000)\nwindow_size = 50  \n\n# Compute the e z-normalized matrix profile \nmatrix_profile = stumpy.stump(your_time_series, m=window_size)\n\noutput_file_path = \"matrix_profile.txt\"\nnp.savetxt(output_file_path, matrix_profile)",
        "instruction": "Develop a Python program using the 'stumpy' API to generate a matrix profile for a given random time series data. The program should compute the e z-normalized matrix profile, and the resulting profile should be saved to a text file.",
        "prediction": ":import stumpy\nimport stumpy\nimport numpy as np\n\n# Generate a random time series data\nyour_random_time series = np.random.rand(1000)\n\n# Compute the Pan Matrix Profile\npan_matrix_profile = pan_matrix_profile(your_random_time series, min_m=3, max_m=None, step=1, percentage=0.01,\npre_scrump=True, normalize=True, p=2.0, T_subseq_isconstant_func=None)\n\n# Save the Pan Matrix Profile to a text file\noutput_text_file = 'e z-normalized Pan Matrix Profile.txt'\nwith open(output_text_file, 'w') as output_text_file:\n    for profile in pan_matrix_profile:\n        print(f\"{profile}\", file=output_text_file)",
        "source": "This is some documentation of stumpy API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n# STUMPY API#\n\n\nOverview\n\n\n|  |  |\n| --- | --- |\n| `stumpy.stump` | Compute the z-normalized matrix profile |\n| `stumpy.stumped` | Compute the z-normalized matrix profile with a distributed dask/ray cluster |\n| `stumpy.gpu\\_stump` | Compute the z-normalized matrix profile with\n\n==================\n Document 1 \n----------------\n# stimp#\n\n\nstumpy.stimp(*T*, *min\\_m=3*, *max\\_m=None*, *step=1*, *percentage=0.01*, *pre\\_scrump=True*, *normalize=True*, *p=2.0*, *T\\_subseq\\_isconstant\\_func=None*)[source]#\nCompute the Pan Matrix Profile\n\n\nThis is based on the SKIMP algorithm.\n\nParameters:\n* **T** (*numpy.ndarray*) \u2013 The time series or sequence for which to compute the pan matrix profile\n* **min\\_m** (*int**,* *default 3*) \u2013 The starting (or minimum) subsequence window size for which a matrix profile\nmay be computed\n* **max\\_m** (*int**,* *default None*) \u2013 The stopping (or maximum) subsequence window size for which a matrix profile\nmay be computed. When max\\_m = Non, this is set to the maximum allowable\nsubsequence window size\n* **step** (*int**,* *default 1*) \u2013 The step between subsequence window sizes\n* **percentage** (*float**,* *default 0.01*) \u2013 The percentage of the full matrix profile to compute for each subsequence\nwindow size. When percentage < 1.0, then the scrump algorithm is used.\nOtherwise, the stump algorithm is used when the exact matrix profile is\nrequested.\n* **pre\\_scrump** (*bool**,* *default True*) \u2013 A flag for whether or not to perform the PreSCRIMP calculation prior to\ncomputing SCRIMP. If set to True, this is equivalent to computing\nSCRIMP++. This parameter is ignored when percentage = 1.0.\n*\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program using the 'stumpy' API to generate a matrix profile for a given random time series data. The program should compute the e z-normalized matrix profile, and the resulting profile should be saved to a text file.\n\n### Code:"
    },
    {
        "api": "supervision",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install ultralytics supervision opencv-python\nimport cv2\nfrom ultralytics import YOLO\nimport supervision as sv\nimport numpy as np\n\nZONE_POLYGON = np.array([\n    [0, 0],\n    [0.5, 0],\n    [0.5, 1],\n    [0, 1]\n])\nwebcam_resolution = (1280, 720)\n\nframe_width, frame_height = webcam_resolution\n\ncap = cv2.VideoCapture(0)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, frame_width)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, frame_height)\n\nmodel = YOLO(\"yolov8s.pt\")\n\nbox_annotator = sv.BoxAnnotator(\n        thickness=2,\n        text_thickness=2,\n        text_scale=1\n    )\n\nzone_polygon = (ZONE_POLYGON * np.array(webcam_resolution)).astype(int)\nzone = sv.PolygonZone(polygon=zone_polygon, frame_resolution_wh=tuple(webcam_resolution))\nzone_annotator = sv.PolygonZoneAnnotator(\n        zone=zone, \n        color=sv.Color.red(),\n        thickness=2,\n        text_thickness=4,\n        text_scale=2\n    )\n\nwhile True:\n  _, frame = cap.read()\n\n  result = model(frame, agnostic_nms=True)[0]\n  detections = sv.Detections.from_ultralytics(result)\n  print(detections)\n  labels = [\n            f\"{model.model.names[class_id]} {confidence:0.2f}\"\n            for _, _, confidence, class_id, _\n            in detections\n        ]\n  frame = box_annotator.annotate(\n            scene=frame, \n            detections=detections, \n            labels=labels\n        )\n\n  zone.trigger(detections=detections)\n  frame = zone_annotator.annotate(scene=frame)      \n        \n  cv2.imshow(\"yolov8\", frame)\n\n  if (cv2.waitKey(30) == 27):\n      break",
        "instruction": "Develop a Python program that utilizes the 'supervision' API for object detection using a YOLO model on a webcam feed. Annotate detected objects with labels and draw a red polygon zone on the video frame. Use the Ultralytics YOLO model for object detection.",
        "prediction": ":pip install supervision opencv-python\nimport cv2\nimport supervision as sv\n\n# Load the YOLO model\nmodel = sv.ObjectDetector.from_pretrained('ultralytics_yolov5')\n\n# Open the webcam feed\nwhile True:\n    frame = cv2.imread('frame.jpg')\n    detections = model.detect(frame)\n    # Draw labels and the detection zone on the frame\n    frame = sv.draw.from_detections(frame, detections)\n    # Display the frame with the detection zone drawn\n    cv2.imshow('Object Detection', frame)\n    if cv2.waitKey(1) == ord('q'):\n        break\n\n# draw detections on the frame\nframe = sv.draw.from_detections(frame, detections)\n\n# Display the frame with the detection zone drawn\ncv2.imshow('Object Detection', frame)\n\nif cv2.waitKey(1) == ord('q'):\n    break\n\n# draw bounding boxes on the frame\nframe = sv.draw.from_detections(frame, detections)\n\n# Display the frame with the bounding boxes drawn\ncv2.imshow('Object Detection', frame)\n\nif cv2.waitKey(1) == ord('q'):\n    break\n\n# draw bounding boxes on the frame\nframe = sv.draw.from_detections(frame, detections)\n\n# Draw bounding boxes on the frame\nframe = sv.draw.from_detections(frame, detections)\n\n# Display the frame with the bounding boxes drawn\ncv2.imshow('Object Detection', frame)\n\nif cv2.waitKey(1) == ord('q'):\n    break\n",
        "source": "This is some documentation of supervision API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n\n# Home\n\n\n\n\n## \ud83d\udc4b Hello\u00b6\n\n\nWe write your reusable computer vision tools. Whether you need to load your dataset from your hard drive, draw detections on an image or video, or count how many detections are in a zone. You can count on us!\n\n\n\n## \ud83d\udcbb Install\u00b6\n\n\nYou can install `supervision` with pip in a\n**3.11>=Python>=3.8** environment.\n\npip install (recommended)\n\n\nheadlessdesktop\n\n\nThe headless installation of `supervision` is designed for environments where graphical user interfaces (GUI) are not needed, making it more lightweight and suitable for server-side applications.\n\n```\npip install supervision\n\n```\n\nIf you require the full version of `supervision` with GUI support you can install the desktop version. This version includes the GUI components of OpenCV, allowing you to display images and videos on the screen.\n\n```\npip install supervision[desktop]\n\n\ngit clone (for development)\n\n\nvirtualenvpoetry\n\n```\n\n# clone repository and navigate to root directory\ngit clone https://github.com/roboflow/supervision.git\ncd supervision\n\n\n# setup python environment and activate it\npython3 -m venv venv\nsource venv/bin/activate\n\n\n# headless install\npip install -e \".\"\n\n\n# desktop install\npip install -e \".[desktop]\"\n\n\n```\n\n# clone repository and navigate to root directory\ngit clone https://github.com/roboflow/supervision.git\ncd supervision\n\n\n# setup python environment and activate it\npoetry env use python 3.10\npoetry shell\n\n\n# headless install\npoetry install\n\n\n# desktop install\npoetry install --extras \"desktop\"\n\n\n\n# Detections\n\n\n\n## advanced filtering\u00b6\n\n\nThe advanced filtering capabilities of the `Detections` class offer users a versatile and efficient way to narrow down\nand refine object detections. This section outlines various filtering methods, including filtering by specific class\nor a set of classes, confidence, object area, bounding box area, relative area, box dimensions, and designated zones.\nEach method is demonstrated with concise code examples to provide users with a clear understanding\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program that utilizes the 'supervision' API for object detection using a YOLO model on a webcam feed. Annotate detected objects with labels and draw a red polygon zone on the video frame. Use the Ultralytics YOLO model for object detection.\n\n### Code:"
    },
    {
        "api": "supervision",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install supervision torchvision\nimport torch\nimport torchvision.transforms as transforms\nimport supervision as sv\n\n# Load the pre-trained ResNet model\nmodel = torch.hub.load('pytorch/vision:v0.9.0', 'resnet18', pretrained=True)\nmodel.eval()\n\n# Define the image transformation pipeline\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Load the image from file\nimage_path = 'image.jpg'\nimage = sv.Image.from_file(image_path)\n\n# Apply the transformation pipeline to the image\ninput_tensor = transform(image)\n\n# Add a batch dimension to the input tensor\ninput_tensor = input_tensor.unsqueeze(0)\n\n# Perform the classification\nwith torch.no_grad():\n    output = model(input_tensor)\n\n# Get the predicted class label and confidence score\n_, predicted_class = torch.max(output, 1)\nconfidence = torch.nn.functional.softmax(output, dim=1)[0, predicted_class]\n\n# Display the predicted class label and confidence score\nprint(f\"Predicted class: {predicted_class.item()}\")\nprint(f\"Confidence: {confidence.item()}\")",
        "instruction": "Develop a Python program that utilizes the 'supervision' API to perform image classification using a pre-trained ResNet model. Load an image from a file, classify it using the ResNet model, and display the predicted class label along with the confidence score.",
        "prediction": ":pip install supervision opencv-python\nimport cv2\nimport supervision as sv\n\n# Load the pre-trained ResNet model\nmodel = sv.Classification.resnet.resnet50.ResNet50()\n\n# Load the image\nfile_path = 'image.jpg'\nimg = sv.Image.from_file(file_path)\n\n# Classify the image using the ResNet model\nclass_label = model.classify(img)\n\n# Display the predicted class label and confidence score\nprint(f\"Class Label: {class_label}\")",
        "source": "This is some documentation of supervision API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n\n# Home\n\n\n\n\n## \ud83d\udc4b Hello\u00b6\n\n\nWe write your reusable computer vision tools. Whether you need to load your dataset from your hard drive, draw detections on an image or video, or count how many detections are in a zone. You can count on us!\n\n\n\n## \ud83d\udcbb Install\u00b6\n\n\nYou can install `supervision` with pip in a\n**3.11>=Python>=3.8** environment.\n\npip install (recommended)\n\n\nheadlessdesktop\n\n\nThe headless installation of `supervision` is designed for environments where graphical user interfaces (GUI) are not needed, making it more lightweight and suitable for server-side applications.\n\n```\npip install supervision\n\n```\n\nIf you require the full version of `supervision` with GUI support you can install the desktop version. This version includes the GUI components of OpenCV, allowing you to display images and videos on the screen.\n\n```\npip install supervision[desktop]\n\n\ngit clone (for development)\n\n\nvirtualenvpoetry\n\n```\n\n# clone repository and navigate to root directory\ngit clone https://github.com/roboflow/supervision.git\ncd supervision\n\n\n# setup python environment and activate it\npython3 -m venv venv\nsource venv/bin/activate\n\n\n# headless install\npip install -e \".\"\n\n\n# desktop install\npip install -e \".[desktop]\"\n\n\n```\n\n# clone repository and navigate to root directory\ngit clone https://github.com/roboflow/supervision.git\ncd supervision\n\n\n# setup python environment and activate it\npoetry env use python 3.10\npoetry shell\n\n\n# headless install\npoetry install\n\n\n# desktop install\npoetry install --extras \"desktop\"\n\n\n\n# Detections\n\n\n\n## advanced filtering\u00b6\n\n\nThe advanced filtering capabilities of the `Detections` class offer users a versatile and efficient way to narrow down\nand refine object detections. This section outlines various filtering methods, including filtering by specific class\nor a set of classes, confidence, object area, bounding box area, relative area, box dimensions, and designated zones.\nEach method is demonstrated with concise code examples to provide users with a clear understanding\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nDevelop a Python program that utilizes the 'supervision' API to perform image classification using a pre-trained ResNet model. Load an image from a file, classify it using the ResNet model, and display the predicted class label along with the confidence score.\n\n### Code:"
    },
    {
        "api": "tensorflow",
        "decoder_input_ids": true,
        "ground-truth": "#pip install tensorflow\nimport tensorflow as tf\nimport numpy as np\n\n# Generate synthetic data for binary classification\ndata_size = 1000\ndata_x = np.random.rand(data_size, 2)\ndata_y = (data_x[:, 0] + data_x[:, 1] > 1).astype(int)\n\n# Build a simple feedforward neural network\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(2,)),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model on the synthetic data\nprint(\"Training............\")\nmodel.fit(data_x, data_y, epochs=10)\n\n\nprint(\"\\nEvaluation............\")\nloss, accuracy = model.evaluate(data_x, data_y)\nprint(\"Accuracy: \",accuracy)\nprint(\"Loss: \",loss)\n\npredictions = model.predict(data_x)\nprint(\"Are all predictions between 0 and 1?\",np.all(predictions >= 0) and np.all(predictions <= 1))",
        "instruction": "Create a Python program that uses the 'tensorflow' API to build a simple feedforward neural network for binary classification. The program should generate synthetic data, compile the model, train it on the data, and evaluate its performance.",
        "prediction": ":pip install tensorflow\nimport tensorflow as tf\nfrom tensorflow.python import client\n\n# Generate synthetic data\nsynthetic_data = true\nif synthetic_data:\n    # Generate synthetic data\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    synthetic_data = true\n    ",
        "source": "This is some documentation of tensorflow API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n Input builders\ndef input_fn_train:\n  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n  # index.\n  pass\ndef input_fn_eval:\n  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n  # index.\n  pass\ndef input_fn_predict:\n  # Returns tf.data.Dataset of (x, None) tuple.\n  pass\nestimator.train(input_fn=input_fn_train)\nmetrics = estimator.evaluate(input_fn=input_fn_eval)\npredictions = estimator.predict(input_fn=input_fn_predict)\n\nInput of `train` and `evaluate` should have following features,\notherwise there will be a `KeyError`:\n\n\n* if `weight_column` is not `None`, a feature with `key=weight_column` whose\nvalue is a `Tensor`.\n* for each `column` in `feature_columns`:\n\t+ if `column` is a `CategoricalColumn`, a feature with `key=column.name`\n\twhose `value` is a `SparseTensor`.\n\t+ if `column` is a `WeightedCategoricalColumn`, two features: the first\n\twith `key` the id column name, the second with `key` the weight column\n\tname. Both features' `value` must be a `SparseTensor`.\n\t+ if `column` is a `DenseColumn`, a feature with `key=column.name`\n\twhose `value` is a `Tensor`.\n\n\nLoss is calculated by using softmax cross entropy.\n\n# tf.compat.v1.estimator.DNNEstimator\n\nAn estimator for TensorFlow DNN models with user-specified head. (deprecated) (deprecated)\n\ntf.compat.v1.estimator.DNNEstimator(\n    head,\n    hidden_units,\n    feature_columns,\n    model_dir=None,\n    optimizer='Adagrad',\n    activation_fn=\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that uses the 'tensorflow' API to build a simple feedforward neural network for binary classification. The program should generate synthetic data, compile the model, train it on the data, and evaluate its performance.\n\n### Code:"
    },
    {
        "api": "tensorflow",
        "decoder_input_ids": true,
        "ground-truth": "#pip install tensorflow\nimport tensorflow as tf\nimport numpy as np\n\n# Generate a synthetic sequence dataset\nseq_length = 100\nsequences = [np.random.randint(0, 10, size=seq_length) for _ in range(1000)]\ntargets = [np.sum(seq) for seq in sequences]\n\n# Preprocess the data\nsequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post')\ntargets = np.array(targets)\n\n# Build a simple recurrent neural network\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=10, output_dim=16, input_length=seq_length),\n    tf.keras.layers.LSTM(64, activation='relu', return_sequences=False),\n    tf.keras.layers.Dense(1)\n])\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='mean_squared_error')\n\n# Train the model on the synthetic sequence data\nmodel.fit(sequences, targets, epochs=5)\n\n# Generate predictions on new sequences\nnew_sequence = np.array([np.random.randint(0, 10, size=seq_length)])\nprediction = model.predict(new_sequence)\nprint(\"Predicted sum of the new sequence:\", prediction[0][0])",
        "instruction": "Create a Python program that uses the 'tensorflow' API to implement a recurrent neural network (RNN) for sequence prediction. The program should use a sequence dataset, preprocess it, build an RNN model, train it, and generate predictions on new sequences.",
        "prediction": ":pip install tensorflow\nimport tensorflow as tf\nimport numpy as np\n\n# Generate a sequence dataset. sequence_dataset = dataset.make_sequence_dataset(length=10, batch_\ufffdsize=10)\nsequence_dataset = tf.data.Dataset.from_generator(generator_fn, output_types=tf.int32, output_shapes=(10))\n\n# Preprocess the sequence dataset\nsequence_dataset = sequence_dataset.repeat(1)\n\n# Build an RNN model for sequence prediction\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(10, 10),\n    tf.keras.layers.LSTM(64, return_sequences=True),\n    tf.keras.layers.LSTM(64)\n  ])\n\n# Generate predictions on new sequences\nsequence_predictions = []\nfor batch in sequence_dataset.batch(length=10, shuffle=False):\n  sequence_predictions.append(model.predict_on_batch(batch))\n\n# Get predictions on new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Get sequences for predictions\nsequence_predictions = sequence_predictions.flatten()\nsequence_predictions = sequence_predictions.numpy()\n\n# Get generated predictions on new sequences\nsequence_predictions = sequence_predictions.astype(int)\n\n# Generate new sequences\nsequence_predictions = sequence_predictions.tolist()\n\n# Generate generated sequences for generation\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate generated sequences for generation\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Get generated sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Get generated sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Get generated sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Get generated sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Get generated sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Get generated sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions = sequence_predictions[-length:]\n\n# Generate new sequences\nsequence_predictions",
        "source": "This is some documentation of tensorflow API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n\n\n* TensorFlow\n* API\n* TensorFlow v2.14.0\n\n# TensorFlow API Versions\n\n\n \n Stay organized with collections\n \n\n \n Save and categorize content based on your preferences.\n \n\n\nThe following versions of the TensorFlow api-docs are currently available.\nMajor features, improvements, and changes of each version are available in the\nrelease notes.\n\n\n\n## TensorFlow 2\n\n\n* r2.12 -\nrelease notes\n* r2.11 -\nrelease notes\n* r2.10 -\nrelease notes\n* r2.9 - \nrelease notes\n* r2.8 - \nrelease notes\n* r2.7- \nrelease notes\n* r2.6- \nrelease notes\n* r2.5- \nrelease notes\n* r2.4- \nrelease notes\n* r2.3- \nrelease notes\n* r2.2- \nrelease notes\n* r2.1- \nrelease notes\n* r2.0- \nrelease notes\n\n\n## TensorFlow 1\n\n\n* r1.15 -\nrelease notes\n* r1.14 -\nrelease notes\n* r1.13 -\nrelease notes\n* r1.12 -\nrelease notes\n* r1.11 -\nrelease notes\n* r1.10 -\nrelease notes\n* r1.9 -\nrelease notes\n* r1.8 -\nrelease notes\n* r1.7 -\nrelease notes\n* r1.6 -\nrelease notes\n* r1.5 -\nrelease notes\n* r1.4 -\nrelease notes\n* r1.3\n\n==================\n Document 1 \n----------------\n Input builders\ndef input_fn_train:\n  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n  # index.\n  pass\ndef input_fn_eval:\n  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n  # index.\n  pass\ndef input_fn_predict:\n  # Returns tf.data.Dataset of (x, None) tuple.\n  pass\nestimator.\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that uses the 'tensorflow' API to implement a recurrent neural network (RNN) for sequence prediction. The program should use a sequence dataset, preprocess it, build an RNN model, train it, and generate predictions on new sequences.\n\n### Code:"
    },
    {
        "api": "validators",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install validators\nimport validators\nimport requests\n\ndef validate_url(url):\n    if validators.url(url):\n        try:\n            response = requests.get(url)\n            if response.status_code == 200:\n                if validators.url(url, public=True, schemes=['https']):\n                    return \"Valid URL, reachable, and secure (HTTPS). Response status code: \" + str(response.status_code)\n                else:\n                    return \"Valid URL, reachable, but not secure (HTTP). Response status code: \" + str(response.status_code)\n            else:\n                return \"Valid URL but unreachable. Response status code: \" + str(response.status_code)\n        except requests.exceptions.RequestException:\n            return \"Valid URL but unreachable.\"\n    else:\n        return \"Invalid URL.\"\n\n# Get user inputs\nuser_url = input(\"Enter a URL: \")\n\n# Validate and display results\nprint(validate_url(user_url))\n",
        "instruction": "Create a Python program using the 'validators' API to validate user-provided URLs, checking if they adhere to the standard URL format. Additionally, the program should also check if the URL is reachable by making a request to it and display the response status code. If the URL is not reachable, the program should display an appropriate error message. Finally, the program should also check if the URL is secure (HTTPS) and display a message indicating its security status.",
        "prediction": ":import validators\nimport validators\nimport requests\n\ndef validate_url(url):\n    if validators.url(url):\n        try:\n            response = requests.get(url)\n            if response.status_code == 200:\n                return \"Valid URL and reachable. Response status code: \" + str(response.status_code)\n            else:\n                return \"Valid URL but unreachable. Response status code: \" + str(response.status_code)\n        except requests.exceptions.RequestException:\n            return \"Valid URL but unreachable.\"\n    else:\n        return \"Invalid URL.\"\n\n# Get user inputs\nuser_url = input(\"Enter a URL: \")\n\n# Validate and display results\nprint(validate_url(user_url))\n",
        "source": "This is some documentation of validators API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n Output: ValidationError(func=url,...)\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `value` | `str` | \nURL string to validate.\n | *required* |\n| `skip_ipv6_addr` | `bool` | \nWhen URL string cannot contain an IPv6 address.\n | `False` |\n| `skip_ipv4_addr` | `bool` | \nWhen URL string cannot contain an IPv4 address.\n | `False` |\n| `may_have_port` | `bool` | \nURL string may contain port number.\n | `True` |\n| `simple_host` | `bool` | \nURL string maybe only hyphens and alpha-numerals.\n | `False` |\n| `strict_query` | `bool` | \nFail validation on query string parsing error.\n | `True` |\n| `rfc_1034` | `bool` | \nAllow trailing dot in domain/host name.\nRef: RFC 1034.\n | `False` |\n| `rfc_2782` | `bool` | \nDomain/Host name is of type service record.\nRef: RFC 2782.\n | `False` |\n\nNote\n* *In version 0.11.3*:\n\t+ Added support for URLs containing localhost.\n* *In version 0.11.0*:\n\t+ Made the regular expression case insensitive.\n* *In version 0.10.3*:\n\t+ Added a `public` parameter.\n* *In version 0.10.2*:\n\t+ Added support for various exotic URLs.\n\t+ Fixed various false positives.\n\nSource code in `/opt/hostedtoolcache/Python/3.11.5/x64/lib/python3.11/site-packages/validators/url.py`\n\n\n|  |  |\n| --- | --- |\n| \n```\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'validators' API to validate user-provided URLs, checking if they adhere to the standard URL format. Additionally, the program should also check if the URL is reachable by making a request to it and display the response status code. If the URL is not reachable, the program should display an appropriate error message. Finally, the program should also check if the URL is secure (HTTPS) and display a message indicating its security status.\n\n### Code:"
    },
    {
        "api": "validators",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install validators\nimport validators\nimport smtplib\n\ndef validate_email(email):\n    if validators.email(email):\n        try:\n            server = smtplib.SMTP('smtp.gmail.com', 587)\n            server.starttls()\n            server.login(\"your_email_address\", \"your_email_password\")\n            response = server.sendmail(\"your_email_address\", email, \"Test message\")\n            server.quit()\n            if response == {}:\n                return \"Valid email address and deliverable.\"\n            else:\n                return \"Valid email address but undeliverable.\"\n        except smtplib.SMTPException:\n            return \"Valid email address but undeliverable.\"\n    else:\n        return \"Invalid email address.\"\n\n# Get user inputs\nuser_email = input(\"Enter an email address: \")\n\n# Validate and display results\nprint(validate_email(user_email))\n",
        "instruction": "Create a Python program using the 'validators' API to validate user-provided email addresses, checking if they adhere to the standard email format. Additionally, the program should also check if the email address is deliverable by making a request to the email server.",
        "prediction": ":import validators\nimport validators\n\ndef validate_email(email):\n    if validators.email(email):\n        if validators.ip_address_or_simple_host(email):\n            return \"Valid email address and deliverable by making a request to the email server.\"\n        else:\n            return \"Valid email address and not deliverable by making a request to the email server.\"\n    else:\n        return \"Invalid email address.\"\n\n# Get user inputs\nuser_email = input(\"Enter an email address: \")\n\n# Validate and display results\nprint(validate_email(user_email))\n",
        "source": "This is some documentation of validators API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n Output: ValidationError(email=email, args={'value': 'bogus@@'})\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `value` | `str` | \neMail string to validate.\n | *required* |\n| `ipv6_address` | `bool` | \nWhen the domain part is an IPv6 address.\n | `False` |\n| `ipv4_address` | `bool` | \nWhen the domain part is an IPv4 address.\n | `False` |\n| `simple_host` | `bool` | \nWhen the domain part is a simple hostname.\n | `False` |\n| `rfc_1034` | `bool` | \nAllow trailing dot in domain name.\nRef: RFC 1034.\n | `False` |\n| `rfc_2782` | `bool` | \nDomain name is of type service record.\nRef: RFC 2782.\n | `False` |\n\n\n| Type | Description |\n| --- | --- |\n| `Literal[True]` | \nIf `value` is a valid eMail.\n |\n| `ValidationError` | \nIf `value` is an invalid eMail.\n |\n\n> \n> *New in version 0.1.0*.\n> \n> \n> \n\nSource code in `/opt/hostedtoolcache/Python/3.11.5/x64/lib/python3.11/site-packages/validators/email.py`\n\n\n|  |  |\n| --- | --- |\n| \n```\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'validators' API to validate user-provided email addresses, checking if they adhere to the standard email format. Additionally, the program should also check if the email address is deliverable by making a request to the email server.\n\n### Code:"
    },
    {
        "api": "xarray",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install xarray\nimport xarray as xr\n\n# Read the NetCDF file\nds = xr.open_dataset('input.nc')\n\n# Extract the variable\nvar = ds['temperature']\n\n# Perform a mathematical operation on the variable\nvar_modified = var * 2\n\n# Save the modified variable to a new NetCDF file\nvar_modified.to_netcdf('output.nc')",
        "instruction": "Create a Python program using the 'xarray' API to read and manipulate a NetCDF file. The program should read a NetCDF file, extract a specific variable, perform a mathematical operation on the variable, and save the modified variable back to a new NetCDF file.",
        "prediction": ":import xarray as xr\nimport xarray as xr\n\n# Read the NetCDF file\nds = xr.open_dataset('input.nc')\n\n# Extract the specific variable\nvar = ds['temperature']\n\n# Perform mathematical operation on the variable\nmodified_variable = var + 10\n\n# Save the modified variable back to a new NetCDF file\nmodified_variable.to_netcdf('output.nc')",
        "source": "This is some documentation of xarray API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n## DataArray methods#\n\n\n|  |  |\n| --- | --- |\n| `load\\_dataarray`(filename\\_or\\_obj,\u00a0\\*\\*kwargs) | Open, load into memory, and close a DataArray from a file or file-like object containing a single data variable. |\n| `open\\_dataarray`(filename\\_or\\_obj,\u00a0\\*[,\u00a0engine,\u00a0...]) | Open an DataArray from a file or file-like object containing a single data variable. |\n| `DataArray.as\\_numpy`() | Coerces wrapped data and coordinates into numpy arrays, returning a DataArray. |\n| `DataArray.from\\_cdms2`(variable) | Convert a cdms2.Variable into an xarray.DataArray |\n| `DataArray.from\\_dict`(d) | Convert a dictionary into an xarray.DataArray |\n| `DataArray.from\\_iris`(cube) | Convert a iris.cube.Cube into an xarray.DataArray |\n| `DataArray.from\\_series`(series[,\u00a0sparse]) | Convert a pandas.Series into an xarray.DataArray. |\n| `DataArray.to\\_cdms2`() | Convert this array into a cdms2.Variable |\n| `DataArray.to\\_dask\\_dataframe`([dim\\_order,\u00a0...]) | Convert this array into a dask.dataframe.DataFrame. |\n| `DataArray.to\\_dataframe`([name,\u00a0dim\\_order]) | Convert this array and its coordinates into a tidy pandas.DataFrame. |\n| `DataArray.to\\_dataset`([dim,\u00a0name,\u00a0promote\\_attrs]) | Convert a DataArray to a Dataset. |\n| `DataArray.to\\_dict`([data,\u00a0encoding]) | Convert this xarray.DataArray into a dictionary following xarray naming conventions. |\n| `DataArray.to\\_index`() | Convert this variable to a pandas.Index. |\n| `DataArray.to\\_iris`() | Convert this array into a iris.\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'xarray' API to read and manipulate a NetCDF file. The program should read a NetCDF file, extract a specific variable, perform a mathematical operation on the variable, and save the modified variable back to a new NetCDF file.\n\n### Code:"
    },
    {
        "api": "xarray",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install xarray\nimport xarray as xr\n\n# Read the NetCDF file\nds = xr.open_dataset('input.nc')\n\n# Select the variable\nvar = ds['temperature']\n\n# Slice the data along one or more dimensions\nsliced_data = var.sel(time=slice('2000-01-01', '2000-12-31'), lat=slice(0, 90))\n\n# Apply a mask based on a condition\nmask = sliced_data > 25\nmasked_data = sliced_data.where(mask)\n\n# Calculate the mean, standard deviation, and maximum value of the masked and sliced data along another specified dimension\nmean = masked_data.mean(dim='time')\nstd_dev = masked_data.std(dim='time')\nmax_value = masked_data.max(dim='time')\n\n# Print the calculated statistics\nprint(\"Mean: \", mean)\nprint(\"Standard Deviation: \", std_dev)\nprint(\"Maximum Value: \", max_value)",
        "instruction": "Create a Python program using the 'xarray' API to perform data slicing, masking, and aggregation on a multi-dimensional data array. The program should read a NetCDF file, select a specific variable, slice the data along one or more dimensions, apply a mask based on a condition, and then calculate the mean, standard deviation, and maximum value of the masked and sliced data along another specified dimension. Finally, the program should print the calculated statistics.",
        "prediction": ":import xarray as rr\nimport numpy as np\nimport pandas as pd\nfrom xarray import open_dataset\n\n# Read the NetCDF file\nds = open_dataset('input.nc')\n\n# Select the variable\nvar = ds['temperature']\n\n# Slice the data along one or more dimensions\nsliced_data = var.sel(time=slice(1, 10), lat=slice(0, 90))\n\n# Apply a mask based on a condition\nmask = sliced_data > 25\nmasked_data = sliced_data.where(mask)\n\n# Calculate the mean, standard deviation, and maximum value of the masked and sliced data along another specified dimension\nmean = masked_data.mean(dim='time')\nstd_dev = masked_data.std(dim='time')\nmax_value = masked_data.max(dim='time')\n\n# Print the calculated statistics\nprint(\"Mean: \", mean)\nprint(\"Standard Deviation: \", std_dev)\nprint(\"Maximum Value: \", max_value)",
        "source": "This is some documentation of xarray API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n## DataArray methods#\n\n\n|  |  |\n| --- | --- |\n| `load\\_dataarray`(filename\\_or\\_obj,\u00a0\\*\\*kwargs) | Open, load into memory, and close a DataArray from a file or file-like object containing a single data variable. |\n| `open\\_dataarray`(filename\\_or\\_obj,\u00a0\\*[,\u00a0engine,\u00a0...]) | Open an DataArray from a file or file-like object containing a single data variable. |\n| `DataArray.as\\_numpy`() | Coerces wrapped data and coordinates into numpy arrays, returning a DataArray. |\n| `DataArray.from\\_cdms2`(variable) | Convert a cdms2.Variable into an xarray.DataArray |\n| `DataArray.from\\_dict`(d) | Convert a dictionary into an xarray.DataArray |\n| `DataArray.from\\_iris`(cube) | Convert a iris.cube.Cube into an xarray.DataArray |\n| `DataArray.from\\_series`(series[,\u00a0sparse]) | Convert a pandas.Series into an xarray.DataArray. |\n| `DataArray.to\\_cdms2`() | Convert this array into a cdms2.Variable |\n| `DataArray.to\\_dask\\_dataframe`([dim\\_order,\u00a0...]) | Convert this array into a dask.dataframe.DataFrame. |\n| `DataArray.to\\_dataframe`([name,\u00a0dim\\_order]) | Convert this array and its coordinates into a tidy pandas.DataFrame. |\n| `DataArray.to\\_dataset`([dim,\u00a0name,\u00a0promote\\_attrs]) | Convert a DataArray to a Dataset. |\n| `DataArray.to\\_dict`([data,\u00a0encoding]) | Convert this xarray.DataArray into a dictionary following xarray naming conventions. |\n| `DataArray.to\\_index`() | Convert this variable to a pandas.Index. |\n| `DataArray.to\\_iris`() | Convert this array into a iris.\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program using the 'xarray' API to perform data slicing, masking, and aggregation on a multi-dimensional data array. The program should read a NetCDF file, select a specific variable, slice the data along one or more dimensions, apply a mask based on a condition, and then calculate the mean, standard deviation, and maximum value of the masked and sliced data along another specified dimension. Finally, the program should print the calculated statistics.\n\n### Code:"
    },
    {
        "api": "ydata-profiling",
        "decoder_input_ids": true,
        "ground-truth": "#! pip install ydata-profiling\nimport pandas as pd\nfrom ydata_profiling import ProfileReport\n\n# Load the JSONL data\ndata = pd.read_json('data.jsonl', lines=True)\n\n# Perform data profiling\nprofiling_report = ProfileReport(data)\n\n# Save the report as an HTML file\nprofiling_report.to_file('jsonl_data_profile.html')",
        "instruction": "Create a Python program that employs the 'ydata-profiling' API to profile a JSONL file named 'data.jsonl'. The profiling should include a count of records, data types of columns, and a table of unique values for a column named 'user_id'. Save the profiling report as an HTML file named 'jsonl_data_profile.html'.",
        "prediction": ":pip install ydata-profiling\nimport pandas as pd\nfrom ydata_profiling import ProfileReport\n\n# Load the JSONL data\ndata = pd.read_json('data.jsonl', lines=True)\n\n# Perform data profiling\nprofiling_report = ProfileReport(data)\n\n# Save the report as an HTML file\nprofiling_report.to_file('jsonl_data_profile.html')",
        "source": "This is some documentation of ydata-profiling API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n Profiling large datasets\n\n\nBy default, `ydata-profiling` comprehensively summarizes the input\ndataset in a way that gives the most insights for data analysis. For\nsmall datasets, these computations can be performed in *quasi*\nreal-time. For larger datasets, deciding upfront which calculations to\nmake might be required. Whether a computation scales to a large datasets\nnot only depends on the exact size of the dataset, but also on its\ncomplexity and on whether fast computations are available. If the\ncomputation time of the profiling becomes a bottleneck,\n`ydata-profiling` offers several alternatives to overcome it.\n\nScale in a fully managed system\n\n\nLooking for an fully managed system that is able to scale the profiling\nfor large datasets? Sign up Fabric\ncommunity for distributed data profiling.\n\n## Pyspark\n\n\nSpark\n\nMinimal mode\n\n\nThis mode was introduced in version v4.0.0\n\n`ydata-profiling` now supports Spark Dataframes profiling. You can find\nan example of the integration\nhere.\n\n\n**Features supported:** - Univariate variables' analysis - Head and Tail\ndataset sample - Correlation matrices: Pearson and Spearman\n\n\n*Coming soon* - Missing values analysis - Interactions - Improved\nhistogram computation\n\n\nKeep an eye on the\nGitHub page to\nfollow the updates on the implementation of Pyspark Dataframes\nsupport.\n\n\n\n## Minimal mode\n\n**Minimal mode**\n\n\nThis mode was introduced in version v2.4.0\n\n`ydata-profiling` includes a minimal configuration file where the most\nexpensive computations are turned off by default. This is the\nrecommended starting point for larger datasets.\n\n```\nprofile = ProfileReport(large\\_dataset, minimal=True)\nprofile.to\\_file(\"output.html\")\n\nThis configuration file can be found here:\nconfig\\_minimal.yaml.\nMore details on settings and configuration are available in\n`../advanced_usage/available_settings`.\n\n\n## Sample the dataset\n\n\nAn alternative way to handle really large datasets is to use a portion\nof it to generate the profiling report. Several users report this is a\ngood way to scale back the computation time while\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that employs the 'ydata-profiling' API to profile a JSONL file named 'data.jsonl'. The profiling should include a count of records, data types of columns, and a table of unique values for a column named 'user_id'. Save the profiling report as an HTML file named 'jsonl_data_profile.html'.\n\n### Code:"
    },
    {
        "api": "ydata-profiling",
        "decoder_input_ids": true,
        "ground-truth": "#! pip install ydata-profiling\nimport pandas as pd\nfrom ydata_profiling import ProfileReport\n\n# Load the CSV data\ndata = pd.read_csv(\"data.csv\")\n\n# Perform data profiling\nprofiling_report = ProfileReport(data)\n\n# Save the report as an HTML file\nprofiling_report.to_file(\"data_profile.html\")",
        "instruction": "Create a Python program that uses the \"ydata-profiling\" API to perform data profiling on a CSV file named \"data.csv.\" The profiling should include statistics like mean, median, and histograms for numeric columns. Save the profiling report as an HTML file named \"data_profile.html.\"",
        "prediction": ":import ydata-profiling as np\nfrom ydata_profiling import ProfileReport\n\n# Load the CSV \"data.csv\" data\ndata = np.read_csv('data.csv')\n\n# Perform data profiling\nprofiling_report = ProfileReport(data)\n\n# Save the report as an HTML file\nprofiling_report.to_file('data_profile.html')",
        "source": "This is some documentation of ydata-profiling API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n# Sample the dataset\n\n\nAn alternative way to handle really large datasets is to use a portion\nof it to generate the profiling report. Several users report this is a\ngood way to scale back the computation time while maintaining\nrepresentativity.\n\n> \n> pandas-profiling is a nifty tool to compute descriptive statistics on a dataframe and issue warnings on columns with many missing values, high skew, high cardinality categorical values, high correlation, etc: https://t.co/57IezPW1nI demo notebook: https://t.co/JpqTO9FK1p\n> \n> \u2014 Olivier Grisel (@ogrisel) January 11, 2018\n\n\n| Sampling a large dataset |\n| --- |\n| \n```\n1\n2\n3\n4\n5\n```\n | \n```\n# Sample 10.000 rows\nsample = large\\_dataset.sample(10000)\n\nprofile = ProfileReport(sample, minimal=True)\nprofile.to\\_file(\"output.html\")\n\n\nThe reader of the report might want to know that the profile is\ngenerated using a sample from the data. This can be done by adding a\ndescription to the report (see `metadata`\nfor details).\n\n\n| Sample 5% of your dataset |\n| --- |\n| \n```\n1\n2\n3\n4\n5\n```\n | \n```\ndescription = \"Disclaimer: this profiling report was generated using a sample of 5% of the original dataset.\"\nsample = large\\_dataset.sample(frac=0.05)\n\nprofile = sample.profile\\_report(dataset={\"description\": description}, minimal=True)\nprofile.to\\_file(\"output.html\")\n\n\n\n## Disable expensive computations\n\n\nTo decrease the computational burden in particularly large datasets but\nstill maintain some information of interest that may stem from them,\nsome computations can be filtered only for certain columns.\nParticularly, a list of targets can be provided to **Interactions**, so\nthat only the interactions with these variables in specific are\ncomputed.\n\n\n| Disable expensive computations |\n| --- |\n| \n```\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that uses the \"ydata-profiling\" API to perform data profiling on a CSV file named \"data.csv.\" The profiling should include statistics like mean, median, and histograms for numeric columns. Save the profiling report as an HTML file named \"data_profile.html.\"\n\n### Code:"
    },
    {
        "api": "langchain",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install langchain\n# Import necessary libraries\nfrom langchain.prompts import PromptTemplate\n\n# define the template\nprompt_template = PromptTemplate(\n    input_variables=[\"adjective\", \"content\"],\n    template = \"Tell me a {adjective} joke about {content}.\"\n)\n\n# can also use invoke syntax (applied to following examples as well)\nprompt_value = prompt_template.invoke({\"adjective\": \"funny\", \"content\": \"AI\"})\nprint(prompt_value)",
        "instruction": "Create a Python program that utilizes the \"langchain\" API to generate a prompt template that take to inputs adjective and content to till the model to generate a joke about the content with the given adjective. Then invoke the template with the inputs \"funny\" and \"AI\" to generate a joke about AI.",
        "prediction": ":import langchain\nfrom langchain.chains import LLMChain\n\nprompt = PromptTemplate(  \n input_variables=[\"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"adjective\", \"",
        "source": "This is some documentation of langchain API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n### Using `LLMChain`\u200b\n\nThe `LLMChain` is most basic building block chain. It takes in a prompt template, formats it with the user input and returns the response from an LLM.\n\nTo use the `LLMChain`, first create a prompt template.\n\n\n```\nfrom langchain.llms import OpenAI  \nfrom langchain.prompts import PromptTemplate  \n  \nllm = OpenAI(temperature=0.9)  \nprompt = PromptTemplate(  \n input\\_variables=[\"product\"],  \n template=\"What is a good name for a company that makes {product}?\",  \n)  \n\n```\nWe can now create a very simple chain that will take user input, format the prompt with it, and then send it to the LLM.\n\n\n```\nfrom langchain.chains import LLMChain  \nchain = LLMChain(llm=llm, prompt=prompt)  \n  # Run the chain only specifying the input variable.  \nprint(chain.run(\"colorful socks\"))  \n\n```\n\n```\n Colorful Toes Co.  \n\n```\nIf there are multiple variables, you can input them all at once using a dictionary.\n\n\n```\nprompt = PromptTemplate(  \n input\\_variables=[\"company\", \"product\"],\n\n==================\n Document 1 \n----------------\n Run the chain only specifying the input variable.  \nprint(chain.run(\"colorful socks\"))  \n\n```\n\n```\n Colorful Toes Co.  \n\n```\nIf there are multiple variables, you can input them all at once using a dictionary.\n\n\n```\nprompt = PromptTemplate(  \n input\\_variables=[\"company\", \"product\"],  \n template=\"What is a good name for {company} that makes {product}?\",  \n)  \nchain = LLMChain(llm=llm, prompt=prompt)  \nprint(chain.run({  \n 'company': \"ABC Startup\",  \n 'product': \"colorful socks\"  \n }))  \n\n```\n Socktopia Colourful Creations.  \n\n```\nYou can use a chat model in an `LLMChain\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that utilizes the \"langchain\" API to generate a prompt template that take to inputs adjective and content to till the model to generate a joke about the content with the given adjective. Then invoke the template with the inputs \"funny\" and \"AI\" to generate a joke about AI.\n\n### Code:"
    },
    {
        "api": "langchain",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install langchain\n# Import necessary libraries\nfrom langchain.document_loaders import WebBaseLoader\n\n# Declare the loader\nloader = WebBaseLoader(\n    web_paths=(\"https://en.wikipedia.org/wiki/Germany\",),\n)\n# Use it to load the text data\ndocs = loader.load()\n\n# Let's look at a piece of text from the first document\ndocs[0].page_content[:1000]",
        "instruction": "Create a Python program that utilizes the \"langchain\" API to load the text from the Wikipedia page on Germany and print the first 1000 characters of the page content.",
        "prediction": ":import langchain\n\n# Load the text on \"Germany\"\nimport langchain\nload_doc = langchain.load_text(\"Germany\")\n\n# Get the first 1000 characters\nresult = load_doc[:1000]\n\n# Print the first 1000 characters\nprint(result)",
        "source": "This is some documentation of langchain API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n Retrieval\n\nMany LLM applications require user-specific data that is not part of the model's training set.\nThe primary way of accomplishing this is through Retrieval Augmented Generation (RAG).\nIn this process, external data is *retrieved* and then passed to the LLM when doing the *generation* step.\n\nLangChain provides all the building blocks for RAG applications - from simple to complex.\nThis section of the documentation covers everything related to the *retrieval* step - e.g. the fetching of the data.\nAlthough this sounds simple, it can be subtly complex.\nThis encompasses several key modules.\n\n\n\n**Document loaders**\n\nLoad documents from many different sources.\nLangChain provides over 100 different document loaders as well as integrations with other major providers in the space,\nlike AirByte and Unstructured.\nWe provide integrations to load all types of documents (HTML, PDF, code) from all types of locations (private s3 buckets, public websites).\n\n**Document transformers**\n\nA key part of retrieval is fetching only the relevant parts of documents.\nThis involves several transformation steps in order to best prepare the documents for retrieval.\nOne of the primary ones here is splitting (or chunking) a large document into smaller chunks.\nLangChain provides several different algorithms for doing this, as well as logic optimized for specific document types (code, markdown, etc).\n\n**Text embedding models**\n\nAnother key part of retrieval has become creating embeddings for documents.\nEmbeddings capture the semantic meaning of the text, allowing you to quickly and\nefficiently find other pieces of text that are similar.\nLangChain provides integrations with over 25 different embedding providers and methods,\nfrom open-source to proprietary API,\nallowing you to choose the one best suited for your needs.\nLangChain provides a standard interface, allowing you to easily swap between models.\n\n**Vector stores**\n\nWith the rise of embeddings, there has emerged a need for databases to support efficient storage and searching of these embeddings.\nLangChain provides integrations with over 50 different vectorstores, from open-source local ones to cloud-hosted proprietary ones,\nallowing you to choose the one best suited for your needs.\nLang\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that utilizes the \"langchain\" API to load the text from the Wikipedia page on Germany and print the first 1000 characters of the page content.\n\n### Code:"
    },
    {
        "api": "langchain",
        "decoder_input_ids": true,
        "ground-truth": "#!pip install langchain\n# Import necessary libraries\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.document_loaders import WebBaseLoader\n\n# Declare the loader\nloader = WebBaseLoader(\n    web_paths=(\"https://en.wikipedia.org/wiki/Sinai_Peninsula\",),\n)\n# Use it to load the text data\ndocs = loader.load()\n\n# Declare the loader\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,\n    add_start_index=True\n)\n\n# Use it to split the data\nall_splits = text_splitter.split_documents(docs)",
        "instruction": "Create a Python program that utilizes the \"langchain\" API to load the text from the Wikipedia page on the Sinai Peninsula and split the text into chunks of 1000 characters with an overlap of 200 characters using Recursive Character Text Splitter.",
        "prediction": ":import langchain\nfrom langchain import load_text\n\n# Load the text\n\ntext = load_text(\"en\", \"Ensenate\ufffdeseni_Sinai_146.html\")\n\n# Split the text into chunks (1000 characters with an overlap of an overlap of 200 characters)\n\nchunks = split_text_chunks(text, 1000, overlap_size=200)\n\n# Print the chunks\n\nfor i, chunk in enumerate(chunks):\n    print(f\"Chunk {i}:\", chunk)",
        "source": "This is some documentation of langchain API:\n\n=====\n\n\n==================\n Document 0 \n----------------\n Retrieval\n\nMany LLM applications require user-specific data that is not part of the model's training set.\nThe primary way of accomplishing this is through Retrieval Augmented Generation (RAG).\nIn this process, external data is *retrieved* and then passed to the LLM when doing the *generation* step.\n\nLangChain provides all the building blocks for RAG applications - from simple to complex.\nThis section of the documentation covers everything related to the *retrieval* step - e.g. the fetching of the data.\nAlthough this sounds simple, it can be subtly complex.\nThis encompasses several key modules.\n\n\n\n**Document loaders**\n\nLoad documents from many different sources.\nLangChain provides over 100 different document loaders as well as integrations with other major providers in the space,\nlike AirByte and Unstructured.\nWe provide integrations to load all types of documents (HTML, PDF, code) from all types of locations (private s3 buckets, public websites).\n\n**Document transformers**\n\nA key part of retrieval is fetching only the relevant parts of documents.\nThis involves several transformation steps in order to best prepare the documents for retrieval.\nOne of the primary ones here is splitting (or chunking) a large document into smaller chunks.\nLangChain provides several different algorithms for doing this, as well as logic optimized for specific document types (code, markdown, etc).\n\n**Text embedding models**\n\nAnother key part of retrieval has become creating embeddings for documents.\nEmbeddings capture the semantic meaning of the text, allowing you to quickly and\nefficiently find other pieces of text that are similar.\nLangChain provides integrations with over 25 different embedding providers and methods,\nfrom open-source to proprietary API,\nallowing you to choose the one best suited for your needs.\nLangChain provides a standard interface, allowing you to easily swap between models.\n\n**Vector stores**\n\nWith the rise of embeddings, there has emerged a need for databases to support efficient storage and searching of these embeddings.\nLangChain provides integrations with over 50 different vectorstores, from open-source local ones to cloud-hosted proprietary ones,\nallowing you to choose the one best suited for your needs.\nLang\n=====\n\nBelow is an instruction that describes a coding task. Write a full code that appropriately completes the request using the above documentation.\n\n### Instruction:\nCreate a Python program that utilizes the \"langchain\" API to load the text from the Wikipedia page on the Sinai Peninsula and split the text into chunks of 1000 characters with an overlap of 200 characters using Recursive Character Text Splitter.\n\n### Code:"
    }
]