[
    {
        "api": "PyDrive2",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install pydrive2\nfrom pydrive2.auth import GoogleAuth\nfrom pydrive2.drive import GoogleDrive\n\n# Function to authenticate and get the GoogleDrive instance\ndef authenticate_drive():\n    gauth = GoogleAuth()\n    gauth.LocalWebserverAuth()  # Authenticate using local webserver\n    return GoogleDrive(gauth)\n\ndef get_file_metadata(file_id):\n    drive = authenticate_drive()\n    \n    try:\n        file = drive.CreateFile({'id': file_id})\n        if not file:\n            print(f\"File with ID '{file_id}' does not exist on Google Drive.\")\n            return\n        \n        print(\"File Metadata:\")\n        print(f\"Title: {file['title']}\")\n        print(f\"Size: {file['fileSize']} bytes\")\n        print(f\"Creation Date: {file['createdDate']}\")\n        print(f\"Last Modified Date: {file['modifiedDate']}\")\n    except Exception as e:\n        print(f\"Error retrieving file metadata: {e}\")\n\nif __name__ == \"__main__\":\n    file_id = input(\"Enter the file ID on Google Drive to retrieve metadata: \")\n    \n    get_file_metadata(file_id)",
        "instruction": "Create a Python program that retrieves the metadata of a file on Google Drive using the PyDrive2 API. The program should take the file ID as input and display the metadata information of the file, including its title, size, creation date, and last modified date. Make sure the program is capable of handling cases where the file ID provided is invalid or when there is an authentication issue.",
        "prediction": "import requests\nimport json\nimport time\nimport sys\nimport os\n\ndef get_file_name(file_id):\n    url = 'https://drive.google.com/uc?export=download&id=' + file_id\n    file_name = requests.get(url).text\n    return file_name\n\ndef get_metadata(file_id):\n    url = 'https://drive.google.com/get_video_info?docid=' + file_id\n    metadata = requests.get(url).text\n    return metadata\n\ndef get_metadata_json(file_id):\n    url = 'https://drive.google.com/get_video_info?docid=' + file_id\n    metadata = requests.get(url).json()\n    return metadata\n\ndef get_metadata_json_file(file_id):\n    url = 'https://drive.google.com/get_video_info?docid=' + file_id\n    metadata = requests.get(url).json()\n    return metadata\n\ndef get_metadata_json_file_name(file_id):\n    url = 'https://drive.google.com/get_video_info?docid=' + file_id\n    metadata = requests.get(url).json()\n    return metadata['title']\n\ndef get_metadata_json_file_size(file_id):\n    url = 'https://drive.google.com/get_video_info?docid=' + file_id\n    metadata = requests.get(url).json()\n    return metadata['fileSize']\n\ndef get_metadata_json_file_creation_date(file_id):\n    url = 'https://drive.google.com/get_video_info?docid=' + file_id\n    metadata = requests.get(url).json()\n    return metadata['creationTime']\n\ndef get_metadata_json_file_last_modified_date(file_id):\n    url = 'https://drive.google.com/get_video_info?docid=' + file_id\n    metadata = requests.get(url).json()\n    return metadata['lastModifiedTime']\n\ndef get_metadata_json_file_last_modified_date_unix(file_id):\n    url = 'https://drive.google.com/get_video_info?docid=' + file_id\n    metadata = requests.get(url).json()\n    return metadata['lastModifiedTime']/1000\n\ndef get_metadata_json_file_last_modified_date_unix_str(file_id):\n    url = 'https://drive.google.com/get_video_info?docid=' + file_id\n    metadata = requests.get(url).json()\n    return time.ctime(metadata['lastModifiedTime']/1000)\n\ndef get_metadata_json_file_last_modified_date_str(file_id):\n    url = 'https://drive.google.com/get_video_info?docid=' + file_id\n    metadata = requests.get(url).json()\n    return time.ctime(metadata['lastModifiedTime']/1000)\n\ndef get_metadata_json_file_last_modified_date_str_full(file_id):\n    url = 'https://drive.google.com/get_video_info?docid=' + file_id\n    metadata = requests.get(url).json()\n    return time.ctime(metadata['lastModifiedTime']/1000)\n\ndef get_metadata_json_file_last_modified_date_str_full_full(file_id):\n    url = 'https://drive.google.com/get_video_info?docid=' + file_id\n    metadata = requests.get(url).json()\n    return time.ctime(metadata['lastModifiedTime']/1000)\n\ndef get_metadata_json_file_last_modified_date_str_full_full_full(file_id):\n    url = 'https://drive.google.com/get_video_info?docid=' + file_id\n    metadata = requests.get(url).json()\n    return time.ctime(metadata['lastModifiedTime']/1000)\n\ndef get_metadata_json_file_last_modified_date_str_full_full_full_full(file_id):\n    url = 'https://drive.google.com/get_",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that retrieves the metadata of a file on Google Drive using the PyDrive2 API. The program should take the file ID as input and display the metadata information of the file, including its title, size, creation date, and last modified date. Make sure the program is capable of handling cases where the file ID provided is invalid or when there is an authentication issue.\n\n### Code:"
    },
    {
        "api": "PyDrive2",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install pydrive2\nfrom pydrive2.auth import GoogleAuth\nfrom pydrive2.drive import GoogleDrive\n\n# Function to authenticate and get the GoogleDrive instance\ndef authenticate_drive():\n    gauth = GoogleAuth()\n    gauth.LocalWebserverAuth()  # Authenticate using local webserver\n    return GoogleDrive(gauth)\n\ndef get_permissions(file_or_folder_id):\n    drive = authenticate_drive()\n    \n    try:\n        file_or_folder = drive.CreateFile({'id': file_or_folder_id})\n        if not file_or_folder:\n            print(f\"File or folder with ID '{file_or_folder_id}' does not exist on Google Drive.\")\n            return\n        \n        permissions = file_or_folder.GetPermissions()\n        \n        print(f\"Permissions for file or folder with ID '{file_or_folder_id}':\")\n        \n        for permission in permissions:\n            print(f\"Email Address: {permission['emailAddress']}, Role: {permission['role']}\")\n    except Exception as e:\n        print(f\"Error retrieving permissions: {e}\")\n\nif __name__ == \"__main__\":\n    file_or_folder_id = input(\"Enter the file or folder ID on Google Drive to retrieve permissions: \")\n    \n    get_permissions(file_or_folder_id)",
        "instruction": "Create a Python program that retrieves the list of permissions for a file or folder on Google Drive using the PyDrive2 API. The program should take the file or folder ID as input and display the list of permissions, including the email addresses and roles of the users or groups that have access to the file or folder. Make sure the program is capable of handling cases where the file or folder ID provided is invalid or when there is an authentication issue.",
        "prediction": "import requests\nimport json\nimport sys\nimport os\n\ndef get_permissions(file_or_folder_id):\n    url = 'https://drive.google.com/drive/v3/files/' + file_or_folder_id + '/permissions'\n    headers = {'Authorization': 'Bearer'+ os.environ['GDRIVE_TOKEN']}\n    response = requests.get(url, headers=headers)\n    return response.json()\n\ndef get_permissions_list(file_or_folder_id):\n    permissions = get_permissions(file_or_folder_id)\n    permissions_list = []\n    for permission in permissions:\n        permissions_list.append(permission['emailAddress'])\n    return permissions_list\n\ndef get_permissions_list_by_email(file_or_folder_id, email):\n    permissions = get_permissions(file_or_folder_id)\n    for permission in permissions:\n        if permission['emailAddress'] == email:\n            return True\n    return False\n\ndef get_permissions_list_by_email_and_role(file_or_folder_id, email, role):\n    permissions = get_permissions(file_or_folder_id)\n    for permission in permissions:\n        if permission['emailAddress'] == email and permission['role'] == role:\n            return True\n    return False\n\ndef get_permissions_list_by_email_and_role_and_type(file_or_folder_id, email, role, type):\n    permissions = get_permissions(file_or_folder_id)\n    for permission in permissions:\n        if permission['emailAddress'] == email and permission['role'] == role and permission['type'] == type:\n            return True\n    return False\n\ndef get_permissions_list_by_email_and_role_and_type_and_role(file_or_folder_id, email, role, type, role2):\n    permissions = get_permissions(file_or_folder_id)\n    for permission in permissions:\n        if permission['emailAddress'] == email and permission['role'] == role and permission['type'] == type and permission['role'] == role2:\n            return True\n    return False\n\ndef get_permissions_list_by_email_and_role_and_type_and_role_and_type(file_or_folder_id, email, role, type, role2, type2):\n    permissions = get_permissions(file_or_folder_id)\n    for permission in permissions:\n        if permission['emailAddress'] == email and permission['role'] == role and permission['type'] == type and permission['role'] == role2 and permission['type'] == type2:\n            return True\n    return False\n\ndef get_permissions_list_by_email_and_role_and_type_and_role_and_type_and_role(file_or_folder_id, email, role, type, role2, type2, role3):\n    permissions = get_permissions(file_or_folder_id)\n    for permission in permissions:\n        if permission['emailAddress'] == email and permission['role'] == role and permission['type'] == type and permission['role'] == role2 and permission['type'] == type2 and permission['role'] == role3:\n            return True\n    return False\n\ndef get_permissions_list_by_email_and_role_and_type_and_role_and_type_and_role_and_type(file_or_folder_id, email, role, type, role2, type2, role3, type3):\n    permissions = get_permissions(file_or_folder_id)\n    for permission in permissions:\n        if permission['emailAddress'] == email and permission['role'] == role and permission['type'] == type and permission['role'] == role2 and permission['type'] == type2 and permission['role'] == role3 and permission['type'] == type3:\n            return True\n    return False\n\ndef get_permissions_list_by_email_and_role_and_type_and_role_and_type_and_role_and_type_and_role(file_or_folder_id, email, role, type, role2, type2, role3, type3, role4):\n    ",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that retrieves the list of permissions for a file or folder on Google Drive using the PyDrive2 API. The program should take the file or folder ID as input and display the list of permissions, including the email addresses and roles of the users or groups that have access to the file or folder. Make sure the program is capable of handling cases where the file or folder ID provided is invalid or when there is an authentication issue.\n\n### Code:"
    },
    {
        "api": "PyDrive2",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install pydrive2\nfrom pydrive2.auth import GoogleAuth\nfrom pydrive2.drive import GoogleDrive\n\n# Function to authenticate and get the GoogleDrive instance\ndef authenticate_drive():\n    gauth = GoogleAuth()\n    gauth.LocalWebserverAuth()  # Authenticate using local webserver\n    return GoogleDrive(gauth)\n\ndef list_files_in_drive_folder(folder_id):\n    drive = authenticate_drive()\n    \n    try:\n        folder = drive.CreateFile({'id': folder_id})\n        if not folder:\n            print(f\"Folder with ID '{folder_id}' does not exist on Google Drive.\")\n            return\n        \n        folder_title = folder['title']\n        print(f\"Listing files in '{folder_title}' folder (ID: {folder_id}):\")\n        \n        file_list = drive.ListFile({'q': f\"'{folder_id}' in parents and trashed=false\"}).GetList()\n        if not file_list:\n            print(\"No files found in this folder.\")\n            return\n        \n        for file in file_list:\n            print(f\"File Name: {file['title']}, File ID: {file['id']}\")\n    except Exception as e:\n        print(f\"Error listing files: {e}\")\n\nif __name__ == \"__main__\":\n    folder_id = input(\"Enter the folder ID on Google Drive to list files from: \")\n    \n    list_files_in_drive_folder(folder_id)",
        "instruction": "Create a Python program that lists all files in a specified folder on Google Drive using the PyDrive2 API. The program should take the folder's ID as input and display a list of file names along with their corresponding IDs. Make sure the program is capable of handling cases where the folder ID provided does not exist or when there is an authentication issue.",
        "prediction": "import urllib.request\nimport json\nimport os\n\ndef listFiles(dirname):\n    r = []\n    for root, dirs, files in os.walk(dirname):\n        for file in files:\n            r.append(os.path.join(root, file))\n    return r\n\ndef listFilesWithExt(dirname, ext):\n    r = []\n    for root, dirs, files in os.walk(dirname):\n        for file in files:\n            if file.endswith(ext):\n                r.append(os.path.join(root, file))\n    return r\n\ndef listFilesWithExtAndPrefix(dirname, ext, prefix):\n    r = []\n    for root, dirs, files in os.walk(dirname):\n        for file in files:\n            if file.endswith(ext) and file.startswith(prefix):\n                r.append(os.path.join(root, file))\n    return r\n\ndef listFilesWithExtAndPrefixAndSuffix(dirname, ext, prefix, suffix):\n    r = []\n    for root, dirs, files in os.walk(dirname):\n        for file in files:\n            if file.endswith(ext) and file.startswith(prefix) and file.endswith(suffix):\n                r.append(os.path.join(root, file))\n    return r\n\ndef listFilesWithExtAndPrefixAndSuffixAndSubdir(dirname, ext, prefix, suffix, subdir):\n    r = []\n    for root, dirs, files in os.walk(dirname):\n        for file in files:\n            if file.endswith(ext) and file.startswith(prefix) and file.endswith(suffix) and subdir in root:\n                r.append(os.path.join(root, file))\n    return r\n\ndef listFilesWithExtAndPrefixAndSuffixAndSubdirAndSubsubdir(dirname, ext, prefix, suffix, subdir, subsubdir):\n    r = []\n    for root, dirs, files in os.walk(dirname):\n        for file in files:\n            if file.endswith(ext) and file.startswith(prefix) and file.endswith(suffix) and subdir in root and subsubdir in root:\n                r.append(os.path.join(root, file))\n    return r\n\ndef listFilesWithExtAndPrefixAndSuffixAndSubdirAndSubsubdirAndSubsubsubdir(dirname, ext, prefix, suffix, subdir, subsubdir, subsubsubdir):\n    r = []\n    for root, dirs, files in os.walk(dirname):\n        for file in files:\n            if file.endswith(ext) and file.startswith(prefix) and file.endswith(suffix) and subdir in root and subsubdir in root and subsubsubdir in root:\n                r.append(os.path.join(root, file))\n    return r\n\ndef listFilesWithExtAndPrefixAndSuffixAndSubdirAndSubsubdirAndSubsubsubdirAndSubsubsubsubdir(dirname, ext, prefix, suffix, subdir, subsubdir, subsubsubdir, subsubsubsubdir):\n    r = []\n    for root, dirs, files in os.walk(dirname):\n        for file in files:\n            if file.endswith(ext) and file.startswith(prefix) and file.endswith(suffix) and subdir in root and subsubdir in root and subsubsubdir in root and subsubsubsubdir in root:\n                r.append(os.path.join(root, file))\n    return r\n\ndef listFilesWithExtAndPrefixAndSuffixAndSubdirAndSubsubdirAndSubsubsubdirAndSubsubsubsubdir(dirname, ext, prefix, suffix, subdir, subsubdir, subsubsubdir, subsubsubsubdir):\n    r = []\n    for root, dirs, files in os.walk(dirname):\n        for file in files:\n            if file.endswith(ext) and file.startswith(prefix) and file.endswith(suffix) and subdir in root and subsubdir in root and subsubsubdir",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that lists all files in a specified folder on Google Drive using the PyDrive2 API. The program should take the folder's ID as input and display a list of file names along with their corresponding IDs. Make sure the program is capable of handling cases where the folder ID provided does not exist or when there is an authentication issue.\n\n### Code:"
    },
    {
        "api": "Pygments",
        "decoder_input_ids": false,
        "ground-truth": "# !pip install Pygments\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import ExcelFormatter\n\n# Read the code file\nwith open(\"code.py\", \"r\") as file:\n    code = file.read()\n\n# Determine the lexer based on the file extension\nlexer = get_lexer_by_name(\"python\")\n\n# Define a custom formatter with a custom color scheme and line numbers\nformatter = ExcelFormatter(style=\"friendly\", linenos=True)\n\n# Apply syntax highlighting to the code with the custom color scheme and line numbers\nhighlighted_code = highlight(code, lexer, formatter)\n\n# Save the highlighted code to an Excel file (XLSX) with the custom color scheme and line numbers\nwith open(\"highlighted_code.xlsx\", \"wb\") as xlsx_file:\n    xlsx_file.write(highlighted_code)",
        "instruction": "Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as an Excel file (XLSX) with a custom color scheme and line numbers.",
        "prediction": "import PyGnuplot as gp\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the number of values\nnum_vals = 40\n\n# Generate random values\nvals = np.random.rand(num_vals)\n\n# Define the data file name\ndata_file_name = 'data/data_file.txt'\n\n# Open the data file for writing\ndata_file = open(data_file_name, 'w')\n\n# Loop through the values and write them to the file\nfor val in vals:\n    data_file.write('%f\\n' % val)\n\n# Close the file\ndata_file.close()\n\n# Define the terminal and data file names\nterminal_file_name = 'terminal/terminal_file.txt'\ndata_file_name = 'data/data_file.txt'\n\n# Open the terminal file for writing\nterminal_file = open(terminal_file_name, 'w')\n\n# Set the output option for the terminal to the data file\nterminal_file.write('set output \"%s\"\\n' % data_file_name)\n\n# Set the title of the graph\nterminal_file.write('set title \"Random Numbers\"\\n')\n\n# Set the x and y labels\nterminal_file.write('set xlabel \"Random Number\"\\n')\nterminal_file.write('set ylabel \"Frequency\"\\n')\n\n# Set the x range\nterminal_file.write('set xrange [0:1]\\n')\n\n# Set the y range\nterminal_file.write('set yrange [0:40]\\n')\n\n# Set the x2 range\nterminal_file.write('set x2range [0:1]\\n')\n\n# Set the style of the graph to a blue continuous line\nterminal_file.write('set style line 1 lt 1 lw 2 lc rgb \"blue\"\\n')\n\n# Set the style of the legend to a box\nterminal_file.write('set style fill solid 1.00 border -1\\n')\n\n# Set the style of the histogram to a blue continuous line\nterminal_file.write('set style histogram cluster gap 1\\n')\n\n# Set the style of the histogram to a blue continuous line\nterminal_file.write('set style histogram cluster gap 1\\n')\n\n# Plot the data\nterminal_file.write('plot \"data/data_file.txt\" u 1:2 w l\\n')\n\n# Close the file\nterminal_file.close()\n\n# Open the terminal file for reading\nterminal_file = open(terminal_file_name, 'r')\n\n# Read the terminal file\nc = terminal_file.read()\n\n# Close the file\nterminal_file.close()\n\n# Define the output file name\noutput_file_name = 'output/output_file.png'\n\n# Set the terminal option to the output file\ngp.c('set terminal pngcairo font \"sans,13\" size 1000,700')\n\n# Set the data file option to the terminal option\ngp.c('set output \"%s\"' % output_file_name)\n\n# Set the terminal option to the data file\ngp.c('set terminal \"datafile\" size 1000,700')\n\n# Set the data file option to the terminal option\ngp.c('set output \"%s\"' % terminal_file_name)\n\n# Set the terminal option to the data file\ngp.c('set terminal \"datafile\" size 1000,700')\n\n# Set the data file option to the terminal option\ngp.c('set output \"%s\"' % terminal_file_name)\n\n# Set the terminal option to the data file\ngp.c('set terminal \"datafile\" size 1000,700')\n\n# Plot the data\ngp.c('plot \"%s\" u 1:2 w l' % data_file_name)\n\n# Save the data\ngp.c('set output \"%s\"' % output_file_name)\n\n# Save the data\ngp.c('set output \"%s\"' % output_file_name)\n\n# Show the data\ngp.c('set output \"%s\"' % output_file_name)\n\n# Show the data\ngp.c('set output \"%s\"' % output_file_name)\n\n# Show the data\ngp.c('set output",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as an Excel file (XLSX) with a custom color scheme and line numbers.\n\n### Code:"
    },
    {
        "api": "Pygments",
        "decoder_input_ids": false,
        "ground-truth": "# !pip install Pygments\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import ExcelFormatter\n\n# Read the code file\nwith open(\"code.py\", \"r\") as file:\n    code = file.read()\n\n# Determine the lexer based on the file extension\nlexer = get_lexer_by_name(\"python\")\n\n# Apply syntax highlighting to the code\nhighlighted_code = highlight(code, lexer, ExcelFormatter())\n\n# Save the highlighted code to an Excel file (XLSX)\nwith open(\"highlighted_code.xlsx\", \"wb\") as xlsx_file:\n    xlsx_file.write(highlighted_code)",
        "instruction": "Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as an Excel file (XLSX).",
        "prediction": "import PyGnuplot as gp\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the number of values\nnum_vals = 40\n\n# Generate random values\nvals = np.random.rand(num_vals)\n\n# Define the file name\nfilename = 'test.dat'\n\n# Open the file\nfile = open(filename, 'w')\n\n# Write the values to the file\nfor val in vals:\n    file.write('%f\\n' % val)\n\n# Close the file\nfile.close()\n\n# Define the file name\nfilename = 'test.dat'\n\n# Open the file\nfile = open(filename, 'r')\n\n# Read the values from the file\nvals = []\nfor line in file:\n    vals.append(float(line))\n\n# Close the file\nfile.close()\n\n# Plot the values\ngp.s([vals], filename='test.png',\n     title='Random Values',\n     xlabel='Index',\n     ylabel='Value',\n     xrange=[0, num_vals],\n     yrange=[0, 1],\n     legend='Random Values',\n     xlog=False,\n     ylog=False,\n     grid=True,\n     set_style='linespoints',\n     set_title_font='arial',\n     set_x_font='arial',\n     set_y_font='arial',\n     set_x_range_font='arial',\n     set_y_range_font='arial',\n     set_x_label_font='arial',\n     set_y_label_font='arial',\n     set_legend_font='arial',\n     set_x_tics_font='arial',\n     set_y_tics_font='arial',\n     set_x_title_font='arial',\n     set_y_title_font='arial',\n     set_x_title_offset=1,\n     set_y_title_offset=1,\n     set_x_title_rotation=0,\n     set_y_title_rotation=0,\n     set_x_title_at_bottom=False,\n     set_y_title_at_bottom=False,\n     set_x_title_at_top=False,\n     set_y_title_at_top=False,\n     set_x_title_at_left=False,\n     set_y_title_at_left=False,\n     set_x_title_at_right=False,\n     set_y_title_at_right=False,\n     set_x_title_at_top_left=False,\n     set_y_title_at_top_left=False,\n     set_x_title_at_top_right=False,\n     set_y_title_at_top_right=False,\n     set_x_title_at_bottom_left=False,\n     set_y_title_at_bottom_left=False,\n     set_x_title_at_bottom_right=False,\n     set_y_title_at_bottom_right=False,\n     set_x_title_pad=0,\n     set_y_title_pad=0,\n     set_x_label_pad=0,\n     set_y_label_pad=0,\n     set_x_label_rotation=0,\n     set_y_label_rotation=0,\n     set_x_label_at_bottom=False,\n     set_y_label_at_bottom=False,\n     set_x_label_at_top=False,\n     set_y_label_at_top=False,\n     set_x_label_at_left=False,\n     set_y_label_at_left=False,\n     set_x_label_at_right=False,\n     set_y_label_at_right=False,\n     set_x_label_at_top_left=False,\n     set_y_label_at_top_left=False,\n     set_x_label_at_top_right=False,\n     set_y_label_at_top_right=False,\n     set_x_label_at_bottom_left=False,\n     set_y_label_at_bottom_left=False,\n     set_x_label_at_bottom_right=",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as an Excel file (XLSX).\n\n### Code:"
    },
    {
        "api": "Pygments",
        "decoder_input_ids": false,
        "ground-truth": "# !pip install Pygments\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import CsvFormatter\n\n# Read the code file\nwith open(\"code.py\", \"r\") as file:\n    code = file.read()\n\n# Determine the lexer based on the file extension\nlexer = get_lexer_by_name(\"python\")\n\n# Apply syntax highlighting to the code\nhighlighted_code = highlight(code, lexer, CsvFormatter())\n\n# Save the highlighted code to a CSV file\nwith open(\"highlighted_code.csv\", \"w\") as csv_file:\n    csv_file.write(highlighted_code)",
        "instruction": "Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a CSV file.",
        "prediction": "import PyPDF2\nimport sys\n\ninputs = sys.argv[1:]\n\ndef merge_pdfs(pdf_list):\n    merger = PyPDF2.PdfFileMerger()\n    for pdf in pdf_list:\n        print(pdf)\n        merger.append(pdf)\n    merger.write('super.pdf')\n\nmerge_pdfs(inputs)\n\n# import PyPDF2\n#\n# inputs = sys.argv[1:]\n#\n# def pdf_combiner(pdf_list):\n#     merger = PyPDF2.PdfFileMerger()\n#     for pdf in pdf_list:\n#         print(pdf)\n#         merger.append(pdf)\n#     merger.write('super.pdf')\n#\n# pdf_combiner(inputs)",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a CSV file.\n\n### Code:"
    },
    {
        "api": "Pygments",
        "decoder_input_ids": false,
        "ground-truth": "# !pip install Pygments\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import CsvFormatter\n\n# Read the code file\nwith open(\"code.py\", \"r\") as file:\n    code = file.read()\n\n# Determine the lexer based on the file extension\nlexer = get_lexer_by_name(\"python\")\n\n# Define a custom formatter with a custom color scheme and line numbers\nformatter = CsvFormatter(style=\"fruity\", linenos=True)\n\n# Apply syntax highlighting to the code with the custom color scheme and line numbers\nhighlighted_code = highlight(code, lexer, formatter)\n\n# Save the highlighted code to a CSV file with the custom color scheme and line numbers\nwith open(\"highlighted_code.csv\", \"w\") as csv_file:\n    csv_file.write(highlighted_code)",
        "instruction": "Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a CSV file with a custom color scheme and line numbers.",
        "prediction": "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton, QLabel, QVBoxLayout, QWidget\nfrom PyQt5.QtCore import Qt\nfrom PyQt5.QtGui import QPixmap\nimport sys\n\nclass Window(QMainWindow):\n    def __init__(self):\n        super().__init__()\n\n        self.setWindowTitle(\"PyQt5 Push Button\")\n        self.setGeometry(100, 100, 600, 600)\n        self.setStyleSheet(\"background-color: #ff0000;\")\n\n        self.label = QLabel(self)\n        self.label.setText(\"This is a label\")\n        self.label.setStyleSheet(\"color: #00ff00;\")\n        self.label.setGeometry(0, 0, 600, 600)\n\n        self.pushButton = QPushButton(self)\n        self.pushButton.setText(\"Click Me!\")\n        self.pushButton.setStyleSheet(\"background-color: #00ff00;\")\n        self.pushButton.setGeometry(0, 0, 600, 600)\n        self.pushButton.clicked.connect(self.clicked)\n\n    def clicked(self):\n        self.label.setText(\"You pressed the button\")\n        self.label.setStyleSheet(\"color: #00ff00;\")\n\nif __name__ == \"__main__\":\n    app = QApplication(sys.argv)\n    window = Window()\n    window.show()\n    sys.exit(app.exec_())\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a CSV file with a custom color scheme and line numbers.\n\n### Code:"
    },
    {
        "api": "Pygments",
        "decoder_input_ids": false,
        "ground-truth": "# !pip install Pygments\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import RtfFormatter\n\n# Read the code file\nwith open(\"code.py\", \"r\") as file:\n    code = file.read()\n\n# Determine the lexer based on the file extension\nlexer = get_lexer_by_name(\"python\")\n\n# Apply syntax highlighting to the code\nhighlighted_code = highlight(code, lexer, RtfFormatter())\n\n# Save the highlighted code to a RTF file\nwith open(\"highlighted_code.rtf\", \"w\") as rtf_file:\n    rtf_file.write(highlighted_code)",
        "instruction": "Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a RTF file.",
        "prediction": "from PyQt5.QtWidgets import QApplication, QWidget, QPushButton, QHBoxLayout, QVBoxLayout, QLabel, QTextEdit, QLineEdit, QComboBox\nfrom PyQt5.QtGui import QIcon\nfrom PyQt5.QtCore import pyqtSlot\n\nimport sys\n\nclass App(QWidget):\n\n    def __init__(self):\n        super().__init__()\n        self.title = 'PyQt5 layout - pythonspot.com'\n        self.left = 10\n        self.top = 10\n        self.width = 320\n        self.height = 100\n        self.initUI()\n    \n    def initUI(self):\n        self.setWindowTitle(self.title)\n        self.setGeometry(self.left, self.top, self.width, self.height)\n        \n        self.createHorizontalLayout()\n        \n        windowLayout = QVBoxLayout()\n        windowLayout.addWidget(self.horizontalGroupBox)\n        self.setLayout(windowLayout)\n        \n        self.show()\n    \n    def createHorizontalLayout(self):\n        self.horizontalGroupBox = QGroupBox(\"Horizontal layout\")\n        layout = QHBoxLayout()\n        \n        layout.addWidget(QPushButton('Left'))\n        layout.addWidget(QPushButton('Center'))\n        layout.addWidget(QPushButton('Right'))\n        \n        self.horizontalGroupBox.setLayout(layout)\n\nif __name__ == '__main__':\n    app = QApplication(sys.argv)\n    ex = App()\n    sys.exit(app.exec_())\n    ",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a RTF file.\n\n### Code:"
    },
    {
        "api": "SQLAlchemy",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install SQLAlchemy\nfrom sqlalchemy import create_engine, Column, Integer, String, Float\nfrom sqlalchemy.orm import sessionmaker, declarative_base\n\n# Create a MySQL database connection\nengine = create_engine('mysql://username:password@localhost/mydatabase')\n\n# Define the Employee class to represent the \"Employee\" entity\nBase = declarative_base()\n\nclass Employee(Base):\n    __tablename__ = 'employees'\n    id = Column(Integer, primary_key=True)\n    first_name = Column(String)\n    last_name = Column(String)\n    position = Column(String)\n    salary = Column(Float)\n\n# Create the table in the database\nBase.metadata.create_all(engine)\n\n# Function to add a new employee to the database\ndef add_employee(first_name, last_name, position, salary):\n    Session = sessionmaker(bind=engine)\n    session = Session()\n\n    employee = Employee(first_name=first_name, last_name=last_name, position=position, salary=salary)\n    session.add(employee)\n    session.commit()\n    session.close()\n\nadd_employee(\"John\", \"Smith\", \"Manager\", 60000.00)\n\nSession = sessionmaker(bind=engine)\nsession = Session()\nemployee = session.query(Employee).filter_by(first_name=\"John\").first()\nsession.close()\n\nprint(\"Employee data: \")\nprint(employee.first_name)\nprint(employee.last_name)\nprint(employee.position)\nprint(employee.salary)",
        "instruction": "Create a Python program that uses the 'SQLAlchemy' API to connect to a MySQL database. Define a 'Employee' entity with attributes like 'id,' 'first_name,' 'last_name,' 'position,' and 'salary.' The program should establish a connection to the MySQL database, create the 'employees' table and add a new employee. Then, retrieve employee details from the database.",
        "prediction": "import sqlalchemy as sa\nfrom sqlalchemy.ext.declarative import declarative_base\n\nBase = declarative_base()\n\nclass Employee(Base):\n    __tablename__ = 'employee'\n    id = sa.Column(sa.Integer, primary_key=True)\n    first_name = sa.Column(sa.String(50))\n    last_name = sa.Column(sa.String(50))\n    position = sa.Column(sa.String(50))\n    salary = sa.Column(sa.Integer)\n    department_id = sa.Column(sa.Integer, sa.ForeignKey('department.id'))\n    department = sa.orm.relationship('Department', back_populates='employees')",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that uses the 'SQLAlchemy' API to connect to a MySQL database. Define a 'Employee' entity with attributes like 'id,' 'first_name,' 'last_name,' 'position,' and 'salary.' The program should establish a connection to the MySQL database, create the 'employees' table and add a new employee. Then, retrieve employee details from the database.\n\n### Code:"
    },
    {
        "api": "apeye",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install apeye\nimport apeye\n\nbase_url = \"https://example.com\"\n\nclient = apeye.URL(base_url)\n\ncookies = client.cookies\nuser_agent = client.user_agent\nreferer = client.referer\n\nprint(\"Cookies:\")\nprint(cookies)\n\nprint(\"\\nUser Agent:\")\nprint(user_agent)\n\nprint(\"\\nReferer:\")\nprint(referer)",
        "instruction": "Develop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the cookies, user agent, and referer information from the URL request.",
        "prediction": "import requests\nimport json\n\ndef get_url(url):\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'\n    }\n    response = requests.get(url, headers=headers)\n    return response.content.decode('utf-8')\n\ndef get_json_data(url):\n    data = get_url(url)\n    data = json.loads(data)\n    return data\n\ndef get_data(url):\n    data = get_json_data(url)\n    return data['data']",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the cookies, user agent, and referer information from the URL request.\n\n### Code:"
    },
    {
        "api": "apeye",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install apeye\nimport apeye\n\nbase_url = \"https://example.com\"\n\nclient = apeye.URL(base_url)\n\nresponse = client.get()\n\nprint(\"Status Code:\")\nprint(response.status_code)\n\nprint(\"\\nHeaders:\")\nprint(response.headers)\n\nprint(\"\\nContent:\")\nprint(response.content)\n\n# Save content to file\nwith open('output.txt', 'w') as file:\n    file.write(response.content)",
        "instruction": "Develop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the status code, headers, and content of the URL response. Additionally, the program should save the content of the response to a file named 'output.txt'.",
        "prediction": "import requests\nimport json\nimport sys\nimport os\n\ndef get_url(url):\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36'\n    }\n    response = requests.get(url, headers=headers)\n    return response.status_code, response.content.decode('utf-8')\n\ndef get_json(url):\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36'\n    }\n    response = requests.get(url, headers=headers)\n    return json.loads(response.content.decode('utf-8'))\n\ndef get_content(url):\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36'\n    }\n    response = requests.get(url, headers=headers)\n    return response.content.decode('utf-8')\n\ndef get_content_from_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        return f.read()\n\ndef get_content_from_file_by_line(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        return f.readlines()\n\ndef get_content_from_file_by_line_strip(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        return [line.strip() for line in f.readlines()]\n\ndef get_content_from_file_by_line_strip_list(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        return [line.strip() for line in f.readlines()]\n\ndef get_content_from_file_by_line_strip_list_to_list(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        return [line.strip() for line in f.readlines()]\n\ndef get_content_from_file_by_line_strip_list_to_list_to_dict(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        return [json.loads(line.strip()) for line in f.readlines()]\n\ndef get_content_from_file_by_line_strip_list_to_list_to_dict_list(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        return [json.loads(line.strip()) for line in f.readlines()]\n\ndef get_content_from_file_by_line_strip_list_to_list_to_dict_list_to_list(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        return [json.loads(line.strip()) for line in f.readlines()]\n\ndef get_content_from_file_by_line_strip_list_to_list_to_dict_list_to_list_to_dict(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        return [json.loads(line.strip()) for line in f.readlines()]\n\ndef get_content_from_file_by_line_strip_list_to_list_to_dict_list_to_list_to_dict_list(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        return [json.loads(line.strip()) for line in f.readlines()]\n\ndef get_content_from_file_by_line_strip_list_",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the status code, headers, and content of the URL response. Additionally, the program should save the content of the response to a file named 'output.txt'.\n\n### Code:"
    },
    {
        "api": "bitstring",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install bitstring\nfrom bitstring import BitArray\n\nbinary1 = input(\"Enter the first binary string: \")\nbinary2 = input(\"Enter the second binary string: \")\n\nbitarray1 = BitArray(bin=binary1)\nbitarray2 = BitArray(bin=binary2)\n\nresult = bitarray1 + bitarray2\n\nprint(\"Result of bitwise concatenation operation:\", result.bin)",
        "instruction": "Create a Python program using the 'bitstring' API to perform a bitwise concatenation operation on two binary strings. The program should take two binary strings as input and perform the bitwise concatenation operation on them. The program should then print the result of the bitwise concatenation operation.",
        "prediction": "import sys\nsys.path.append('../')\nfrom binascii import hexlify\nfrom struct import pack\nfrom struct import unpack\nfrom struct import calcsize as sizeof\nfrom binascii import unhexlify\nfrom binascii import b2a_hex\nfrom binascii import a2b_hex\nfrom binascii import b2a_base64\nfrom binascii import a2b_base64\nfrom binascii import b2a_qp\nfrom binascii import a2b_qp\nfrom binascii import b2a_uu\nfrom binascii import a2b_uu\nfrom binascii import b2a_hex\nfrom binascii import a2b_hex\nfrom binascii import b2a_base64\nfrom binascii import a2b_base64\nfrom binascii import b2a_qp\nfrom binascii import a2b_qp\nfrom binascii import b2a_uu\nfrom binascii import a2b_uu\nfrom binascii import b2a_hex\nfrom binascii import a2b_hex\nfrom binascii import b2a_base64\nfrom binascii import a2b_base64\nfrom binascii import b2a_qp\nfrom binascii import a2b_qp\nfrom binascii import b2a_uu\nfrom binascii import a2b_uu\nfrom binascii import b2a_hex\nfrom binascii import a2b_hex\nfrom binascii import b2a_base64\nfrom binascii import a2b_base64\nfrom binascii import b2a_qp\nfrom binascii import a2b_qp\nfrom binascii import b2a_uu\nfrom binascii import a2b_uu\nfrom binascii import b2a_hex\nfrom binascii import a2b_hex\nfrom binascii import b2a_base64\nfrom binascii import a2b_base64\nfrom binascii import b2a_qp\nfrom binascii import a2b_qp\nfrom binascii import b2a_uu\nfrom binascii import a2b_uu\nfrom binascii import b2a_hex\nfrom binascii import a2b_hex\nfrom binascii import b2a_base64\nfrom binascii import a2b_base64\nfrom binascii import b2a_qp\nfrom binascii import a2b_qp\nfrom binascii import b2a_uu\nfrom binascii import a2b_uu\nfrom binascii import b2a_hex\nfrom binascii import a2b_hex\nfrom binascii import b2a_base64\nfrom binascii import a2b_base64\nfrom binascii import b2a_qp\nfrom binascii import a2b_qp\nfrom binascii import b2a_uu\nfrom binascii import a2b_uu\nfrom binascii import b2a_hex\nfrom binascii import a2b_hex\nfrom binascii import b2a_base64\nfrom binascii import a2b_base64\nfrom binascii import b2a_qp\nfrom binascii import a2b_qp\nfrom binascii import b2a_uu\nfrom binascii import a2b_uu\nfrom binascii import b2a_hex\nfrom binascii import a2b_hex\nfrom binascii import b2a_base64\nfrom binascii import a2b_base64\nfrom binascii import b2a_qp\nfrom binascii import a2b_qp\nfrom binascii import b2a_uu\nfrom binascii import a2b_uu\nfrom binascii import b2a_hex\nfrom binascii import a2b_hex\nfrom binascii import b2a_base64\nfrom binascii import a2b_base64\nfrom binascii import b2a_qp\nfrom binascii import a2",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'bitstring' API to perform a bitwise concatenation operation on two binary strings. The program should take two binary strings as input and perform the bitwise concatenation operation on them. The program should then print the result of the bitwise concatenation operation.\n\n### Code:"
    },
    {
        "api": "category-encoders",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install category_encoders\nimport category_encoders as cat_encoder\nimport pandas as pd\n\n# creating the dictionary\ndictionary = {'OUTLOOK': ['Rainy', 'Rainy',\n                          'Overcast', 'Sunny',\n                          'Sunny', 'Sunny',\n                          'Overcast'],\n              'TEMPERATURE': ['Hot', 'Hot', 'Hot',\n                          'Mild', 'Cool',\n                          'Cool', 'Cool'],\n              'PLAY': ['No', 'No', 'Yes',\n                       'Yes', 'Yes', 'No',\n                       'Yes']}\n\n# converting the dictionary to DataFrame\ndf = pd.DataFrame(dictionary)\n\n# creating an object PolynomialEncoder\nencoder = cat_encoder.PolynomialEncoder(cols = ['OUTLOOK', 'TEMPERATURE'])\n\n# fitting the columns to a data frame\ndf_category_encoder = encoder.fit_transform(df[['OUTLOOK', 'TEMPERATURE']])\n\nprint(df_category_encoder)",
        "instruction": "Create a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using polynomial encoding and print the resulting data frame.",
        "prediction": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('data/categories.csv')\ndf.head()\n\ndf.info()\n\ndf.describe()\n\ndf.columns\n\ndf.category_name.nunique()\n\ndf.category_name.value_counts()\n\ndf.category_name.value_counts().plot(kind='bar')\n\ndf.category_name.value_counts().plot(kind='pie')\n\ndf.category_name.value_counts().plot(kind='barh')\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10))\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g')\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True)\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True, cumulative=True)\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True, cumulative=True,\n                                     label='cumulative')\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True, cumulative=True,\n                                     label='cumulative', alpha=0.5)\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True, cumulative=True,\n                                     label='cumulative', alpha=0.5, width=0.8)\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True, cumulative=True,\n                                     label='cumulative', alpha=0.5, width=0.8, align='center')\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True, cumulative=True,\n                                     label='cumulative', alpha=0.5, width=0.8, align='center',\n                                     color='c', edgecolor='k')\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True, cumulative=True,\n                                     label='cumulative', alpha=0.5, width=0.8, align='center',\n                                     color='c', edgecolor='k', linewidth=2)\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True, cumulative=True,\n                                     label='cumulative', alpha=0.5, width=0.8, align='center',\n                                     color='c', edgecolor='k', linewidth=2,\n                                     hatch='/')\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True, cumulative=True,\n                                     label='cumulative', alpha=0.5, width=0.8, align='center',\n                                     color='c', edgecolor='k', linewidth=2,\n                                     hatch='/', alpha=0.3)\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True, cumulative=True,\n                                     label='cumulative', alpha=0.5, width=0.8, align='center',\n                                     color='c', edgecolor='k', linewidth=2,\n                                     hatch='/', alpha=0.3, width=0.8)\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True, cumulative=True,\n                                     label='cumulative', alpha=0.5, width=0.8, align='center',\n                                     color='c', edgecolor='k', linewidth=2,\n",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using polynomial encoding and print the resulting data frame.\n\n### Code:"
    },
    {
        "api": "category-encoders",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install category_encoders\nimport category_encoders as cat_encoder\nimport pandas as pd\n\n# creating the dictionary\ndictionary = {'OUTLOOK': ['Rainy', 'Rainy',\n                          'Overcast', 'Sunny',\n                          'Sunny', 'Sunny',\n                          'Overcast'],\n              'TEMPERATURE': ['Hot', 'Hot', 'Hot',\n                          'Mild', 'Cool',\n                          'Cool', 'Cool'],\n              'PLAY': ['No', 'No', 'Yes',\n                       'Yes', 'Yes', 'No',\n                       'Yes']}\n\n# converting the dictionary to DataFrame\ndf = pd.DataFrame(dictionary)\n\n# creating an object JamesSteinEncoder\nencoder = cat_encoder.JamesSteinEncoder(cols = ['OUTLOOK', 'TEMPERATURE'])\n\n# fitting the columns to a data frame\ndf_category_encoder = encoder.fit_transform(df[['OUTLOOK', 'TEMPERATURE']])\n\nprint(df_category_encoder)",
        "instruction": "Create a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using James-Stein encoding and print the resulting data frame.",
        "prediction": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('data/categories.csv')\ndf.head()\n\ndf.info()\n\ndf.describe()\n\ndf.columns\n\ndf.category_name.unique()\n\ndf.category_name.value_counts()\n\ndf.category_name.value_counts().plot(kind='bar')\n\ndf.category_name.value_counts().plot(kind='pie')\n\ndf.category_name.value_counts().plot(kind='barh')\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10))\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g')\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True)\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True, cumulative=True)\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True, cumulative=True,\n                                     label='cumulative')\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True, cumulative=True,\n                                     label='cumulative', alpha=0.5)\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True, cumulative=True,\n                                     label='cumulative', alpha=0.5, width=0.8)\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True, cumulative=True,\n                                     label='cumulative', alpha=0.5, width=0.8, align='center')\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True, cumulative=True,\n                                     label='cumulative', alpha=0.5, width=0.8, align='center',\n                                     color='c', edgecolor='k')\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True, cumulative=True,\n                                     label='cumulative', alpha=0.5, width=0.8, align='center',\n                                     color='c', edgecolor='k', linewidth=2)\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True, cumulative=True,\n                                     label='cumulative', alpha=0.5, width=0.8, align='center',\n                                     color='c', edgecolor='k', linewidth=2,\n                                     hatch='/')\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True, cumulative=True,\n                                     label='cumulative', alpha=0.5, width=0.8, align='center',\n                                     color='c', edgecolor='k', linewidth=2,\n                                     hatch='/', alpha=0.3)\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True, cumulative=True,\n                                     label='cumulative', alpha=0.5, width=0.8, align='center',\n                                     color='c', edgecolor='k', linewidth=2,\n                                     hatch='/', alpha=0.3, width=0.8)\n\ndf.category_name.value_counts().plot(kind='barh', figsize=(10, 10), color='g', log=True, cumulative=True,\n                                     label='cumulative', alpha=0.5, width=0.8, align='center',\n                                     color='c', edgecolor='k', linewidth=2,\n                               ",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using James-Stein encoding and print the resulting data frame.\n\n### Code:"
    },
    {
        "api": "chardet",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install chardet\nimport chardet\n\nwith open('mixed_encodings.txt', 'r') as file:\n    for line in file:\n        result = chardet.detect(line.encode())\n        print(f\"Line: {line.strip()}\")\n        print(f\"Detected Encoding: {result['encoding']}\")",
        "instruction": "Develop a Python program that reads a text file containing mixed encodings and uses the 'chardet' API to detect the encoding of each line. The program should then print the encoding detection results for each line in the file.",
        "prediction": "import sys\nimport re\nimport chardet\n\ndef detect_encoding(file):\n    with open(file, 'rb') as f:\n        rawdata = f.read()\n    return chardet.detect(rawdata)['encoding']\n\ndef detect_encoding_file(file):\n    with open(file, 'rb') as f:\n        rawdata = f.read()\n    return chardet.detect(rawdata)['encoding']\n\ndef detect_encoding_str(str):\n    return chardet.detect(str)['encoding']\n\ndef detect_encoding_str_file(file):\n    with open(file, 'rb') as f:\n        rawdata = f.read()\n    return chardet.detect(rawdata)['encoding']\n\ndef detect_encoding_str_file_by_line(file):\n    encoding = detect_encoding_str_file(file)\n    with open(file, 'r', encoding=encoding) as f:\n        for line in f:\n            print(line)\n            break\n\ndef detect_encoding_str_file_by_line_by_line(file):\n    encoding = detect_encoding_str_file(file)\n    with open(file, 'r', encoding=encoding) as f:\n        for line in f:\n            print(line)\n            break\n\ndef detect_encoding_str_file_by_line_by_line_by_line(file):\n    encoding = detect_encoding_str_file(file)\n    with open(file, 'r', encoding=encoding) as f:\n        for line in f:\n            print(line)\n            break\n\ndef detect_encoding_str_file_by_line_by_line_by_line_by_line(file):\n    encoding = detect_encoding_str_file(file)\n    with open(file, 'r', encoding=encoding) as f:\n        for line in f:\n            print(line)\n            break\n\ndef detect_encoding_str_file_by_line_by_line_by_line_by_line_by_line(file):\n    encoding = detect_encoding_str_file(file)\n    with open(file, 'r', encoding=encoding) as f:\n        for line in f:\n            print(line)\n            break\n\ndef detect_encoding_str_file_by_line_by_line_by_line_by_line_by_line_by_line(file):\n    encoding = detect_encoding_str_file(file)\n    with open(file, 'r', encoding=encoding) as f:\n        for line in f:\n            print(line)\n            break\n\ndef detect_encoding_str_file_by_line_by_line_by_line_by_line_by_line_by_line_by_line(file):\n    encoding = detect_encoding_str_file(file)\n    with open(file, 'r', encoding=encoding) as f:\n        for line in f:\n            print(line)\n            break\n\ndef detect_encoding_str_file_by_line_by_line_by_line_by_line_by_line_by_line_by_line_by_line(file):\n    encoding = detect_encoding_str_file(file)\n    with open(file, 'r', encoding=encoding) as f:\n        for line in f:\n            print(line)\n            break\n\ndef detect_encoding_str_file_by_line_by_line_by_line_by_line_by_line_by_line_by_line_by_line_by_line(file):\n    encoding = detect_encoding_str_file(file)\n    with open(file, 'r', encoding=encoding) as f:\n        for line in f:\n            print(line)\n            break\n\ndef detect_encoding_str_file_by_line_by_line_by_line_by_line_by_line_by_line_by_line_by_line_by_line_by_line_by_line(file):\n    encoding = detect_encoding_str_file(file)\n    with",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program that reads a text file containing mixed encodings and uses the 'chardet' API to detect the encoding of each line. The program should then print the encoding detection results for each line in the file.\n\n### Code:"
    },
    {
        "api": "chardet",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install chardet\nimport chardet\nimport json\n\nwith open('data.json', 'r') as jsonfile:\n    data = json.load(jsonfile)\n\ndetected_data = {}\nfor key, value in data.items():\n    result = chardet.detect(value.encode())\n    encoding = result['encoding']\n    detected_data[key] = value.encode(encoding).decode(encoding)\n\nwith open('detected_data.json', 'w') as output_file:\n    json.dump(detected_data, output_file, ensure_ascii=False)",
        "instruction": "Develop a Python program that reads a JSON file with text data in various encodings, uses the 'chardet' API to detect the encoding of each value, and then saves the JSON data with the detected encodings to a new file.",
        "prediction": "import json\nimport sys\nimport os\nimport csv\nimport re\nimport argparse\nimport codecs\nimport unicodedata\n\ndef read_json(file_name):\n    with open(file_name, 'r') as f:\n        data = json.load(f)\n    return data\n\ndef read_json_from_file(file_name):\n    with open(file_name, 'r') as f:\n        data = json.load(f)\n    return data\n\ndef read_json_from_file_line_by_line(file_name):\n    with open(file_name, 'r') as f:\n        for line in f:\n            data = json.loads(line)\n            yield data\n\ndef read_json_from_file_line_by_line_with_filter(file_name, filter_key):\n    with open(file_name, 'r') as f:\n        for line in f:\n            data = json.loads(line)\n            if filter_key in data:\n                yield data[filter_key]\n\ndef read_json_from_file_line_by_line_with_filter_and_key(file_name, filter_key, key):\n    with open(file_name, 'r') as f:\n        for line in f:\n            data = json.loads(line)\n            if filter_key in data:\n                yield data[filter_key][key]\n\ndef read_json_from_file_line_by_line_with_filter_and_key_and_value(file_name, filter_key, key, value):\n    with open(file_name, 'r') as f:\n        for line in f:\n            data = json.loads(line)\n            if filter_key in data:\n                if data[filter_key][key] == value:\n                    yield data[filter_key]\n\ndef read_json_from_file_line_by_line_with_filter_and_key_and_value_and_value(file_name, filter_key, key, value):\n    with open(file_name, 'r') as f:\n        for line in f:\n            data = json.loads(line)\n            if filter_key in data:\n                if data[filter_key][key] == value:\n                    yield data[filter_key][key]\n\ndef read_json_from_file_line_by_line_with_filter_and_key_and_value_and_value_and_value(file_name, filter_key, key, value):\n    with open(file_name, 'r') as f:\n        for line in f:\n            data = json.loads(line)\n            if filter_key in data:\n                if data[filter_key][key] == value:\n                    yield data[filter_key][key]\n\ndef read_json_from_file_line_by_line_with_filter_and_key_and_value_and_value_and_value_and_value(file_name, filter_key, key, value):\n    with open(file_name, 'r') as f:\n        for line in f:\n            data = json.loads(line)\n            if filter_key in data:\n                if data[filter_key][key] == value:\n                    yield data[filter_key][key]\n\ndef read_json_from_file_line_by_line_with_filter_and_key_and_value_and_value_and_value_and_value_and_value(file_name, filter_key, key, value):\n    with open(file_name, 'r') as f:\n        for line in f:\n            data = json.loads(line)\n            if filter_key in data:\n                if data[filter_key][key] == value:\n                    yield data[filter_key][key]\n\ndef read_json_from_file_line_by_line_with_filter_and_key_and_value_and_value_and_value_and_value_and_value_and_value(file_name, filter_key, key, value):\n    with open(file_name, 'r') as f:\n        for line in f:\n            data = json.loads(line)\n            if filter_key in data:\n                if data[filter",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program that reads a JSON file with text data in various encodings, uses the 'chardet' API to detect the encoding of each value, and then saves the JSON data with the detected encodings to a new file.\n\n### Code:"
    },
    {
        "api": "chardet",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install chardet\nfrom chardet.universaldetector import UniversalDetector\n\ntext = \"Here is a text with numbers 22 and special charcaters ##%&\"\ntext = text.encode(encoding='ascii',errors='strict')\n\ndetector = UniversalDetector()\ndetector.feed(text)\ndetector.close()\n\nprint(detector.result)",
        "instruction": "Develop a Python program that uses the 'chardet' API to detect the encoding of a given text. The program should encode the text in ASCII and use the detector to identify the encoding. Print the result of the encoding detection.",
        "prediction": "import cv2\nimport sys\nimport os\nimport numpy as np\n\ndef detect_encoding(file):\n    img = cv2.imread(file)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n    for (x, y, w, h) in faces:\n        cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n        roi_gray = gray[y:y+h, x:x+w]\n        roi_color = img[y:y+h, x:x+w]\n    cv2.imshow('img', img)\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n    return faces\n\ndef detect_encoding_from_camera():\n    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n    cap = cv2.VideoCapture(0)\n    while True:\n        ret, frame = cap.read()\n        if ret == False:\n            continue\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n        for (x, y, w, h) in faces:\n            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n            roi_gray = gray[y:y+h, x:x+w]\n            roi_color = frame[y:y+h, x:x+w]\n        cv2.imshow('img', frame)\n        k = cv2.waitKey(30) & 0xff\n        if k == 27:\n            break\n    cap.release()\n    cv2.destroyAllWindows()\n\ndef encode_faces(file):\n    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n    img = cv2.imread(file)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n    for (x, y, w, h) in faces:\n        cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n        roi_gray = gray[y:y+h, x:x+w]\n        roi_color = img[y:y+h, x:x+w]\n    cv2.imwrite('encoded_faces/' + file, img)\n\ndef encode_faces_from_camera():\n    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n    cap = cv2.VideoCapture(0)\n    while True:\n        ret, frame = cap.read()\n        if ret == False:\n            continue\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n        for (x, y, w, h) in faces:\n            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n            roi_gray = gray[y:y+h, x:x+w]\n            roi_color = frame[y:y+h, x:x+w]\n        cv2.imshow('img', frame)\n        k = cv2.waitKey(30) & 0xff\n        if k == 27:\n            break\n    cap.release()\n    cv2.destroyAllWindows()\n\ndef encode_faces_from_camera_to_folder():\n    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program that uses the 'chardet' API to detect the encoding of a given text. The program should encode the text in ASCII and use the detector to identify the encoding. Print the result of the encoding detection.\n\n### Code:"
    },
    {
        "api": "click",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install click\nimport click\n\n@click.command()\n@click.option('--number1', prompt='Enter the first number: ',\n              help='First number.')\n@click.option('--number2', prompt='Enter the second number: ',\n              help='Second number.')\ndef main(number1, number2):\n  number1 = int(number1)\n  number2 = int(number2)\n  print(\"The sum is:\", number1 + number2)\n\nif __name__ == '__main__':\n    main()",
        "instruction": "Create a Python program using the 'click' API to build a command-line tool. The program should accept two input options: 'number1' and 'number2'. It should then calculate the sum of 'number1' and 'number2' and print the result.",
        "prediction": "import click\n\n@click.command()\n@click.option('--number1', prompt='Enter a number',\n              help='This is the first number',\n              type=int)\n@click.option('--number2', prompt='Enter a number',\n              help='This is the second number',\n              type=int)\n@click.option('--operation', prompt='Enter an operation',\n              help='This is the operation',\n              type=click.Choice(['add','subtract','multiply', 'divide']))\ndef calculate(number1, number2, operation):\n    if operation == 'add':\n        result = number1 + number2\n    elif operation =='subtract':\n        result = number1 - number2\n    elif operation =='multiply':\n        result = number1 * number2\n    elif operation == 'divide':\n        result = number1 / number2\n    else:\n        click.echo('Invalid operation')\n    click.echo('Result: {}'.format(result))\n\nif __name__ == '__main__':\n    calculate()\n    ",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'click' API to build a command-line tool. The program should accept two input options: 'number1' and 'number2'. It should then calculate the sum of 'number1' and 'number2' and print the result.\n\n### Code:"
    },
    {
        "api": "datacompy",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install datacompy\nfrom io import StringIO\nimport pandas as pd\nimport datacompy\n\ndata1 = \"\"\"employee_id, name\n1, islam mesabah\n\"\"\"\n\ndata2 = \"\"\"employee_id, name\n1, islam mesabah\n\"\"\"\n\ndf1 = pd.read_csv(StringIO(data1))\ndf2 = pd.read_csv(StringIO(data2))\n\ncompare = datacompy.Compare(\n  df1,\n  df2,\n  # You can also specify a list\n  # of columns\n  join_columns = 'employee_id',\n\n  # Optional, defaults to 'df1'\n  df1_name = 'Original',\n\n  # Optional, defaults to 'df2'\n  df2_name = 'New'\n  )\ncompare.matches(ignore_extra_columns = False)\nprint(compare.report())",
        "instruction": "Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on the 'employee_id' column, and produce a comparison report. Ensure that it reports matches and does not ignore extra columns.",
        "prediction": "import datacompy\nimport pandas as pd\n\ndf1 = pd.read_csv('data/df1.csv')\ndf2 = pd.read_csv('data/df2.csv')\n\ndf1.columns = ['employee_id', 'name', 'department', 'role', 'is_manager', 'hire_date']\ndf2.columns = ['employee_id', 'name', 'department', 'role', 'is_manager', 'hire_date']\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on the 'employee_id' column, and produce a comparison report. Ensure that it reports matches and does not ignore extra columns.\n\n### Code:"
    },
    {
        "api": "datacompy",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install datacompy\nfrom io import StringIO\nimport pandas as pd\nimport datacompy\n\ndata1 = \"\"\"employee_id, name, department\n1, islam mesabah, IT\n2, john doe, HR\n\"\"\"\n\ndata2 = \"\"\"employee_id, name, department\n1, islam mesabah, IT\n2, john doe, HR\n\"\"\"\n\ndf1 = pd.read_csv(StringIO(data1))\ndf2 = pd.read_csv(StringIO(data2))\n\ncompare = datacompy.Compare(\n  df1,\n  df2,\n  # You can also specify a list\n  # of columns\n  join_columns = ['employee_id', 'department'],\n\n  # Optional, defaults to 'df1'\n  df1_name = 'Original',\n\n  # Optional, defaults to 'df2'\n  df2_name = 'New'\n  )\ncompare.matches(ignore_extra_columns = True)\nprint(compare.report())",
        "instruction": "Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on multiple columns ('employee_id' and 'department'), and produce a comparison report. Ensure that it reports matches and ignores extra columns.",
        "prediction": "import datacompy\nimport pandas as pd\n\ndf1 = pd.read_csv('data/df1.csv')\ndf2 = pd.read_csv('data/df2.csv')\n\ndf1.columns = ['employee_id', 'department', 'name','salary']\ndf2.columns = ['employee_id', 'department', 'name','salary']\n\ndf1.set_index('employee_id', inplace=True)\ndf2.set_index('employee_id', inplace=True)\n\ndf1.sort_index(inplace=True)\ndf2.sort_index(inplace=True)\n\ndf1.drop(columns=['department', 'name'], inplace=True)\ndf2.drop(columns=['department', 'name'], inplace=True)\n\ndf1.columns = ['employee_id','salary']\ndf2.columns = ['employee_id','salary']\n\ndf1.set_index('employee_id', inplace=True)\ndf2.set_index('employee_id', inplace=True)\n\ndf1.sort_index(inplace=True)\ndf2.sort_index(inplace=True)\n\ndf1.drop(columns=['salary'], inplace=True)\ndf2.drop(columns=['salary'], inplace=True)\n\ndf1.columns = ['department', 'name']\ndf2.columns = ['department', 'name']\n\ndf1.set_index('department', inplace=True)\ndf2.set_index('department', inplace=True)\n\ndf1.sort_index(inplace=True)\ndf2.sort_index(inplace=True)\n\ndf1.drop(columns=['name'], inplace=True)\ndf2.drop(columns=['name'], inplace=True)\n\ndf1.columns = ['department', 'name']\ndf2.columns = ['department', 'name']\n\ndf1.set_index('department', inplace=True)\ndf2.set_index('department', inplace=True)\n\ndf1.sort_index(inplace=True)\ndf2.sort_index(inplace=True)\n\ndf1.drop(columns=['name'], inplace=True)\ndf2.drop(columns=['name'], inplace=True)\n\ndf1.columns = ['department', 'name']\ndf2.columns = ['department', 'name']\n\ndf1.set_index('department', inplace=True)\ndf2.set_index('department', inplace=True)\n\ndf1.sort_index(inplace=True)\ndf2.sort_index(inplace=True)\n\ndf1.drop(columns=['name'], inplace=True)\ndf2.drop(columns=['name'], inplace=True)\n\ndf1.columns = ['department', 'name']\ndf2.columns = ['department', 'name']\n\ndf1.set_index('department', inplace=True)\ndf2.set_index('department', inplace=True)\n\ndf1.sort_index(inplace=True)\ndf2.sort_index(inplace=True)\n\ndf1.drop(columns=['name'], inplace=True)\ndf2.drop(columns=['name'], inplace=True)\n\ndf1.columns = ['department', 'name']\ndf2.columns = ['department', 'name']\n\ndf1.set_index('department', inplace=True)\ndf2.set_index('department', inplace=True)\n\ndf1.sort_index(inplace=True)\ndf2.sort_index(inplace=True)\n\ndf1.drop(columns=['name'], inplace=True)\ndf2.drop(columns=['name'], inplace=True)\n\ndf1.columns = ['department', 'name']\ndf2.columns = ['department', 'name']\n\ndf1.set_index('department', inplace=True)\ndf2",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on multiple columns ('employee_id' and 'department'), and produce a comparison report. Ensure that it reports matches and ignores extra columns.\n\n### Code:"
    },
    {
        "api": "datacompy",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install datacompy\nfrom io import StringIO\nimport pandas as pd\nimport datacompy\n\ndata1 = \"\"\"employee_id, name, age\n1, islam mesabah, 25\n2, john doe, 30\n\"\"\"\n\ndata2 = \"\"\"employee_id, name, age\n1, islam mesabah, 25\n2, john doe, 30\n\"\"\"\n\ndf1 = pd.read_csv(StringIO(data1))\ndf2 = pd.read_csv(StringIO(data2))\n\ncompare = datacompy.Compare(\n  df1,\n  df2,\n  # You can also specify a list\n  # of columns\n  join_columns = 'employee_id',\n\n  # Optional, defaults to 'df1'\n  df1_name = 'Original',\n\n  # Optional, defaults to 'df2'\n  df2_name = 'New'\n  )\ncompare.matches(ignore_extra_columns = True)\nprint(compare.report())",
        "instruction": "Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on a specific column ('employee_id'), and produce a comparison report. Ensure that it reports matches and ignores extra columns.",
        "prediction": "import datacompy\nimport pandas as pd\n\ndf1 = pd.read_csv('data/df1.csv')\ndf2 = pd.read_csv('data/df2.csv')\n\ndf1.columns = ['employee_id', 'name', 'department', 'role', 'is_manager', 'hire_date']\ndf2.columns = ['employee_id', 'name', 'department', 'role', 'is_manager', 'hire_date']\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on a specific column ('employee_id'), and produce a comparison report. Ensure that it reports matches and ignores extra columns.\n\n### Code:"
    },
    {
        "api": "datacompy",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install datacompy\nfrom io import StringIO\nimport pandas as pd\nimport datacompy\n\ndata1 = \"\"\"employee_id, name, age\n1, islam mesabah, 25\n2, john doe, 30\n\"\"\"\n\ndata2 = \"\"\"employee_id, name, age\n1, islam mesabah, 25\n2, john doe, 30\n\"\"\"\n\ndf1 = pd.read_csv(StringIO(data1))\ndf2 = pd.read_csv(StringIO(data2))\n\ncompare = datacompy.Compare(\n  df1,\n  df2,\n  # You can also specify a list\n  # of columns\n  join_columns = 'employee_id',\n\n  # Optional, defaults to 'df1'\n  df1_name = 'Original',\n\n  # Optional, defaults to 'df2'\n  df2_name = 'New'\n  )\ncompare.matches(ignore_extra_columns = False)\nprint(compare.report())",
        "instruction": "Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on a specific column ('employee_id'), and produce a comparison report. Ensure that it reports matches and includes extra columns in the comparison.",
        "prediction": "import datacompy\nimport pandas as pd\n\ndf1 = pd.read_csv('data/df1.csv')\ndf2 = pd.read_csv('data/df2.csv')\n\ndf1.columns = ['employee_id', 'name', 'department', 'role', 'is_manager', 'hire_date']\ndf2.columns = ['employee_id', 'name', 'department', 'role', 'is_manager', 'hire_date']\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on a specific column ('employee_id'), and produce a comparison report. Ensure that it reports matches and includes extra columns in the comparison.\n\n### Code:"
    },
    {
        "api": "datacompy",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install datacompy\nfrom io import StringIO\nimport pandas as pd\nimport datacompy\n\ndata1 = \"\"\"employee_id, name, department\n1, islam mesabah, IT\n2, john doe, HR\n\"\"\"\n\ndata2 = \"\"\"employee_id, name, department\n1, islam mesabah, IT\n2, john doe, HR\n\"\"\"\n\ndf1 = pd.read_csv(StringIO(data1))\ndf2 = pd.read_csv(StringIO(data2))\n\ncompare = datacompy.Compare(\n  df1,\n  df2,\n  # You can also specify a list\n  # of columns\n  join_columns = ['employee_id', 'department'],\n\n  # Optional, defaults to 'df1'\n  df1_name = 'Original',\n\n  # Optional, defaults to 'df2'\n  df2_name = 'New'\n  )\ncompare.matches(ignore_extra_columns = False)\nprint(compare.report())",
        "instruction": "Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on multiple columns ('employee_id' and 'department'), and produce a comparison report. Ensure that it reports matches and does not ignore extra columns.",
        "prediction": "import datacompy\nimport pandas as pd\n\ndf1 = pd.read_csv('data/df1.csv')\ndf2 = pd.read_csv('data/df2.csv')\n\ndf1.columns = ['employee_id', 'department', 'name','salary']\ndf2.columns = ['employee_id', 'department', 'name','salary']\n\ndf1.set_index('employee_id', inplace=True)\ndf2.set_index('employee_id', inplace=True)\n\ndf1.sort_index(inplace=True)\ndf2.sort_index(inplace=True)\n\ndf1.drop(columns=['department', 'name'], inplace=True)\ndf2.drop(columns=['department', 'name'], inplace=True)\n\ndf1.columns = ['employee_id','salary']\ndf2.columns = ['employee_id','salary']\n\ndf1.set_index('employee_id', inplace=True)\ndf2.set_index('employee_id', inplace=True)\n\ndf1.sort_index(inplace=True)\ndf2.sort_index(inplace=True)\n\ndf1.drop(columns=['salary'], inplace=True)\ndf2.drop(columns=['salary'], inplace=True)\n\ndf1.columns = ['department', 'name']\ndf2.columns = ['department', 'name']\n\ndf1.set_index('department', inplace=True)\ndf2.set_index('department', inplace=True)\n\ndf1.sort_index(inplace=True)\ndf2.sort_index(inplace=True)\n\ndf1.drop(columns=['name'], inplace=True)\ndf2.drop(columns=['name'], inplace=True)\n\ndf1.columns = ['department', 'name']\ndf2.columns = ['department', 'name']\n\ndf1.set_index('department', inplace=True)\ndf2.set_index('department', inplace=True)\n\ndf1.sort_index(inplace=True)\ndf2.sort_index(inplace=True)\n\ndf1.drop(columns=['name'], inplace=True)\ndf2.drop(columns=['name'], inplace=True)\n\ndf1.columns = ['department', 'name']\ndf2.columns = ['department', 'name']\n\ndf1.set_index('department', inplace=True)\ndf2.set_index('department', inplace=True)\n\ndf1.sort_index(inplace=True)\ndf2.sort_index(inplace=True)\n\ndf1.drop(columns=['name'], inplace=True)\ndf2.drop(columns=['name'], inplace=True)\n\ndf1.columns = ['department', 'name']\ndf2.columns = ['department', 'name']\n\ndf1.set_index('department', inplace=True)\ndf2.set_index('department', inplace=True)\n\ndf1.sort_index(inplace=True)\ndf2.sort_index(inplace=True)\n\ndf1.drop(columns=['name'], inplace=True)\ndf2.drop(columns=['name'], inplace=True)\n\ndf1.columns = ['department', 'name']\ndf2.columns = ['department', 'name']\n\ndf1.set_index('department', inplace=True)\ndf2.set_index('department', inplace=True)\n\ndf1.sort_index(inplace=True)\ndf2.sort_index(inplace=True)\n\ndf1.drop(columns=['name'], inplace=True)\ndf2.drop(columns=['name'], inplace=True)\n\ndf1.columns = ['department', 'name']\ndf2.columns = ['department', 'name']\n\ndf1.set_index('department', inplace=True)\ndf2",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on multiple columns ('employee_id' and 'department'), and produce a comparison report. Ensure that it reports matches and does not ignore extra columns.\n\n### Code:"
    },
    {
        "api": "datacompy",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install datacompy\nfrom io import StringIO\nimport pandas as pd\nimport datacompy\n\ndata1 = \"\"\"employee_id, name, age\n1, islam mesabah, 25\n2, john doe, 30\n\"\"\"\n\ndata2 = \"\"\"employee_id, name, age\n1, islam mesabah, 25\n2, john doe, 30\n\"\"\"\n\ndf1 = pd.read_csv(StringIO(data1))\ndf2 = pd.read_csv(StringIO(data2))\n\ncompare = datacompy.Compare(\n  df1,\n  df2,\n  # You can also specify a list\n  # of columns\n  join_columns = 'employee_id',\n\n  # Optional, defaults to 'df1'\n  df1_name = 'Original',\n\n  # Optional, defaults to 'df2'\n  df2_name = 'New'\n  )\ncompare.matches(ignore_extra_columns = False)\nprint(compare.report())",
        "instruction": "Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on a specific column ('employee_id'), and produce a comparison report. Ensure that it reports matches and does not ignore extra columns.",
        "prediction": "import datacompy\nimport pandas as pd\n\ndf1 = pd.read_csv('data/df1.csv')\ndf2 = pd.read_csv('data/df2.csv')\n\ndf1.columns = ['employee_id', 'name', 'department', 'role', 'is_manager', 'hire_date']\ndf2.columns = ['employee_id', 'name', 'department', 'role', 'is_manager', 'hire_date']\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m-%d')\n\ndf1['hire_date'] = pd.to_datetime(df1['hire_date'])\ndf2['hire_date'] = pd.to_datetime(df2['hire_date'])\n\ndf1['hire_date'] = df1['hire_date'].dt.strftime('%Y-%m-%d')\ndf2['hire_date'] = df2['hire_date'].dt.strftime('%Y-%m",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on a specific column ('employee_id'), and produce a comparison report. Ensure that it reports matches and does not ignore extra columns.\n\n### Code:"
    },
    {
        "api": "datacompy",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install datacompy\nfrom io import StringIO\nimport pandas as pd\nimport datacompy\n\ndata1 = \"\"\"employee_id, name, department\n1, islam mesabah, IT\n2, john doe, HR\n\"\"\"\n\ndata2 = \"\"\"employee_id, name, department\n1, islam mesabah, IT\n2, john doe, HR\n\"\"\"\n\ndf1 = pd.read_csv(StringIO(data1))\ndf2 = pd.read_csv(StringIO(data2))\n\ncompare = datacompy.Compare(\n  df1,\n  df2,\n  # You can also specify a list\n  # of columns\n  join_columns = ['employee_id', 'department'],\n\n  # Optional, defaults to 'df1'\n  df1_name = 'Original',\n\n  # Optional, defaults to 'df2'\n  df2_name = 'New'\n  )\ncompare.matches(ignore_extra_columns = False)\nprint(compare.report())",
        "instruction": "Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on multiple columns ('employee_id' and 'department'), and produce a comparison report. Ensure that it reports matches and includes extra columns in the comparison.",
        "prediction": "import datacompy\nimport pandas as pd\n\ndf1 = pd.read_csv('data/df1.csv')\ndf2 = pd.read_csv('data/df2.csv')\n\ndf1.columns = ['employee_id', 'department', 'name','salary']\ndf2.columns = ['employee_id', 'department', 'name','salary']\n\ndf1['department'] = df1['department'].str.lower()\ndf2['department'] = df2['department'].str.lower()\n\ndf1['department'] = df1['department'].str.replace(' ', '')\ndf2['department'] = df2['department'].str.replace(' ', '')\n\ndf1['department'] = df1['department'].str.replace('-', '')\ndf2['department'] = df2['department'].str.replace('-', '')\n\ndf1['department'] = df1['department'].str.replace('.', '')\ndf2['department'] = df2['department'].str.replace('.', '')\n\ndf1['department'] = df1['department'].str.replace('/', '')\ndf2['department'] = df2['department'].str.replace('/', '')\n\ndf1['department'] = df1['department'].str.replace('\\\\', '')\ndf2['department'] = df2['department'].str.replace('\\\\', '')\n\ndf1['department'] = df1['department'].str.replace('_', '')\ndf2['department'] = df2['department'].str.replace('_', '')\n\ndf1['department'] = df1['department'].str.replace('-', '')\ndf2['department'] = df2['department'].str.replace('-', '')\n\ndf1['department'] = df1['department'].str.replace('.', '')\ndf2['department'] = df2['department'].str.replace('.', '')\n\ndf1['department'] = df1['department'].str.replace('/', '')\ndf2['department'] = df2['department'].str.replace('/', '')\n\ndf1['department'] = df1['department'].str.replace('\\\\', '')\ndf2['department'] = df2['department'].str.replace('\\\\', '')\n\ndf1['department'] = df1['department'].str.replace('_', '')\ndf2['department'] = df2['department'].str.replace('_', '')\n\ndf1['department'] = df1['department'].str.replace('-', '')\ndf2['department'] = df2['department'].str.replace('-', '')\n\ndf1['department'] = df1['department'].str.replace('.', '')\ndf2['department'] = df2['department'].str.replace('.', '')\n\ndf1['department'] = df1['department'].str.replace('/', '')\ndf2['department'] = df2['department'].str.replace('/', '')\n\ndf1['department'] = df1['department'].str.replace('\\\\', '')\ndf2['department'] = df2['department'].str.replace('\\\\', '')\n\ndf1['department'] = df1['department'].str.replace('_', '')\ndf2['department'] = df2['department'].str.replace('_', '')\n\ndf1['department'] = df1['department'].str.replace('-', '')\ndf2['department'] = df2['department'].str.replace('-', '')\n\ndf1['department'] = df1['department'].str.replace('.', '')\ndf2['department'] = df2['department'].str.replace('.', '')\n\ndf1['department'] = df1['department'].str.replace('/', '')\ndf2['department'] = df2['department'].str.replace('",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on multiple columns ('employee_id' and 'department'), and produce a comparison report. Ensure that it reports matches and includes extra columns in the comparison.\n\n### Code:"
    },
    {
        "api": "datasets",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install datasets\nfrom datasets import load_dataset\n\n# Load SNLI dataset\ndataset = load_dataset(\"snli\")\n\n# Get the training genres\ntrain_genres = dataset[\"train\"][\"genre\"]\n\n# Count unique genre labels\nunique_genres = set(train_genres)\n\nprint(\"Number of unique genres in the SNLI training set:\", len(unique_genres))",
        "instruction": "Develop a Python program using the 'datasets' API to load the SNLI dataset and count the number of unique genres in the training set. The program should load the SNLI dataset, access the training genres, and count the unique genre labels.",
        "prediction": "import urllib.request\nimport json\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nurl = 'https://raw.githubusercontent.com/datasets/s-and-p-500-companies/master/data/constituents.json'\n\nresponse = urllib.request.urlopen(url)\ndata = json.loads(response.read())\n\nsymbols = []\nfor item in data:\n    symbols.append(item['symbol'])\n\nsymbols = symbols[:50]\n\nfor symbol in symbols:\n    url = 'https://finnhub.io/api/v1/stock/candle?symbol=' + symbol + '&resolution=D&count=1000&token=c6nqnqr5f5f5h6g5'\n    response = urllib.request.urlopen(url)\n    data = json.loads(response.read())\n    df = pd.DataFrame(data)\n    df.to_csv('data/' + symbol + '.csv')\n    print(symbol)\n\ndf = pd.DataFrame()\nfor symbol in symbols:\n    df = df.append(pd.read_csv('data/' + symbol + '.csv'))\n\ndf.to_csv('data/all.csv')\n\ndf = pd.read_csv('data/all.csv')\ndf = df.drop(columns=['s', 'p', 't', 'v'])\ndf = df.dropna()\ndf = df.set_index('t')\ndf = df.sort_index()\n\ndf.to_csv('data/all.csv')\n\ndf = pd.read_csv('data/all.csv')\ndf = df.drop(columns=['s', 'p', 't', 'v'])\ndf = df.dropna()\ndf = df.set_index('t')\ndf = df.sort_index()\n\ndf.to_csv('data/all.csv')\n\ndf = pd.read_csv('data/all.csv')\ndf = df.drop(columns=['s', 'p', 't', 'v'])\ndf = df.dropna()\ndf = df.set_index('t')\ndf = df.sort_index()\n\ndf.to_csv('data/all.csv')\n\ndf = pd.read_csv('data/all.csv')\ndf = df.drop(columns=['s', 'p', 't', 'v'])\ndf = df.dropna()\ndf = df.set_index('t')\ndf = df.sort_index()\n\ndf.to_csv('data/all.csv')\n\ndf = pd.read_csv('data/all.csv')\ndf = df.drop(columns=['s', 'p', 't', 'v'])\ndf = df.dropna()\ndf = df.set_index('t')\ndf = df.sort_index()\n\ndf.to_csv('data/all.csv')\n\ndf = pd.read_csv('data/all.csv')\ndf = df.drop(columns=['s', 'p', 't', 'v'])\ndf = df.dropna()\ndf = df.set_index('t')\ndf = df.sort_index()\n\ndf.to_csv('data/all.csv')\n\ndf = pd.read_csv('data/all.csv')\ndf = df.drop(columns=['s', 'p', 't', 'v'])\ndf = df.dropna()\ndf = df.set_index('t')\ndf = df.sort_index()\n\ndf.to_csv('data/all.csv')\n\ndf = pd.read_csv('data/all.csv')\ndf = df.drop(columns=['s', 'p', 't', 'v'])\ndf = df.dropna()\ndf = df.set_index('t')\ndf = df.sort_index()\n\ndf.to_csv('data/all.csv')\n\ndf = pd.read_csv('data/all.csv')\ndf = df.drop(columns=['s', 'p', 't', 'v'])\ndf = df.dropna()\ndf",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program using the 'datasets' API to load the SNLI dataset and count the number of unique genres in the training set. The program should load the SNLI dataset, access the training genres, and count the unique genre labels.\n\n### Code:"
    },
    {
        "api": "datasets",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install datasets\nfrom datasets import load_dataset\nimport random\n\n# Load SAMSum dataset\ndataset = load_dataset(\"samsum\")\n\n# Select a random conversation\nrandom_conversation = random.choice(dataset[\"train\"][\"dialog\"])\n\nprint(\"Random Conversation:\")\nfor message in random_conversation:\n    print(message)",
        "instruction": "Develop a Python program using the 'datasets' API to load the SAMSum dataset and extract a random conversation. The program should load the SAMSum dataset, select a random conversation, and print its content.",
        "prediction": "import urllib.request\nimport json\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef get_data(url):\n    with urllib.request.urlopen(url) as url:\n        data = json.loads(url.read().decode())\n    return data\n\ndef get_random_conversation(data):\n    conversations = data['conversations']\n    conversation_id = np.random.choice(conversations)\n    conversation = data['conversations'][conversation_id]\n    return conversation\n\ndef get_random_conversation_data(data):\n    conversation = get_random_conversation(data)\n    conversation_data = []\n    for message in conversation['messages']:\n        conversation_data.append(message['content'])\n    return conversation_data\n\ndef get_random_conversation_data_df(data):\n    conversation_data = get_random_conversation_data(data)\n    conversation_data_df = pd.DataFrame(conversation_data)\n    return conversation_data_df\n\ndef get_random_conversation_data_df_with_labels(data):\n    conversation_data = get_random_conversation_data(data)\n    conversation_data_df = pd.DataFrame(conversation_data)\n    conversation_data_df['labels'] = conversation_data_df['content'].apply(lambda x: x.split('\\t')[0])\n    return conversation_data_df\n\ndef get_random_conversation_data_df_with_labels_and_labels_encoded(data):\n    conversation_data = get_random_conversation_data(data)\n    conversation_data_df = pd.DataFrame(conversation_data)\n    conversation_data_df['labels'] = conversation_data_df['content'].apply(lambda x: x.split('\\t')[0])\n    conversation_data_df['labels_encoded'] = conversation_data_df['labels'].apply(lambda x: 1 if x == 'positive' else 0)\n    return conversation_data_df\n\ndef get_random_conversation_data_df_with_labels_and_labels_encoded_and_labels_encoded_one_hot(data):\n    conversation_data = get_random_conversation_data(data)\n    conversation_data_df = pd.DataFrame(conversation_data)\n    conversation_data_df['labels'] = conversation_data_df['content'].apply(lambda x: x.split('\\t')[0])\n    conversation_data_df['labels_encoded'] = conversation_data_df['labels'].apply(lambda x: 1 if x == 'positive' else 0)\n    conversation_data_df['labels_encoded_one_hot'] = conversation_data_df['labels_encoded'].apply(lambda x: [1 if x == i else 0 for i in range(2)])\n    return conversation_data_df\n\ndef get_random_conversation_data_df_with_labels_and_labels_encoded_and_labels_encoded_one_hot_and_labels_encoded_one_hot_one_hot(data):\n    conversation_data = get_random_conversation_data(data)\n    conversation_data_df = pd.DataFrame(conversation_data)\n    conversation_data_df['labels'] = conversation_data_df['content'].apply(lambda x: x.split('\\t')[0])\n    conversation_data_df['labels_encoded'] = conversation_data_df['labels'].apply(lambda x: 1 if x == 'positive' else 0)\n    conversation_data_df['labels_encoded_one_hot'] = conversation_data_df['labels_encoded'].apply(lambda x: [1 if x == i else 0 for i in range(2)])\n    conversation_data_df",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program using the 'datasets' API to load the SAMSum dataset and extract a random conversation. The program should load the SAMSum dataset, select a random conversation, and print its content.\n\n### Code:"
    },
    {
        "api": "datasets",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install datasets\nfrom datasets import load_dataset\nimport random\n\n# Load WikiText-103 dataset\ndataset = load_dataset(\"wikitext\", \"103\")\n\n# Select a random article\nrandom_article = random.choice(dataset[\"train\"][\"text\"])\n\nprint(\"Random Article:\")\nprint(random_article)",
        "instruction": "Develop a Python program using the 'datasets' API to load the WikiText-103 dataset and extract a random article. The program should load the WikiText-103 dataset, select a random article, and print its content.",
        "prediction": "import urllib.request\nimport json\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nurl = 'https://api.datasets.imdb.com/data/title.basics.tsv.gz'\nresponse = urllib.request.urlopen(url)\ndata = response.read()\ndata = data.decode('utf-8')\ndata = data.split('\\n')\n\ndf = pd.DataFrame(columns=['tconst', 'titleType', 'primaryTitle','startYear', 'genres'])\n\nfor i in range(len(data)):\n    if i == 0:\n        continue\n    else:\n        row = data[i].split('\\t')\n        df.loc[i-1] = [row[0], row[1], row[2], row[3], row[5]]\n\ndf.to_csv('title.basics.tsv', sep='\\t', index=False)\n\ndf = pd.read_csv('title.basics.tsv', sep='\\t')\ndf = df.drop(columns=['tconst', 'titleType', 'primaryTitle','startYear', 'genres'])\ndf = df.drop_duplicates()\ndf = df.dropna()\ndf = df.reset_index(drop=True)\n\ndf['genres'] = df['genres'].str.split(',')\n\ndf['genres'] = df['genres'].apply(lambda x: [i.strip() for i in x])\n\ndf['genres'] = df['genres'].apply(lambda x: [i.strip() for i in x if i.strip()!= '\\\\N'])\n\ndf['genres'] = df['genres'].apply(lambda x: [i.strip() for i in x if i.strip()!= '\\\\N'])\n\ndf['genres'] = df['genres'].apply(lambda x: [i.strip() for i in x if i.strip()!= '\\\\N'])\n\ndf['genres'] = df['genres'].apply(lambda x: [i.strip() for i in x if i.strip()!= '\\\\N'])\n\ndf['genres'] = df['genres'].apply(lambda x: [i.strip() for i in x if i.strip()!= '\\\\N'])\n\ndf['genres'] = df['genres'].apply(lambda x: [i.strip() for i in x if i.strip()!= '\\\\N'])\n\ndf['genres'] = df['genres'].apply(lambda x: [i.strip() for i in x if i.strip()!= '\\\\N'])\n\ndf['genres'] = df['genres'].apply(lambda x: [i.strip() for i in x if i.strip()!= '\\\\N'])\n\ndf['genres'] = df['genres'].apply(lambda x: [i.strip() for i in x if i.strip()!= '\\\\N'])\n\ndf['genres'] = df['genres'].apply(lambda x: [i.strip() for i in x if i.strip()!= '\\\\N'])\n\ndf['genres'] = df['genres'].apply(lambda x: [i.strip() for i in x if i.strip()!= '\\\\N'])\n\ndf['genres'] = df['genres'].apply(lambda x: [i.strip() for i in x if i.strip()!= '\\\\N'])\n\ndf['genres'] = df['genres'].apply(lambda x: [i.strip() for i in x if i.strip()!= '\\\\N'])\n\ndf['genres'] = df['genres'].apply(lambda x: [i.strip() for i in x if i.strip()!= '\\\\N'])\n\ndf['genres'] = df['genres'].apply(lambda x: [i.strip() for i in x if i.strip()!= '\\\\N'])\n\ndf['genres'] = df['genres'].apply(lambda x: [i.strip() for i in x if i.strip()!= '\\\\N'])\n\ndf['genres'] = df['genres'].apply(lambda x: [i.strip() for i in x",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program using the 'datasets' API to load the WikiText-103 dataset and extract a random article. The program should load the WikiText-103 dataset, select a random article, and print its content.\n\n### Code:"
    },
    {
        "api": "datasets",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install datasets\nfrom datasets import load_dataset\nimport random\n\n# Load TED MultiTranslation dataset\ndataset = load_dataset(\"ted_multi\")\n\n# Select a random translated speech\nrandom_speech = random.choice(dataset[\"train\"][\"translation\"])\n\nsource_language = random_speech[\"en\"]\ntarget_language = random_speech[\"fr\"]\n\nprint(f\"Source Transcript (English): {source_language}\")\nprint(f\"Target Transcript (French): {target_language}\")",
        "instruction": "Develop a Python program using the 'datasets' API to load the TED MultiTranslation dataset and retrieve a random translated speech. The program should load the TED MultiTranslation dataset, select a random translated speech, and print the source and target language transcripts.",
        "prediction": "import json\nimport urllib.request\nimport datetime\nimport random\nimport string\nimport os\nimport requests\n\ndef load_data():\n    with open('data.json') as json_file:\n        data = json.load(json_file)\n    return data\n\ndef get_random_string(length):\n    letters = string.ascii_lowercase\n    result_str = ''.join(random.choice(letters) for i in range(length))\n    return result_str\n\ndef get_random_number(length):\n    numbers = string.digits\n    result_str = ''.join(random.choice(numbers) for i in range(length))\n    return result_str\n\ndef get_random_datetime():\n    start_date = datetime.datetime(2020, 1, 1, 0, 0, 0)\n    end_date = datetime.datetime(2020, 1, 1, 0, 0, 0) + datetime.timedelta(days=1)\n    time_between_dates = end_date - start_date\n    days_between_dates = time_between_dates.days\n    random_number_of_days = random.randrange(days_between_dates)\n    random_date = start_date + datetime.timedelta(days=random_number_of_days)\n    return random_date\n\ndef get_random_datetime_string():\n    start_date = datetime.datetime(2020, 1, 1, 0, 0, 0)\n    end_date = datetime.datetime(2020, 1, 1, 0, 0, 0) + datetime.timedelta(days=1)\n    time_between_dates = end_date - start_date\n    days_between_dates = time_between_dates.days\n    random_number_of_days = random.randrange(days_between_dates)\n    random_date = start_date + datetime.timedelta(days=random_number_of_days)\n    random_date_string = random_date.strftime(\"%Y-%m-%d\")\n    return random_date_string\n\ndef get_random_datetime_string_with_timezone():\n    start_date = datetime.datetime(2020, 1, 1, 0, 0, 0)\n    end_date = datetime.datetime(2020, 1, 1, 0, 0, 0) + datetime.timedelta(days=1)\n    time_between_dates = end_date - start_date\n    days_between_dates = time_between_dates.days\n    random_number_of_days = random.randrange(days_between_dates)\n    random_date = start_date + datetime.timedelta(days=random_number_of_days)\n    random_date_string = random_date.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    return random_date_string\n\ndef get_random_datetime_string_with_timezone_and_milliseconds():\n    start_date = datetime.datetime(2020, 1, 1, 0, 0, 0)\n    end_date = datetime.datetime(2020, 1, 1, 0, 0, 0) + datetime.timedelta(days=1)\n    time_between_dates = end_date - start_date\n    days_between_dates = time_between_dates.days\n    random_number_of_days = random.randrange(days_between_dates)\n    random_date = start_date + datetime.timedelta(days=random_number_of_days)\n    random_date_string = random_date.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n    return random_date_string\n\ndef get_random_datetime_string_with_timezone_and_milliseconds_and_microseconds():\n    start_date = datetime.datetime(2020, 1, 1, 0, 0, 0)\n    end_date = datetime.datetime(2020, 1, 1, 0, 0, 0) + datetime.timedelta(days=1)\n    time_between_dates = end_date - start_date\n    days_between_dates = time_between_dates.days\n    random",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program using the 'datasets' API to load the TED MultiTranslation dataset and retrieve a random translated speech. The program should load the TED MultiTranslation dataset, select a random translated speech, and print the source and target language transcripts.\n\n### Code:"
    },
    {
        "api": "diffusers",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install diffusers\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-3-diffusers\",\n    use_auth_token=True,\n    revision=\"fp16\",\n    torch_dtype=torch.float16\n)\npipe = pipe.to(\"cpu\")\n\nprompt = \"What are the benefits of reading books?\"\ncompletions = pipe(prompt)[\"sample\"][0]\n\nprint(completions)",
        "instruction": "Write a Python program that utilizes the 'diffusers' API to generate a list of possible completions for the provided prompt, 'What are the benefits of reading books?'",
        "prediction": "import diffusers\nimport diffusers.diffusers as diffusers\nimport diffusers.diffusers_api as diffusers_api\nimport diffusers.diffusers_api_client as diffusers_api_client\nimport diffusers.diffusers_api_client_async as diffusers_api_client_async\nimport diffusers.diffusers_api_client_async_asyncio as diffusers_api_client_async_asyncio\nimport diffusers.diffusers_api_client_async_aiohttp as diffusers_api_client_async_aiohttp\nimport diffusers.diffusers_api_client_async_aiohttp_asyncio as diffusers_api_client_async_aiohttp_asyncio\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp as diffusers_api_client_async_aiohttp_aiohttp\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_asyncio as diffusers_api_client_async_aiohttp_aiohttp_asyncio\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_aiohttp as diffusers_api_client_async_aiohttp_aiohttp_aiohttp\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_aiohttp_asyncio as diffusers_api_client_async_aiohttp_aiohttp_aiohttp_asyncio\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp as diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_asyncio as diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_asyncio\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp as diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_asyncio as diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_asyncio\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp as diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_asyncio as diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_asyncio\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp as diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_asyncio as diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_asyncio\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp as diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aio",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nWrite a Python program that utilizes the 'diffusers' API to generate a list of possible completions for the provided prompt, 'What are the benefits of reading books?'\n\n### Code:"
    },
    {
        "api": "diffusers",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install diffusers\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-3-diffusers\",\n    use_auth_token=True,\n    revision=\"fp16\",\n    torch_dtype=torch.float16\n)\npipe = pipe.to(\"cpu\")\n\nprompt = \"What are the benefits of regular exercise?\"\ncompletions = pipe(prompt)[\"sample\"][0]\n\nprint(completions)",
        "instruction": "Write a Python program that utilizes the 'diffusers' API to generate a list of possible completions for the provided prompt, 'What are the benefits of regular exercise?'",
        "prediction": "import diffusers\nimport diffusers.diffusers as diffusers\nimport diffusers.diffusers_api as diffusers_api\nimport diffusers.diffusers_api_client as diffusers_api_client\nimport diffusers.diffusers_api_client_async as diffusers_api_client_async\nimport diffusers.diffusers_api_client_async_asyncio as diffusers_api_client_async_asyncio\nimport diffusers.diffusers_api_client_async_aiohttp as diffusers_api_client_async_aiohttp\nimport diffusers.diffusers_api_client_async_aiohttp_asyncio as diffusers_api_client_async_aiohttp_asyncio\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp as diffusers_api_client_async_aiohttp_aiohttp\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_asyncio as diffusers_api_client_async_aiohttp_aiohttp_asyncio\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_aiohttp as diffusers_api_client_async_aiohttp_aiohttp_aiohttp\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_aiohttp_asyncio as diffusers_api_client_async_aiohttp_aiohttp_aiohttp_asyncio\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp as diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_asyncio as diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_asyncio\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp as diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_asyncio as diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_asyncio\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp as diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_asyncio as diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_asyncio\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp as diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_asyncio as diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_asyncio\nimport diffusers.diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp as diffusers_api_client_async_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aiohttp_aio",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nWrite a Python program that utilizes the 'diffusers' API to generate a list of possible completions for the provided prompt, 'What are the benefits of regular exercise?'\n\n### Code:"
    },
    {
        "api": "emoji",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install emoji\nimport emoji\n\ndef has_emoji_code(txt, codes):\n  for char in txt:\n    for code in codes:\n      if emoji.demojize(char) == code:\n        return True\n  return False\n\ntxts = [\"I am so happy \ud83d\ude00\", \n          \"Python is :thumbs_up:\", \n          \"Funny \ud83d\ude02\", \n          \"I liked it :red_heart:\",\n          \"I am so happy\",\n          \"This is a normal text\"]\n\ncodes_to_check = [\":thumbs_up:\", \":red_heart:\", \":smile:\"]\n\nfor txt in txts:\n  print(has_emoji_code(txt, codes_to_check))\n",
        "instruction": "Create a Python program that uses the 'emoji' API to check if a given text contains any specific emoji codes. The program should take a text and a list of emoji codes as input and return True if the text contains any emojis that match the specified codes, and False otherwise. Run the program on a list of sample texts containing emojis and non-emojis, and specify a list of emoji codes to check for each text.",
        "prediction": "import urllib.request\nimport json\nimport emoji\n\ndef check_emoji(text):\n    for character in text:\n        if character in emoji.UNICODE_EMOJI:\n            return True\n    return False\n\ndef check_emoji_list(text):\n    for character in text:\n        if character in emoji.UNICODE_EMOJI_ALIAS:\n            return True\n    return False\n\ndef check_emoji_list_2(text):\n    for character in text:\n        if character in emoji.UNICODE_EMOJI_ALIAS_UNICODE:\n            return True\n    return False\n\ndef check_emoji_list_3(text):\n    for character in text:\n        if character in emoji.UNICODE_EMOJI_ALIAS_UNICODE_ENGLISH:\n            return True\n    return False\n\ndef check_emoji_list_4(text):\n    for character in text:\n        if character in emoji.UNICODE_EMOJI_ALIAS_UNICODE_ENGLISH_NAME_TO_EMOJI:\n            return True\n    return False\n\ndef check_emoji_list_5(text):\n    for character in text:\n        if character in emoji.UNICODE_EMOJI_ALIAS_UNICODE_ENGLISH_NAME_TO_EMOJI_ALIAS:\n            return True\n    return False\n\ndef check_emoji_list_6(text):\n    for character in text:\n        if character in emoji.UNICODE_EMOJI_ALIAS_UNICODE_ENGLISH_NAME_TO_EMOJI_ALIAS_UNICODE:\n            return True\n    return False\n\ndef check_emoji_list_7(text):\n    for character in text:\n        if character in emoji.UNICODE_EMOJI_ALIAS_UNICODE_ENGLISH_NAME_TO_EMOJI_ALIAS_UNICODE_ENGLISH:\n            return True\n    return False\n\ndef check_emoji_list_8(text):\n    for character in text:\n        if character in emoji.UNICODE_EMOJI_ALIAS_UNICODE_ENGLISH_NAME_TO_EMOJI_ALIAS_UNICODE_ENGLISH_SYMBOLS:\n            return True\n    return False\n\ndef check_emoji_list_9(text):\n    for character in text:\n        if character in emoji.UNICODE_EMOJI_ALIAS_UNICODE_ENGLISH_NAME_TO_EMOJI_ALIAS_UNICODE_ENGLISH_SYMBOLS_NUMBERS:\n            return True\n    return False\n\ndef check_emoji_list_10(text):\n    for character in text:\n        if character in emoji.UNICODE_EMOJI_ALIAS_UNICODE_ENGLISH_NAME_TO_EMOJI_ALIAS_UNICODE_ENGLISH_SYMBOLS_NUMBERS_SYMBOLS:\n            return True\n    return False\n\ndef check_emoji_list_11(text):\n    for character in text:\n        if character in emoji.UNICODE_EMOJI_ALIAS_UNICODE_ENGLISH_NAME_TO_EMOJI_ALIAS_UNICODE_ENGLISH_SYMBOLS_NUMBERS_SYMBOLS_CIRCLES:\n            return True\n    return False\n\ndef check_emoji_list_12(text):\n    for character in text:\n        if character in emoji.UNICODE_EMOJI_ALIAS_UNICODE_ENGLISH_NAME_TO_EMOJI_ALIAS_UNICODE_ENGLISH_SYMBOLS_NUMBERS_SYMBOLS_CIRCLES_SYMBOLS:\n            return True\n    return False\n\ndef check_emoji_list_13(text):\n    for character in text:\n        if character in emoji.UNICODE_EMOJI_ALIAS_UNICODE_ENGLISH_NAME_TO_EMOJI_ALIAS_UNICODE_ENGLISH_SYMBOLS_NUMBERS_SYMBOLS_CIRCL",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that uses the 'emoji' API to check if a given text contains any specific emoji codes. The program should take a text and a list of emoji codes as input and return True if the text contains any emojis that match the specified codes, and False otherwise. Run the program on a list of sample texts containing emojis and non-emojis, and specify a list of emoji codes to check for each text.\n\n### Code:"
    },
    {
        "api": "emoji",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install emoji\nimport emoji\n\ndef has_emoji_name(txt, names):\n  for char in txt:\n    for name in names:\n      if emoji.demojize(char).lower().find(name.lower()) != -1:\n        return True\n  return False\n\ntxts = [\"I am so happy \ud83d\ude00\", \n          \"Python is :thumbs_up:\", \n          \"Funny \ud83d\ude02\", \n          \"I liked it :red_heart:\",\n          \"I am so happy\",\n          \"This is a normal text\"]\n\nnames_to_check = [\"happy\", \"thumbs\", \"red\"]\n\nfor txt in txts:\n  print(has_emoji_name(txt, names_to_check))\n",
        "instruction": "Create a Python program that uses the 'emoji' API to check if a given text contains any specific emoji names. The program should take a text and a list of emoji names as input and return True if the text contains any emojis that match the specified names, and False otherwise. Run the program on a list of sample texts containing emojis and non-emojis, and specify a list of emoji names to check for each text.",
        "prediction": "import urllib.request\nimport urllib.parse\nimport urllib.error\nimport json\nimport ssl\n\napi_key = False\n# If you have a Google Places API key, enter it here\n# api_key = 'AIzaSy___IDByT70'\n# https://developers.google.com/maps/documentation/geocoding/intro\n\nif api_key is False:\n    api_key = 42\n    serviceurl = 'http://py4e-data.dr-chuck.net/json?'\nelse :\n    serviceurl = 'https://maps.googleapis.com/maps/api/geocode/json?'\n\n# Ignore SSL certificate errors\nctx = ssl.create_default_context()\nctx.check_hostname = False\nctx.verify_mode = ssl.CERT_NONE\n\nwhile True:\n    address = input('Enter location: ')\n    if len(address) < 1: break\n\n    parms = dict()\n    parms['address'] = address\n    if api_key is not False: parms['key'] = api_key\n    url = serviceurl + urllib.parse.urlencode(parms)\n\n    print('Retrieving', url)\n    uh = urllib.request.urlopen(url, context=ctx)\n    data = uh.read().decode()\n    print('Retrieved', len(data), 'characters')\n\n    try:\n        js = json.loads(data)\n    except:\n        js = None\n\n    if not js or'status' not in js or js['status']!= 'OK':\n        print('==== Failure To Retrieve ====')\n        print(data)\n        continue\n\n    print(json.dumps(js, indent=4))\n\n    lat = js['results'][0]['geometry']['location']['lat']\n    lng = js['results'][0]['geometry']['location']['lng']\n    print('lat', lat, 'lng', lng)\n    location = js['results'][0]['formatted_address']\n    print(location)\n    ",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that uses the 'emoji' API to check if a given text contains any specific emoji names. The program should take a text and a list of emoji names as input and return True if the text contains any emojis that match the specified names, and False otherwise. Run the program on a list of sample texts containing emojis and non-emojis, and specify a list of emoji names to check for each text.\n\n### Code:"
    },
    {
        "api": "evaluate",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install datasets\n#!pip install evaluate\nfrom datasets import load_dataset\nfrom evaluate import evaluator\n\ndata = load_dataset(\"imdb\", split=\"test\").shuffle(seed=42).select(range(1000))\ntask_evaluator = evaluator(\"text-classification\")\n\n# Pass a model name or path\neval_results = task_evaluator.compute(\n    model_or_pipeline=\"lvwerra/distilbert-imdb\",\n    data=data,\n    label_mapping={\"NEGATIVE\": 0, \"POSITIVE\": 1}\n)\n\nprint(eval_results)",
        "instruction": "Create a Python program that uses the 'evaluate' API to perform sentiment analysis on a dataset. The program should load the IMDb dataset, shuffle it, select the first 1000 examples, and then use the 'lvwerra/distilbert-imdb' model to classify the text as either 'NEGATIVE' or 'POSITIVE.' The results should be printed.",
        "prediction": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\n\ndataset = pd.read_csv('../data/Social_Network_Ads.csv')\nX = dataset.iloc[:, [2, 3]].values\ny = dataset.iloc[:, 4].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\n\nclassifier = LogisticRegression(random_state=0)\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\n\nfrom matplotlib.colors import ListedColormap\n\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),\n                     np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha=0.75, cmap=ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c=ListedColormap(('red', 'green'))(i), label=j)\nplt.title('Logistic Regression (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\nfrom matplotlib.colors import ListedColormap\n\nX_set, y_set = X_test, y_test\nX1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),\n                     np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha=0.75, cmap=ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c=ListedColormap(('red', 'green'))(i), label=j)\nplt.title('Logistic Regression (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that uses the 'evaluate' API to perform sentiment analysis on a dataset. The program should load the IMDb dataset, shuffle it, select the first 1000 examples, and then use the 'lvwerra/distilbert-imdb' model to classify the text as either 'NEGATIVE' or 'POSITIVE.' The results should be printed.\n\n### Code:"
    },
    {
        "api": "evaluate",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install evaluate\nfrom evaluate import evaluator\n\n# Load the text summarization model\nsummarization_model = \"facebook/bart-large-cnn\"\n\ndef perform_summarization(text):\n    summarization_evaluator = evaluator(\"text-summarization\")\n\n    # Use the model for text summarization\n    summary = summarization_evaluator.compute(\n        model_or_pipeline=summarization_model,\n        data=text\n    )\n\n    return summary\n\n# User input\nuser_text = input(\"Enter the text for summarization: \")\nresult = perform_summarization(user_text)\n\nprint(\"Summarized Text:\")\nprint(result[0][\"summary_text\"])\n",
        "instruction": "Create a Python program that uses the 'evaluate' API to perform text summarization on a given text. The program should take a text as input, use the 'facebook/bart-large-cnn' model for text summarization, and print the summarized text.",
        "prediction": "import requests\nimport json\nimport sys\nimport os\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))\n\nfrom config import Config\n\ndef get_text_summary(text):\n    url = 'https://api.deepai.org/api/summarization'\n    payload = {\n        'text': text,\n        'api_key': Config.DEEPAI_API_KEY\n    }\n    result = requests.post(url, data=payload)\n    return result.json()['output']",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that uses the 'evaluate' API to perform text summarization on a given text. The program should take a text as input, use the 'facebook/bart-large-cnn' model for text summarization, and print the summarized text.\n\n### Code:"
    },
    {
        "api": "evaluate",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install evaluate\nfrom evaluate import evaluator\n\n# Load the text summarization model\nsummarization_model = \"facebook/bart-large-cnn\"\n\ndef perform_summarization(text):\n    summarization_evaluator = evaluator(\"text-summarization\")\n\n    # Use the model for text summarization\n    summary = summarization_evaluator.compute(\n        model_or_pipeline=summarization_model,\n        data=text\n    )\n\n    return summary\n\n# User input\nuser_text = input(\"Enter the text for summarization: \")\nresult = perform_summarization(user_text)\n\nprint(\"Summarized Text:\")\nprint(result[0][\"summary_text\"])\n",
        "instruction": "Create a Python program that uses the 'evaluate' API to perform text summarization on a given text. The program should allow the user to input a text, then use the 'facebook/bart-large-cnn' model for text summarization and print the summarized text.",
        "prediction": "import requests\nimport json\nimport sys\nimport os\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))\n\nfrom config import Config\n\ndef get_text_summary(text):\n    url = 'https://api.deepai.org/api/summarization'\n    payload = {\n        'text': text,\n        'api_key': Config.DEEPAI_API_KEY\n    }\n    result = requests.post(url, data=payload)\n    return result.json()['output']",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that uses the 'evaluate' API to perform text summarization on a given text. The program should allow the user to input a text, then use the 'facebook/bart-large-cnn' model for text summarization and print the summarized text.\n\n### Code:"
    },
    {
        "api": "gymnasium",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install ale_py gymnasium\n#!pip install swig\n#!pip install gymnasium[box2d]\n#!pip install gymnasium[classic-control]\nimport gymnasium as gym\nfrom gymnasium.utils import play\n\nenv = gym.make(\"Taxi-v3\")\n\nmapping = {\"w\": 0,\n            \"a\": 3,\n            \"s\": 2,\n            \"d\": 1}\n\ndefault_action = 0\nplay.play(env, keys_to_action=mapping, noop=default_action)",
        "instruction": "Develop a Python program using the 'gymnasium' API to play the 'Taxi-v3' environment. The program should control the taxi within the environment using keyboard inputs (W, A, S, D) to move up, left, down, and right, respectively.",
        "prediction": "import gym\nimport gym_taxi\nimport gym_taxi.envs\nimport gym_taxi.envs.taxi_env\nimport gym_taxi.envs.taxi_env_v3\nimport gym_taxi.envs.taxi_env_v4\nimport gym_taxi.envs.taxi_env_v5\nimport gym_taxi.envs.taxi_env_v6\nimport gym_taxi.envs.taxi_env_v7\nimport gym_taxi.envs.taxi_env_v8\nimport gym_taxi.envs.taxi_env_v9\nimport gym_taxi.envs.taxi_env_v10\nimport gym_taxi.envs.taxi_env_v11\nimport gym_taxi.envs.taxi_env_v12\nimport gym_taxi.envs.taxi_env_v13\nimport gym_taxi.envs.taxi_env_v14\nimport gym_taxi.envs.taxi_env_v15\nimport gym_taxi.envs.taxi_env_v16\nimport gym_taxi.envs.taxi_env_v17\nimport gym_taxi.envs.taxi_env_v18\nimport gym_taxi.envs.taxi_env_v19\nimport gym_taxi.envs.taxi_env_v20\nimport gym_taxi.envs.taxi_env_v21\nimport gym_taxi.envs.taxi_env_v22\nimport gym_taxi.envs.taxi_env_v23\nimport gym_taxi.envs.taxi_env_v24\nimport gym_taxi.envs.taxi_env_v25\nimport gym_taxi.envs.taxi_env_v26\nimport gym_taxi.envs.taxi_env_v27\nimport gym_taxi.envs.taxi_env_v28\nimport gym_taxi.envs.taxi_env_v29\nimport gym_taxi.envs.taxi_env_v30\nimport gym_taxi.envs.taxi_env_v31\nimport gym_taxi.envs.taxi_env_v32\nimport gym_taxi.envs.taxi_env_v33\nimport gym_taxi.envs.taxi_env_v34\nimport gym_taxi.envs.taxi_env_v35\nimport gym_taxi.envs.taxi_env_v36\nimport gym_taxi.envs.taxi_env_v37\nimport gym_taxi.envs.taxi_env_v38\nimport gym_taxi.envs.taxi_env_v39\nimport gym_taxi.envs.taxi_env_v40\nimport gym_taxi.envs.taxi_env_v41\nimport gym_taxi.envs.taxi_env_v42\nimport gym_taxi.envs.taxi_env_v43\nimport gym_taxi.envs.taxi_env_v44\nimport gym_taxi.envs.taxi_env_v45\nimport gym_taxi.envs.taxi_env_v46\nimport gym_taxi.envs.taxi_env_v47\nimport gym_taxi.envs.taxi_env_v48\nimport gym_taxi.envs.taxi_env_v49\nimport gym_taxi.envs.taxi_env_v50\nimport gym_taxi.envs.taxi_env_v51\nimport gym_taxi.envs.taxi_env_v52\nimport gym_taxi.envs.taxi_env_v53\nimport gym_taxi.envs.taxi_env_v54\nimport gym_taxi.envs.taxi_env_v55\nimport gym_taxi.envs.taxi_env_v56\nimport gym_taxi.envs.taxi_env_v57\nimport gym_taxi.envs.taxi_env_v58\nimport gym_taxi.envs.taxi_env_v59\nimport gym_taxi.envs.taxi_env_v60\nimport gym_taxi.",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program using the 'gymnasium' API to play the 'Taxi-v3' environment. The program should control the taxi within the environment using keyboard inputs (W, A, S, D) to move up, left, down, and right, respectively.\n\n### Code:"
    },
    {
        "api": "gymnasium",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install ale_py gymnasium\n#!pip install swig\n#!pip install gymnasium[box2d]\n#!pip install gymnasium[classic-control]\nimport gymnasium as gym\n\nenv = gym.make(\"RoboschoolInvertedPendulum-v1\")\n\ndone = False\nwhile not done:\n    action = env.action_space.sample()\n    observation, reward, done, info = env.step(action)\n    print(\"Action:\", action)\n    print(\"Observation:\", observation)\n    print(\"Reward:\", reward)\n    print(\"Done:\", done)\n    print(\"Info:\", info)",
        "instruction": "Develop a Python program using the 'gymnasium' API to play the 'RoboschoolInvertedPendulum-v1' environment. The program should control the inverted pendulum within the environment using continuous actions to apply torque to the joint.",
        "prediction": "gymnasium = Gymnasium(gymnasium_id=gymnasium_id, gymnasium_name=gymnasium_name, gymnasium_address=gymnasium_address, gymnasium_contact=gymnasium_contact, gymnasium_capacity=gymnasium_capacity, gymnasium_status=gymnasium_status)\n#         gymnasium.save()\n#         return gymnasium\n#\n#     def update_gymnasium(self, gymnasium_id, gymnasium_name, gymnasium_address, gymnasium_contact, gymnasium_capacity, gymnasium_status):\n#         gymnasium = Gymnasium.objects.get(gymnasium_id=gymnasium_id)\n#         gymnasium.gymnasium_name = gymnasium_name\n#         gymnasium.gymnasium_address = gymnasium_address\n#         gymnasium.gymnasium_contact = gymnasium_contact\n#         gymnasium.gymnasium_capacity = gymnasium_capacity\n#         gymnasium.gymnasium_status = gymnasium_status\n#         gymnasium.save()\n#         return gymnasium\n#\n#     def delete_gymnasium(self, gymnasium_id):\n#         gymnasium = Gymnasium.objects.get(gymnasium_id=gymnasium_id)\n#         gymnasium.delete()\n#         return gymnasium\n#\n#     def get_gymnasium_by_gymnasium_id(self, gymnasium_id):\n#         gymnasium = Gymnasium.objects.get(gymnasium_id=gymnasium_id)\n#         return gymnasium\n#\n#     def get_gymnasium_by_gymnasium_name(self, gymnasium_name):\n#         gymnasium = Gymnasium.objects.get(gymnasium_name=gymnasium_name)\n#         return gymnasium\n#\n#     def get_gymnasium_by_gymnasium_capacity(self, gymnasium_capacity):\n#         gymnasium = Gymnasium.objects.get(gymnasium_capacity=gymnasium_capacity)\n#         return gymnasium\n#\n#     def get_gymnasium_by_gymnasium_status(self, gymnasium_status):\n#         gymnasium = Gymnasium.objects.get(gymnasium_status=gymnasium_status)\n#         return gymnasium\n#\n#     def get_gymnasium_by_gymnasium_capacity_and_gymnasium_status(self, gymnasium_capacity, gymnasium_status):\n#         gymnasium = Gymnasium.objects.filter(gymnasium_capacity=gymnasium_capacity, gymnasium_status=gymnasium_status)\n#         return gymnasium\n#\n#     def get_gymnasium_by_gymnasium_capacity_and_gymnasium_status_and_gymnasium_name(self, gymnasium_capacity, gymnasium_status, gymnasium_name):\n#         gymnasium = Gymnasium.objects.filter(gymnasium_capacity=gymnasium_capacity, gymnasium_status=gymnasium_status, gymnasium_name=gymnasium_name)\n#         return gymnasium\n#\n#     def get_gymnasium_by_gymnasium_capacity_and_gymnasium_status_and_gymnasium_name_and_gymnasium_address(self, gymnasium_capacity, gymnasium_status, gymnasium_name, gymnasium_address):\n#         gymnasium = Gymnasium.objects.filter(gymnasium_capacity=gymnasium_capacity, gymnasium_status=gymnasium_status, gymnasium_name=gymnasium_name, gymnasium_address=gymnasium_address)\n#         return gymnasium\n#\n#     def get_gymnasium_by_gy",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program using the 'gymnasium' API to play the 'RoboschoolInvertedPendulum-v1' environment. The program should control the inverted pendulum within the environment using continuous actions to apply torque to the joint.\n\n### Code:"
    },
    {
        "api": "gymnasium",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install ale_py gymnasium\n#!pip install swig\n#!pip install gymnasium[box2d]\n#!pip install gymnasium[classic-control]\nimport gymnasium as gym\nfrom gymnasium.utils import play\n\nenv = gym.make(\"FrozenLake-v0\")\n\nmapping = {\"w\": 0,\n            \"a\": 3,\n            \"s\": 2,\n            \"d\": 1}\n\ndefault_action = 0\nplay.play(env, keys_to_action=mapping, noop=default_action)",
        "instruction": "Develop a Python program using the 'gymnasium' API to play the 'FrozenLake-v0' environment. The program should control the agent within the environment using keyboard inputs (W, A, S, D) to move up, left, down, and right, respectively.",
        "prediction": "gymnasium.gymnasium_id\nINNER JOIN gymnasium_gymnasium_gymnasium_gymnasium_id\nON gymnasium_gymnasium_gymnasium_id.gymnasium_gymnasium_id = gymnasium_gymnasium_gymnasium_id.gymnasium_gymnasium_id\nINNER JOIN gymnasium_gymnasium_gymnasium_gymnasium_id\nON gymnasium_gymnasium_gymnasium_id.gymnasium_gymnasium_id = gymnasium_gymnasium_gymnasium_id.gymnasium_gymnasium_id\nINNER JOIN gymnasium_gymnasium_gymnasium_gymnasium_id\nON gymnasium_gymnasium_gymnasium_id.gymnasium_gymnasium_id = gymnasium_gymnasium_gymnasium_id.gymnasium_gymnasium_id\nINNER JOIN gymnasium_gymnasium_gymnasium_gymnasium_id\nON gymnasium_gymnasium_gymnasium_id.gymnasium_gymnasium_id = gymnasium_gymnasium_gymnasium_id.gymnasium_gymnasium_id\nINNER JOIN gymnasium_gymnasium_gymnasium_gymnasium_id\nON gymnasium_gymnasium_gymnasium_id.gymnasium_gymnasium_id = gymnasium_gymnasium_gymnasium_id.gymnasium_gymnasium_id\nINNER JOIN gymnasium_gymnasium_gymnasium_gymnasium_id\nON gymnasium_gymnasium_id.gymnasium_gymnasium_id = gymnasium_gymnasium_gymnasium_id.gymnasium_gymnasium_id\nINNER JOIN gymnasium_gymnasium_gymnasium_gymnasium_id\nON gymnasium_gymnasium_id.gymnasium_gymnasium_id = gymnasium_gymnasium_gymnasium_id.gymnasium_gymnasium_id\nINNER JOIN gymnasium_gymnasium_gymnasium_gymnasium_id\nON gymnasium_gymnasium_id.gymnasium_gymnasium_id = gymnasium_gymnasium_gymnasium_id.gymnasium_gymnasium_id\nINNER JOIN gymnasium_gymnasium_gymnasium_gymnasium_id\nON gymnasium_gymnasium_id.gymnasium_gymnasium_id = gymnasium_gymnasium_gymnasium_id.gymnasium_gymnasium_id\nINNER JOIN gymnasium_gymnasium_gymnasium_gymnasium_id\nON gymnasium_gymnasium_id.gymnasium_gymnasium_id = gymnasium_gymnasium_gymnasium_id.gymnasium_gymnasium_id\nINNER JOIN gymnasium_gymnasium_gymnasium_gymnasium_id\nON gymnasium_gymnasium_id.gymnasium_gymnasium_id = gymnasium_gymnasium_gymnasium_id.gymnasium_gymnasium_id\nINNER JOIN gymnasium_gymnasium_gymnasium_gymnasium_id\nON gymnasium_gymnasium_id.gymnasium_gymnasium_id = gymnasium_gymnasium_gymnasium_id.gymnasium_gymnasium_id\nINNER JOIN gymnasium_gymnasium_gymnasium_gymnasium_id\nON gymnasium_gymnasium_id.gymnasium_gymnasium_id = gymnasium_gymnasium_gymnasium_",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program using the 'gymnasium' API to play the 'FrozenLake-v0' environment. The program should control the agent within the environment using keyboard inputs (W, A, S, D) to move up, left, down, and right, respectively.\n\n### Code:"
    },
    {
        "api": "holidays",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install holidays\nimport holidays\n\n# Define the date to check\ndate_to_check = \"2023-07-04\" \n\n# Specify the country or region (e.g., 'US' for the United States)\ncountry_code = 'US'\n\n# Create a dictionary of holidays for the specified country or region\nholiday_dict = holidays.country_holidays(country_code)\n\n# Check if the date is a public holiday\nis_holiday = date_to_check in holiday_dict\n\n# Display the result\nif is_holiday:\n    print(f\"{date_to_check} is a public holiday in {country_code}.\")\nelse:\n    print(f\"{date_to_check} is not a public holiday in {country_code}.\")",
        "instruction": "Create a Python program using the 'holidays' API to check if a specified date is a public holiday in a given country or region.",
        "prediction": "import requests\nimport json\n\ndef check_public_holidays(date):\n    url = 'https://api.holidays.calendar.google.com/v1/calendars/primary/public_holidays?key=AIzaSyBJZJZ-JZ9ZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZJZ",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'holidays' API to check if a specified date is a public holiday in a given country or region.\n\n### Code:"
    },
    {
        "api": "holidays",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install holidays\nimport holidays\n\n# Specify the country or region (e.g., 'US' for the United States)\ncountry_code = 'US'\n\n# Specify the state or province code (e.g., 'CA' for California)\nstate_code = 'CA'\n\n# Specify the region (e.g., 'LA' for Los Angeles)\nregion = 'LA'\n\n# Specify the year for which you want to list holidays\nyear = 2023\n\n# Create a dictionary of holidays for the specified region and year\nholiday_dict = holidays.RegionHoliday(country_code, region, observed=True, years=year)\n\n# List all the public holidays\nholiday_list = sorted(holiday_dict.keys())\n\n# Display the list of holidays\nprint(f\"Public holidays in {region}, {state_code}, {country_code} for the year {year}:\")\nfor holiday in holiday_list:\n    print(holiday)",
        "instruction": "Create a Python program using the 'holidays' API to list all the public holidays in a specific region (e.g., a city or district) within a country and state for a given year.",
        "prediction": "import requests\nimport json\n\ndef get_holidays(country, state, year):\n    url = 'https://holidayapi.dev/v1/holidays/' + country + '/' + state + '/' + year\n    response = requests.get(url)\n    if response.status_code == 200:\n        return json.loads(response.content)\n    else:\n        return None\n\ndef get_holidays_by_year(country, state, year):\n    holidays = get_holidays(country, state, year)\n    if holidays:\n        return holidays['holidays']\n    else:\n        return None\n\ndef get_holidays_by_country(country, state):\n    holidays = get_holidays(country, state, '2020')\n    if holidays:\n        return holidays['holidays']\n    else:\n        return None\n\ndef get_holidays_by_state(country, state):\n    holidays = get_holidays(country, state, '2020')\n    if holidays:\n        return holidays['state']\n    else:\n        return None\n\ndef get_holidays_by_country_and_state(country, state):\n    holidays = get_holidays(country, state, '2020')\n    if holidays:\n        return holidays['country']\n    else:\n        return None\n\ndef get_holidays_by_country_and_state_and_year(country, state, year):\n    holidays = get_holidays(country, state, year)\n    if holidays:\n        return holidays['country']\n    else:\n        return None\n\ndef get_holidays_by_country_and_state_and_year_and_month(country, state, year, month):\n    holidays = get_holidays(country, state, year)\n    if holidays:\n        return holidays['country']\n    else:\n        return None\n\ndef get_holidays_by_country_and_state_and_year_and_month_and_day(country, state, year, month, day):\n    holidays = get_holidays(country, state, year)\n    if holidays:\n        return holidays['country']\n    else:\n        return None\n\ndef get_holidays_by_country_and_state_and_year_and_month_and_day_and_hour(country, state, year, month, day, hour):\n    holidays = get_holidays(country, state, year)\n    if holidays:\n        return holidays['country']\n    else:\n        return None\n\ndef get_holidays_by_country_and_state_and_year_and_month_and_day_and_hour_and_minute(country, state, year, month, day, hour, minute):\n    holidays = get_holidays(country, state, year)\n    if holidays:\n        return holidays['country']\n    else:\n        return None\n\ndef get_holidays_by_country_and_state_and_year_and_month_and_day_and_hour_and_minute_and_second(country, state, year, month, day, hour, minute, second):\n    holidays = get_holidays(country, state, year)\n    if holidays:\n        return holidays['country']\n    else:\n        return None\n\ndef get_holidays_by_country_and_state_and_year_and_month_and_day_and_hour_and_minute_and_second_and_microsecond(country, state, year, month, day, hour, minute, second, microsecond):\n    holidays = get_holidays(country, state, year)\n    if holidays:\n        return holidays['country']\n    else:\n        return None\n\ndef get_holidays_by_country_and_state_and_year_and_month_and_day_and_hour_and_minute_and_second_and_microsecond_and_nanosecond(country, state, year, month, day, hour, minute, second, microsecond, nanosecond):\n    holidays = get_holidays(country, state, year)\n    if holidays:\n        return holidays['country']\n    else:\n        return None\n\ndef get_holidays_by_country_and_state_and_year_and_month_and_day_and_hour_and_minute_and_second_and_micro",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'holidays' API to list all the public holidays in a specific region (e.g., a city or district) within a country and state for a given year.\n\n### Code:"
    },
    {
        "api": "hypothesis",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install hypothesis\nimport re\nimport unittest\nfrom hypothesis import given, strategies as st\n\ndef is_valid_date(date):\n    pattern = r\"^\\d{4}-\\d{2}-\\d{2}$\"\n    return re.match(pattern, date) is not None\n\ndef has_valid_dates(lst):\n    return any(is_valid_date(s) for s in lst)\n\nclass HasValidDatesTest(unittest.TestCase):\n    @given(st.lists(st.text(min_size=10, max_size=10, alphabet=st.characters(whitelist_categories=('Nd', 'Pd')))))\n    def test_has_valid_dates(self, lst):\n        result = has_valid_dates(lst)\n        self.assertEqual(result, any(is_valid_date(s) for s in lst))\n\nif __name__ == \"__main__\":\n    unittest.main()",
        "instruction": "Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any valid dates in the format 'YYYY-MM-DD'. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the date checking function.",
        "prediction": "import hypothesis.strategies as st\nfrom hypothesis import given\n\nfrom date_functions import DateFunctions\n\n\n@given(st.dates(), st.dates())\ndef test_date_functions_valid_date(date_1, date_2):\n    date_functions = DateFunctions()\n    assert date_functions.valid_date(date_1, date_2)\n\n\n@given(st.dates(), st.dates())\ndef test_date_functions_invalid_date(date_1, date_2):\n    date_functions = DateFunctions()\n    assert not date_functions.valid_date(date_1, date_2)\n    assert not date_functions.valid_date(date_2, date_1)\n    assert not date_functions.valid_date(date_1, date_1)\n    assert not date_functions.valid_date(date_2, date_2)\n    assert not date_functions.valid_date(date_1, date_2)\n    assert not date_functions.valid_date(date_2, date_1)\n    assert not date_functions.valid_date(date_1, date_1)\n    assert not date_functions.valid_date(date_2, date_2)\n    assert not date_functions.valid_date(date_1, date_2)\n    assert not date_functions.valid_date(date_2, date_1)\n    assert not date_functions.valid_date(date_1, date_1)\n    assert not date_functions.valid_date(date_2, date_2)\n    assert not date_functions.valid_date(date_1, date_2)\n    assert not date_functions.valid_date(date_2, date_1)\n    assert not date_functions.valid_date(date_1, date_1)\n    assert not date_functions.valid_date(date_2, date_2)\n    assert not date_functions.valid_date(date_1, date_2)\n    assert not date_functions.valid_date(date_2, date_1)\n    assert not date_functions.valid_date(date_1, date_1)\n    assert not date_functions.valid_date(date_2, date_2)\n    assert not date_functions.valid_date(date_1, date_2)\n    assert not date_functions.valid_date(date_2, date_1)\n    assert not date_functions.valid_date(date_1, date_1)\n    assert not date_functions.valid_date(date_2, date_2)\n    assert not date_functions.valid_date(date_1, date_2)\n    assert not date_functions.valid_date(date_2, date_1)\n    assert not date_functions.valid_date(date_1, date_1)\n    assert not date_functions.valid_date(date_2, date_2)\n    assert not date_functions.valid_date(date_1, date_2)\n    assert not date_functions.valid_date(date_2, date_1)\n    assert not date_functions.valid_date(date_1, date_1)\n    assert not date_functions.valid_date(date_2, date_2)\n    assert not date_functions.valid_date(date_1, date_2)\n    assert not date_functions.valid_date(date_2, date_1)\n    assert not date_functions.valid_date(date_1, date_1)\n    assert not date_functions.valid_date(date_2, date_2)\n    assert not date_functions.valid_date(date_1, date_2)\n    assert not date_functions.valid_date(date_2, date_1)\n    assert not date_functions.valid_date(date_1, date_1)\n    assert not date_functions.valid_date(date_2, date_2)\n    assert not date_functions.valid_date(date_1, date_2)\n    assert not date_functions.valid_date(date_2, date_1)\n    assert not date_fun",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any valid dates in the format 'YYYY-MM-DD'. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the date checking function.\n\n### Code:"
    },
    {
        "api": "hypothesis",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install hypothesis\nimport math\nimport unittest\nfrom hypothesis import given, strategies as st\n\ndef is_prime(n):\n    if n < 2:\n        return False\n    for i in range(2, int(math.sqrt(n))+1):\n        if n % i == 0:\n            return False\n    return True\n\nclass PrimeNumberTest(unittest.TestCase):\n    @given(st.integers(min_value=0, max_value=100))\n    def test_is_prime(self, n):\n        result = is_prime(n)\n        self.assertEqual(result, all(n % i != 0 for i in range(2, int(math.sqrt(n))+1)))\n\nif __name__ == \"__main__\":\n    unittest.main()",
        "instruction": "Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given number is a prime number. The 'hypothesis' library will be used to generate various input numbers and verify the correctness of the prime number checking function.",
        "prediction": "import hypothesis.strategies as st\nfrom hypothesis import given\n\nfrom. import strategies\n\n\n@given(st.integers(min_value=0, max_value=2**32 - 1), st.integers(min_value=0, max_value=2**32 - 1))\ndef test_is_prime(n, m):\n    assert strategies.is_prime(n) == (n in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31))\n    assert strategies.is_prime(m) == (m in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31))\n    assert strategies.is_prime(n * m) == (n * m in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31))\n    assert strategies.is_prime(n * m + 1) == (n * m + 1 in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31))\n    assert strategies.is_prime(n * m - 1) == (n * m - 1 in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31))\n    assert strategies.is_prime(n * m + 2) == (n * m + 2 in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31))\n    assert strategies.is_prime(n * m - 2) == (n * m - 2 in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31))\n    assert strategies.is_prime(n * m + 3) == (n * m + 3 in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31))\n    assert strategies.is_prime(n * m - 3) == (n * m - 3 in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31))\n    assert strategies.is_prime(n * m + 4) == (n * m + 4 in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31))\n    assert strategies.is_prime(n * m - 4) == (n * m - 4 in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31))\n    assert strategies.is_prime(n * m + 5) == (n * m + 5 in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31))\n    assert strategies.is_prime(n * m - 5) == (n * m - 5 in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31))\n    assert strategies.is_prime(n * m + 6) == (n * m + 6 in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31))\n    assert strategies.is_prime(n * m - 6) == (n * m - 6 in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31))\n    assert strategies.is_prime(n * m + 7) == (n * m + 7 in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31))\n    assert strategies.is_prime(n * m - 7) == (n * m - 7 in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31))\n    assert strategies.is_prime(n * m + 8) == (n * m + 8 in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31))\n    assert strategies.is_prime(n * m - 8) == (n * m - 8 in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31))\n    assert strategies.is_prime(n * m + 9) == (n * m + 9 in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31))\n    assert strategies.is_prime(n * m - 9) == (n * m - 9 in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31))",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given number is a prime number. The 'hypothesis' library will be used to generate various input numbers and verify the correctness of the prime number checking function.\n\n### Code:"
    },
    {
        "api": "hypothesis",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install hypothesis\nimport re\nimport unittest\nfrom hypothesis import given, strategies as st\n\ndef is_valid_ipv4_address(ip_address):\n    pattern = r\"^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$\"\n    return re.match(pattern, ip_address) is not None\n\nclass ValidIPv4AddressTest(unittest.TestCase):\n    @given(st.text(min_size=7, max_size=15, alphabet=st.characters(whitelist_categories=('Nd', 'Pd'))))\n    def test_is_valid_ipv4_address(self, ip_address):\n        result = is_valid_ipv4_address(ip_address)\n        self.assertEqual(result, re.match(r\"^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$\", ip_address) is not None)\n\nif __name__ == \"__main__\":\n    unittest.main()",
        "instruction": "Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given string is a valid IPv4 address. The 'hypothesis' library will be used to generate various input strings and verify the correctness of the IPv4 address validation function.",
        "prediction": "import hypothesis.strategies as st\nfrom hypothesis import given\n\nfrom. import strategies\n\n\n@given(st.data(), st.text())\ndef test_valid_ipv4_address(data, ip):\n    if ip == '0.0.0.0':\n        return\n    try:\n        ipaddress.ip_address(ip)\n    except ValueError:\n        data.draw(strategies.invalid_ipv4_address_strategy)\n    else:\n        data.draw(strategies.valid_ipv4_address_strategy)\n\n\n@given(st.data(), st.text())\ndef test_valid_ipv6_address(data, ip):\n    if ip == '::':\n        return\n    try:\n        ipaddress.ip_address(ip)\n    except ValueError:\n        data.draw(strategies.invalid_ipv6_address_strategy)\n    else:\n        data.draw(strategies.valid_ipv6_address_strategy)\n\n\n@given(st.data(), st.text())\ndef test_valid_ipv4_address_with_port(data, ip):\n    if ip == '0.0.0.0':\n        return\n    try:\n        ipaddress.ip_address(ip)\n    except ValueError:\n        data.draw(strategies.invalid_ipv4_address_with_port_strategy)\n    else:\n        data.draw(strategies.valid_ipv4_address_with_port_strategy)\n\n\n@given(st.data(), st.text())\ndef test_valid_ipv6_address_with_port(data, ip):\n    if ip == '::':\n        return\n    try:\n        ipaddress.ip_address(ip)\n    except ValueError:\n        data.draw(strategies.invalid_ipv6_address_with_port_strategy)\n    else:\n        data.draw(strategies.valid_ipv6_address_with_port_strategy)\n",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given string is a valid IPv4 address. The 'hypothesis' library will be used to generate various input strings and verify the correctness of the IPv4 address validation function.\n\n### Code:"
    },
    {
        "api": "hypothesis",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install hypothesis\nimport re\nimport unittest\nfrom hypothesis import given, strategies as st\n\ndef has_email_addresses(lst):\n    pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\"\n    return any(re.match(pattern, s) for s in lst)\n\nclass HasEmailAddressesTest(unittest.TestCase):\n    @given(st.lists(st.text()))\n    def test_has_email_addresses(self, lst):\n        result = has_email_addresses(lst)\n        self.assertEqual(result, any(re.match(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", s) for s in lst))\n\nif __name__ == \"__main__\":\n    unittest.main()",
        "instruction": "Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any email addresses. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the email address checking function.",
        "prediction": "import hypothesis.strategies as st\nfrom hypothesis import given\n\nfrom email_address_parser.email_address_parser import EmailAddressParser\n\nEMAIL_ADDRESS_STRATEGY = st.text(st.characters(min_codepoint=0x0021, max_codepoint=0x7E), min_size=1)\n\n\n@given(EMAIL_ADDRESS_STRATEGY)\ndef test_email_address_parser_valid_email_address(email_address):\n    EmailAddressParser.validate_email_address(email_address)\n    assert EmailAddressParser.validate_email_address(email_address) is True\n    assert EmailAddressParser.validate_email_address(email_address.lower()) is True\n    assert EmailAddressParser.validate_email_address(email_address.upper()) is True\n    assert EmailAddressParser.validate_email_address(email_address.title()) is True\n    assert EmailAddressParser.validate_email_address(email_address.capitalize()) is True\n    assert EmailAddressParser.validate_email_address(email_address.strip()) is True\n    assert EmailAddressParser.validate_email_address(email_address.replace(\" \", \"\")) is True\n    assert EmailAddressParser.validate_email_address(email_address.replace(\" \", \".\")) is True\n    assert EmailAddressParser.validate_email_address(email_address.replace(\" \", \".\").replace(\".\", \"\")) is True\n    assert EmailAddressParser.validate_email_address(email_address.replace(\" \", \".\").replace(\".\", \".\")) is True\n    assert EmailAddressParser.validate_email_address(email_address.replace(\" \", \".\").replace(\".\", \".\").replace(\".\", \"\")) is True\n    assert EmailAddressParser.validate_email_address(email_address.replace(\" \", \".\").replace(\".\", \".\").replace(\".\", \"\")) is True\n    assert EmailAddressParser.validate_email_address(email_address.replace(\" \", \".\").replace(\".\", \".\").replace(\".\", \"\")) is True\n    assert EmailAddressParser.validate_email_address(email_address.replace(\" \", \".\").replace(\".\", \".\").replace(\".\", \"\")) is True\n    assert EmailAddressParser.validate_email_address(email_address.replace(\" \", \".\").replace(\".\", \".\").replace(\".\", \"\")) is True\n    assert EmailAddressParser.validate_email_address(email_address.replace(\" \", \".\").replace(\".\", \".\").replace(\".\", \"\")) is True\n    assert EmailAddressParser.validate_email_address(email_address.replace(\" \", \".\").replace(\".\", \".\").replace(\".\", \"\")) is True\n    assert EmailAddressParser.validate_email_address(email_address.replace(\" \", \".\").replace(\".\", \".\").replace(\".\", \"\")) is True\n    assert EmailAddressParser.validate_email_address(email_address.replace(\" \", \".\").replace(\".\", \".\").replace(\".\", \"\")) is True\n    assert EmailAddressParser.validate_email_address(email_address.replace(\" \", \".\").replace(\".\", \".\").replace(\".\", \"\")) is True\n    assert EmailAddressParser.validate_email_address(email_address.replace(\" \", \".\").replace(\".\", \".\").replace(\".\", \"\")) is True\n    assert EmailAddressParser.validate_email_address(email_address.replace(\" \", \".\").replace(\".\", \".\").replace(\".\", \"\")) is True\n    assert EmailAddressParser.validate_email_address(email_address.replace(\" \", \".\").replace(\".\", \".\").replace(\".\", \"\")) is True\n    assert EmailAddressParser.validate_email_address(email_address.replace(\" \", \".\").replace(\".\", \".\").replace(\".\", \"\")) is True\n    assert EmailAddressParser.validate_email_address(email_address.replace(\" \", \".\").replace(\".\", \".\").replace(\".\", \"\")) is True\n    assert EmailAddressParser.validate_email_address(email_address.replace(\" \", \".\").replace(\".\", \".\").replace(\".\", \"\")) is True\n    assert EmailAddressParser.validate_email_address(email_address.replace(\" \", \".\").replace(\".\", \".\").replace(\".\", \"\")) is True\n    assert EmailAddressParser.validate_email_address(email_",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any email addresses. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the email address checking function.\n\n### Code:"
    },
    {
        "api": "ibis-framework",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install 'ibis-framework[duckdb]'\nimport ibis\nimport pandas as pd\n\nibis.options.interactive = True\n\ndf = pd.DataFrame(\n    [[\"a\", 1, 2], [\"b\", 3, 4], [\"c\", 5, 6]],\n    columns=[\"one\", \"two\", \"three\"],\n    index=[5, 6, 7],\n)\nt = ibis.memtable(df, name=\"t\")\n\n# Add a new column to the table\ntransformed_table = t.mutate(four=t.two + t.three)\n\n# Rename existing columns\ntransformed_table = transformed_table.rename({\"one\": \"new_one\", \"two\": \"new_two\", \"three\": \"new_three\", \"four\": \"new_four\"})\n\nprint(\"Transformed table: \")\nprint(transformed_table)",
        "instruction": "Create a Python program using the 'ibis-framework' API to perform data transformation operations on a data table. The program should define a sample data table using a Pandas DataFrame and convert it to an 'ibis' table. You need to interact with the 'ibis-framework' library to load the data, transform the table by adding a new column, and rename existing columns.",
        "prediction": "import pymysql\n\n# Open database connection\ndb = pymysql.connect(\"localhost\",\"root\",\"root\",\"mydata\" )\n\n# prepare a cursor object using cursor() method\ncursor = db.cursor()\n\n# Prepare SQL query to INSERT a record into the database.\nsql = \"SELECT * FROM data\"\n\ntry:\n   # Execute the SQL command\n   cursor.execute(sql)\n   # Fetch all the rows in a list of lists.\n   results = cursor.fetchall()\n   for row in results:\n      print(row)\nexcept:\n   print (\"Error: unable to fetch data\")\n\n# disconnect from server\ndb.close()",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'ibis-framework' API to perform data transformation operations on a data table. The program should define a sample data table using a Pandas DataFrame and convert it to an 'ibis' table. You need to interact with the 'ibis-framework' library to load the data, transform the table by adding a new column, and rename existing columns.\n\n### Code:"
    },
    {
        "api": "ibis-framework",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install 'ibis-framework[duckdb]'\nimport ibis\nimport pandas as pd\n\nibis.options.interactive = True\n\ndf = pd.DataFrame(\n    [[\"a\", 1, 2], [\"b\", 3, 4]],\n    columns=[\"one\", \"two\", \"three\"],\n    index=[5, 6],\n)\nt = ibis.memtable(df, name=\"t\")\nprint(\"Ibis data table: \")\nprint(t)\nprint(\"Ibis data schema: \")\nprint(t.schema())",
        "instruction": "Create a Python program using the 'ibis-framework' API to work with data tables and schemas. The program should define a sample data table using a Pandas DataFrame and convert it to an 'ibis' table. You need to interact with the 'ibis-framework' library to load the data, define a schema, and display the table and its schema.",
        "prediction": "// IMPORTANT: The schema is stored in a.py file in order to take advantage of the\n// int() and float() type coercion functions. Otherwise it could easily have\n// been stored as a JSON or another serialized format.\n\nschema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"id\": {\"type\": \"string\"},\n        \"name\": {\"type\": \"string\"},\n        \"description\": {\"type\": \"string\"},\n        \"price\": {\"type\": \"number\"},\n        \"tax\": {\"type\": \"number\"},\n        \"tags\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n    },\n    \"required\": [\"id\", \"name\", \"price\", \"tax\"],\n}\n\n// get_model() is a convenience function that given a schema and an id\n// returns a model instance built with the data stored in the database.\n\ndef get_model(schema, id):\n    data = get_data(id)\n    return model(data, many=False)\n\n// get_models() is a convenience function that given a schema and a set of ids\n// returns a list of model instances built with the data stored in the database.\n\ndef get_models(schema, ids):\n    data = get_data(ids)\n    return [model(item, many=False) for item in data]\n\n// get_data() is a convenience function that given an id returns the data\n// stored in the database for that id.\n\ndef get_data(id):\n    return data_source.get(id)\n\n// model() is a convenience function that given data and an optional many\n// parameter returns an instance of the model.\n\ndef model(data, many=True):\n    return model_type(data, many=many)",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'ibis-framework' API to work with data tables and schemas. The program should define a sample data table using a Pandas DataFrame and convert it to an 'ibis' table. You need to interact with the 'ibis-framework' library to load the data, define a schema, and display the table and its schema.\n\n### Code:"
    },
    {
        "api": "json-tricks",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install json_tricks\nfrom json_tricks import dumps\nimport math\n\ndata = {\n    'pi': math.pi,\n    'nan_value': math.nan,\n    'infinity_value': math.inf\n}\n\njson_str = dumps(data, special_values={'NaN': 'NaN', 'Infinity': 'Infinity'}, indent=4)\n\nprint(json_str)",
        "instruction": "Create a Python program that uses 'json-tricks' to handle JSON data with custom serialization of NaN and Infinity. Define a dictionary with floating-point values that include NaN and Infinity. Serialize the dictionary into a JSON string using 'dumps' and ensure that the custom serialization of NaN and Infinity is applied.",
        "prediction": "import json\nimport sys\n\ndef main():\n    if len(sys.argv)!= 2:\n        print(\"Usage: json-tricks.py <input-file>\")\n        return\n    \n    with open(sys.argv[1], 'r') as f:\n        data = json.load(f)\n    \n    for d in data:\n        d['nano_time'] = d['nano_time'] / 1000000000.0\n        d['system_time'] = d['system_time'] / 1000000000.0\n        d['user_time'] = d['user_time'] / 1000000000.0\n        d['cpu_percent'] = d['cpu_percent'] / 100.0\n        d['cpu_times'] = d['cpu_times'] / 1000000000.0\n        d['memory_percent'] = d['memory_percent'] / 100.0\n        d['memory_info'] = d['memory_info'] / 1000000000.0\n        d['memory_full_info'] = d['memory_full_info'] / 1000000000.0\n        d['num_ctx_switches'] = d['num_ctx_switches'] / 1000000000.0\n        d['num_fds'] = d['num_fds'] / 1000000000.0\n        d['num_threads'] = d['num_threads'] / 1000000000.0\n        d['open_files'] = d['open_files'] / 1000000000.0\n        d['connections'] = d['connections'] / 1000000000.0\n        d['num_fds_closed'] = d['num_fds_closed'] / 1000000000.0\n        d['num_connections_dropped'] = d['num_connections_dropped'] / 1000000000.0\n        d['num_connections_accepted'] = d['num_connections_accepted'] / 1000000000.0\n        d['num_connections_established'] = d['num_connections_established'] / 1000000000.0\n        d['num_connections_reset'] = d['num_connections_reset'] / 1000000000.0\n        d['num_connections_closed'] = d['num_connections_closed'] / 1000000000.0\n        d['num_connections_aborted'] = d['num_connections_aborted'] / 1000000000.0\n        d['num_connections_failed'] = d['num_connections_failed'] / 1000000000.0\n        d['num_connections_timed_out'] = d['num_connections_timed_out'] / 1000000000.0\n        d['num_connections_resets'] = d['num_connections_resets'] / 1000000000.0\n        d['num_connections_closed_by_peer'] = d['num_connections_closed_by_peer'] / 1000000000.0\n        d['num_connections_closed_by_us'] = d['num_connections_closed_by_us'] / 1000000000.0\n        d['num_connections_accepted_by_peer'] = d['num_connections_accepted_by_peer'] / 1000000000.0\n        d['num_connections_accepted_by_us'] = d['num_connections_accepted_by_us'] / 1000000000.0\n        d['num_connections_finished'] = d['num_connections_finished'] / 1000000000.0\n        d['num_connections_closed_by_peer_after_timed_out'] = d['num_connections_closed_by_peer_after_timed_out'] / 1000000000.0\n        d['num_connections_closed_by_us_after_timed_out'] = d['num_connections_closed_by_us_after_timed_out'] / 1000000000.0\n        d['num_connections_accepted_by_peer_after_timed_out'] = d['num_connections_accepted_by_peer_after_timed_out'] / 1000000000.0\n        d['num_connections_accepted_by_us_after_timed_out'] = d['num_connections_accepted_by_us_after_timed_out'] / 1000000000.0\n        d['num_connections_finished_after_timed_out'] = d['num_connections_finished_after_timed_",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that uses 'json-tricks' to handle JSON data with custom serialization of NaN and Infinity. Define a dictionary with floating-point values that include NaN and Infinity. Serialize the dictionary into a JSON string using 'dumps' and ensure that the custom serialization of NaN and Infinity is applied.\n\n### Code:"
    },
    {
        "api": "json-tricks",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install json_tricks\nfrom json_tricks import dumps\n\nnested_data = {\n    'name': 'John',\n    'info': {\n        'age': 30,\n        'address': {\n            'street': '123 Main St',\n            'city': 'Cityville'\n        },\n        'hobbies': ['Reading', 'Traveling']\n    }\n}\n\njson_str = dumps(nested_data, indent=4)\n\nprint(json_str)",
        "instruction": "Develop a Python program that showcases the use of 'json-tricks' to handle JSON data with nested objects. Create a dictionary containing nested dictionaries and lists. Serialize the dictionary into a JSON string using 'dumps' and ensure that the nested structure is preserved.",
        "prediction": "import json\nimport sys\n\ndef main():\n    if len(sys.argv)!= 2:\n        print(\"Usage: json-tricks.py <json-file>\")\n        return\n    \n    with open(sys.argv[1]) as f:\n        data = json.load(f)\n    \n    print(data)\n    \n    data[\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][\"data\"][",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program that showcases the use of 'json-tricks' to handle JSON data with nested objects. Create a dictionary containing nested dictionaries and lists. Serialize the dictionary into a JSON string using 'dumps' and ensure that the nested structure is preserved.\n\n### Code:"
    },
    {
        "api": "json-tricks",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install json_tricks\nfrom json_tricks import dumps, loads\n\ndef custom_encoder(obj):\n    if isinstance(obj, complex):\n        return {'__complex__': True, 'real': obj.real, 'imag': obj.imag}\n    raise TypeError(repr(obj) + ' is not JSON serializable')\n\ndef custom_decoder(dct):\n    if '__complex__' in dct:\n        return complex(dct['real'], dct['imag'])\n    return dct\n\ndata = {\n    'complex_number': 2 + 3j,\n    'regular_number': 42\n}\n\njson_str = dumps(data, default=custom_encoder, indent=4)\nparsed_data = loads(json_str, object_hook=custom_decoder)\n\nprint(parsed_data['complex_number'])\nprint(parsed_data['regular_number'])",
        "instruction": "Create a Python program that showcases the use of 'json-tricks' for handling JSON data with custom encoder and decoder functions. Define custom functions for encoding and decoding data. Use these functions to serialize and deserialize a dictionary containing special data types. Ensure that the custom encoding and decoding functions are applied to the data.",
        "prediction": "import json\nimport sys\nimport os\nimport json_tricks as json\n\ndef json_encoder(obj):\n    if isinstance(obj, type(sys.float_info.epsilon)):\n        return float(obj)\n    else:\n        return obj.__dict__\n\ndef json_decoder(obj):\n    if '__type__' in obj:\n        return obj['__type__']\n    else:\n        return obj\n\ndef encode(data):\n    return json.dumps(data, default=json_encoder)\n\ndef decode(data):\n    return json.loads(data, object_hook=json_decoder)\n\ndef encode_file(file_path):\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n    return encode(data)\n\ndef decode_file(file_path):\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n    return decode(data)\n\ndef encode_json(data):\n    return json.dumps(data, default=json_encoder)\n\ndef decode_json(data):\n    return json.loads(data, object_hook=json_decoder)\n\ndef encode_json_file(file_path):\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n    return encode_json(data)\n\ndef decode_json_file(file_path):\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n    return decode_json(data)\n\ndef encode_json_str(data):\n    return json.dumps(data, default=json_encoder)\n\ndef decode_json_str(data):\n    return json.loads(data, object_hook=json_decoder)\n\ndef encode_json_str_file(file_path):\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n    return encode_json_str(data)\n\ndef decode_json_str_file(file_path):\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n    return decode_json_str(data)\n\ndef encode_json_str_to_file(file_path, data):\n    with open(file_path, 'w') as f:\n        f.write(encode_json_str(data))\n\ndef decode_json_str_from_file(file_path):\n    with open(file_path, 'r') as f:\n        data = f.read()\n    return decode_json_str(data)\n\ndef encode_json_str_to_file_atomic(file_path, data):\n    with open(file_path, 'w') as f:\n        f.write(encode_json_str(data))\n\ndef decode_json_str_from_file_atomic(file_path):\n    with open(file_path, 'r') as f:\n        data = f.read()\n    return decode_json_str(data)\n\ndef encode_json_str_to_file_atomic(file_path, data):\n    with open(file_path, 'w') as f:\n        f.write(encode_json_str(data))\n\ndef decode_json_str_from_file_atomic(file_path):\n    with open(file_path, 'r') as f:\n        data = f.read()\n    return decode_json_str(data)\n\ndef encode_json_str_to_file_atomic_compressed(file_path, data):\n    with open(file_path, 'w') as f:\n        f.write(encode_json_str(data))\n\ndef decode_json_str_from_file_atomic_compressed(file_path):\n    with open(file_path, 'r') as f:\n        data = f.read()\n    return decode_json_str(data)\n\ndef encode_json_str_to_file_atomic_compressed(file_path, data):\n    with open(file_path, 'w') as f:\n        f.write(encode_json_str(data))\n\ndef decode_json_",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that showcases the use of 'json-tricks' for handling JSON data with custom encoder and decoder functions. Define custom functions for encoding and decoding data. Use these functions to serialize and deserialize a dictionary containing special data types. Ensure that the custom encoding and decoding functions are applied to the data.\n\n### Code:"
    },
    {
        "api": "json-tricks",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install json_tricks\nfrom json_tricks import dumps\n\nclass CustomObject:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\ndata = {\n    'person': CustomObject('Frank', 45)\n}\n\njson_str = dumps(data, object_serialization={\n    'person': lambda obj: {'name': obj.name, 'age': obj.age}\n}, indent=4)\n\nprint(json_str)",
        "instruction": "Develop a Python program that demonstrates the use of 'json-tricks' for handling JSON data with custom object serialization. Define a dictionary that includes custom Python objects and serialize it into a JSON string using 'dumps'. Specify custom serialization options for the objects.",
        "prediction": "import json\n\ndef main():\n    mydict = {\n        'name': '\u9a86\u660a',\n        'age': 38,\n        'qq': 957658,\n        'friends': ['\u738b\u5927\u9524', '\u767d\u5143\u82b3'],\n        'cars': [\n            {'brand': 'BYD','max_speed': 180},\n            {'brand': 'Audi','max_speed': 280},\n            {'brand': 'Benz','max_speed': 320}\n        ]\n    }\n    try:\n        with open('data.json', 'w', encoding='utf-8') as fs:\n            json.dump(mydict, fs)\n    except IOError as e:\n        print(e)\n    print('\u4fdd\u5b58\u6570\u636e\u5b8c\u6210!')\n\nif __name__ == '__main__':\n    main()",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program that demonstrates the use of 'json-tricks' for handling JSON data with custom object serialization. Define a dictionary that includes custom Python objects and serialize it into a JSON string using 'dumps'. Specify custom serialization options for the objects.\n\n### Code:"
    },
    {
        "api": "json-tricks",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install json_tricks\nfrom json_tricks import dumps, loads\n\npeople = [\n    {'name': 'Alice', 'age': 25, 'address': '123 Main St'},\n    {'name': 'Bob', 'age': 30, 'address': '456 Elm St'},\n    {'name': 'Charlie', 'age': 35, 'address': '789 Oak St'}\n]\n\njson_str = dumps(people, indent=4)\nparsed_people = loads(json_str)\n\nprint(parsed_people)\nprint(parsed_people[0]['name'])  # Should print 'Alice'.",
        "instruction": "Create a Python program that demonstrates the use of 'json-tricks' for handling JSON arrays. Define a list of dictionaries, each representing a person's information, such as name, age, and address. Serialize this list of dictionaries into a JSON string using the 'dumps' function. Then, deserialize the JSON string back into a list of dictionaries using the 'loads' function.",
        "prediction": "import json\nimport sys\n\ndef json_to_dict(json_file):\n    with open(json_file) as f:\n        data = json.load(f)\n    return data\n\ndef dict_to_json(data, json_file):\n    with open(json_file, 'w') as f:\n        json.dump(data, f, indent=4)\n\ndef dict_to_json_string(data):\n    return json.dumps(data, indent=4)\n\ndef json_to_dict_string(json_string):\n    return json.loads(json_string)\n\ndef dict_to_json_file(data, json_file):\n    with open(json_file, 'w') as f:\n        json.dump(data, f, indent=4)\n\ndef json_to_dict_file(json_file):\n    with open(json_file) as f:\n        data = json.load(f)\n    return data\n\ndef dict_to_json_string_file(data, json_file):\n    with open(json_file, 'w') as f:\n        json.dump(data, f, indent=4)\n\ndef json_to_dict_string_file(json_string, json_file):\n    with open(json_file, 'w') as f:\n        json.dump(json_string, f, indent=4)\n\ndef dict_to_json_file_string(data, json_file):\n    with open(json_file, 'w') as f:\n        json.dump(data, f, indent=4)\n\ndef json_to_dict_file_string(json_file):\n    with open(json_file) as f:\n        data = json.load(f)\n    return data\n\ndef dict_to_json_file_string_file(data, json_file):\n    with open(json_file, 'w') as f:\n        json.dump(data, f, indent=4)\n\ndef json_to_dict_string_file(json_string, json_file):\n    with open(json_file, 'w') as f:\n        json.dump(json_string, f, indent=4)\n\ndef json_to_dict_string_file_string(json_string, json_file):\n    with open(json_file, 'w') as f:\n        json.dump(json_string, f, indent=4)\n\ndef dict_to_json_file_string_file(data, json_file):\n    with open(json_file, 'w') as f:\n        json.dump(data, f, indent=4)\n\ndef json_to_dict_file_string_file(json_file):\n    with open(json_file) as f:\n        data = json.load(f)\n    return data\n\ndef dict_to_json_file_string_file_string(data, json_file):\n    with open(json_file, 'w') as f:\n        json.dump(data, f, indent=4)\n\ndef dict_to_json_file_string_file_string_file(data, json_file):\n    with open(json_file, 'w') as f:\n        json.dump(data, f, indent=4)\n\ndef json_to_dict_file_string_file_string(json_file):\n    with open(json_file) as f:\n        data = json.load(f)\n    return data\n\ndef dict_to_json_file_string_file_string_file(json_file):\n    with open(json_file, 'w') as f:\n        json.dump(data, f, indent=4)\n\ndef dict_to_json_file_string_file_string_file_string(data, json_file):\n    with open(json_file, 'w') as f:\n        json.dump(data, f, indent=4)\n\ndef dict_to_json_file_string_file_string_file_string_file(data, json_file):\n    with open(json_file, 'w') as f:\n        json.dump(data, f, indent=4)\n\ndef dict_to_json_file_string_file_string_file_string_file_string(data, json_file):\n    with open",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that demonstrates the use of 'json-tricks' for handling JSON arrays. Define a list of dictionaries, each representing a person's information, such as name, age, and address. Serialize this list of dictionaries into a JSON string using the 'dumps' function. Then, deserialize the JSON string back into a list of dictionaries using the 'loads' function.\n\n### Code:"
    },
    {
        "api": "jsonschema",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install jsonschema\nimport jsonschema\n\n# Define a JSON schema with conditional validation based on properties\nschema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"isAdult\": {\"type\": \"boolean\"},\n        \"age\": {\"type\": \"integer\"}\n    },\n    \"if\": {\"properties\": {\"isAdult\": {\"const\": True}}, \"then\": {\"required\": [\"age\"]}}\n}\n\n# Create a JSON object to validate\njson_data = {\n    \"isAdult\": True,\n    \"age\": 25\n}\n\n# Validate the JSON data against the schema with conditional validation\ntry:\n    jsonschema.validate(instance=json_data, schema=schema)\n    print(\"JSON data is valid according to the schema with conditional validation.\")\nexcept jsonschema.exceptions.ValidationError as e:\n    print(f\"JSON data is not valid according to the schema with conditional validation. Error: {e}\")\n",
        "instruction": "Create a Python program that uses 'jsonschema' to validate a JSON object against a schema with conditional validation based on properties.",
        "prediction": "import json\nimport jsonschema\nimport sys\nimport os\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nfrom schema import schema\n\ndef validate(data):\n    try:\n        jsonschema.validate(data, schema)\n    except jsonschema.ValidationError as e:\n        return e.message\n    return None\n\nif __name__ == \"__main__\":\n    data = json.load(sys.stdin)\n    error = validate(data)\n    if error:\n        print(error, file=sys.stderr)\n        sys.exit(1)\n    sys.exit(0)\n",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that uses 'jsonschema' to validate a JSON object against a schema with conditional validation based on properties.\n\n### Code:"
    },
    {
        "api": "jsonschema",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install jsonschema\nimport jsonschema\n\n# Define a JSON schema with format-based date validation for a property, custom keyword validation, conditional validation based on property values, and pattern-based validation for another property\nschema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"birthDate\": {\"type\": \"string\", \"format\": \"date\"},\n        \"isStudent\": {\"type\": \"boolean\"},\n        \"studentID\": {\"type\": \"string\"},\n        \"email\": {\"type\": \"string\", \"pattern\": \"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\"}\n    },\n    \"custom_keywords\": {\n        \"student_required\": {\n            \"if\": {\"properties\": {\"isStudent\": {\"const\": True}}, \"then\": {\"required\": [\"studentID\"]}}\n        },\n        \"student_email_required\": {\n            \"if\": {\n                \"properties\": {\"isStudent\": {\"const\": True}, \"studentID\": {\"pattern\": \"^[A-Z]{3}[0-9]{3}$\"}},\n                \"then\": {\"required\": [\"email\"]}\n            }\n        }\n    }\n}\n\n# Create a JSON object to validate\njson_data = {\n    \"birthDate\": \"1995-01-15\",\n    \"isStudent\": True,\n    \"studentID\": \"ABC123\",\n    \"email\": \"student@example.com\"\n}\n\n# Custom keyword validation function\ndef validate_custom_keywords(validator, custom_keywords, instance, schema):\n    if instance[\"isStudent\"] and \"studentID\" in instance and not instance[\"studentID\"].startswith(\"ABC\"):\n        yield jsonschema.exceptions.ValidationError(\"Student ID must start with 'ABC' for students.\")\n    if instance[\"isStudent\"] and \"studentID\" in instance and not instance[\"email\"].endswith(\"student.com\"):\n        yield jsonschema.exceptions.ValidationError(\"Invalid email format for student with ID starting with 'ABC'.\")\n\n# Add the custom keyword validator to the validator registry\njsonschema.validators.validator_for(schema).VALIDATORS[\"custom_keywords\"] = validate_custom_keywords\n\n# Validate the JSON data against the schema with format-based date validation, custom keywords, conditional validation, and pattern-based validation\ntry:\n    jsonschema.validate(instance=json_data, schema=schema, format_checker=jsonschema.FormatChecker())\n    print(\"JSON data is valid according to the schema with format-based date validation, custom keywords, conditional validation, and pattern-based validation.\")\nexcept jsonschema.exceptions.ValidationError as e:\n    print(f\"JSON data is not valid according to the schema. Error: {e}\")\n",
        "instruction": "Create a Python program that utilizes 'jsonschema' to validate a JSON object against a schema with format-based date validation for a property, custom keyword validation, conditional validation based on property values, and pattern-based validation for another property.",
        "prediction": "import json\nimport sys\nimport os\nimport jsonschema\nimport jsonschema.exceptions\nimport jsonschema.validators\nimport jsonschema.validators.exceptions\nimport jsonschema.validators.validator_for\nimport jsonschema.validators.validator_for\nimport jsonschema.validators.draft_2019_09\nimport jsonschema.validators.draft_2019_09.validators\nimport jsonschema.validators.draft_2019_09.validators.validators\nimport jsonschema.validators.draft_2019_09.errors\nimport jsonschema.validators.draft_2019_09.types\nimport jsonschema.validators.draft_2019_09.types.object\nimport jsonschema.validators.draft_2019_09.types.object.properties\nimport jsonschema.validators.draft_2019_09.types.object.properties.patternProperties\nimport jsonschema.validators.draft_2019_09.types.object.properties.patternProperties.properties\nimport jsonschema.validators.draft_2019_09.types.object.properties.patternProperties.properties.properties\nimport jsonschema.validators.draft_2019_09.types.object.properties.patternProperties.properties.properties.items\nimport jsonschema.validators.draft_2019_09.types.object.properties.patternProperties.properties.properties.items.enum\nimport jsonschema.validators.draft_2019_09.types.object.properties.patternProperties.properties.properties.items.enum.validators\nimport jsonschema.validators.draft_2019_09.types.object.properties.patternProperties.properties.properties.items.enum.validators.0\nimport jsonschema.validators.draft_2019_09.types.object.properties.patternProperties.properties.properties.items.enum.validators.0.validator\nimport jsonschema.validators.draft_2019_09.types.object.properties.patternProperties.properties.properties.items.enum.validators.0.validator.META_SCHEMA\nimport jsonschema.validators.draft_2019_09.types.object.properties.patternProperties.properties.properties.items.enum.validators.0.validator.META_SCHEMA.properties\nimport jsonschema.validators.draft_2019_09.types.object.properties.patternProperties.properties.properties.items.enum.validators.0.validator.META_SCHEMA.properties.patternProperties\nimport jsonschema.validators.draft_2019_09.types.object.properties.patternProperties.properties.properties.items.enum.validators.0.validator.META_SCHEMA.properties.patternProperties\nimport jsonschema.validators.draft_2019_09.types.object.properties.patternProperties.properties.properties.items.enum.validators.0.validator.META_SCHEMA.properties.patternProperties.items\nimport jsonschema.validators.draft_2019_09.types.object.properties.patternProperties.properties.properties.items.enum.validators.0.validator.META_SCHEMA.properties.patternProperties.items.properties\nimport jsonschema.validators.draft_2019_09.types.object.properties.patternProperties.properties.properties.items.enum.validators.0.validator.META_SCHEMA.properties.patternProperties.items.properties\nimport jsonschema.validators.draft_2019_09.types.object.properties.patternProperties.properties.properties.items.enum.validators.0.validator.META_SCHEMA.properties.patternProperties.items.properties.properties\nimport jsonschema.validators.draft_2019_09.types.object.properties.patternProperties.properties.properties.items.enum.validators.0.validator.META_SCHEMA.properties.patternProperties.items.properties.properties\nimport jsonschema.validators.draft_2019_09.types.object.properties.patternProperties.properties.properties.items.enum.validators.0.validator.META_SCHEMA.properties.patternProperties.items.properties.properties\nimport jsonschema",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that utilizes 'jsonschema' to validate a JSON object against a schema with format-based date validation for a property, custom keyword validation, conditional validation based on property values, and pattern-based validation for another property.\n\n### Code:"
    },
    {
        "api": "jsonschema",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install jsonschema\nimport jsonschema\n\n# Define a JSON schema with custom keyword validation\nschema = {\n    \"type\": \"object\",\n    \"custom_keyword\": \"my_custom_value\"\n}\n\n# Create a JSON object to validate\njson_data = {\n    \"custom_keyword\": \"my_custom_value\"\n}\n\n# Custom keyword validation function\ndef validate_custom_keyword(validator, custom_keyword, instance, schema):\n    if instance != custom_keyword:\n        yield jsonschema.exceptions.ValidationError(f\"Value does not match the custom keyword: {custom_keyword}\")\n\n# Add the custom keyword validator to the validator registry\njsonschema.validators.validator_for(schema).VALIDATORS[\"custom_keyword\"] = validate_custom_keyword\n\n# Validate the JSON data against the schema with custom keyword validation\ntry:\n    jsonschema.validate(instance=json_data, schema=schema)\n    print(\"JSON data is valid according to the schema with custom keyword validation.\")\nexcept jsonschema.exceptions.ValidationError as e:\n    print(f\"JSON data is not valid according to the schema with custom keyword validation. Error: {e}\")\n",
        "instruction": "Create a Python program that utilizes 'jsonschema' to validate a JSON object against a schema that includes custom keyword validation.",
        "prediction": "import json\nimport sys\nimport os\nimport jsonschema\nimport jsonschema.exceptions\nimport jsonschema.validators\nimport jsonschema.validators.exceptions\nimport jsonschema.validators.validator\nimport jsonschema.validators.validator_for\nimport jsonschema.validators.compound\nimport jsonschema.validators.compound.draft4\nimport jsonschema.validators.compound.draft6\nimport jsonschema.validators.compound.draft7\nimport jsonschema.validators.compound.draft201909\nimport jsonschema.validators.compound.draft202012\nimport jsonschema.validators.extended\nimport jsonschema.validators.extended.draft3\nimport jsonschema.validators.extended.draft4\nimport jsonschema.validators.extended.draft6\nimport jsonschema.validators.extended.draft7\nimport jsonschema.validators.extended.draft201909\nimport jsonschema.validators.extended.draft202012\nimport jsonschema.validators.extended.draft202101\nimport jsonschema.validators.extended.draft202101.properties\nimport jsonschema.validators.extended.draft202101.items\nimport jsonschema.validators.extended.draft202101.arrays\nimport jsonschema.validators.extended.draft202101.additionalItems\nimport jsonschema.validators.extended.draft202101.additionalProperties\nimport jsonschema.validators.extended.draft202101.dependencies\nimport jsonschema.validators.extended.draft202101.contains",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that utilizes 'jsonschema' to validate a JSON object against a schema that includes custom keyword validation.\n\n### Code:"
    },
    {
        "api": "langchain",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install langchain\n# Import necessary libraries\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.chains import LLMChain\n\n# Define the GPT-3.5 Turbo model\nllm_model = \"gpt-3.5-turbo\"\n\n# Initialize a ChatOpenAI instance with the specified model and temperature\nllm = ChatOpenAI(temperature=0.5, model=llm_model)\n\n# Define a template for chat prompts, where {country} will be replaced with the actual country name\nprompt = ChatPromptTemplate.from_template(\"What is the capital of {country}?\")\n\n# Initialize an LLMChain with the ChatOpenAI instance and the prompt template\nchain = LLMChain(llm=llm, prompt=prompt)\n\n# Specify the country you want to inquire about\ncountry = \"Germany\"\n\n# Run the chain to get a response about the capital of the specified country\nchain.run(country)",
        "instruction": "Create a Python program that utilizes the \"langchain\" API to interact with the GPT-3.5 Turbo model. Define a chat prompt template that includes a variable {country}. Use this template to inquire about the capital of a specific country, e.g., \"What is the capital of {country}?\" Replace {country} with \"Germany\" and retrieve the model's response. Save the response.",
        "prediction": "import urllib.request\nimport urllib.parse\nimport json\nimport time\n\ndef get_data(country):\n    url = 'https://api.gpt.2.ai/api/v1/generate'\n    params = {\n        'count': 1,\n        'temperature': 0.7,\n        'prefix': country,\n        'nsamples': 1,\n        'batch_size': 1,\n        'length': 100,\n        'include_prefix': True,\n       'return_as_list': True,\n        'top_k': 40,\n        'top_p': 0.9,\n        'include_prefix': True,\n        'num_iterations': 3,\n        'early_stopping': True,\n        'eos_token': '",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that utilizes the \"langchain\" API to interact with the GPT-3.5 Turbo model. Define a chat prompt template that includes a variable {country}. Use this template to inquire about the capital of a specific country, e.g., \"What is the capital of {country}?\" Replace {country} with \"Germany\" and retrieve the model's response. Save the response.\n\n### Code:"
    },
    {
        "api": "linear-operator",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install linear-operator\nimport torch\nfrom linear_operator import to_linear_operator\nfrom scipy.linalg import eigh\n\n# Define a square matrix M\nM = torch.Tensor([[4, 2], [2, 5]])\n\n# Compute the eigenvalues and eigenvectors\nM_op = to_linear_operator(M)\neigenvalues, eigenvectors = eigh(M_op.to_dense())\n\n# Print the eigenvalues and eigenvectors\nprint(\"Eigenvalues:\", eigenvalues)\nprint(\"Eigenvectors:\", eigenvectors)",
        "instruction": "Create a Python program that utilizes the 'linear-operator' API to compute the eigenvalues and eigenvectors of a given square matrix. The program should print the computed eigenvalues and eigenvectors.",
        "prediction": "// https://leetcode.com/problems/squares-of-a-sorted-array/\n\nclass Solution:\n    def sortedSquares(self, A: List[int]) -> List[int]:\n        \n        result = []\n        \n        for i in range(len(A)):\n            result.append(A[i]**2)\n            \n        result.sort()\n        \n        return result\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that utilizes the 'linear-operator' API to compute the eigenvalues and eigenvectors of a given square matrix. The program should print the computed eigenvalues and eigenvectors.\n\n### Code:"
    },
    {
        "api": "linear-operator",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install linear-operator\nimport torch\nfrom linear_operator import to_linear_operator\nfrom scipy.linalg import sinm\n\n# Define a matrix M\nM = torch.Tensor([[1, 2], [3, 4]])\n\n# Compute the matrix sine\nM_op = to_linear_operator(M)\nmatrix_sine = sinm(M_op.to_dense())\n\n# Print the matrix sine\nprint(\"Matrix Sine:\", matrix_sine)",
        "instruction": "Create a Python program that utilizes the 'linear-operator' API to compute the matrix sine of a given matrix. The program should print the computed matrix sine.",
        "prediction": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n\n# Create new Figure and an Axes which fills it.\nfig = plt.figure(figsize=(7, 7))\nax = fig.add_axes([0, 0, 1, 1], frameon=False)\nax.set_xlim(-3, 3), ax.set_xticks([])\nax.set_ylim(-3, 3), ax.set_yticks([])\n\n# Create rain data\nn_drops = 50\nrain_drops = np.zeros(n_drops, dtype=[('position', float, 2),\n                                      ('size', float, 1),\n                                      ('growth', float, 1),\n                                      ('color', float, 4)])\n\n# Initialize the raindrops in random positions and with\n# random growth rates.\nrain_drops['position'] = np.random.uniform(-1, 1, (n_drops, 2))\nrain_drops['growth'] = np.random.uniform(50, 200, n_drops)\n\n# Construct the scatter which we will update during animation\n# as the raindrops develop.\nscat = ax.scatter(rain_drops['position'][:, 0], rain_drops['position'][:, 1],\n                  s=rain_drops['size'], lw=0.5, edgecolors=rain_drops['color'],\n                  facecolors='none')\n\n\ndef update(frame_number):\n    # Get an index which we can use to re-spawn the oldest raindrop.\n    current_index = frame_number % n_drops\n\n    # Make all colors more transparent as time progresses.\n    rain_drops['color'][:, 3] -= 1.0/len(rain_drops)\n    rain_drops['color'][:, 3] = np.clip(rain_drops['color'][:, 3], 0, 1)\n\n    # Make all circles bigger.\n    rain_drops['size'] += rain_drops['growth']\n\n    # Pick a new position for oldest rain drop, resetting its size,\n    # color and growth factor.\n    rain_drops['position'][current_index] = np.random.uniform(-1, 1, 2)\n    rain_drops['size'][current_index] = 5\n    rain_drops['color'][current_index] = (0, 0, 0, 1)\n    rain_drops['growth'][current_index] = np.random.uniform(50, 200)\n\n    # Update the scatter collection, with the new colors, sizes and positions.\n    scat.set_edgecolors(rain_drops['color'])\n    scat.set_sizes(rain_drops['size'])\n    scat.set_offsets(rain_drops['position'])\n\n\n# Construct the animation, using the update function as the animation\n# director.\nanimation = FuncAnimation(fig, update, interval=10)\nplt.show()\n# animation.save('matplotlib_animation.mp4', fps=30, extra_args=['-vcodec', 'libx264'])\nprint(\"Done\")\n",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that utilizes the 'linear-operator' API to compute the matrix sine of a given matrix. The program should print the computed matrix sine.\n\n### Code:"
    },
    {
        "api": "linear-operator",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install linear-operator\nimport torch\nfrom linear_operator import to_linear_operator\nfrom scipy.linalg import sqrtm\n\n# Define a matrix M\nM = torch.Tensor([[1, 2], [3, 4]])\n\n# Compute the matrix square root\nM_op = to_linear_operator(M)\nmatrix_sqrt = sqrtm(M_op.to_dense())\n\n# Print the matrix square root\nprint(\"Matrix Square Root:\", matrix_sqrt)",
        "instruction": "Create a Python program that utilizes the 'linear-operator' API to compute the matrix square root of a given matrix. The program should print the computed matrix square root.",
        "prediction": "// https://leetcode.com/problems/matrix-block-sum\n\nclass Solution:\n    def matrixBlockSum(self, mat: List[List[int]], K: int) -> List[List[int]]:\n        \n        m = len(mat)\n        n = len(mat[0])\n        \n        for i in range(m):\n            for j in range(n):\n                if i > 0:\n                    mat[i][j] += mat[i-1][j]\n                if j > 0:\n                    mat[i][j] += mat[i][j-1]\n                if i > 0 and j > 0:\n                    mat[i][j] -= mat[i-1][j-1]\n        \n        res = [[0 for _ in range(n)] for _ in range(m)]\n        \n        for i in range(m):\n            for j in range(n):\n                r1, c1 = max(0, i-K), max(0, j-K)\n                r2, c2 = min(m-1, i+K), min(n-1, j+K)\n                res[i][j] = mat[r2][c2]\n                if r1 > 0:\n                    res[i][j] -= mat[r1-1][c2]\n                if c1 > 0:\n                    res[i][j] -= mat[r2][c1-1]\n                if r1 > 0 and c1 > 0:\n                    res[i][j] += mat[r1-1][c1-1]\n        \n        return res\n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    ",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that utilizes the 'linear-operator' API to compute the matrix square root of a given matrix. The program should print the computed matrix square root.\n\n### Code:"
    },
    {
        "api": "llama-index",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install llama-cpp-python\n#!pip install llama-index html2text trafilatura\n\nfrom llama_index import GPTListIndex\nfrom llama_index import TrafilaturaWebReader\n\ndef scrape_and_index_robotics_info(url, *topics):\n    documents = TrafilaturaWebReader().load_data([url])\n    index = GPTListIndex.from_documents(documents)\n    engine = index.as_query_engine()\n\n    results = {}\n    for topic in topics:\n        results[topic] = engine.query(topic)\n\n    return results\n\nif __name__ == \"__main__\":\n    url = \"https://en.wikipedia.org/wiki/Robotics\"\n    topics = [\"History of robotics\", \"Types of robotics\"]\n    extracted_data = scrape_and_index_robotics_info(url, *topics)\n    \n    for topic, data in extracted_data.items():\n        print(f\"{topic}:\\n{data}\")",
        "instruction": "Develop a Python program using the 'llama-index' API to scrape and index data from 'https://en.wikipedia.org/wiki/Robotics.' Extract information about the history and types of robotics. Print the extracted information.",
        "prediction": "import requests\nimport json\nimport sys\nimport os\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nfrom config import *\n\ndef get_data(url):\n    response = requests.get(url)\n    return response.json()\n\ndef get_history(url):\n    history = get_data(url)\n    history_list = []\n    for history_item in history:\n        history_list.append(history_item['history'])\n    return history_list\n\ndef get_history_history(url):\n    history = get_history(url)\n    history_history_list = []\n    for history_item in history:\n        history_history_list.append(history_item['history'])\n    return history_history_list\n\ndef get_history_history_history(url):\n    history = get_history_history(url)\n    history_history_history_list = []\n    for history_item in history:\n        history_history_history_list.append(history_item['history'])\n    return history_history_history_list\n\ndef get_history_history_history_history(url):\n    history = get_history_history_history(url)\n    history_history_history_history_list = []\n    for history_item in history:\n        history_history_history_history_list.append(history_item['history'])\n    return history_history_history_history_list\n\ndef get_history_history_history_history_history(url):\n    history = get_history_history_history_history(url)\n    history_history_history_history_history_list = []\n    for history_item in history:\n        history_history_history_history_history_list.append(history_item['history'])\n    return history_history_history_history_history_list\n\ndef get_history_history_history_history_history_history(url):\n    history = get_history_history_history_history_history(url)\n    history_history_history_history_history_history_list = []\n    for history_item in history:\n        history_history_history_history_history_history_list.append(history_item['history'])\n    return history_history_history_history_history_history_list\n\ndef get_history_history_history_history_history_history_history(url):\n    history = get_history_history_history_history_history_history(url)\n    history_history_history_history_history_history_history_list = []\n    for history_item in history:\n        history_history_history_history_history_history_history_list.append(history_item['history'])\n    return history_history_history_history_history_history_history_list\n\ndef get_history_history_history_history_history_history_history_history(url):\n    history = get_history_history_history_history_history_history_history(url)\n    history_history_history_history_history_history_history_history_list = []\n    for history_item in history:\n        history_history_history_history_history_history_history_history_list.append(history_item['history'])\n    return history_history_history_history_history_history_history_history_list\n\ndef get_history_history_history_history_history_history_history_history_history(url):\n    history = get_history_history_history_history_history_history_history_history(url)\n    history_history_history_history_history_history_history_history_history_list = []\n    for history_item in history:\n        history_history_history_history_history_history_history_history_history_list.append(history_item['history'])\n    return history_history_history_history_history_history_history_history_history_list\n\ndef get_history_history_history_history_history_history_history_history_history_history(url):\n    history = get_history_history_history_history_history_history_history_history_history(url)\n    history_history_history_history_history_history_history_history_history_history_list = []\n    for history_item in history:\n        history_history_history_history",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program using the 'llama-index' API to scrape and index data from 'https://en.wikipedia.org/wiki/Robotics.' Extract information about the history and types of robotics. Print the extracted information.\n\n### Code:"
    },
    {
        "api": "llama-index",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install llama-cpp-python\n#!pip install llama-index html2text trafilatura\n\nfrom llama_index import GPTListIndex\nfrom llama_index import TrafilaturaWebReader\n\ndef scrape_and_index_machine_vision_info(url, *topics):\n    documents = TrafilaturaWebReader().load_data([url])\n    index = GPTListIndex.from_documents(documents)\n    engine = index.as_query_engine()\n\n    results = {}\n    for topic in topics:\n        results[topic] = engine.query(topic)\n\n    return results\n\nif __name__ == \"__main__\":\n    url = \"https://en.wikipedia.org/wiki/Machine_vision\"\n    topics = [\"History of machine vision\", \"Applications of machine vision\"]\n    extracted_data = scrape_and_index_machine_vision_info(url, *topics)\n\n    for topic, data in extracted_data.items():\n        print(f\"{topic}:\\n{data}\")",
        "instruction": "Write a Python program using the 'llama-index' API to scrape and index data from 'https://en.wikipedia.org/wiki/Machine_vision.' Extract information about the history and applications of machine vision. Print the extracted information.",
        "prediction": "import requests\nimport json\nimport sys\nimport os\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nfrom config import *\n\ndef get_data(url):\n    response = requests.get(url)\n    return response.json()\n\ndef get_data_from_api(url):\n    response = requests.get(url)\n    return response.json()\n\ndef get_data_from_api_with_param(url, param):\n    response = requests.get(url, params=param)\n    return response.json()\n\ndef get_data_from_api_with_param_and_header(url, param, header):\n    response = requests.get(url, params=param, headers=header)\n    return response.json()\n\ndef get_data_from_api_with_param_and_header_and_auth(url, param, header, auth):\n    response = requests.get(url, params=param, headers=header, auth=auth)\n    return response.json()\n\ndef get_data_from_api_with_param_and_header_and_auth_and_cert(url, param, header, auth, cert):\n    response = requests.get(url, params=param, headers=header, auth=auth, cert=cert)\n    return response.json()\n\ndef get_data_from_api_with_param_and_header_and_auth_and_cert_and_verify(url, param, header, auth, cert, verify):\n    response = requests.get(url, params=param, headers=header, auth=auth, cert=cert, verify=verify)\n    return response.json()\n\ndef get_data_from_api_with_param_and_header_and_auth_and_cert_and_verify_and_allow_redirects(url, param, header, auth, cert, verify, allow_redirects):\n    response = requests.get(url, params=param, headers=header, auth=auth, cert=cert, verify=verify, allow_redirects=allow_redirects)\n    return response.json()\n\ndef get_data_from_api_with_param_and_header_and_auth_and_cert_and_verify_and_allow_redirects_and_timeout(url, param, header, auth, cert, verify, allow_redirects, timeout):\n    response = requests.get(url, params=param, headers=header, auth=auth, cert=cert, verify=verify, allow_redirects=allow_redirects, timeout=timeout)\n    return response.json()\n\ndef get_data_from_api_with_param_and_header_and_auth_and_cert_and_verify_and_allow_redirects_and_timeout_and_proxies(url, param, header, auth, cert, verify, allow_redirects, timeout, proxies):\n    response = requests.get(url, params=param, headers=header, auth=auth, cert=cert, verify=verify, allow_redirects=allow_redirects, timeout=timeout, proxies=proxies)\n    return response.json()\n\ndef get_data_from_api_with_param_and_header_and_auth_and_cert_and_verify_and_allow_redirects_and_timeout_and_proxies_and_stream(url, param, header, auth, cert, verify, allow_redirects, timeout, proxies, stream):\n    response = requests.get(url, params=param, headers=header, auth=auth, cert=cert, verify=verify, allow_redirects=allow_redirects, timeout=timeout, proxies=proxies, stream=stream)\n    return response.json()\n\ndef get_data_from_api_with_param_and_header_and_auth_and_cert_and_verify_and_allow_redirects_and_timeout_and_proxies_and_stream_and_allow_redirects(url, param, header, auth, cert, verify, allow_redirects, timeout, proxies, stream, allow_redirects):\n    response = requests.get(url, params=param, headers=header, auth=",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nWrite a Python program using the 'llama-index' API to scrape and index data from 'https://en.wikipedia.org/wiki/Machine_vision.' Extract information about the history and applications of machine vision. Print the extracted information.\n\n### Code:"
    },
    {
        "api": "mlflow",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install mlflow\nimport mlflow\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\n# Generate a synthetic dataset\nX = np.random.rand(300, 2)\n\n# Start an MLflow run\nwith mlflow.start_run() as run:\n    # Train a K-means clustering model\n    kmeans = KMeans(n_clusters=3, random_state=42)\n    kmeans.fit(X)\n    \n    # Log hyperparameters\n    mlflow.log_params(kmeans.get_params())\n    \n    # Log the trained model\n    mlflow.sklearn.log_model(kmeans, \"kmeans_model\")\n",
        "instruction": "Develop a program that uses the MLflow API to train a K-means clustering model on a synthetic dataset. Log the model and its hyperparameters.",
        "prediction": "// https://leetcode.com/problems/k-closest-points-to-origin\n\nclass Solution:\n    def kClosest(self, points: List[List[int]], K: int) -> List[List[int]]:\n        \n        def dist(point):\n            return point[0]**2 + point[1]**2\n        \n        points.sort(key = dist)\n        return points[:K]\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        ",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a program that uses the MLflow API to train a K-means clustering model on a synthetic dataset. Log the model and its hyperparameters.\n\n### Code:"
    },
    {
        "api": "mlflow",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install mlflow\nimport mlflow\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n# Generate a synthetic binary classification dataset\nX = np.random.rand(100, 10)\ny = np.random.randint(2, size=100)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Start an MLflow run\nwith mlflow.start_run() as run:\n    # Train a logistic regression classifier\n    logreg = LogisticRegression(random_state=42)\n    logreg.fit(X_train, y_train)\n    \n    # Log hyperparameters\n    mlflow.log_params(logreg.get_params())\n    \n    # Log the trained model\n    mlflow.sklearn.log_model(logreg, \"logistic_regression_model\")\n",
        "instruction": "Develop a program that uses the MLflow API to train a logistic regression classifier on a synthetic binary classification dataset. Log the model and its hyperparameters.",
        "prediction": "// https://leetcode.com/problems/binary-tree-level-order-traversal-ii\n\n# Definition for a binary tree node.\n# class TreeNode:\n#     def __init__(self, x):\n#         self.val = x\n#         self.left = None\n#         self.right = None\n\nclass Solution:\n    def levelOrderBottom(self, root: TreeNode) -> List[List[int]]:\n        if not root:\n            return []\n        \n        res = []\n        queue = [root]\n        while queue:\n            level = []\n            for _ in range(len(queue)):\n                node = queue.pop(0)\n                level.append(node.val)\n                if node.left:\n                    queue.append(node.left)\n                if node.right:\n                    queue.append(node.right)\n            res.append(level)\n        return res[::-1]\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a program that uses the MLflow API to train a logistic regression classifier on a synthetic binary classification dataset. Log the model and its hyperparameters.\n\n### Code:"
    },
    {
        "api": "mlflow",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install mlflow\nimport mlflow\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load the Iris dataset\ndata = load_iris()\nX, y = data.data, data.target\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Start an MLflow run\nwith mlflow.start_run() as run:\n    # Train a Decision Tree classifier\n    clf = DecisionTreeClassifier()\n    clf.fit(X_train, y_train)\n    \n    # Log hyperparameters\n    mlflow.log_param(\"criterion\", clf.criterion)\n    mlflow.log_param(\"max_depth\", clf.max_depth)\n    \n    # Make predictions on the test set\n    y_pred = clf.predict(X_test)\n    \n    # Calculate and log accuracy as a metric\n    accuracy = accuracy_score(y_test, y_pred)\n    mlflow.log_metric(\"accuracy\", accuracy)\n    \n    # Save the model as an artifact\n    mlflow.sklearn.log_model(clf, \"decision_tree_model\")",
        "instruction": "Create a program that uses the MLflow API to train a simple decision tree classifier on the Iris dataset. Log the model, its hyperparameters, and accuracy as a metric. Save the model as an artifact.",
        "prediction": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n# Load dataset\ndata = pd.read_csv('data/data.csv')\n\n# Split dataset into training and test data\nX = data.iloc[:, :-1].values\ny = data.iloc[:, -1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Create and train model\nmodel = LogisticRegression(solver='liblinear', random_state=0)\nmodel.fit(X_train, y_train)\n\n# Predict test data\ny_pred = model.predict(X_test)\n\n# Print accuracy score\nprint('Accuracy score: ', accuracy_score(y_test, y_pred))\n\n# Print confusion matrix\nprint('Confusion matrix: \\n', confusion_matrix(y_test, y_pred))\n\n# Print classification report\nprint('Classification report: \\n', classification_report(y_test, y_pred))\n\n# Plot confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(8, 8))\nax.imshow(cm)\nax.grid(False)\nax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\nax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\nax.set_ylim(1.5, -0.5)\nfor i in range(2):\n    for j in range(2):\n        ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\nplt.show()\n\n# Plot ROC curve\ny_pred_proba = model.predict_proba(X_test)[::, 1]\nfpr, tpr, _ = roc_curve(y_test, y_pred_proba)\nroc_auc = auc(fpr, tpr)\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a program that uses the MLflow API to train a simple decision tree classifier on the Iris dataset. Log the model, its hyperparameters, and accuracy as a metric. Save the model as an artifact.\n\n### Code:"
    },
    {
        "api": "more-itertools",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install more-itertools\nfrom more_itertools import consecutive_groups\n\nstrings = [\"123\", \"456\", \"789\", \"abc\", \"def\", \"ghi\", \"123456789\"]\npartitioned_sublists = list(consecutive_groups(strings))\nfiltered_sublists = [sublist for sublist in partitioned_sublists if all(word.isdigit() for word in sublist)]\nlengths = [len(sublist) for sublist in filtered_sublists]\n\nprint(\"Filtered Sublists:\")\nfor sublist in filtered_sublists:\n    print(list(sublist))\n\nprint(\"Lengths:\")\nprint(lengths)",
        "instruction": "Create a Python program using the 'more-itertools' API to demonstrate the grouping and filtering of elements in a list based on a condition. The program should take a list of strings, group them into consecutive sublists using the 'consecutive_groups' function from 'more-itertools', and filter out sublists that contain only strings with all numeric characters. Finally, print the filtered sublists and calculate the length of each sublist and print the lengths.",
        "prediction": "from collections import defaultdict\nfrom more_itertools import consecutive_groups\n\ndef consecutive_groups(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_2(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_3(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_4(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_5(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_6(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_7(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_8(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_9(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_10(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_11(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_12(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_13(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_14(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_15(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_16(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_17(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_18(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_19(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_20(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_21(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_22(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_23(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_24(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_25(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_26(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_27(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_28(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_29(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_30(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_31(lst):\n    return consecutive_groups(l",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'more-itertools' API to demonstrate the grouping and filtering of elements in a list based on a condition. The program should take a list of strings, group them into consecutive sublists using the 'consecutive_groups' function from 'more-itertools', and filter out sublists that contain only strings with all numeric characters. Finally, print the filtered sublists and calculate the length of each sublist and print the lengths.\n\n### Code:"
    },
    {
        "api": "more-itertools",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install more-itertools\nfrom more_itertools import consecutive_groups\n\nstrings = [\"APPLE\", \"BANANA\", \"CHERRY\", \"DATE\", \"ELDERBERRY\", \"FIG\", \"GRAPE\"]\npartitioned_sublists = list(consecutive_groups(strings))\nfiltered_sublists = [sublist for sublist in partitioned_sublists if all(word.isupper() for word in sublist)]\nlengths = [len(sublist) for sublist in filtered_sublists]\n\nprint(\"Filtered Sublists:\")\nfor sublist in filtered_sublists:\n    print(list(sublist))\n\nprint(\"Lengths:\")\nprint(lengths)",
        "instruction": "Create a Python program using the 'more-itertools' API to demonstrate the grouping and filtering of elements in a list based on a condition. The program should take a list of strings, group them into consecutive sublists using the 'consecutive_groups' function from 'more-itertools', and filter out sublists that contain only strings with all uppercase letters. Finally, print the filtered sublists and calculate the length of each sublist and print the lengths.",
        "prediction": "from collections import defaultdict\nfrom more_itertools import consecutive_groups\n\ndef consecutive_groups(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_2(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_3(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_4(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_5(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_6(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_7(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_8(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_9(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_10(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_11(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_12(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_13(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_14(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_15(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_16(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_17(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_18(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_19(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_20(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_21(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_22(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_23(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_24(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_25(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_26(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_27(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_28(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_29(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_30(lst):\n    return consecutive_groups(lst, consecutive=lambda a, b: a == b)\n\ndef consecutive_groups_31(lst):\n    return consecutive_groups(l",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'more-itertools' API to demonstrate the grouping and filtering of elements in a list based on a condition. The program should take a list of strings, group them into consecutive sublists using the 'consecutive_groups' function from 'more-itertools', and filter out sublists that contain only strings with all uppercase letters. Finally, print the filtered sublists and calculate the length of each sublist and print the lengths.\n\n### Code:"
    },
    {
        "api": "numpy",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install numpy\nimport numpy as np\n\n# Generate a random matrix of size (3x3)\noriginal_matrix = np.random.rand(3, 3)\nprint(\"Original Matrix: \", original_matrix)\n\n# Compute the matrix's transpose\ntransposed_matrix = np.transpose(original_matrix)\nprint(\"Transposed Matrix: \", transposed_matrix)\n\n# Calculate the matrix product\nmatrix_product = np.dot(original_matrix, transposed_matrix)\nprint(\"Matrix Product: \", matrix_product)",
        "instruction": "Create a Python program that uses the 'numpy' API to perform matrix operations, including generating a random matrix, computing its transpose, and calculating the matrix product.",
        "prediction": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef generate_random_matrix(n, m):\n    return np.random.randint(0, 10, (n, m))\n\ndef transpose_matrix(matrix):\n    return np.transpose(matrix)\n\ndef matrix_product(matrix_a, matrix_b):\n    return np.matmul(matrix_a, matrix_b)\n\ndef matrix_transpose_product(matrix_a, matrix_b):\n    return np.matmul(matrix_a, transpose_matrix(matrix_b))\n\ndef matrix_matrix_product(matrix_a, matrix_b):\n    return np.matmul(matrix_a, matrix_b)\n\ndef matrix_matrix_transpose_product(matrix_a, matrix_b):\n    return np.matmul(matrix_a, transpose_matrix(matrix_b))\n\ndef matrix_matrix_product_with_transpose(matrix_a, matrix_b):\n    return np.matmul(matrix_a, matrix_b)\n\ndef matrix_matrix_product_with_transpose_and_matrix_product(matrix_a, matrix_b):\n    return np.matmul(matrix_a, matrix_b)\n\ndef matrix_matrix_product_with_transpose_and_matrix_matrix_product(matrix_a, matrix_b):\n    return np.matmul(matrix_a, matrix_b)\n\ndef matrix_matrix_product_with_transpose_and_matrix_matrix_transpose_product(matrix_a, matrix_b):\n    return np.matmul(matrix_a, transpose_matrix(matrix_b))\n\ndef matrix_matrix_product_with_transpose_and_matrix_matrix_transpose_product_with_matrix_product(matrix_a, matrix_b):\n    return np.matmul(matrix_a, transpose_matrix(matrix_b))\n\ndef matrix_matrix_product_with_transpose_and_matrix_matrix_transpose_product_with_matrix_matrix_product(matrix_a, matrix_b):\n    return np.matmul(matrix_a, transpose_matrix(matrix_b))\n\ndef matrix_matrix_product_with_transpose_and_matrix_matrix_transpose_product_with_matrix_matrix_transpose_product(matrix_a, matrix_b):\n    return np.matmul(matrix_a, transpose_matrix(matrix_b))\n\ndef matrix_matrix_product_with_transpose_and_matrix_matrix_transpose_product_with_matrix_matrix_transpose_product_with_matrix_product(matrix_a, matrix_b):\n    return np.matmul(matrix_a, transpose_matrix(matrix_b))\n\ndef matrix_matrix_product_with_transpose_and_matrix_matrix_transpose_product_with_matrix_matrix_transpose_product_with_matrix_matrix_product(matrix_a, matrix_b):\n    return np.matmul(matrix_a, transpose_matrix(matrix_b))\n\ndef matrix_matrix_product_with_transpose_and_matrix_matrix_transpose_product_with_matrix_matrix_transpose_product_with_matrix_matrix_transpose_product(matrix_a, matrix_b):\n    return np.matmul(matrix_a, transpose_matrix(matrix_b))\n\ndef matrix_matrix_product_with_transpose_and_matrix_matrix_transpose_product_with_matrix_matrix_transpose_product_with_matrix_matrix_transpose_product_with_matrix_product(matrix_a, matrix_b):\n    return np.matmul(matrix_a, transpose_matrix(matrix_b))\n\ndef matrix_matrix_product_with_transpose_and_matrix",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that uses the 'numpy' API to perform matrix operations, including generating a random matrix, computing its transpose, and calculating the matrix product.\n\n### Code:"
    },
    {
        "api": "numpy",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install numpy\nimport numpy as np\nimport csv\n\n# Read the CSV file\ndata = []\nwith open('data.csv', 'r') as file:\n    reader = csv.reader(file)\n    for row in reader:\n        data.append(row)\n\n# Convert the data to a numpy array\ndata_array = np.array(data, dtype=float)\n\n# Calculate the mean of each column\nmean = np.mean(data_array, axis=0)\nprint(\"Mean: \", mean)\n\n# Calculate the median of each column\nmedian = np.median(data_array, axis=0)\nprint(\"Median: \", median)\n\n# Calculate the standard deviation of each column\nstd_dev = np.std(data_array, axis=0)\nprint(\"Standard Deviation: \", std_dev)",
        "instruction": "Create a Python program that uses the 'numpy' API to perform statistical operations on a given dataset. The program should read a CSV file containing numerical data, calculate the mean, median, and standard deviation of each column, and display the results.",
        "prediction": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef read_file(filename):\n    data = np.loadtxt(filename)\n    return data\n\ndef calculate_mean(data):\n    mean = np.mean(data, axis=0)\n    return mean\n\ndef calculate_covariance(data):\n    covariance = np.cov(data, rowvar=False)\n    return covariance\n\ndef calculate_eigenvalues_and_eigenvectors(covariance):\n    eigenvalues, eigenvectors = np.linalg.eig(covariance)\n    return eigenvalues, eigenvectors\n\ndef calculate_variance(eigenvalues):\n    variance = np.sqrt(eigenvalues)\n    return variance\n\ndef calculate_eigen_projection(eigenvectors, data):\n    eigen_projection = np.dot(eigenvectors.T, data.T)\n    return eigen_projection\n\ndef calculate_eigen_projection_mean(eigen_projection):\n    eigen_projection_mean = np.mean(eigen_projection, axis=1)\n    return eigen_projection_mean\n\ndef calculate_eigen_projection_median(eigen_projection):\n    eigen_projection_median = np.median(eigen_projection, axis=1)\n    return eigen_projection_median\n\ndef calculate_eigen_projection_std(eigen_projection):\n    eigen_projection_std = np.std(eigen_projection, axis=1)\n    return eigen_projection_std\n\ndef calculate_eigen_projection_skew(eigen_projection):\n    eigen_projection_skew = np.std(eigen_projection, axis=1)\n    return eigen_projection_skew\n\ndef calculate_eigen_projection_kurtosis(eigen_projection):\n    eigen_projection_kurtosis = np.std(eigen_projection, axis=1)\n    return eigen_projection_kurtosis\n\ndef calculate_eigen_projection_skew_and_kurtosis(eigen_projection):\n    eigen_projection_skew_and_kurtosis = np.std(eigen_projection, axis=1)\n    return eigen_projection_skew_and_kurtosis\n\ndef calculate_eigen_projection_mean_and_median(eigen_projection):\n    eigen_projection_mean_and_median = np.mean(eigen_projection, axis=1)\n    return eigen_projection_mean_and_median\n\ndef calculate_eigen_projection_mean_and_median_and_std(eigen_projection):\n    eigen_projection_mean_and_median_and_std = np.std(eigen_projection, axis=1)\n    return eigen_projection_mean_and_median_and_std\n\ndef calculate_eigen_projection_mean_and_median_and_std_and_skew(eigen_projection):\n    eigen_projection_mean_and_median_and_std_and_skew = np.std(eigen_projection, axis=1)\n    return eigen_projection_mean_and_median_and_std_and_skew\n\ndef calculate_eigen_projection_mean_and_median_and_std_and_skew_and_kurtosis(eigen_projection):\n    eigen_projection_mean_and_median_and_std_and_skew_and_kurtosis = np.std(eigen_projection, axis=1)\n    return eigen_projection_mean_and_median_and_std_and_skew_and_kurtosis\n\ndef calculate_eigen_projection_mean_and_median_and_std_and_skew_and_kurtosis_and_max(eigen_projection):\n    eigen_projection_mean_and_median_and_std_and_",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that uses the 'numpy' API to perform statistical operations on a given dataset. The program should read a CSV file containing numerical data, calculate the mean, median, and standard deviation of each column, and display the results.\n\n### Code:"
    },
    {
        "api": "optuna",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install optuna scikit-learn\nimport optuna\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import f1_score\n\n# Generate a custom dataset\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n\n# Define the machine learning task: Random Forest Classifier\ndef objective(trial):\n    # Define hyperparameters to search\n    n_estimators = trial.suggest_int(\"n_estimators\", 50, 200)\n    max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n\n    # Create and evaluate the random forest classifier with the chosen hyperparameters\n    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n    f1_scores = cross_val_score(clf, X, y, cv=5, scoring='f1')\n\n    return np.mean(f1_scores)\n\n# Create a study and optimize hyperparameters\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=50)\n\n    # Get the best hyperparameters\n    best_params = study.best_params\n    n_estimators = best_params[\"n_estimators\"]\n    max_depth = best_params[\"max_depth\"]\n\n    print(\"Best Hyperparameters:\", best_params)\n\n    # Create and evaluate the random forest classifier with the best hyperparameters\n    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n    f1_scores = cross_val_score(clf, X, y, cv=5, scoring='f1')\n    mean_f1_score = np.mean(f1_scores)\n\n    print(\"Cross-Validated F1 Score with Best Hyperparameters:\", mean_f1_score)\n",
        "instruction": "Create a Python program using the 'optuna' API to optimize the hyperparameters of a random forest classifier. The program should search for the best combination of 'n_estimators' and 'max_depth' hyperparameters for a random forest classifier on a custom dataset and print the cross-validated F1 score.",
        "prediction": "import optuna\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\n\ndef objective(trial):\n    param = {\n        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n       'max_depth': trial.suggest_int('max_depth', 2, 10),\n       'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n       'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n        'n_jobs': -1\n    }\n    clf = RandomForestClassifier(**param)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    clf.fit(X_train, y_train)\n    pred = clf.predict(X_test)\n    f1 = f1_score(y_test, pred)\n    return f1\n\nif __name__ == '__main__':\n    df = pd.read_csv('../data/train_data.csv')\n    X = df.drop(['Survived'], axis=1)\n    y = df['Survived']\n    study = optuna.create_study()\n    study.optimize(objective, n_trials=100)\n    print(study.best_params)\n    print(study.best_value)\n    print(study.best_trial)\n    print(study.trials)\n    print(study.trials[0].params)\n    print(study.trials[0].value)\n    print(study.trials[0].state)\n    print(study.trials[0].user_attrs)\n    print(study.trials[0].system_attrs)\n    print(study.trials[0].intermediate_values)\n    print(study.trials[0].params)\n    print(study.trials[0].datetime_start)\n    print(study.trials[0].datetime_complete)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[0].params)\n    print(study.trials[",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'optuna' API to optimize the hyperparameters of a random forest classifier. The program should search for the best combination of 'n_estimators' and 'max_depth' hyperparameters for a random forest classifier on a custom dataset and print the cross-validated F1 score.\n\n### Code:"
    },
    {
        "api": "optuna",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install optuna scikit-learn\nimport optuna\nimport numpy as np\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.datasets import make_classification\n\n# Generate a custom dataset\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n\n# Define the machine learning task: Linear Discriminant Analysis (LDA)\ndef objective(trial):\n    # Define hyperparameters to search\n    n_components = trial.suggest_int(\"n_components\", 1, 20)\n    solver = trial.suggest_categorical(\"solver\", [\"svd\", \"lsqr\", \"eigen\"])\n\n    # Create and evaluate the LDA model with the chosen hyperparameters\n    lda = LinearDiscriminantAnalysis(n_components=n_components, solver=solver)\n    lda.fit(X, y)\n    explained_variance = np.sum(lda.explained_variance_ratio_)\n\n    return explained_variance\n\n# Create a study and optimize hyperparameters\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=50)\n\n    # Get the best hyperparameters\n    best_params = study.best_params\n    n_components = best_params[\"n_components\"]\n    solver = best_params[\"solver\"]\n\n    print(\"Best Hyperparameters:\", best_params)\n\n    # Create and evaluate the LDA model with the best hyperparameters\n    lda = LinearDiscriminantAnalysis(n_components=n_components, solver=solver)\n    lda.fit(X, y)\n    explained_variance = np.sum(lda.explained_variance_ratio_\n\n)\n\n    print(\"Explained Variance Ratio with Best Hyperparameters:\", explained_variance)\n",
        "instruction": "Create a Python program using the 'optuna' API to optimize the hyperparameters of a linear discriminant analysis (LDA) model. The program should search for the best combination of 'n_components' and 'solver' hyperparameters for an LDA model on a custom dataset and print the explained variance ratio.",
        "prediction": "import optuna\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_error\n\ndef objective(trial):\n    data = pd.read_csv('../data/train.csv')\n    data = data.drop(['Id'], axis=1)\n    X = data.drop(['SalePrice'], axis=1)\n    y = data['SalePrice']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    model = Lasso(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    return mean_squared_error(y_test, pred)\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=100)\nprint(study.best_params)\nprint(study.best_value)\nprint(study.best_trial)\n",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'optuna' API to optimize the hyperparameters of a linear discriminant analysis (LDA) model. The program should search for the best combination of 'n_components' and 'solver' hyperparameters for an LDA model on a custom dataset and print the explained variance ratio.\n\n### Code:"
    },
    {
        "api": "pandas",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install pandas\nimport pandas as pd\n\n# Read the JSON file into a DataFrame\ndf = pd.read_json('sales_data.json')\n\n# Calculate total sales revenue for each product\nproduct_sales = df.groupby('Product')['Quantity'].sum() * df.groupby('Product')['Price'].sum()\nproduct_sales = product_sales.reset_index()\nproduct_sales.columns = ['Product', 'Total Revenue']\n\n# Identify the product with the highest revenue\nhighest_revenue_product = product_sales[product_sales['Total Revenue'] == product_sales['Total Revenue'].max()]\n\n# Create a summary DataFrame\nsummary_df = pd.concat([highest_revenue_product, product_sales], axis=1)\n\nprint(\"Summary Dataframe: \")\nprint(summary_df)",
        "instruction": "Create a Python program that uses the 'pandas' API to read a JSON file containing sales data, calculate total sales revenue for each product, identify the product with the highest revenue, and create a summary DataFrame.",
        "prediction": "import pandas as pd\nimport numpy as np\nimport json\nimport requests\nimport time\n\ndef get_sales_data(file_path):\n    data = pd.read_csv(file_path)\n    return data\n\ndef get_sales_data_from_api(file_path):\n    data = pd.read_json(file_path)\n    return data\n\ndef get_sales_data_from_api_v2(file_path):\n    data = pd.read_json(file_path, lines=True)\n    return data\n\ndef get_sales_data_from_api_v3(file_path):\n    data = pd.read_json(file_path, lines=True, orient='records')\n    return data\n\ndef get_sales_data_from_api_v4(file_path):\n    data = pd.read_json(file_path, lines=True, orient='records', convert_dates=['Date'])\n    return data\n\ndef get_sales_data_from_api_v5(file_path):\n    data = pd.read_json(file_path, lines=True, orient='records', convert_dates=['Date'], date_unit='s')\n    return data\n\ndef get_sales_data_from_api_v6(file_path):\n    data = pd.read_json(file_path, lines=True, orient='records', convert_dates=['Date'], date_unit='s')\n    return data\n\ndef get_sales_data_from_api_v7(file_path):\n    data = pd.read_json(file_path, lines=True, orient='records', convert_dates=['Date'], date_unit='s')\n    return data\n\ndef get_sales_data_from_api_v8(file_path):\n    data = pd.read_json(file_path, lines=True, orient='records', convert_dates=['Date'], date_unit='s')\n    return data\n\ndef get_sales_data_from_api_v9(file_path):\n    data = pd.read_json(file_path, lines=True, orient='records', convert_dates=['Date'], date_unit='s')\n    return data\n\ndef get_sales_data_from_api_v10(file_path):\n    data = pd.read_json(file_path, lines=True, orient='records', convert_dates=['Date'], date_unit='s')\n    return data\n\ndef get_sales_data_from_api_v11(file_path):\n    data = pd.read_json(file_path, lines=True, orient='records', convert_dates=['Date'], date_unit='s')\n    return data\n\ndef get_sales_data_from_api_v12(file_path):\n    data = pd.read_json(file_path, lines=True, orient='records', convert_dates=['Date'], date_unit='s')\n    return data\n\ndef get_sales_data_from_api_v13(file_path):\n    data = pd.read_json(file_path, lines=True, orient='records', convert_dates=['Date'], date_unit='s')\n    return data\n\ndef get_sales_data_from_api_v14(file_path):\n    data = pd.read_json(file_path, lines=True, orient='records', convert_dates=['Date'], date_unit='s')\n    return data\n\ndef get_sales_data_from_api_v15(file_path):\n    data = pd.read_json(file_path, lines=True, orient='records', convert_dates=['Date'], date_unit='s')\n    return data\n\ndef get_sales_data_from_api_v16(file_path):\n    data = pd.read_json(file_path, lines=True, orient='records', convert_dates=['Date'], date_unit='s')\n    return data\n\ndef get_sales_data_from_api_v17(file_path):\n    data = pd",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that uses the 'pandas' API to read a JSON file containing sales data, calculate total sales revenue for each product, identify the product with the highest revenue, and create a summary DataFrame.\n\n### Code:"
    },
    {
        "api": "pandas",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install pandas\nimport pandas as pd\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv('sales_data.csv')\n\n# Calculate the total sales revenue for each customer\ncustomer_total_revenue = df.groupby('Customer')['Revenue'].sum()\ncustomer_total_revenue = customer_total_revenue.reset_index()\ncustomer_total_revenue.columns = ['Customer', 'Total Revenue']\n\n# Identify the customer with the highest sales revenue\nhighest_total_revenue_customer = customer_total_revenue[customer_total_revenue['Total Revenue'] == customer_total_revenue['Total Revenue'].max()]\n\n# Create a summary DataFrame\nsummary_df = pd.concat([highest_total_revenue_customer, customer_total_revenue], axis=1)\n\nprint(\"Summary Dataframe: \")\nprint(summary_df)",
        "instruction": "Create a Python program that uses the 'pandas' API to read a CSV file containing sales data, calculate the total sales revenue for each customer, identify the customer with the highest sales revenue, and create a summary DataFrame.",
        "prediction": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('sales_data.csv')\n\ndf['total_revenue'] = df['total_revenue'].str.replace('$', '')\ndf['total_revenue'] = df['total_revenue'].str.replace(',', '')\n\ndf['total_revenue'] = df['total_revenue'].astype(float)\n\ndf['total_sales'] = df['total_sales'].str.replace('$', '')\ndf['total_sales'] = df['total_sales'].str.replace(',', '')\n\ndf['total_sales'] = df['total_sales'].astype(float)\n\ndf['avg_sales'] = df['total_sales'] / df['total_customers']\n\ndf['avg_sales'] = df['avg_sales'].astype(float)\n\ndf['avg_sales'] = df['avg_sales'].round(2)\n\ndf['avg_sales'] = df['avg_sales'].astype(str)\n\ndf['avg_sales'] = df['avg_sales'].str.replace('.', '')\n\ndf['avg_sales'] = df['avg_sales'].str.replace('nan', '0')\n\ndf['avg_sales'] = df['avg_sales'].astype(float)\n\ndf['avg_sales'] = df['avg_sales'].round(2)\n\ndf['avg_sales'] = df['avg_sales'].astype(str)\n\ndf['avg_sales'] = df['avg_sales'].str.replace('.', '')\n\ndf['avg_sales'] = df['avg_sales'].str.replace('nan', '0')\n\ndf['avg_sales'] = df['avg_sales'].astype(float)\n\ndf['avg_sales'] = df['avg_sales'].round(2)\n\ndf['avg_sales'] = df['avg_sales'].astype(str)\n\ndf['avg_sales'] = df['avg_sales'].str.replace('.', '')\n\ndf['avg_sales'] = df['avg_sales'].str.replace('nan', '0')\n\ndf['avg_sales'] = df['avg_sales'].astype(float)\n\ndf['avg_sales'] = df['avg_sales'].round(2)\n\ndf['avg_sales'] = df['avg_sales'].astype(str)\n\ndf['avg_sales'] = df['avg_sales'].str.replace('.', '')\n\ndf['avg_sales'] = df['avg_sales'].str.replace('nan', '0')\n\ndf['avg_sales'] = df['avg_sales'].astype(float)\n\ndf['avg_sales'] = df['avg_sales'].round(2)\n\ndf['avg_sales'] = df['avg_sales'].astype(str)\n\ndf['avg_sales'] = df['avg_sales'].str.replace('.', '')\n\ndf['avg_sales'] = df['avg_sales'].str.replace('nan', '0')\n\ndf['avg_sales'] = df['avg_sales'].astype(float)\n\ndf['avg_sales'] = df['avg_sales'].round(2)\n\ndf['avg_sales'] = df['avg_sales'].astype(str)\n\ndf['avg_sales'] = df['avg_sales'].str.replace('.', '')\n\ndf['avg_sales'] = df['avg_sales'].str.replace('nan', '0')\n\ndf['avg_sales'] = df['avg_sales'].astype(",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that uses the 'pandas' API to read a CSV file containing sales data, calculate the total sales revenue for each customer, identify the customer with the highest sales revenue, and create a summary DataFrame.\n\n### Code:"
    },
    {
        "api": "pandas",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install pandas\nimport pandas as pd\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv('customer_data.csv')\n\n# Calculate the average age for each occupation\noccupation_avg_age = df.groupby('Occupation')['Age'].mean()\noccupation_avg_age = occupation_avg_age.reset_index()\noccupation_avg_age.columns = ['Occupation', 'Average Age']\n\n# Identify the occupation with the highest average age\nhighest_avg_age_occupation = occupation_avg_age[occupation_avg_age['Average Age'] == occupation_avg_age['Average Age'].max()]\n\n# Create a summary DataFrame\nsummary_df = pd.concat([highest_avg_age_occupation, occupation_avg_age], axis=1)\n\nprint(\"Summary Dataframe: \")\nprint(summary_df)",
        "instruction": "Create a Python program that uses the 'pandas' API to read a CSV file containing customer data, calculate the average age for each occupation, identify the occupation with the highest average age, and create a summary DataFrame.",
        "prediction": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('data/customer_data.csv')\ndf.head()\n\ndf.describe()\n\ndf.info()\n\ndf.columns\n\ndf.columns = ['ID', 'Name', 'Age', 'Gender', 'Address', 'City', 'State', 'Zip', 'Phone', 'Email', 'SIN', 'CreditCard', 'Data']\n\ndf.head()\n\ndf.columns\n\ndf.Age.hist()\n\ndf.Age.mean()\n\ndf.Age.median()\n\ndf.Age.mode()\n\ndf.Age.std()\n\ndf.Age.var()\n\ndf.Age.skew()\n\ndf.Age.kurt()\n\ndf.Age.quantile(0.25)\n\ndf.Age.quantile(0.75)\n\ndf.Age.quantile(0.50)\n\ndf.Age.quantile(0.95)\n\ndf.Age.quantile(0.05)\n\ndf.Age.quantile(0.01)\n\ndf.Age.quantile(0.99)\n\ndf.Age.quantile(0.50, interpolation='midpoint')\n\ndf.Age.quantile(0.50, interpolation='nearest')\n\ndf.Age.quantile(0.50, interpolation='linear')\n\ndf.Age.quantile(0.50, interpolation='lower')\n\ndf.Age.quantile(0.50, interpolation='higher')\n\ndf.Age.quantile(0.50, interpolation='midpoint')\n\ndf.Age.quantile(0.50, interpolation='nearest')\n\ndf.Age.quantile(0.50, interpolation='linear')\n\ndf.Age.quantile(0.50, interpolation='lower')\n\ndf.Age.quantile(0.50, interpolation='higher')\n\ndf.Age.quantile(0.50, interpolation='midpoint')\n\ndf.Age.quantile(0.50, interpolation='nearest')\n\ndf.Age.quantile(0.50, interpolation='linear')\n\ndf.Age.quantile(0.50, interpolation='lower')\n\ndf.Age.quantile(0.50, interpolation='higher')\n\ndf.Age.quantile(0.50, interpolation='midpoint')\n\ndf.Age.quantile(0.50, interpolation='nearest')\n\ndf.Age.quantile(0.50, interpolation='linear')\n\ndf.Age.quantile(0.50, interpolation='lower')\n\ndf.Age.quantile(0.50, interpolation='higher')\n\ndf.Age.quantile(0.50, interpolation='midpoint')\n\ndf.Age.quantile(0.50, interpolation='nearest')\n\ndf.Age.quantile(0.50, interpolation='linear')\n\ndf.Age.quantile(0.50, interpolation='lower')\n\ndf.Age.quantile(0.50, interpolation='higher')\n\ndf.Age.quantile(0.50, interpolation='midpoint')\n\ndf.Age.quantile(0.50, interpolation='nearest')\n\ndf.Age.quantile(0.50, interpolation='linear')\n\ndf.Age.quantile(0.50, interpolation='lower')\n\ndf.Age.quantile(0.50, interpolation='higher')\n\ndf.Age.quantile(0.50, interpolation='midpoint')\n\ndf.Age.quantile(0.50, interpolation='nearest')\n\ndf.Age.quantile(0.50, interpolation='linear')\n\ndf.Age.quantile(0.50, interpolation='lower')\n\ndf.Age.quantile(0.50, interpolation='higher')\n\ndf.Age.quantile(0.50, interpolation='midpoint')\n\ndf.Age.quantile(0.50, interpolation='nearest')\n\ndf.Age.quantile(0.50, interpolation='linear')\n\ndf.Age.quantile(0.50, interpolation='lower')\n\ndf.Age.quantile(0.50, interpolation='higher')\n\n",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that uses the 'pandas' API to read a CSV file containing customer data, calculate the average age for each occupation, identify the occupation with the highest average age, and create a summary DataFrame.\n\n### Code:"
    },
    {
        "api": "peft",
        "decoder_input_ids": false,
        "ground-truth": "# Install necessary libraries\n#!pip install transformers\n#!pip install peft\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Load Models\nbase_model = \"gpt-3.5-turbo\"\npeft_model = \"gpt-3.5-turbo\"\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(base_model)\n\n# Load peft\nmodel = PeftModel.from_pretrained(model, peft_model)\nmodel = model.eval()\n\n# Generate code for a language translation application\ntranslation_app_code = model.generate_translation_app_code(max_length=500)\n\n# Print generated language translation application code\nprint(translation_app_code)",
        "instruction": "Create a Python program that utilizes the 'peft' API to generate code for a basic language translation application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a language translation application that can translate text between different languages.",
        "prediction": "import peft\nimport peft.pe as pe\nimport peft.pe.gpt as gpt\nimport peft.pe.gpt.gpt_pb2 as gpt_pb2\nimport peft.pe.gpt.gpt_pb2_grpc as gpt_pb2_grpc\nimport peft.pe.gpt.gpt_pb2_grpc as gpt_pb2_grpc\nimport peft.pe.gpt.gpt_pb2 as gpt_pb2\nimport peft.pe.gpt.gpt_pb2_grpc as gpt_pb2_grpc\nimport peft.pe.gpt.gpt_pb2 as gpt_pb2\nimport peft.pe.gpt.gpt_pb2_grpc as gpt_pb2_grpc\nimport peft.pe.gpt.gpt_pb2 as gpt_pb2\nimport peft.pe.gpt.gpt_pb2_grpc as gpt_pb2_grpc\nimport peft.pe.gpt.gpt_pb2 as gpt_pb2\nimport peft.pe.gpt.gpt_pb2_grpc as gpt_pb2_grpc\nimport peft.pe.gpt.gpt_pb2 as gpt_pb2\nimport peft.pe.gpt.gpt_pb2_grpc as gpt_pb2_grpc\nimport peft.pe.gpt.gpt_pb2 as gpt_pb2\nimport peft.pe.gpt.gpt_pb2_grpc as gpt_pb2_grpc\nimport peft.pe.gpt.gpt_pb2 as gpt_pb2\nimport peft.pe.gpt.gpt_pb2_grpc as gpt_pb2_grpc\nimport peft.pe.gpt.gpt_pb2 as gpt_pb2\nimport peft.pe.gpt.gpt_pb2_grpc as gpt_pb2_grpc\nimport peft.pe.gpt.gpt_pb2 as gpt_pb2\nimport peft.pe.gpt.gpt_pb2_grpc as gpt_pb2_grpc\nimport peft.pe.gpt.gpt_pb2 as gpt_pb2\nimport peft.pe.gpt.gpt_pb2_grpc as gpt_pb2_grpc\nimport peft.pe.gpt.gpt_pb2 as gpt_pb2\nimport peft.pe.gpt.gpt_pb2_grpc as gpt_pb2_grpc\nimport peft.pe.gpt.gpt_pb2 as gpt_pb2\nimport peft.pe.gpt.gpt_pb2_grpc as gpt_pb2_grpc\nimport peft.pe.gpt.gpt_pb2 as gpt_pb2\nimport peft.pe.gpt.gpt_pb2_grpc as gpt_pb2_grpc\nimport peft.pe.gpt.gpt_pb2 as gpt_pb2\nimport peft.pe.gpt.gpt_pb2_grpc as gpt_pb2_grpc\nimport peft.pe.gpt.gpt_pb2 as gpt_pb2\nimport peft.pe.gpt.gpt_pb2_grpc as gpt_pb2_grpc\nimport peft.pe.gpt.gpt_pb2 as gpt_pb2\nimport peft.pe.gpt.gpt_pb2_grpc as gpt_pb2_grpc\nimport peft.pe.gpt.gpt_pb2 as gpt_pb2\nimport peft.pe.gpt.gpt_pb2_grpc as gpt_pb2_grpc\nimport peft.pe.gpt.gpt_pb2 as gpt_pb2\nimport peft.pe.gpt.gpt_pb2_grpc as gpt_pb2_grpc\nimport peft.pe.gpt.gpt_pb2 as gpt_pb2\nimport peft.pe.gpt.gpt_pb2_grpc as gpt_pb2_grpc\nimport peft.pe.gpt.gpt_pb",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that utilizes the 'peft' API to generate code for a basic language translation application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a language translation application that can translate text between different languages.\n\n### Code:"
    },
    {
        "api": "peft",
        "decoder_input_ids": false,
        "ground-truth": "# Install necessary libraries\n#!pip install transformers\n#!pip install peft\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Load Models\nbase_model = \"gpt-3.5-turbo\"\npeft_model = \"gpt-3.5-turbo\"\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(base_model)\n\n# Load peft\nmodel = PeftModel.from_pretrained(model, peft_model)\nmodel = model.eval()\n\n# Generate code for a basic e-commerce website\necommerce_website_code = model.generate_ecommerce_website_code(max_length=500)\n\n# Print generated e-commerce website code\nprint(ecommerce_website_code)",
        "instruction": "Develop a Python program using the 'peft' API to generate code for a basic e-commerce website. Utilize the 'gpt-3.5-turbo' model for website code generation. The program should generate code for a simple e-commerce website with product listings, a shopping cart, and checkout functionality.",
        "prediction": "import requests\nimport json\n\nfrom. import models\n\ndef get_products(category_id):\n    url = 'https://api.peft.tech/v1/products'\n    params = {\n        'category_id': category_id,\n        'limit': 100,\n        'offset': 0\n    }\n    response = requests.get(url, params=params)\n    products = json.loads(response.text)\n    return products['data']\n\ndef get_product_details(product_id):\n    url = 'https://api.peft.tech/v1/products/{}'.format(product_id)\n    response = requests.get(url)\n    product = json.loads(response.text)\n    return product['data']\n\ndef get_product_details_by_sku(sku):\n    url = 'https://api.peft.tech/v1/products/{}'.format(sku)\n    response = requests.get(url)\n    product = json.loads(response.text)\n    return product['data']\n\ndef get_product_details_by_sku_and_store(sku, store_id):\n    url = 'https://api.peft.tech/v1/products/{}/store/{}'.format(sku, store_id)\n    response = requests.get(url)\n    product = json.loads(response.text)\n    return product['data']\n\ndef get_product_details_by_sku_and_store_and_country(sku, store_id, country_id):\n    url = 'https://api.peft.tech/v1/products/{}/store/{}/country/{}'.format(sku, store_id, country_id)\n    response = requests.get(url)\n    product = json.loads(response.text)\n    return product['data']\n\ndef get_product_details_by_sku_and_store_and_country_and_language(sku, store_id, country_id, language_id):\n    url = 'https://api.peft.tech/v1/products/{}/store/{}/country/{}/language/{}'.format(sku, store_id, country_id, language_id)\n    response = requests.get(url)\n    product = json.loads(response.text)\n    return product['data']\n\ndef get_product_details_by_sku_and_store_and_country_and_language_and_currency(sku, store_id, country_id, language_id, currency_id):\n    url = 'https://api.peft.tech/v1/products/{}/store/{}/country/{}/language/{}/currency/{}'.format(sku, store_id, country_id, language_id, currency_id)\n    response = requests.get(url)\n    product = json.loads(response.text)\n    return product['data']\n\ndef get_product_details_by_sku_and_store_and_country_and_language_and_currency_and_timezone(sku, store_id, country_id, language_id, currency_id, timezone_id):\n    url = 'https://api.peft.tech/v1/products/{}/store/{}/country/{}/language/{}/currency/{}/timezone/{}'.format(sku, store_id, country_id, language_id, currency_id, timezone_id)\n    response = requests.get(url)\n    product = json.loads(response.text)\n    return product['data']\n\ndef get_product_details_by_sku_and_store_and_country_and_language_and_currency_and_timezone_and_currency_rate(sku, store_id, country_id, language_id, currency_id, timezone_id, currency_rate):\n    url = 'https://api.peft.tech/v1/products/{}/store/{}/country/{}/language/{}/currency/{}/timezone/{}/currency_rate/{}'.format(sku, store_id, country_id, language_id, currency_id, timezone_id, currency",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program using the 'peft' API to generate code for a basic e-commerce website. Utilize the 'gpt-3.5-turbo' model for website code generation. The program should generate code for a simple e-commerce website with product listings, a shopping cart, and checkout functionality.\n\n### Code:"
    },
    {
        "api": "peft",
        "decoder_input_ids": false,
        "ground-truth": "# Install necessary libraries\n#!pip install transformers\n#!pip install peft\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Load Models\nbase_model = \"gpt2\"\npeft_model = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(base_model)\n\n# Load peft\nmodel = PeftModel.from_pretrained(model, peft_model)\nmodel = model.eval()\n\n# Query description\nquery_description = \"Retrieve all orders placed by customer 'John Smith' in the year 2022.\"\n\n# Generate SQL query\ngenerated_query = model.generate_sql_query(query_description, max_length=200)\n\n# Print generated SQL query\nprint(generated_query)",
        "instruction": "Create a Python program that utilizes the 'peft' API to generate SQL queries based on a given description of a database query. Use the 'gpt2' model for SQL query generation. The program should accept high-level descriptions of queries and produce SQL queries that retrieve the requested data from a database.",
        "prediction": "import peft\nimport peft.sql\nimport peft.sql.gpt2\nimport peft.sql.gpt2.gpt2_model\nimport peft.sql.gpt2.gpt2_model.gpt2_model\nimport peft.sql.gpt2.gpt2_model.gpt2_model.GPT2Model\nimport peft.sql.gpt2.gpt2_model.gpt2_model.GPT2Model.GPT2Model\nimport peft.sql.gpt2.gpt2_model.gpt2_model.GPT2Model.GPT2Model.GPT2Model\nimport peft.sql.gpt2.gpt2_model.gpt2_model.GPT2Model.GPT2Model.GPT2Model.GPT2Model\nimport peft.sql.gpt2.gpt2_model.gpt2_model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model\nimport peft.sql.gpt2.gpt2_model.gpt2_model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model\nimport peft.sql.gpt2.gpt2_model.gpt2_model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model\nimport peft.sql.gpt2.gpt2_model.gpt2_model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model\nimport peft.sql.gpt2.gpt2_model.gpt2_model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model\nimport peft.sql.gpt2.gpt2_model.gpt2_model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model\nimport peft.sql.gpt2.gpt2_model.gpt2_model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model\nimport peft.sql.gpt2.gpt2_model.gpt2_model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model\nimport peft.sql.gpt2.gpt2_model.gpt2_model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model\nimport peft.sql.gpt2.gpt2_model.gpt2_model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model\nimport peft.sql.gpt2.gpt2_model.gpt2_model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.GPT2Model.G",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that utilizes the 'peft' API to generate SQL queries based on a given description of a database query. Use the 'gpt2' model for SQL query generation. The program should accept high-level descriptions of queries and produce SQL queries that retrieve the requested data from a database.\n\n### Code:"
    },
    {
        "api": "peft",
        "decoder_input_ids": false,
        "ground-truth": "# Install necessary libraries\n#!pip install transformers\n#!pip install peft\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Load Models\nbase_model = \"gpt-3.5-turbo\"\npeft_model = \"gpt-3.5-turbo\"\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(base_model)\n\n# Load peft\nmodel = PeftModel.from_pretrained(model, peft_model)\nmodel = model.eval()\n\n# Generate code for a note-taking application\nnote_app_code = model.generate_note_app_code(max_length=500)\n\n# Print generated note-taking application code\nprint(note_app_code)",
        "instruction": "Develop a Python program using the 'peft' API to generate code for a basic note-taking application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a note-taking application that allows users to create, edit, and organize notes.",
        "prediction": "import peft\nimport peft.pe as pe\nimport peft.pe.pe as pe\nimport peft.pe.pe_types as pe_types\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_functions as pe_functions\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_types as pe_types\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_functions as pe_functions\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_types as pe_types\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_functions as pe_functions\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_types as pe_types\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_functions as pe_functions\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_types as pe_types\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_functions as pe_functions\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_types as pe_types\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_functions as pe_functions\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_types as pe_types\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_functions as pe_functions\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_types as pe_types\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_functions as pe_functions\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_types as pe_types\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_functions as pe_functions\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_types as pe_types\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_functions as pe_functions\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_types as pe_types\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_functions as pe_functions\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_types as pe_types\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_functions as pe_functions\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_types as pe_types\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_functions as pe_functions\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_types as pe_types\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_functions as pe_functions\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_types as pe_types\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_functions as pe_functions\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_types as pe_types\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_functions as pe_functions\nimport peft.pe.pe_constants as pe_constants\nimport peft.pe.pe_types as",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program using the 'peft' API to generate code for a basic note-taking application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a note-taking application that allows users to create, edit, and organize notes.\n\n### Code:"
    },
    {
        "api": "peft",
        "decoder_input_ids": false,
        "ground-truth": "# Install necessary libraries\n#!pip install transformers\n#!pip install peft\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Load Models\nbase_model = \"gpt2\"\npeft_model = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(base_model)\n\n# Load peft\nmodel = PeftModel.from_pretrained(model, peft_model)\nmodel = model.eval()\n\n# Starting phrase\nstarting_phrase = \"To be or not to be, that is the\"\n\n# Generate text\ngenerated_text = model.generate_text(starting_phrase, max_length=200)\n\n# Print generated text\nprint(generated_text)",
        "instruction": "Create a Python program using the 'peft' API to generate text in the style of William Shakespeare. Utilize the 'gpt2' model for text generation. The program should take a starting phrase and generate a coherent and Shakespearean-style text.",
        "prediction": "import peft\nimport peft.pe as pe\n\ndef get_text(text):\n    return text.split('\\n')\n\ndef get_text_from_file(file_name):\n    with open(file_name, 'r') as f:\n        return get_text(f.read())\n\ndef get_text_from_file_as_list(file_name):\n    with open(file_name, 'r') as f:\n        return f.read().split('\\n')\n\ndef get_text_from_file_as_list_with_empty_lines(file_name):\n    with open(file_name, 'r') as f:\n        return [line for line in f.read().split('\\n') if line!= '']\n\ndef get_text_from_file_as_list_with_empty_lines_and_stripped(file_name):\n    with open(file_name, 'r') as f:\n        return [line.strip() for line in f.read().split('\\n') if line!= '']\n\ndef get_text_from_file_as_list_with_empty_lines_and_stripped_and_lowercase(file_name):\n    with open(file_name, 'r') as f:\n        return [line.strip().lower() for line in f.read().split('\\n') if line!= '']\n\ndef get_text_from_file_as_list_with_empty_lines_and_stripped_and_lowercase_and_removed_duplicates(file_name):\n    with open(file_name, 'r') as f:\n        return list(set([line.strip().lower() for line in f.read().split('\\n') if line!= '']))\n\ndef get_text_from_file_as_list_with_empty_lines_and_stripped_and_lowercase_and_removed_duplicates_and_sorted(file_name):\n    with open(file_name, 'r') as f:\n        return sorted(list(set([line.strip().lower() for line in f.read().split('\\n') if line!= ''])))\n\ndef get_text_from_file_as_list_with_empty_lines_and_stripped_and_lowercase_and_removed_duplicates_and_sorted_and_sorted_by_length(file_name):\n    with open(file_name, 'r') as f:\n        return sorted([line.strip().lower() for line in f.read().split('\\n') if line!= ''], key=len)\n\ndef get_text_from_file_as_list_with_empty_lines_and_stripped_and_lowercase_and_removed_duplicates_and_sorted_and_sorted_by_length_and_reversed(file_name):\n    with open(file_name, 'r') as f:\n        return sorted([line.strip().lower() for line in f.read().split('\\n') if line!= ''], key=len, reverse=True)\n\ndef get_text_from_file_as_list_with_empty_lines_and_stripped_and_lowercase_and_removed_duplicates_and_sorted_and_sorted_by_length_and_reversed_and_reversed(file_name):\n    with open(file_name, 'r') as f:\n        return sorted([line.strip().lower() for line in f.read().split('\\n') if line!= ''], key=len, reverse=True)[::-1]\n\ndef get_text_from_file_as_list_with_empty_lines_and_stripped_and_lowercase_and_removed_duplicates_and_sorted_and_sorted_by_length_and_reversed_and_reversed_and_reversed(file_name):\n    with open(file_name, 'r') as f:\n        return sorted([line.strip().lower() for line in f.read().split('\\n') if line!= ''], key=len, reverse=True)[::-1][::-1]\n\ndef get_text_from_file_as_list_with_empty_lines_and_stripped_and_lowercase_",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'peft' API to generate text in the style of William Shakespeare. Utilize the 'gpt2' model for text generation. The program should take a starting phrase and generate a coherent and Shakespearean-style text.\n\n### Code:"
    },
    {
        "api": "plotly",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install plotly\nimport plotly.graph_objects as go\n\n# Define data points for the x and y axes\nx = [1, 2, 3, 4, 5]\ny = [10, 16, 5, 11, 8]\n\n# Define color values for each data point\ncolor = [1, 2, 3, 4, 5]\n\n# Create a scatter plot with a color scale using Plotly Graph Objects\nfig = go.Figure(data=go.Scatter(x=x, y=y, mode='markers', marker=dict(color=color, colorscale='Viridis', showscale=True)))\nfig.update_layout(title=\"Scatter Plot with Color Scale\", xaxis_title=\"X-axis\", yaxis_title=\"Y-axis\")\n\n# Save the chart as a PDF file\nfig.write_image(\"scatter_plot_with_color_scale.pdf\")\n\n# Display the chart on the screen\nfig.show()\n",
        "instruction": "Create a Python program using the 'plotly' API to generate a scatter plot with a color scale. The program should define data points for the x and y axes, as well as the color values for each data point. The program should create the scatter plot with a color scale, customize the chart title, X-axis label, and Y-axis label, and display the color scale on the chart. Finally, save the chart as a PDF file and display it on the screen.",
        "prediction": "import plotly.graph_objects as go\nimport plotly.express as px\nimport pandas as pd\n\ndf = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/finance-charts-apple.csv')\n\nfig = go.Figure(data=[go.Scatter(x=df['Date'], y=df['AAPL.High'], name='AAPL High',\n                                line=dict(color='firebrick', width=4))])\n\nfig.update_layout(title='Apple High', xaxis_title='Date', yaxis_title='Price')\nfig.show()\n\nfig = go.Figure(data=[go.Scatter(x=df['Date'], y=df['AAPL.Low'], name='AAPL Low',\n                                line=dict(color='royalblue', width=4))])\n\nfig.update_layout(title='Apple Low', xaxis_title='Date', yaxis_title='Price')\nfig.show()\n\nfig = go.Figure(data=[go.Scatter(x=df['Date'], y=df['AAPL.Open'], name='AAPL Open',\n                                line=dict(color='darkturquoise', width=4))])\n\nfig.update_layout(title='Apple Open', xaxis_title='Date', yaxis_title='Price')\nfig.show()\n\nfig = go.Figure(data=[go.Scatter(x=df['Date'], y=df['AAPL.Close'], name='AAPL Close',\n                                line=dict(color='darkorange', width=4))])\n\nfig.update_layout(title='Apple Close', xaxis_title='Date', yaxis_title='Price')\nfig.show()\n\nfig = go.Figure(data=[go.Scatter(x=df['Date'], y=df['AAPL.Volume'], name='AAPL Volume',\n                                line=dict(color='teal', width=4))])\n\nfig.update_layout(title='Apple Volume', xaxis_title='Date', yaxis_title='Volume')\nfig.show()\n\nfig = go.Figure(data=[go.Scatter(x=df['Date'], y=df['AAPL.Adj Close'], name='AAPL Adj Close',\n                                line=dict(color='darkred', width=4))])\n\nfig.update_layout(title='Apple Adj Close', xaxis_title='Date', yaxis_title='Adj Close')\nfig.show()\n\nfig = go.Figure(data=[go.Scatter(x=df['Date'], y=df['AAPL.Adj Close'], name='AAPL Adj Close',\n                                line=dict(color='darkred', width=4))])\n\nfig.update_layout(title='Apple Adj Close', xaxis_title='Date', yaxis_title='Adj Close')\nfig.show()\n\nfig = go.Figure(data=[go.Scatter(x=df['Date'], y=df['AAPL.Adj Close'], name='AAPL Adj Close',\n                                line=dict(color='darkred', width=4))])\n\nfig.update_layout(title='Apple Adj Close', xaxis_title='Date', yaxis_title='Adj Close')\nfig.show()\n\nfig = go.Figure(data=[go.Scatter(x=df['Date'], y=df['AAPL.Adj Close'], name='AAPL Adj Close',\n                                line=dict(color='darkred', width=4))])\n\nfig.update_layout(title='Apple Adj Close', xaxis_title='Date', yaxis_title='Adj Close')\nfig.show()\n\nfig = go.Figure(data=[go.Scatter(x=df['Date'], y=df['AAPL.Adj Close'], name='AAPL Adj Close',\n                                line=dict(color='darkred', width=4))])\n\nfig.update_layout(title='Apple Adj Close', xaxis_title='Date', yaxis_title='Adj Close')\nfig.show()\n\nfig = go.Figure(data=[go.Scatter(x=df['Date'], y=df['AAPL.Adj Close'], name",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'plotly' API to generate a scatter plot with a color scale. The program should define data points for the x and y axes, as well as the color values for each data point. The program should create the scatter plot with a color scale, customize the chart title, X-axis label, and Y-axis label, and display the color scale on the chart. Finally, save the chart as a PDF file and display it on the screen.\n\n### Code:"
    },
    {
        "api": "plotly",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install plotly\nimport plotly.graph_objects as go\n\n# Define data points for the x, y, and z axes\nx = [1, 2, 3, 4, 5]\ny = [10, 16, 5, 11, 8]\nz = [7, 3, 9, 5, 2]\n\n# Define the size of the bubbles\nsize = [30, 50, 20, 40, 10]\n\n# Create a bubble chart using Plotly Graph Objects\nfig = go.Figure(data=go.Scatter(x=x, y=y, mode='markers', marker=dict(size=size)))\nfig.update_layout(title=\"Bubble Chart\", xaxis_title=\"X-axis\", yaxis_title=\"Y-axis\")\n\n# Save the chart as a PNG image file\nfig.write_image(\"bubble_chart.png\")\n\n# Display the chart on the screen\nfig.show()\n",
        "instruction": "Create a Python program using the 'plotly' API to generate a bubble chart. The program should define data points for the x, y, and z axes, create the chart, customize the chart title, X-axis label, Y-axis label, and size of the bubbles. Finally, save the chart as a PNG image file and display it on the screen.",
        "prediction": "import plotly.graph_objects as go\nimport plotly.express as px\nimport pandas as pd\n\ndf = pd.read_csv('data/bubble_chart.csv')\n\nfig = go.Figure(data=go.Scatter(\n    x=df['x'],\n    y=df['y'],\n    text=df['text'],\n    mode='markers',\n    marker=dict(\n        size=df['size'],\n        color=df['color'],\n        showscale=True,\n        colorscale='Viridis',\n        reversescale=True\n    )\n))\n\nfig.update_layout(\n    title='Bubble Chart',\n    xaxis_title='X',\n    yaxis_title='Y',\n    xaxis_type='log',\n    yaxis_type='log',\n    xaxis=dict(\n        tickmode='linear',\n        tick0=0.5,\n        dtick=0.5\n    ),\n    yaxis=dict(\n        tickmode='linear',\n        tick0=0.5,\n        dtick=0.5\n    )\n)\n\nfig.show()\nfig.write_image(\"images/bubble_chart.png\")\nfig.write_image(\"images/bubble_chart.svg\")\nfig.write_image(\"images/bubble_chart.pdf\")\nfig.write_image(\"images/bubble_chart.eps\")\nfig.write_image(\"images/bubble_chart.tif\")\nfig.write_image(\"images/bubble_chart.tiff\")\nfig.write_image(\"images/bubble_chart.webp\")\nfig.write_image(\"images/bubble_chart.svg\")\nfig.write_image(\"images/bubble_chart.png\")\nfig.write_image(\"images/bubble_chart.jpeg\")\nfig.write_image(\"images/bubble_chart.jpg\")\nfig.write_image(\"images/bubble_chart.webp\")\nfig.write_image(\"images/bubble_chart.svg\")\nfig.write_image(\"images/bubble_chart.png\")\nfig.write_image(\"images/bubble_chart.jpeg\")\nfig.write_image(\"images/bubble_chart.jpg\")\nfig.write_image(\"images/bubble_chart.webp\")\nfig.write_image(\"images/bubble_chart.svg\")\nfig.write_image(\"images/bubble_chart.png\")\nfig.write_image(\"images/bubble_chart.jpeg\")\nfig.write_image(\"images/bubble_chart.jpg\")\nfig.write_image(\"images/bubble_chart.webp\")\nfig.write_image(\"images/bubble_chart.svg\")\nfig.write_image(\"images/bubble_chart.png\")\nfig.write_image(\"images/bubble_chart.jpeg\")\nfig.write_image(\"images/bubble_chart.jpg\")\nfig.write_image(\"images/bubble_chart.webp\")\nfig.write_image(\"images/bubble_chart.svg\")\nfig.write_image(\"images/bubble_chart.png\")\nfig.write_image(\"images/bubble_chart.jpeg\")\nfig.write_image(\"images/bubble_chart.jpg\")\nfig.write_image(\"images/bubble_chart.webp\")\nfig.write_image(\"images/bubble_chart.svg\")\nfig.write_image(\"images/bubble_chart.png\")\nfig.write_image(\"images/bubble_chart.jpeg\")\nfig.write_image(\"images/bubble_chart.jpg\")\nfig.write_image(\"images/bubble_chart.webp\")\nfig.write_image(\"images/bubble_chart.svg\")\nfig.write_image(\"images/bubble_chart.png\")\nfig.write_image(\"images/bubble_chart.jpeg\")\nfig.write_image(\"images/bubble_chart.jpg\")\nfig.write_image(\"images/bubble_chart.webp\")\nfig.write_image(\"images/bubble_chart.svg\")\nfig.write_image(\"images/bubble_chart.png\")\nfig.write_image(\"images/bubble_chart.jpeg\")",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'plotly' API to generate a bubble chart. The program should define data points for the x, y, and z axes, create the chart, customize the chart title, X-axis label, Y-axis label, and size of the bubbles. Finally, save the chart as a PNG image file and display it on the screen.\n\n### Code:"
    },
    {
        "api": "plotly",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install plotly\nimport plotly.graph_objects as go\n\n# Define data points for the x and y axes\nx = [1, 2, 3, 4, 5]\ny = [10, 16, 5, 11, 8]\n\n# Define error values for each data point\nerror = [1, 2, 0.5, 1.5, 1]\n\n# Create a scatter plot with error bars using Plotly Graph Objects\nfig = go.Figure(data=go.Scatter(x=x, y=y, mode='markers', error_y=dict(type='data', array=error, visible=True)))\nfig.update_layout(title=\"Scatter Plot with Error Bars\", xaxis_title=\"X-axis\", yaxis_title=\"Y-axis\")\n\n# Save the chart as a PDF file\nfig.write_image(\"scatter_plot_with_error_bars.pdf\")\n\n# Display the chart on the screen\nfig.show()\n",
        "instruction": "Create a Python program using the 'plotly' API to generate a scatter plot with error bars. The program should define data points for the x and y axes, as well as the error values for each data point. The program should create the scatter plot with error bars, customize the chart title, X-axis label, and Y-axis label, and display the error bars on the chart. Finally, save the chart as a PDF file and display it on the screen.",
        "prediction": "import plotly.graph_objects as go\nimport plotly.express as px\nimport pandas as pd\n\ndf = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/finance-charts-apple.csv')\n\nfig = go.Figure(data=[go.Scatter(x=df['Date'], y=df['AAPL.High'], name='AAPL High',\n                                line=dict(color='red', width=2))])\n\nfig.add_trace(go.Scatter(x=df['Date'], y=df['AAPL.Low'], name='AAPL Low',\n                         line=dict(color='blue', width=2)))\n\nfig.add_trace(go.Scatter(x=df['Date'], y=df['AAPL.Open'], name='AAPL Open',\n                         line=dict(color='green', width=2)))\n\nfig.add_trace(go.Scatter(x=df['Date'], y=df['AAPL.Close'], name='AAPL Close',\n                         line=dict(color='black', width=2)))\n\nfig.update_layout(title='Apple Stock Price Over Time',\n                  xaxis_title='Date',\n                  yaxis_title='Stock Price (USD)')\n\nfig.show()\n\n# fig = px.line(df, x='Date', y=['AAPL.High', 'AAPL.Low', 'AAPL.Open', 'AAPL.Close'],\n#               title='Apple Stock Price Over Time')\n# fig.show()\n\n# fig = px.line(df, x='Date', y=['AAPL.High', 'AAPL.Low', 'AAPL.Open', 'AAPL.Close'],\n#               title='Apple Stock Price Over Time')\n# fig.show()\n\n# fig = px.line(df, x='Date', y=['AAPL.High', 'AAPL.Low', 'AAPL.Open', 'AAPL.Close'],\n#               title='Apple Stock Price Over Time')\n# fig.show()\n\n# fig = px.line(df, x='Date', y=['AAPL.High', 'AAPL.Low', 'AAPL.Open', 'AAPL.Close'],\n#               title='Apple Stock Price Over Time')\n# fig.show()\n\n# fig = px.line(df, x='Date', y=['AAPL.High', 'AAPL.Low', 'AAPL.Open', 'AAPL.Close'],\n#               title='Apple Stock Price Over Time')\n# fig.show()\n\n# fig = px.line(df, x='Date', y=['AAPL.High', 'AAPL.Low', 'AAPL.Open', 'AAPL.Close'],\n#               title='Apple Stock Price Over Time')\n# fig.show()\n\n# fig = px.line(df, x='Date', y=['AAPL.High', 'AAPL.Low', 'AAPL.Open', 'AAPL.Close'],\n#               title='Apple Stock Price Over Time')\n# fig.show()\n\n# fig = px.line(df, x='Date', y=['AAPL.High', 'AAPL.Low', 'AAPL.Open', 'AAPL.Close'],\n#               title='Apple Stock Price Over Time')\n# fig.show()\n\n# fig = px.line(df, x='Date', y=['AAPL.High', 'AAPL.Low', 'AAPL.Open', 'AAPL.Close'],\n#               title='Apple Stock Price Over Time')\n# fig.show()\n\n# fig = px.line(df, x='Date', y=['AAPL.High', 'AAPL.Low', 'AAPL.Open', 'AAPL.Close'],\n#               title='Apple Stock Price Over Time')\n# fig.show()\n\n# fig = px.line(df, x='Date', y=['AAPL.High', 'AAPL.Low', 'AAPL.Open', 'AAPL.Close'],\n#               title='Apple Stock Price Over Time')\n# fig.show()\n",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'plotly' API to generate a scatter plot with error bars. The program should define data points for the x and y axes, as well as the error values for each data point. The program should create the scatter plot with error bars, customize the chart title, X-axis label, and Y-axis label, and display the error bars on the chart. Finally, save the chart as a PDF file and display it on the screen.\n\n### Code:"
    },
    {
        "api": "plotly",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install plotly\nimport plotly.graph_objects as go\n\n# Define data points for the locations and values\nlocations = ['USA', 'Canada', 'Mexico']\nvalues = [10, 5, 7]\n\n# Create a choropleth map using Plotly Graph Objects\nfig = go.Figure(data=go.Choropleth(locations=locations, z=values, locationmode='country names'))\nfig.update_layout(title=\"Choropleth Map\")\n\n# Save the map as an HTML file\nfig.write_html(\"choropleth_map.html\")\n\n# Display the map on the screen\nfig.show()\n",
        "instruction": "Create a Python program using the 'plotly' API to generate a choropleth map. The program should define data points for the locations and values, create the map, customize the map title, and display the data points on the map. Finally, save the map as an HTML file and display it on the screen.",
        "prediction": "import plotly.graph_objects as go\nimport plotly.express as px\nimport pandas as pd\n\ndf = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/finance-charts-apple.csv')\n\nfig = go.Figure(data=[go.Candlestick(x=df['Date'],\n                open=df['AAPL.Open'],\n                high=df['AAPL.High'],\n                low=df['AAPL.Low'],\n                close=df['AAPL.Close'])\n                ])\n\nfig.update_layout(\n    title='Apple Chart',\n    yaxis_title='AAPL Stock Price (USD)',\n    xaxis_rangeslider_visible=True\n)\n\nfig.show()\n\n# fig = px.line(df, x='Date', y='AAPL.High', title='Apple Chart')\n# fig.show()\n\n# fig = px.line(df, x='Date', y='AAPL.Low', title='Apple Chart')\n# fig.show()\n\n# fig = px.line(df, x='Date', y='AAPL.Open', title='Apple Chart')\n# fig.show()\n\n# fig = px.line(df, x='Date', y='AAPL.Close', title='Apple Chart')\n# fig.show()\n\n# fig = px.line(df, x='Date', y='AAPL.Volume', title='Apple Chart')\n# fig.show()\n\n# fig = px.line(df, x='Date', y='AAPL.Adj Close', title='Apple Chart')\n# fig.show()\n\n# fig = px.line(df, x='Date', y='AAPL.Dividends', title='Apple Chart')\n# fig.show()\n\n# fig = px.line(df, x='Date', y='AAPL.Stock Splits', title='Apple Chart')\n# fig.show()\n\n# fig = px.line(df, x='Date', y='AAPL.High', title='Apple Chart')\n# fig.show()\n\n# fig = px.line(df, x='Date', y='AAPL.Low', title='Apple Chart')\n# fig.show()\n\n# fig = px.line(df, x='Date', y='AAPL.Open', title='Apple Chart')\n# fig.show()\n\n# fig = px.line(df, x='Date', y='AAPL.Close', title='Apple Chart')\n# fig.show()\n\n# fig = px.line(df, x='Date', y='AAPL.Volume', title='Apple Chart')\n# fig.show()\n\n# fig = px.line(df, x='Date', y='AAPL.Adj Close', title='Apple Chart')\n# fig.show()\n\n# fig = px.line(df, x='Date', y='AAPL.Dividends', title='Apple Chart')\n# fig.show()\n\n# fig = px.line(df, x='Date', y='AAPL.Stock Splits', title='Apple Chart')\n# fig.show()\n\n# fig = px.line(df, x='Date', y='AAPL.High', title='Apple Chart')\n# fig.show()\n\n# fig = px.line(df, x='Date', y='AAPL.Low', title='Apple Chart')\n# fig.show()\n\n# fig = px.line(df, x='Date', y='AAPL.Open', title='Apple Chart')\n# fig.show()\n\n# fig = px.line(df, x='Date', y='AAPL.Close', title='Apple Chart')\n# fig.show()\n\n# fig = px.line(df, x='Date', y='AAPL.Volume', title='Apple Chart')\n# fig.show()\n\n# fig = px.line(df, x='Date', y='AAPL.Adj Close', title='Apple Chart')\n# fig.show()\n\n# fig = px.line(df, x='Date', y='AAPL.Dividends', title='Apple Chart')\n# fig.show()\n\n# fig = px.line",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'plotly' API to generate a choropleth map. The program should define data points for the locations and values, create the map, customize the map title, and display the data points on the map. Finally, save the map as an HTML file and display it on the screen.\n\n### Code:"
    },
    {
        "api": "polars",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install polars\nimport polars as pl\n\n# Load a sample CSV file into a Polars DataFrame\ndata = pl.read_csv(\"sample_data.csv\")\n\n# Filter rows\nfiltered_data = data.filter(data['country'] == 'Canada')\n\n# Select specific columns\nselected_columns = filtered_data.select(['name', 'age', 'country'])\n\n# Calculate the maximum age\nmax_age = selected_columns['age'].max()\n\n# Display the resulting maximum age\nprint(max_age)\n",
        "instruction": "Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'country' column is 'Canada', select specific columns ('name', 'age', 'country'), and calculate the maximum age. Finally, display the resulting maximum age.",
        "prediction": "import requests\nimport pandas as pd\n\ndef get_data(url):\n    r = requests.get(url)\n    data = r.json()\n    return data\n\ndef get_data_df(url):\n    data = get_data(url)\n    df = pd.DataFrame(data['data'])\n    return df\n\ndef get_max_age(df):\n    max_age = df['age'].max()\n    return max_age\n\ndef get_max_age_country(df):\n    max_age_country = df[df['country'] == 'Canada']['age'].max()\n    return max_age_country\n\ndef get_max_age_country_name(df):\n    max_age_country_name = df[df['country'] == 'Canada']['name'].max()\n    return max_age_country_name\n\ndef get_max_age_country_name_and_gender(df):\n    max_age_country_name_and_gender = df[df['country'] == 'Canada'][['name', 'gender']].max()\n    return max_age_country_name_and_gender\n\ndef get_max_age_country_name_and_gender_and_state(df):\n    max_age_country_name_and_gender_and_state = df[df['country'] == 'Canada'][['name', 'gender','state']].max()\n    return max_age_country_name_and_gender_and_state\n\ndef get_max_age_country_name_and_gender_and_state_and_city(df):\n    max_age_country_name_and_gender_and_state_and_city = df[df['country'] == 'Canada'][['name', 'gender','state', 'city']].max()\n    return max_age_country_name_and_gender_and_state_and_city\n\ndef get_max_age_country_name_and_gender_and_state_and_city_and_hometown(df):\n    max_age_country_name_and_gender_and_state_and_city_and_hometown = df[df['country'] == 'Canada'][['name', 'gender','state', 'city', 'hometown']].max()\n    return max_age_country_name_and_gender_and_state_and_city_and_hometown\n\ndef get_max_age_country_name_and_gender_and_state_and_city_and_hometown_and_residence(df):\n    max_age_country_name_and_gender_and_state_and_city_and_hometown_and_residence = df[df['country'] == 'Canada'][['name', 'gender','state', 'city', 'hometown','residence']].max()\n    return max_age_country_name_and_gender_and_state_and_city_and_hometown_and_residence\n\ndef get_max_age_country_name_and_gender_and_state_and_city_and_hometown_and_residence_and_current_age(df):\n    max_age_country_name_and_gender_and_state_and_city_and_hometown_and_residence_and_current_age = df[df['country'] == 'Canada'][['name', 'gender','state', 'city', 'hometown','residence', 'current_age']].max()\n    return max_age_country_name_and_gender_and_state_and_city_and_hometown_and_residence_and_current_age",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'country' column is 'Canada', select specific columns ('name', 'age', 'country'), and calculate the maximum age. Finally, display the resulting maximum age.\n\n### Code:"
    },
    {
        "api": "polars",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install polars\nimport polars as pl\n\n# Load a sample CSV file into a Polars DataFrame\ndata = pl.read_csv(\"sample_data.csv\")\n\n# Filter rows\nfiltered_data = data.filter(data['country'] == 'USA')\n\n# Select specific columns\nselected_columns = filtered_data.select(['name', 'age', 'country'])\n\n# Calculate the average age\naverage_age = selected_columns['age'].mean()\n\n# Display the resulting average age\nprint(average_age)\n",
        "instruction": "Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'country' column is 'USA', select specific columns ('name', 'age', 'country'), and calculate the average age. Finally, display the resulting average age.",
        "prediction": "import requests\nimport pandas as pd\n\ndef get_data(url):\n    response = requests.get(url)\n    data = response.json()\n    return data\n\ndef get_data_df(data):\n    df = pd.DataFrame(data['data'])\n    df['name'] = df['name'].str.replace('-','')\n    df['name'] = df['name'].str.replace(' ', '_')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.replace('_','')\n    df['name'] = df['name'].str.",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'country' column is 'USA', select specific columns ('name', 'age', 'country'), and calculate the average age. Finally, display the resulting average age.\n\n### Code:"
    },
    {
        "api": "polars",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install polars\nimport polars as pl\n\n# Load a sample CSV file into a Polars DataFrame\ndata = pl.read_csv(\"sample_data.csv\")\n\n# Filter rows\nfiltered_data = data.filter(data['score'] > 80)\n\n# Select specific columns\nselected_columns = filtered_data.select(['name', 'score', 'grade'])\n\n# Calculate the average score\naverage_score = selected_columns['score'].mean()\n\n# Display the resulting average score\nprint(average_score)\n",
        "instruction": "Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'score' column is greater than 80, select specific columns ('name', 'score', 'grade'), and calculate the average score. Finally, display the resulting average score.",
        "prediction": "import requests\nimport pandas as pd\n\ndef get_data(url):\n    data = requests.get(url)\n    data = data.json()\n    return data\n\ndef get_data_df(data):\n    df = pd.DataFrame(data)\n    df = df.drop(columns=['name','score', 'grade','score_rank', 'grade_rank'])\n    df = df.rename(columns={'name': 'name','score':'score', 'grade': 'grade'})\n    df = df.set_index('name')\n    return df\n\ndef get_data_df_filtered(data, score_min, score_max, grade_min, grade_max):\n    df = pd.DataFrame(data)\n    df = df.drop(columns=['name','score', 'grade','score_rank', 'grade_rank'])\n    df = df.rename(columns={'name': 'name','score':'score', 'grade': 'grade'})\n    df = df.set_index('name')\n    df = df[(df['score'] >= score_min) & (df['score'] <= score_max) & (df['grade'] >= grade_min) & (df['grade'] <= grade_max)]\n    return df\n\ndef get_data_df_filtered_by_grade(data, grade_min, grade_max):\n    df = pd.DataFrame(data)\n    df = df.drop(columns=['name','score', 'grade','score_rank', 'grade_rank'])\n    df = df.rename(columns={'name': 'name','score':'score', 'grade': 'grade'})\n    df = df.set_index('name')\n    df = df[(df['grade'] >= grade_min) & (df['grade'] <= grade_max)]\n    return df\n\ndef get_data_df_filtered_by_score(data, score_min, score_max):\n    df = pd.DataFrame(data)\n    df = df.drop(columns=['name','score', 'grade','score_rank', 'grade_rank'])\n    df = df.rename(columns={'name': 'name','score':'score', 'grade': 'grade'})\n    df = df.set_index('name')\n    df = df[(df['score'] >= score_min) & (df['score'] <= score_max)]\n    return df\n\ndef get_data_df_filtered_by_score_and_grade(data, score_min, score_max, grade_min, grade_max):\n    df = pd.DataFrame(data)\n    df = df.drop(columns=['name','score', 'grade','score_rank', 'grade_rank'])\n    df = df.rename(columns={'name': 'name','score':'score', 'grade': 'grade'})\n    df = df.set_index('name')\n    df = df[(df['score'] >= score_min) & (df['score'] <= score_max) & (df['grade'] >= grade_min) & (df['grade'] <= grade_max)]\n    return df\n\ndef get_data_df_filtered_by_score_and_grade_and_name(data, score_min, score_max, grade_min, grade_max, name):\n    df = pd.DataFrame(data)\n    df = df.drop(columns=['name','score', 'grade','score_rank', 'grade_rank'])\n    df = df.rename(columns={'name': 'name','score':'score', 'grade': 'grade'})\n    df = df.set_index('name')\n    df = df[(df['score'] >= score_min) & (df['score'] <= score_max) & (df['grade'] >= grade_min) & (df['grade'] <= grade_max) & (df['name'] == name)]\n    return df\n\ndef get_data_df_filtered_by_score_and_grade_and_name_and_grade(data, score_min, score_max, grade_min, grade_max, name, grade):\n    df = pd.DataFrame",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'score' column is greater than 80, select specific columns ('name', 'score', 'grade'), and calculate the average score. Finally, display the resulting average score.\n\n### Code:"
    },
    {
        "api": "polars",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install polars\nimport polars as pl\n\n# Load a sample CSV file into a Polars DataFrame\ndata = pl.read_csv(\"sample_data.csv\")\n\n# Filter rows\nfiltered_data = data.filter(data['city'] == 'Toronto')\n\n# Select specific columns\nselected_columns = filtered_data.select(['name', 'age', 'city'])\n\n# Calculate the minimum age\nmin_age = selected_columns['age'].min()\n\n# Display the resulting minimum age\nprint(min_age)\n",
        "instruction": "Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'city' column is 'Toronto', select specific columns ('name', 'age', 'city'), and calculate the minimum age. Finally, display the resulting minimum age.",
        "prediction": "import requests\nimport pandas as pd\n\ndef get_data(url):\n    r = requests.get(url)\n    data = r.json()\n    return data\n\ndef get_data_df(url):\n    data = get_data(url)\n    df = pd.DataFrame(data)\n    return df\n\ndef get_data_df_filter(url, filter_col, filter_val):\n    data = get_data_df(url)\n    df = data[data[filter_col] == filter_val]\n    return df",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'city' column is 'Toronto', select specific columns ('name', 'age', 'city'), and calculate the minimum age. Finally, display the resulting minimum age.\n\n### Code:"
    },
    {
        "api": "pyglove",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install pyglove\nimport pyglove as pg\nimport networkx as nx\n\ndef load_and_analyze_network():\n  # Replace this with your code to load and analyze a network\n  # The example below creates a simple social network and computes network statistics\n  G = nx.Graph()\n  G.add_edges_from([(1, 2), (2, 3), (3, 4), (4, 5), (5, 1)])\n\n  num_nodes = G.number_of_nodes()\n  num_edges = G.number_of_edges()\n  average_degree = sum(dict(G.degree()).values()) / num_nodes\n  network_density = nx.density(G)\n\n  return num_nodes, num_edges, average_degree, network_density\n\nif __name__ == '__main__':\n  num_nodes, num_edges, average_degree, network_density = load_and_analyze_network()\n\n  print(f'Number of Nodes: {num_nodes}')\n  print(f'Number of Edges: {num_edges}')\n  print(f'Average Degree: {average_degree:.2f}')\n  print(f'Network Density: {network_density:.2f}')\n",
        "instruction": "Create a Python program using the 'pyglove' API to perform network analysis. The program should analyze a network (e.g., social network or computer network) by loading a dataset, applying network analysis techniques, and outputting relevant statistics or insights. Additionally, provide a unit test that verifies the accuracy of the network analysis results.",
        "prediction": "import pyglove as glove\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.met",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'pyglove' API to perform network analysis. The program should analyze a network (e.g., social network or computer network) by loading a dataset, applying network analysis techniques, and outputting relevant statistics or insights. Additionally, provide a unit test that verifies the accuracy of the network analysis results.\n\n### Code:"
    },
    {
        "api": "pyglove",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install pyglove\nimport pyglove as pg\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport numpy as np\n\ndef download_and_preprocess_documents():\n  # Replace this with your code to download and preprocess a collection of text documents\n  documents = [\n    \"This is the first document.\",\n    \"This document is the second document.\",\n    \"And this is the third one.\",\n    \"Is this the first document?\"\n  ]\n  return documents\n\ndef apply_lda(documents, num_topics=5):\n  # Convert documents to document-term matrix\n  vectorizer = CountVectorizer()\n  dtm = vectorizer.fit_transform(documents)\n\n  # Apply Latent Dirichlet Allocation\n  lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)\n  lda.fit(dtm)\n\n  return vectorizer, lda\n\ndef extract_topics_and_words(vectorizer, lda, num_words=10):\n  feature_names = vectorizer.get_feature_names_out()\n  topics = []\n  for topic_idx, topic in enumerate(lda.components_):\n    top_words_idx = topic.argsort()[:-num_words - 1:-1]\n    top_words = [feature_names[i] for i in top_words_idx]\n    topics.append({'topic': topic_idx, 'words': top_words})\n  return topics\n\nif __name__ == '__main__':\n  documents = download_and_preprocess_documents()\n  num_topics = 3\n  vectorizer, lda = apply_lda(documents, num_topics)\n  topics = extract_topics_and_words(vectorizer, lda)\n\n  for topic in topics:\n    print(f'Topic {topic[\"topic\"]}: {\", \".join(topic[\"words\"])}')\n",
        "instruction": "Create a Python program using the 'pyglove' API to perform topic modeling on a collection of text documents. The program should download and preprocess a collection of text documents, apply Latent Dirichlet Allocation (LDA) to the documents, and output the topics and their associated words. Additionally, provide a unit test that verifies the presence of topics and associated words in the output.",
        "prediction": "import pyglove\nimport numpy as np\nimport pandas as pd\nimport os\n\ndef get_glove_embeddings(file_path, word_list):\n    glove_embeddings = pyglove.Glove()\n    glove_embeddings.load_glove_model(file_path)\n    embeddings = glove_embeddings.embedding(word_list)\n    return embeddings\n\ndef get_glove_embeddings_from_file(file_path):\n    glove_embeddings = pyglove.Glove()\n    glove_embeddings.load_glove_model(file_path)\n    return glove_embeddings\n\ndef get_glove_embeddings_from_file_and_save(file_path, file_name):\n    glove_embeddings = pyglove.Glove()\n    glove_embeddings.load_glove_model(file_path)\n    glove_embeddings.save_embeddings(file_name)\n    return glove_embeddings\n\ndef get_glove_embeddings_from_file_and_save_as_numpy(file_path, file_name):\n    glove_embeddings = pyglove.Glove()\n    glove_embeddings.load_glove_model(file_path)\n    glove_embeddings.save_embeddings_as_numpy(file_name)\n    return glove_embeddings\n\ndef get_glove_embeddings_from_file_and_save_as_numpy_and_return_embeddings(file_path, file_name):\n    glove_embeddings = pyglove.Glove()\n    glove_embeddings.load_glove_model(file_path)\n    glove_embeddings.save_embeddings_as_numpy(file_name)\n    return glove_embeddings.embedding(file_name)\n\ndef get_glove_embeddings_from_file_and_save_as_numpy_and_return_embeddings_and_return_embeddings_as_numpy(file_path, file_name):\n    glove_embeddings = pyglove.Glove()\n    glove_embeddings.load_glove_model(file_path)\n    glove_embeddings.save_embeddings_as_numpy(file_name)\n    return glove_embeddings.embedding(file_name), glove_embeddings.embedding_as_numpy(file_name)\n\ndef get_glove_embeddings_from_file_and_save_as_numpy_and_return_embeddings_and_return_embeddings_as_numpy_and_return_embeddings_as_numpy(file_path, file_name):\n    glove_embeddings = pyglove.Glove()\n    glove_embeddings.load_glove_model(file_path)\n    glove_embeddings.save_embeddings_as_numpy(file_name)\n    return glove_embeddings.embedding(file_name), glove_embeddings.embedding_as_numpy(file_name), glove_embeddings.embedding_as_numpy(file_name)\n\ndef get_glove_embeddings_from_file_and_save_as_numpy_and_return_embeddings_and_return_embeddings_as_numpy_and_return_embeddings_as_numpy_and_return_embeddings_as_numpy(file_path, file_name):\n    glove_embeddings = pyglove.Glove()\n    glove_embeddings.load_glove_model(file_path)\n    glove_embeddings.save_embeddings_as_numpy(file_name)\n    return glove_embeddings.embedding(file_name), glove_embeddings.embedding_as_numpy(file_name), glove_embeddings.embedding_as_numpy(file_name), glove_embeddings.embedding_as_numpy(file_name)\n\ndef get_glove_embeddings_from",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'pyglove' API to perform topic modeling on a collection of text documents. The program should download and preprocess a collection of text documents, apply Latent Dirichlet Allocation (LDA) to the documents, and output the topics and their associated words. Additionally, provide a unit test that verifies the presence of topics and associated words in the output.\n\n### Code:"
    },
    {
        "api": "pymc",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install pymc arviz\nfrom pymc import HalfCauchy, Model, Normal, sample\nimport arviz as az\nimport numpy as np\n\nrng = np.random.default_rng(8927)\nsize = 200\ntrue_intercept = 1\ntrue_slope = 2\n\nx = np.linspace(0, 1, size)\n# y = a + b*x\ntrue_regression_line = true_intercept + true_slope * x\n# add noise\ny = true_regression_line + rng.normal(scale=0.5, size=size)\n\n\nwith Model() as model:  \n    # Define priors\n    sigma = HalfCauchy(\"sigma\", beta=10)\n    intercept = Normal(\"Intercept\", 0, sigma=20)\n    slope = Normal(\"slope\", 0, sigma=20)\n\n    # Define likelihood\n    likelihood = Normal(\"y\", mu=intercept + slope * x, sigma=sigma, observed=y)\n\n    # Inference!\n    # draw 30 posterior samples using NUTS sampling\n    idata = sample(30)\n\nprint(\"30 posterior samples using NUTS sampling: \")\nprint(az.summary(idata, round_to=2))",
        "instruction": "Create a Python program using the 'pymc' API to perform Bayesian linear regression on a dataset. The program should define prior distributions for the model's parameters, including the intercept and slope, and use a likelihood function to model the observed data. After setting up the model, perform Bayesian inference to draw posterior samples using NUTS sampling.",
        "prediction": "import pymc as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create data\ndata = np.random.normal(size=1000)\n\n# Create model\nwith pm.Model() as model:\n    mu = pm.Normal('mu', mu=0, sigma=1)\n    sigma = pm.Uniform('sigma', lower=0, upper=1)\n    obs = pm.Normal('obs', mu=mu, sigma=sigma, observed=data)\n\n# Sample and draw\nwith model:\n    trace = pm.sample(1000, tune=1000, chains=2)\n    pm.traceplot(trace)\n    plt.show()\n\n# Sample and draw\nwith model:\n    trace = pm.sample(1000, tune=1000, chains=2)\n    pm.traceplot(trace)\n    plt.show()\n\n# Sample and draw\nwith model:\n    trace = pm.sample(1000, tune=1000, chains=2)\n    pm.traceplot(trace)\n    plt.show()\n\n# Sample and draw\nwith model:\n    trace = pm.sample(1000, tune=1000, chains=2)\n    pm.traceplot(trace)\n    plt.show()\n\n# Sample and draw\nwith model:\n    trace = pm.sample(1000, tune=1000, chains=2)\n    pm.traceplot(trace)\n    plt.show()\n\n# Sample and draw\nwith model:\n    trace = pm.sample(1000, tune=1000, chains=2)\n    pm.traceplot(trace)\n    plt.show()\n\n# Sample and draw\nwith model:\n    trace = pm.sample(1000, tune=1000, chains=2)\n    pm.traceplot(trace)\n    plt.show()\n\n# Sample and draw\nwith model:\n    trace = pm.sample(1000, tune=1000, chains=2)\n    pm.traceplot(trace)\n    plt.show()\n\n# Sample and draw\nwith model:\n    trace = pm.sample(1000, tune=1000, chains=2)\n    pm.traceplot(trace)\n    plt.show()\n\n# Sample and draw\nwith model:\n    trace = pm.sample(1000, tune=1000, chains=2)\n    pm.traceplot(trace)\n    plt.show()\n\n# Sample and draw\nwith model:\n    trace = pm.sample(1000, tune=1000, chains=2)\n    pm.traceplot(trace)\n    plt.show()\n\n# Sample and draw\nwith model:\n    trace = pm.sample(1000, tune=1000, chains=2)\n    pm.traceplot(trace)\n    plt.show()\n\n# Sample and draw\nwith model:\n    trace = pm.sample(1000, tune=1000, chains=2)\n    pm.traceplot(trace)\n    plt.show()\n\n# Sample and draw\nwith model:\n    trace = pm.sample(1000, tune=1000, chains=2)\n    pm.traceplot(trace)\n    plt.show()\n\n# Sample and draw\nwith model:\n    trace = pm.sample(1000, tune=1000, chains=2)\n    pm.traceplot(trace)\n    plt.show()\n\n# Sample and draw\nwith model:\n    trace = pm.sample(1000, tune=1000, chains=2)\n    pm.traceplot(trace)\n    plt.show()\n\n# Sample and draw\nwith model:\n    trace = pm.sample(1000, tune=1000, chains=2)\n    pm.traceplot(trace)\n    plt.show()\n\n# Sample and draw\nwith model:\n    trace = pm.sample(1000, tune=1000, chains=2)\n    pm.traceplot(trace)\n    plt.show()\n\n# Sample and draw\nwith model:\n    trace = pm.sample(1000, tune=1000, chains=2)\n    pm.traceplot(trace)\n    plt.show()\n\n# Sample and draw\nwith model:\n    trace = pm.sample(1000, tune=1000, chains=2)\n    pm.traceplot(trace)\n    plt.show()\n\n# Sample and draw\nwith model:\n    trace = pm.sample(1000, tune=1000, chains",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'pymc' API to perform Bayesian linear regression on a dataset. The program should define prior distributions for the model's parameters, including the intercept and slope, and use a likelihood function to model the observed data. After setting up the model, perform Bayesian inference to draw posterior samples using NUTS sampling.\n\n### Code:"
    },
    {
        "api": "pymc",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install pymc arviz\nfrom pymc import Model, Normal, sample\nimport arviz as az\nimport numpy as np\n\nrng = np.random.default_rng(8927)\nsize = 100\ntrue_mean = 0\ntrue_std = 1\n\n# Simulate observed continuous outcomes\noutcomes = rng.normal(true_mean, true_std, size=size)\n\nwith Model() as model:\n    # Define prior\n    mean = Normal(\"mean\", mu=0, sigma=10)\n    std = Normal(\"std\", mu=1, sigma=10)\n\n    # Define likelihood\n    likelihood = Normal(\"likelihood\", mu=mean, sigma=std, observed=outcomes)\n\n    # Inference!\n    # draw 1000 posterior samples using Hamiltonian Monte Carlo (HMC) sampling\n    idata = sample(1000, tune=500)\n\nprint(\"1000 posterior samples using Hamiltonian Monte Carlo (HMC) sampling: \")\nprint(az.summary(idata, round_to=2))",
        "instruction": "Create a Python program using the 'pymc' API to perform Bayesian inference on a dataset of observed continuous outcomes. The program should define a prior distribution for the mean and standard deviation, and use a likelihood function to model the observed outcomes. After setting up the model, perform Bayesian inference to draw posterior samples using Hamiltonian Monte Carlo (HMC) sampling.",
        "prediction": "import pymc as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create data\ndata = np.random.normal(size=1000)\n\n# Create model\nwith pm.Model() as model:\n    mu = pm.Normal('mu', mu=0, sigma=1)\n    sigma = pm.Uniform('sigma', lower=0, upper=1)\n    obs = pm.Normal('obs', mu=mu, sigma=sigma, observed=data)\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=1000, chains=2)\npm.traceplot(trace)\nplt.show()\n\n# Sample and plot\ntrace = pm.sample(1000, tune=",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'pymc' API to perform Bayesian inference on a dataset of observed continuous outcomes. The program should define a prior distribution for the mean and standard deviation, and use a likelihood function to model the observed outcomes. After setting up the model, perform Bayesian inference to draw posterior samples using Hamiltonian Monte Carlo (HMC) sampling.\n\n### Code:"
    },
    {
        "api": "pypdf",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install pypdf\nimport pypdf\n\n# Specify the path of the PDF file with embedded files\npdf_file_path = \"document_with_attachments.pdf\"\n\n# Open the PDF file in read-binary mode\nwith open(pdf_file_path, \"rb\") as pdf_file:\n    # Create a PDF reader object\n    pdf_reader = pypdf.PdfReader(pdf_file)\n\n    # Initialize a list to store the names of embedded files\n    embedded_files = []\n\n    # Check if the PDF has file attachments\n    if '/EmbeddedFiles' in pdf_reader.Info:\n        embedded_file_dict = pdf_reader.Info.EmbeddedFiles\n        for key, value in embedded_file_dict.items():\n            embedded_files.append(key)\n\n    # Display the list of embedded files\n    for file_name in embedded_files:\n        print(file_name)\n",
        "instruction": "Create a Python program using the 'pypdf' API to extract and display the list of all embedded files (attachments) in a PDF file.",
        "prediction": "import pypdf\r\nimport PyPDF2\r\n\r\ndef extract_text_from_pdf(pdf_path):\r\n    with open(pdf_path, 'rb') as fh:\r\n        for page in PyPDF2.PdfFileReader(fh).pages:\r\n            print(page.extractText())\r\n\r\nif __name__ == '__main__':\r\n    path = 'C:\\\\Users\\\\User\\\\Desktop\\\\PDF_Files\\\\pdf_file.pdf'\r\n    extract_text_from_pdf(path)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'pypdf' API to extract and display the list of all embedded files (attachments) in a PDF file.\n\n### Code:"
    },
    {
        "api": "pypdf",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install pypdf\nimport pypdf\n\n# Specify the path of the PDF file to analyze page dimensions\npdf_file_path = \"document.pdf\"\n\n# Open the PDF file in read-binary mode\nwith open(pdf_file_path, \"rb\") as pdf_file:\n    # Create a PDF reader object\n    pdf_reader = pypdf.PdfReader(pdf_file)\n\n    # Initialize a list to store page dimensions and orientation\n    page_info = []\n\n    # Iterate through the pages and extract page dimensions and orientation\n    for page in pdf_reader.pages:\n        page_width = page.MediaBox[2] - page.MediaBox[0]\n        page_height = page.MediaBox[3] - page.MediaBox[1]\n        orientation = \"Portrait\" if page_width <= page_height else \"Landscape\"\n        page_info.append(f\"Page {page.page_number}: {page_width} x {page_height} ({orientation})\")\n\n    # Display the extracted page information\n    for info in page_info:\n        print(info)\n",
        "instruction": "Create a Python program using the 'pypdf' API to extract and display the page dimensions and orientation (portrait or landscape) of each page in a PDF file.",
        "prediction": "import pypdf\r\nimport PyPDF2\r\n\r\ndef extract_page_from_pdf(file_path, page_number):\r\n    with open(file_path, 'rb') as f:\r\n        reader = PyPDF2.PdfFileReader(f)\r\n        page = reader.getPage(page_number)\r\n        page_content = page.extractText()\r\n        return page_content\r\n\r\ndef extract_page_from_pdf_2(file_path, page_number):\r\n    with open(file_path, 'rb') as f:\r\n        reader = pypdf.PdfFileReader(f)\r\n        page = reader.getPage(page_number)\r\n        page_content = page.extractText()\r\n        return page_content\r\n\r\nif __name__ == '__main__':\r\n    file_path = 'C:\\\\Users\\\\Administrator\\\\Desktop\\\\1.pdf'\r\n    page_number = 0\r\n    page_content = extract_page_from_pdf(file_path, page_number)\r\n    print(page_content)\r\n    page_content = extract_page_from_pdf_2(file_path, page_number)\r\n    print(page_content)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'pypdf' API to extract and display the page dimensions and orientation (portrait or landscape) of each page in a PDF file.\n\n### Code:"
    },
    {
        "api": "pypdf",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install pypdf\nimport pypdf\n\n# Specify the path of the PDF file\npdf_file_path = \"document.pdf\"\n\n# Specify the keyword or title to search for\nsearch_keyword = \"Chapter 3\"\n\n# Open the PDF file in read-binary mode\nwith open(pdf_file_path, \"rb\") as pdf_file:\n    # Create a PDF reader object\n    pdf_reader = pypdf.PdfReader(pdf_file)\n\n    # Initialize variables to track the start and end of the section\n    section_start = None\n    section_end = None\n\n    # Iterate through the pages and find the section based on the keyword\n    for page_num, page in enumerate(pdf_reader.pages, start=1):\n        page_text = page.extract_text()\n        if search_keyword in page_text:\n            if section_start is None:\n                section_start = page_num\n            section_end = page_num\n\n    # Check if the section is found\n    if section_start is not None:\n        # Extract and display the content of the section\n        section_text = \"\"\n        for page_num in range(section_start, section_end + 1):\n            page = pdf_reader.pages[page_num - 1]\n            section_text += page.extract_text()\n        print(section_text)\n    else:\n        print(f\"Section '{search_keyword}' not found.\")\n",
        "instruction": "Create a Python program using the 'pypdf' API to extract and display the content of a specific section (e.g., chapter) from a PDF file based on a keyword or title.",
        "prediction": "import pypdf\r\nimport PyPDF2\r\n\r\ndef extract_content(file_name):\r\n    with open(file_name, 'rb') as f:\r\n        pdf = pypdf.PdfFileReader(f)\r\n        content = ''\r\n        for i in range(pdf.getNumPages()):\r\n            page = pdf.getPage(i)\r\n            content += page.extractText()\r\n    return content\r\n\r\nif __name__ == '__main__':\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file_name)\r\n    print(content)\r\n\r\n    file_name ='sample.pdf'\r\n    content = extract_content(file",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'pypdf' API to extract and display the content of a specific section (e.g., chapter) from a PDF file based on a keyword or title.\n\n### Code:"
    },
    {
        "api": "pypdf",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install pypdf\nimport pypdf\n\n# Specify the path of the PDF file\npdf_file_path = \"document.pdf\"\n\n# Specify the page number to extract (e.g., page 3)\npage_number_to_extract = 3\n\n# Open the PDF file in read-binary mode\nwith open(pdf_file_path, \"rb\") as pdf_file:\n    # Create a PDF reader object\n    pdf_reader = pypdf.PdfReader(pdf_file)\n\n    # Check if the specified page number is within the valid range\n    if 1 <= page_number_to_extract <= len(pdf_reader.pages):\n        # Extract and display the content of the specified page\n        page = pdf_reader.pages[page_number_to_extract - 1]\n        page_text = page.extract_text()\n        print(page_text)\n    else:\n        print(\"Page number is out of range.\")\n",
        "instruction": "Create a Python program using the 'pypdf' API to extract and display the content of a specific page (e.g., page 3) from a PDF file.",
        "prediction": "import pypdf\r\nimport PyPDF2\r\n\r\ndef extract_text_from_pdf(pdf_path):\r\n    with open(pdf_path, 'rb') as fh:\r\n        for page in PyPDF2.PdfFileReader(fh).pages:\r\n            print(page.extractText())\r\n\r\nif __name__ == '__main__':\r\n    path = 'C:\\\\Users\\\\Siddharth\\\\Desktop\\\\Python\\\\Python_Automation\\\\Automate_The_Boring_Stuff\\\\files\\\\example.pdf'\r\n    extract_text_from_pdf(path)\r\n\r\n# Output\r\n# Hello World",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'pypdf' API to extract and display the content of a specific page (e.g., page 3) from a PDF file.\n\n### Code:"
    },
    {
        "api": "pytorch-lightning",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install pytorch-lightning\nimport torch\nimport pytorch_lightning as pl\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\n\nclass DataModule(pl.LightningDataModule):\n    def __init__(\n        self,\n        model_name=\"google/bert_uncased_L-2_H-128_A-2\",\n        batch_size=64,\n        max_length=128,\n    ):\n        super().__init__()\n\n        self.batch_size = batch_size\n        self.max_length = max_length\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    def prepare_data(self):\n        cola_dataset = load_dataset(\"glue\", \"cola\")\n        self.train_data = cola_dataset[\"train\"]\n        self.val_data = cola_dataset[\"validation\"]\n\n    def tokenize_data(self, example):\n        return self.tokenizer(\n            example[\"sentence\"],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_length,\n        )\n\n    def setup(self, stage=None):\n        if stage == \"fit\" or stage is None:\n            self.train_data = self.train_data.map(self.tokenize_data, batched=True)\n            self.train_data.set_format(\n                type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n            )\n\n            self.val_data = self.val_data.map(self.tokenize_data, batched=True)\n            self.val_data.set_format(\n                type=\"torch\",\n                columns=[\"input_ids\", \"attention_mask\", \"label\"],\n                output_all_columns=True,\n            )\n\n    def train_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.train_data, batch_size=self.batch_size, shuffle=True\n        )\n\n    def val_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.val_data, batch_size=self.batch_size, shuffle=False\n        )\n\n\nif __name__ == \"__main__\":\n    data_model = DataModule()\n    data_model.prepare_data()\n    data_model.setup()\n    print(next(iter(data_model.train_dataloader()))[\"input_ids\"].shape)",
        "instruction": "Create a Python program using the 'pytorch-lightning' API to define a 'DataModule' class that prepares and tokenizes data for a natural language processing task. The 'DataModule' should load a dataset using the 'datasets' and 'transformers' libraries, tokenize the data with a specific model tokenizer, and set up data loaders. The program should demonstrate the use of 'pytorch-lightning' to create a 'DataModule' instance, prepare data, set it up, and print the shape of a batch from the data loader.",
        "prediction": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn import init\n\nclass DataModule(nn.Module):\n    def __init__(self, input_dim, output_dim, hidden_dim, num_layers, dropout, device):\n        super(DataModule, self).__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.device = device\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.hidden = None\n        self.cell = None\n        self.init_weights()\n\n    def init_weights(self):\n        init.xavier_normal_(self.fc.weight)\n        init.xavier_normal_(self.lstm.weight_hh_l0)\n        init.xavier_normal_(self.lstm.weight_ih_l0)\n        init.constant_(self.lstm.bias_hh_l0, 0)\n        init.constant_(self.lstm.bias_ih_l0, 0)\n\n    def init_hidden(self, batch_size):\n        self.hidden = (Variable(torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(self.device)),\n                       Variable(torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(self.device)))\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        self.init_hidden(batch_size)\n        out, (self.hidden, self.cell) = self.lstm(x, self.hidden)\n        out = self.fc(out[:, -1, :])\n        return out",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'pytorch-lightning' API to define a 'DataModule' class that prepares and tokenizes data for a natural language processing task. The 'DataModule' should load a dataset using the 'datasets' and 'transformers' libraries, tokenize the data with a specific model tokenizer, and set up data loaders. The program should demonstrate the use of 'pytorch-lightning' to create a 'DataModule' instance, prepare data, set it up, and print the shape of a batch from the data loader.\n\n### Code:"
    },
    {
        "api": "pytorch-lightning",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install pytorch-lightning\nimport pytorch_lightning as pl\nimport torch\n\n# Define a custom callback for logging losses\nclass LossLoggingCallback(pl.Callback):\n    def on_epoch_end(self, trainer, pl_module):\n        train_loss = trainer.callback_metrics[\"train_loss_step\"]\n        val_loss = trainer.callback_metrics[\"val_loss_step\"]\n        print(f\"Epoch {trainer.current_epoch}: Train Loss - {train_loss:.4f}, Val Loss - {val_loss:.4f}\")\n\n# Create a LightningModule for demonstration\nclass ExampleModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(10, 1)\n\n    def forward(self, x):\n        return self.l1(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = torch.nn.functional.mse_loss(y_hat, y)\n        self.log(\"train_loss_step\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = torch.nn.functional.mse_loss(y_hat, y)\n        self.log(\"val_loss_step\", loss)\n\nif __name__ == \"__main__\":\n    model = ExampleModel()\n\n    trainer = pl.Trainer(\n        gpus=0,\n        max_epochs=5,\n        callbacks=[LossLoggingCallback()],\n    )\n\n    trainer.fit(model)",
        "instruction": "Create a Python program using the 'pytorch-lightning' API to define a custom callback that logs the training loss and validation loss during training. The program should demonstrate the use of this custom callback to monitor and log losses during model training.",
        "prediction": "import torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\nfrom torchvision.utils import make_grid\n\nfrom pytorch_lightning import LightningDataModule, LightningModule, Trainer\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n\nfrom model import Model\n\nclass MNISTDataModule(LightningDataModule):\n    def __init__(self, data_dir: str = './', batch_size: int = 64, num_workers: int = 0):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n\n    def prepare_data(self):\n        MNIST(self.data_dir, train=True, download=True)\n        MNIST(self.data_dir, train=False, download=True)\n\n    def setup(self, stage=None):\n        if stage == 'fit' or stage is None:\n            mnist_train = MNIST(self.data_dir, train=True, transform=transforms.ToTensor())\n            self.train_dataset = mnist_train\n\n        if stage == 'test' or stage is None:\n            mnist_test = MNIST(self.data_dir, train=False, transform=transforms.ToTensor())\n            self.test_dataset = mnist_test\n\n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n\n    def test_dataloader(self):\n        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)\n\nclass MNISTModel(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = Model()\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss(logits, y)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss(logits, y)\n        _, predicted = torch.max(logits, 1)\n        num_correct = (predicted == y).sum()\n        return {'val_loss': loss, 'val_acc': num_correct / x.shape[0]}\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        avg_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n        self.log('val_loss', avg_loss, prog_bar=True)\n        self.log('val_acc', avg_acc, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss(logits, y)\n        _, predicted = torch.max(logits, 1)\n        num_correct = (predicted == y).sum()\n        return {'test_loss': loss, 'test_acc': num_correct / x.shape[0]}\n\n    def test_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n        avg_acc = torch.stack([x['test_acc'] for x in outputs]).mean()\n        self.log('test_loss', avg_loss, prog_bar=True)\n        self.log('test_acc', avg_acc, prog_bar=True)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n        return optimizer\n\n    def loss(self, logits, labels):\n        return torch.nn.",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'pytorch-lightning' API to define a custom callback that logs the training loss and validation loss during training. The program should demonstrate the use of this custom callback to monitor and log losses during model training.\n\n### Code:"
    },
    {
        "api": "rapidfuzz",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install rapidfuzz\nfrom rapidfuzz import fuzz, process\n\ntarget_strings = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\", \"fig\", \"grape\"]\n\nquery_string = \"cranberry\"\n\nprint(\"Query: \", query_string)\n\nresults = process.extract(query_string, target_strings, scorer=fuzz.WRatio, limit=len(target_strings))\n\nfor result in results:\n    target, score, _ = result\n    token_ratio_score = fuzz.token_ratio(query_string, target)\n    print(f\"{target}: Weighted Ratio Score {score}, Token Ratio Score {token_ratio_score}\")",
        "instruction": "Create a Python program using the 'rapidfuzz' API to demonstrate fuzzy string matching. The program should take a query string and compare it to a list of target strings using the 'fuzz.WRatio' scorer from 'rapidfuzz'. Print the query string and the similarity score for each target string. Additionally, the program should calculate and print the token ratio similarity score for each target string. The goal is to find similar strings in the list based on both the weighted ratio and token ratio similarity scores.",
        "prediction": "import urllib.request\nimport urllib.parse\nimport json\n\ndef get_fuzz_list(target_str):\n    fuzz_list = []\n    for i in range(0, len(target_str)):\n        for j in range(0, len(target_str)):\n            fuzz_list.append(target_str[i:j])\n    return fuzz_list\n\ndef get_fuzz_list_2(target_str):\n    fuzz_list = []\n    for i in range(0, len(target_str)):\n        for j in range(0, len(target_str)):\n            for k in range(0, len(target_str)):\n                fuzz_list.append(target_str[i:j:k])\n    return fuzz_list\n\ndef get_fuzz_list_3(target_str):\n    fuzz_list = []\n    for i in range(0, len(target_str)):\n        for j in range(0, len(target_str)):\n            for k in range(0, len(target_str)):\n                for l in range(0, len(target_str)):\n                    fuzz_list.append(target_str[i:j:k:l])\n    return fuzz_list\n\ndef get_fuzz_list_4(target_str):\n    fuzz_list = []\n    for i in range(0, len(target_str)):\n        for j in range(0, len(target_str)):\n            for k in range(0, len(target_str)):\n                for l in range(0, len(target_str)):\n                    for m in range(0, len(target_str)):\n                        fuzz_list.append(target_str[i:j:k:l:m])\n    return fuzz_list\n\ndef get_fuzz_list_5(target_str):\n    fuzz_list = []\n    for i in range(0, len(target_str)):\n        for j in range(0, len(target_str)):\n            for k in range(0, len(target_str)):\n                for l in range(0, len(target_str)):\n                    for m in range(0, len(target_str)):\n                        for n in range(0, len(target_str)):\n                            fuzz_list.append(target_str[i:j:k:l:m:n])\n    return fuzz_list\n\ndef get_fuzz_list_6(target_str):\n    fuzz_list = []\n    for i in range(0, len(target_str)):\n        for j in range(0, len(target_str)):\n            for k in range(0, len(target_str)):\n                for l in range(0, len(target_str)):\n                    for m in range(0, len(target_str)):\n                        for n in range(0, len(target_str)):\n                            for o in range(0, len(target_str)):\n                                fuzz_list.append(target_str[i:j:k:l:m:n:o])\n    return fuzz_list\n\ndef get_fuzz_list_7(target_str):\n    fuzz_list = []\n    for i in range(0, len(target_str)):\n        for j in range(0, len(target_str)):\n            for k in range(0, len(target_str)):\n                for l in range(0, len(target_str)):\n                    for m in range(0, len(target_str)):\n                        for n in range(0, len(target_str)):\n                            for o in range(0, len(target_str)):\n                                for p in range(0, len(target_str)):\n                                    fuzz_list.append(target_str[i:j:k:l:m:n:o:p])\n    return fuzz_list\n\ndef get_fuzz_list_8(target_str):\n    fuzz_list = []\n    for i in range(0, len(target_str)):\n        for j in range(0, len(target_str)):\n            for k in range(0, len(target_str)):\n                for l in range(",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'rapidfuzz' API to demonstrate fuzzy string matching. The program should take a query string and compare it to a list of target strings using the 'fuzz.WRatio' scorer from 'rapidfuzz'. Print the query string and the similarity score for each target string. Additionally, the program should calculate and print the token ratio similarity score for each target string. The goal is to find similar strings in the list based on both the weighted ratio and token ratio similarity scores.\n\n### Code:"
    },
    {
        "api": "rapidfuzz",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install rapidfuzz\nfrom rapidfuzz import fuzz, process\n\ntarget_strings = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\", \"fig\", \"grape\"]\n\nquery_string = \"cranberry\"\n\nprint(\"Query: \", query_string)\n\nresults = process.extract(query_string, target_strings, scorer=fuzz.WRatio, limit=len(target_strings))\n\nfor result in results:\n    target, score, _ = result\n    token_set_ratio_score = fuzz.token_set_ratio(query_string, target)\n    print(f\"{target}: Weighted Ratio Score {score}, Token Set Ratio Score {token_set_ratio_score}\")",
        "instruction": "Create a Python program using the 'rapidfuzz' API to demonstrate fuzzy string matching. The program should take a query string and compare it to a list of target strings using the 'fuzz.WRatio' scorer from 'rapidfuzz'. Print the query string and the similarity score for each target string. Additionally, the program should calculate and print the token set ratio similarity score for each target string. The goal is to find similar strings in the list based on both the weighted ratio and token set ratio similarity scores.",
        "prediction": "import urllib.request\nimport urllib.parse\nimport json\n\ndef get_fuzz_list(target_str):\n    fuzz_list = []\n    for i in range(0, len(target_str)):\n        for j in range(0, len(target_str)):\n            fuzz_list.append(target_str[i:j])\n    return fuzz_list\n\ndef get_fuzz_list_2(target_str):\n    fuzz_list = []\n    for i in range(0, len(target_str)):\n        for j in range(0, len(target_str)):\n            for k in range(0, len(target_str)):\n                fuzz_list.append(target_str[i:j:k])\n    return fuzz_list\n\ndef get_fuzz_list_3(target_str):\n    fuzz_list = []\n    for i in range(0, len(target_str)):\n        for j in range(0, len(target_str)):\n            for k in range(0, len(target_str)):\n                for l in range(0, len(target_str)):\n                    fuzz_list.append(target_str[i:j:k:l])\n    return fuzz_list\n\ndef get_fuzz_list_4(target_str):\n    fuzz_list = []\n    for i in range(0, len(target_str)):\n        for j in range(0, len(target_str)):\n            for k in range(0, len(target_str)):\n                for l in range(0, len(target_str)):\n                    for m in range(0, len(target_str)):\n                        fuzz_list.append(target_str[i:j:k:l:m])\n    return fuzz_list\n\ndef get_fuzz_list_5(target_str):\n    fuzz_list = []\n    for i in range(0, len(target_str)):\n        for j in range(0, len(target_str)):\n            for k in range(0, len(target_str)):\n                for l in range(0, len(target_str)):\n                    for m in range(0, len(target_str)):\n                        for n in range(0, len(target_str)):\n                            fuzz_list.append(target_str[i:j:k:l:m:n])\n    return fuzz_list\n\ndef get_fuzz_list_6(target_str):\n    fuzz_list = []\n    for i in range(0, len(target_str)):\n        for j in range(0, len(target_str)):\n            for k in range(0, len(target_str)):\n                for l in range(0, len(target_str)):\n                    for m in range(0, len(target_str)):\n                        for n in range(0, len(target_str)):\n                            for o in range(0, len(target_str)):\n                                fuzz_list.append(target_str[i:j:k:l:m:n:o])\n    return fuzz_list\n\ndef get_fuzz_list_7(target_str):\n    fuzz_list = []\n    for i in range(0, len(target_str)):\n        for j in range(0, len(target_str)):\n            for k in range(0, len(target_str)):\n                for l in range(0, len(target_str)):\n                    for m in range(0, len(target_str)):\n                        for n in range(0, len(target_str)):\n                            for o in range(0, len(target_str)):\n                                for p in range(0, len(target_str)):\n                                    fuzz_list.append(target_str[i:j:k:l:m:n:o:p])\n    return fuzz_list\n\ndef get_fuzz_list_8(target_str):\n    fuzz_list = []\n    for i in range(0, len(target_str)):\n        for j in range(0, len(target_str)):\n            for k in range(0, len(target_str)):\n                for l in range(",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'rapidfuzz' API to demonstrate fuzzy string matching. The program should take a query string and compare it to a list of target strings using the 'fuzz.WRatio' scorer from 'rapidfuzz'. Print the query string and the similarity score for each target string. Additionally, the program should calculate and print the token set ratio similarity score for each target string. The goal is to find similar strings in the list based on both the weighted ratio and token set ratio similarity scores.\n\n### Code:"
    },
    {
        "api": "rich",
        "decoder_input_ids": false,
        "ground-truth": "# Install rich\n#!pip install rich\nfrom rich.console import Console\nfrom rich.table import Table\n\n# Define a list of famous historical quotes\nquotes = [\n    {\n        \"text\": \"Those who cannot remember the past are condemned to repeat it.\",\n        \"author\": \"George Santayana\",\n        \"source\": \"The Life of Reason\",\n    },\n    {\n        \"text\": \"In the end, we will remember not the words of our enemies, but the silence of our friends.\",\n        \"author\": \"Martin Luther King Jr.\",\n        \"source\": \"Strength to Love\",\n    },\n    {\n        \"text\": \"Never interrupt your enemy when he is making a mistake.\",\n        \"author\": \"Napoleon Bonaparte\",\n        \"source\": \"Letter to Charles-Maurice Talleyrand-Perigord\",\n    },\n    {\n        \"text\": \"We must, indeed, all hang together or, most assuredly, we shall all hang separately.\",\n        \"author\": \"Benjamin Franklin\",\n        \"source\": \"In the Continental Congress just before signing the Declaration of Independence\",\n    },\n]\n\n# Create a rich table for famous historical quotes\nconsole = Console()\ntable = Table(title=\"Famous Historical Quotes\", style=\"italic\")\ntable.add_column(\"Quote\", style=\"bold\")\ntable.add_column(\"Author\", style=\"bold\")\ntable.add_column(\"Source\", style=\"bold\")\n\n# Populate the table with quote data\nfor quote in quotes:\n    table.add_row(quote[\"text\"], quote[\"author\"], quote[\"source\"])\n\n# Display the famous historical quotes\nconsole.print(table)\n\n",
        "instruction": "Create a Python program using the 'rich' API to display a colorful and styled list of famous historical quotes. The program should include the quote text, the author, and the source of the quote.",
        "prediction": ".rich.console.Console()\n\nconsole.print(\"[bold red]This is a [bold green]bold[/bold green] and [bold blue]colored[/bold blue] text!\", style=\"bold #00FF00 on #0000FF\")\nconsole.print(\"[bold red]This is a [bold green]bold[/bold green] and [bold blue]colored[/bold blue] text!\", style=\"bold #00FF00 on #0000FF\")\nconsole.print(\"[bold red]This is a [bold green]bold[/bold green] and [bold blue]colored[/bold blue] text!\", style=\"bold #00FF00 on #0000FF\")\nconsole.print(\"[bold red]This is a [bold green]bold[/bold green] and [bold blue]colored[/bold blue] text!\", style=\"bold #00FF00 on #0000FF\")\nconsole.print(\"[bold red]This is a [bold green]bold[/bold green] and [bold blue]colored[/bold blue] text!\", style=\"bold #00FF00 on #0000FF\")\nconsole.print(\"[bold red]This is a [bold green]bold[/bold green] and [bold blue]colored[/bold blue] text!\", style=\"bold #00FF00 on #0000FF\")\nconsole.print(\"[bold red]This is a [bold green]bold[/bold green] and [bold blue]colored[/bold blue] text!\", style=\"bold #00FF00 on #0000FF\")\nconsole.print(\"[bold red]This is a [bold green]bold[/bold green] and [bold blue]colored[/bold blue] text!\", style=\"bold #00FF00 on #0000FF\")\nconsole.print(\"[bold red]This is a [bold green]bold[/bold green] and [bold blue]colored[/bold blue] text!\", style=\"bold #00FF00 on #0000FF\")\nconsole.print(\"[bold red]This is a [bold green]bold[/bold green] and [bold blue]colored[/bold blue] text!\", style=\"bold #00FF00 on #0000FF\")\nconsole.print(\"[bold red]This is a [bold green]bold[/bold green] and [bold blue]colored[/bold blue] text!\", style=\"bold #00FF00 on #0000FF\")\nconsole.print(\"[bold red]This is a [bold green]bold[/bold green] and [bold blue]colored[/bold blue] text!\", style=\"bold #00FF00 on #0000FF\")\nconsole.print(\"[bold red]This is a [bold green]bold[/bold green] and [bold blue]colored[/bold blue] text!\", style=\"bold #00FF00 on #0000FF\")\nconsole.print(\"[bold red]This is a [bold green]bold[/bold green] and [bold blue]colored[/bold blue] text!\", style=\"bold #00FF00 on #0000FF\")\nconsole.print(\"[bold red]This is a [bold green]bold[/bold green] and [bold blue]colored[/bold blue] text!\", style=\"bold #00FF00 on #0000FF\")\nconsole.print(\"[bold red]This is a [bold green]bold[/bold green] and [bold blue]colored[/bold blue] text!\", style=\"bold #00FF00 on #0000FF\")\nconsole.print(\"[bold red]This is a [bold green]bold[/bold green] and [bold blue]colored[/bold blue] text!\", style=\"bold #00FF00 on #0000FF\")\nconsole.print(\"[bold red]This is a [bold green]bold[/bold green] and [bold blue]colored[/bold blue] text!\", style=\"bold #00FF00 on #0000FF\")\nconsole.print(\"[bold red]This is a [bold green]bold[/bold green] and [bold blue]colored[/bold blue] text!\", style=\"bold #00FF00 on #0000FF\")\nconsole.print(\"[bold red]This is a [bold green]bold[/bold green] and [bold blue]colored[/bold blue] text!\", style=\"bold #00FF00 on #0000FF\")\nconsole.print(\"[bold red]This is a [bold green]bold[/bold green] and [bold blue]colored[/bold blue] text!\", style=\"bold #00FF00 on #0000FF\")\nconsole.print(\"[bold red]This is a [bold green]bold[/bold green] and [bold blue]colored[/bold blue] text!\", style=\"bold #00FF00 on #0000FF\")\nconsole.print(\"[bold red]This is a [bold green]bold[/bold green] and [bold blue]",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'rich' API to display a colorful and styled list of famous historical quotes. The program should include the quote text, the author, and the source of the quote.\n\n### Code:"
    },
    {
        "api": "rich",
        "decoder_input_ids": false,
        "ground-truth": "# Install rich\n#!pip install rich\nimport time\nfrom rich.console import Console\n\n# Create a real-time digital clock\nconsole = Console()\nwhile True:\n    console.clear()\n    current_time = time.strftime(\"%H:%M:%S\")\n    console.print(current_time, style=\"bold yellow on green\")\n    time.sleep(1)\n\n",
        "instruction": "Generate a Python program using the 'rich' API to display a stylish and colored digital clock that shows the current time. The program should update the time every second.",
        "prediction": "import rich\nimport rich.console\nimport rich.markdown\nimport rich.panel\nimport rich.syntax\nimport rich.table\nimport rich.text\nimport rich.theme\nimport rich.style\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich.style\nimport rich.text\nimport rich.theme\nimport rich.console\nimport rich.panel\nimport rich.syntax\nimport rich",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nGenerate a Python program using the 'rich' API to display a stylish and colored digital clock that shows the current time. The program should update the time every second.\n\n### Code:"
    },
    {
        "api": "rich",
        "decoder_input_ids": false,
        "ground-truth": "# Install rich\n#!pip install rich\nfrom rich.console import Console\nfrom rich.table import Table\n\n# Define a list of inspirational quotes\nquotes = [\n    {\n        \"text\": \"The future belongs to those who believe in the beauty of their dreams.\",\n        \"author\": \"Eleanor Roosevelt\",\n        \"source\": \"You Learn by Living: Eleven Keys for a More Fulfilling Life\",\n    },\n    {\n        \"text\": \"The only limit to our realization of tomorrow will be our doubts of today.\",\n        \"author\": \"Franklin D. Roosevelt\",\n        \"source\": \"Great Speeches\",\n    },\n    {\n        \"text\": \"Believe you can and you're halfway there.\",\n        \"author\": \"Theodore Roosevelt\",\n        \"source\": \"Quoted in Half-Hearted Enemies: Nova Scotia Texts\",\n    },\n    {\n        \"text\": \"The greatest glory in living lies not in never falling, but in rising every time we fall.\",\n        \"author\": \"Nelson Mandela\",\n        \"source\": \"Long Walk to Freedom: The Autobiography of Nelson Mandela\",\n    },\n]\n\n# Create a rich table for inspirational quotes\nconsole = Console()\ntable = Table(title=\"Inspirational Quotes\", style=\"italic\")\ntable.add_column(\"Quote\", style=\"bold\")\ntable.add_column(\"Author\", style=\"bold\")\ntable.add_column(\"Source\", style=\"bold\")\n\n# Populate the table with quote data\nfor quote in quotes:\n    table.add_row(quote[\"text\"], quote[\"author\"], quote[\"source\"])\n\n# Display the inspirational quotes\nconsole.print(table)\n\n",
        "instruction": "Generate a Python program using the 'rich' API to display a styled and colorful list of inspirational quotes. The program should include the quote text, the author, and the source of the quote.",
        "prediction": "import rich\nimport rich.console\nimport rich.panel\nimport rich.style\nimport rich.syntax\nimport rich.table\nimport rich.text\nimport rich.theme\n\nconsole = rich.console.Console()\n\ntheme = rich.theme.Theme({\n    \"info\": \"green\",\n    \"warning\": \"yellow\",\n    \"error\": \"bold red\",\n    \"question\": \"cyan\",\n    \"inspect\": \"cyan\",\n    \"prompt\": \"cyan\",\n    \"header\": \"bold magenta\",\n    \"strong\": \"bold\",\n    \"link\": \"cyan\",\n    \"list\": \"cyan\",\n    \"table\": \"cyan\",\n    \"table.header\": \"bold cyan\",\n    \"table.footer\": \"cyan\",\n    \"table.cell\": \"cyan\",\n    \"table.title\": \"bold cyan\",\n    \"table.caption\": \"cyan\",\n    \"table.title_row\": \"bold cyan\",\n    \"table.header_row\": \"bold cyan\",\n    \"table.footer_row\": \"cyan\",\n    \"table.row\": \"cyan\",\n    \"table.cell_row\": \"cyan\",\n    \"table.title_cell\": \"bold cyan\",\n    \"table.header_cell\": \"bold cyan\",\n    \"table.footer_cell\": \"cyan\",\n    \"table.cell_cell\": \"cyan\",\n    \"table.title_cell_row\": \"bold cyan\",\n    \"table.header_cell_row\": \"bold cyan\",\n    \"table.footer_cell_row\": \"cyan\",\n    \"table.cell_cell_row\": \"cyan\",\n    \"table.title_row_row\": \"bold cyan\",\n    \"table.header_row_row\": \"bold cyan\",\n    \"table.footer_row_row\": \"cyan\",\n    \"table.cell_row_row\": \"cyan\",\n    \"table.title_row_cell\": \"bold cyan\",\n    \"table.header_row_cell\": \"bold cyan\",\n    \"table.footer_row_cell\": \"cyan\",\n    \"table.cell_row_cell\": \"cyan\",\n    \"table.title_row_cell_row\": \"bold cyan\",\n    \"table.header_row_cell_row\": \"bold cyan\",\n    \"table.footer_row_cell_row\": \"cyan\",\n    \"table.cell_row_cell_row\": \"cyan\",\n    \"table.title_row_row_cell\": \"bold cyan\",\n    \"table.header_row_row_cell\": \"bold cyan\",\n    \"table.footer_row_row_cell\": \"cyan\",\n    \"table.cell_row_row_cell\": \"cyan\",\n    \"table.title_row_row_cell_row\": \"bold cyan\",\n    \"table.header_row_row_cell_row\": \"bold cyan\",\n    \"table.footer_row_row_cell_row\": \"cyan\",\n    \"table.cell_row_row_cell_row\": \"cyan\",\n    \"table.title_row_row_row_cell\": \"bold cyan\",\n    \"table.header_row_row_row_cell\": \"bold cyan\",\n    \"table.footer_row_row_row_cell\": \"cyan\",\n    \"table.cell_row_row_row_cell\": \"cyan\",\n    \"table.title_row_row_row_row_cell\": \"bold cyan\",\n    \"table.header_row_row_row_row_cell\": \"bold cyan\",\n    \"table.footer_row_row_row_row_cell\": \"cyan\",\n    \"table.cell_row_row_row_row_cell\": \"cyan\",\n    \"table.title_row_row_row_row_row_cell\": \"bold cyan\",\n    \"table.header_row_row_row_row_row_cell\": \"bold cyan\",\n    \"table.footer_row_row_row_row_row_cell\": \"cyan\",\n    \"table.cell_row_row_row_row_row_cell\": \"cyan\",\n    \"table.title_row_row_row_row_row_row_cell\": \"bold cyan\",\n    \"table.header_row_row_row_row_",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nGenerate a Python program using the 'rich' API to display a styled and colorful list of inspirational quotes. The program should include the quote text, the author, and the source of the quote.\n\n### Code:"
    },
    {
        "api": "rich",
        "decoder_input_ids": false,
        "ground-truth": "# Install rich\n#!pip install rich\nimport time\nfrom rich.progress import Progress\n\n# Simulate video rendering process\ndef render_video():\n    for _ in range(100):\n        time.sleep(0.1)\n\n# Create a styled progress bar for video rendering\nwith Progress() as progress:\n    task = progress.add_task(\"[cyan]Rendering video...\", total=100)\n    progress.console.show_cursor(False)\n    \n    # Simulate video rendering and update the progress\n    for _ in range(100):\n        time.sleep(0.1)\n        progress.update(task, completed=1)\n        progress.refresh()\n\n# Display completion message\nprogress.console.show_cursor(True)\nprint(\"[green]Video rendering completed!\")\n\n",
        "instruction": "Generate a Python program using the 'rich' API to display a colorful and styled progress bar that shows the progress of a video rendering process. The program should update the progress as the rendering continues.",
        "prediction": "import rich\nimport rich.progress\nimport rich.progress.track\nimport rich.progress.bar\nimport rich.progress.spinner\nimport rich.progress.text\nimport rich.progress.track\nimport rich.progress.Progress\nimport rich.progress.BarColumn\nimport rich.progress.TextColumn\nimport rich.progress.TimeElapsedColumn\nimport rich.progress.TimeRemainingColumn\nimport rich.progress.ProgressColumn\nimport rich.progress.ProgressList",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nGenerate a Python program using the 'rich' API to display a colorful and styled progress bar that shows the progress of a video rendering process. The program should update the progress as the rendering continues.\n\n### Code:"
    },
    {
        "api": "rustworkx",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install rustworkx\nimport rustworkx as rx\n\ngraph = rx.PyGraph()\n\n# Each time add node is called, it returns a new node index\na = graph.add_node(\"A\")\nb = graph.add_node(\"B\")\nc = graph.add_node(\"C\")\nd = graph.add_node(\"D\")\n\n# add_edges_from takes tuples of node indices and weights,\n# and returns edge indices\nedge_indices = graph.add_edges_from([(a, b, 1.5), (a, d, 2.0), (b, c, 2.5),(d, c, 1.5)])\nprint(edge_indices)\n\n# Returns the path \npath_mapping = rx.dijkstra_shortest_paths(graph, a, c, weight_fn=float)\nprint(path_mapping)",
        "instruction": "Create a Python program using the 'rustworkx' API to define a directed graph and find the shortest path between two nodes. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish weighted edges between these nodes and print the indices of the added edges. Then, use Dijkstra's shortest path algorithm to find and print the shortest path from node 'A' to node 'C'",
        "prediction": "from typing import List, Dict, Union, Tuple\nfrom collections import defaultdict\nfrom itertools import combinations\nfrom functools import reduce\nfrom math import factorial\nfrom time import time\n\nfrom. import Graph, Node, Edge, EdgeType, EdgeDirection, EdgeWeight, EdgeWeightType\nfrom. import GraphType, GraphWeightType, GraphWeightTypeInt, GraphWeightTypeFloat\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeightTypeInt, GraphWeightTypeFloat, GraphWeightTypeBool\nfrom. import GraphWeight",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'rustworkx' API to define a directed graph and find the shortest path between two nodes. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish weighted edges between these nodes and print the indices of the added edges. Then, use Dijkstra's shortest path algorithm to find and print the shortest path from node 'A' to node 'C'\n\n### Code:"
    },
    {
        "api": "rustworkx",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install rustworkx\nimport rustworkx as rx\n\ngraph = rx.PyGraph()\n\n# Each time add node is called, it returns a new node index\na = graph.add_node(\"A\")\nb = graph.add_node(\"B\")\nc = graph.add_node(\"C\")\nd = graph.add_node(\"D\")\n\n# add_edges_from takes tuples of node indices and weights,\n# and returns edge indices\nedge_indices = graph.add_edges_from([(a, b, 1.5), (a, d, 2.0), (b, c, 2.5),(d, c, 1.5)])\nprint(edge_indices)\n\n# Returns the minimum spanning tree\nmst = rx.minimum_spanning_tree(graph, weight_fn=float)\nprint(mst)",
        "instruction": "Create a Python program using the 'rustworkx' API to define an undirected graph and find the minimum spanning tree. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish weighted edges between these nodes and print the indices of the added edges. Then, use Kruskal's algorithm to find and print the minimum spanning tree of the graph.",
        "prediction": "from typing import List, Union\n\nfrom.graph import Graph\nfrom.node import Node\nfrom.edge import Edge\nfrom.node_type import NodeType\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom.node_type_enum import NodeTypeEnum\nfrom",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'rustworkx' API to define an undirected graph and find the minimum spanning tree. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish weighted edges between these nodes and print the indices of the added edges. Then, use Kruskal's algorithm to find and print the minimum spanning tree of the graph.\n\n### Code:"
    },
    {
        "api": "scikit-learn",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install scikit-learn\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.tree import DecisionTreeClassifier\n\n#Load the LFW dataset\nlfw = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\nX, y = lfw.data, lfw.target\n\n#Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n#Initialize the Decision Tree classifier\ndt_classifier = DecisionTreeClassifier()\n\n#Train the classifier\ndt_classifier.fit(X_train, y_train)\n\n#Make predictions\ny_pred = dt_classifier.predict(X_test)\n\n#Calculate accuracy\naccuracy = (y_pred == y_test).mean()\nprint(f'Accuracy: {accuracy:.2f}')\n\n#Serialize and save the model\njoblib.dump(dt_classifier, 'lfw_classifier.pkl')\n",
        "instruction": "Create a Python program that uses the 'scikit-learn' API to load the LFW (Labeled Faces in the Wild) dataset, split it into training and testing sets, train a Decision Tree classifier, make predictions, calculate accuracy, and save the trained model.",
        "prediction": "import numpy as np\nimport sklearn.datasets as skdata\nimport sklearn.model_selection as skms\nimport sklearn.tree as sktree\nimport sklearn.metrics as skmetrics\n\n# Load the data\ndata = skdata.load_lfw_people()\n\n# Split the data into training and testing\ntrain_data, test_data, train_labels, test_labels = skms.train_test_split(\n    data.data, data.target, test_size=0.25, random_state=42)\n\n# Create the model\nmodel = sktree.DecisionTreeClassifier()\n\n# Train the model\nmodel.fit(train_data, train_labels)\n\n# Make predictions\npredictions = model.predict(test_data)\n\n# Evaluate the model\naccuracy = skmetrics.accuracy_score(test_labels, predictions)\n\nprint('Accuracy: {:.2f}'.format(accuracy))\n\n# Save the model\nmodel.save_model('model.pkl')\n\n# Load the model\nmodel = sktree.DecisionTreeClassifier()\nmodel.load_model('model.pkl')\n\n# Make predictions\npredictions = model.predict(test_data)\n\n# Evaluate the model\naccuracy = skmetrics.accuracy_score(test_labels, predictions)\n\nprint('Accuracy: {:.2f}'.format(accuracy))\n\n# Create the model\nmodel = sktree.DecisionTreeClassifier()\n\n# Train the model\nmodel.fit(train_data, train_labels)\n\n# Make predictions\npredictions = model.predict(test_data)\n\n# Evaluate the model\naccuracy = skmetrics.accuracy_score(test_labels, predictions)\n\nprint('Accuracy: {:.2f}'.format(accuracy))\n\n# Create the model\nmodel = sktree.DecisionTreeClassifier()\n\n# Train the model\nmodel.fit(train_data, train_labels)\n\n# Make predictions\npredictions = model.predict(test_data)\n\n# Evaluate the model\naccuracy = skmetrics.accuracy_score(test_labels, predictions)\n\nprint('Accuracy: {:.2f}'.format(accuracy))\n\n# Create the model\nmodel = sktree.DecisionTreeClassifier()\n\n# Train the model\nmodel.fit(train_data, train_labels)\n\n# Make predictions\npredictions = model.predict(test_data)\n\n# Evaluate the model\naccuracy = skmetrics.accuracy_score(test_labels, predictions)\n\nprint('Accuracy: {:.2f}'.format(accuracy))\n\n# Create the model\nmodel = sktree.DecisionTreeClassifier()\n\n# Train the model\nmodel.fit(train_data, train_labels)\n\n# Make predictions\npredictions = model.predict(test_data)\n\n# Evaluate the model\naccuracy = skmetrics.accuracy_score(test_labels, predictions)\n\nprint('Accuracy: {:.2f}'.format(accuracy))\n\n# Create the model\nmodel = sktree.DecisionTreeClassifier()\n\n# Train the model\nmodel.fit(train_data, train_labels)\n\n# Make predictions\npredictions = model.predict(test_data)\n\n# Evaluate the model\naccuracy = skmetrics.accuracy_score(test_labels, predictions)\n\nprint('Accuracy: {:.2f}'.format(accuracy))\n\n# Create the model\nmodel = sktree.DecisionTreeClassifier()\n\n# Train the model\nmodel.fit(train_data, train_labels)\n\n# Make predictions\npredictions = model.predict(test_data)\n\n# Evaluate the model\naccuracy = skmetrics.accuracy_score(test_labels, predictions)\n\nprint('Accuracy: {:.2f}'.format(accuracy))\n\n# Create the model\nmodel = sktree.DecisionTreeClassifier()\n\n# Train the model\nmodel.fit(train_data, train_labels)\n\n# Make predictions\npredictions = model.predict(test_data)\n\n# Evaluate the model\naccuracy = skmetrics.accuracy_score(test_",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that uses the 'scikit-learn' API to load the LFW (Labeled Faces in the Wild) dataset, split it into training and testing sets, train a Decision Tree classifier, make predictions, calculate accuracy, and save the trained model.\n\n### Code:"
    },
    {
        "api": "scikit-learn",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install scikit-learn\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Load the LFW dataset\nlfw = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\nX, y = lfw.data, lfw.target\n\n#Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n#Initialize the Random Forest classifier\nrf_classifier = RandomForestClassifier(n_estimators=100)\n\n#Train the classifier\nrf_classifier.fit(X_train, y_train)\n\n#Make predictions\ny_pred = rf_classifier.predict(X_test)\n\n#Calculate accuracy\naccuracy = (y_pred == y_test).mean()\nprint(f'Accuracy: {accuracy:.2f}')\n\n#Serialize and save the model\njoblib.dump(rf_classifier, 'lfw_classifier.pkl')\n",
        "instruction": "Create a Python program that uses the 'scikit-learn' API to load the LFW (Labeled Faces in the Wild) dataset, split it into training and testing sets, train a Random Forest classifier, make predictions, calculate accuracy, and save the trained model.",
        "prediction": "import numpy as np\nimport sklearn.datasets as skdata\nimport sklearn.model_selection as skms\nimport sklearn.svm as sksvm\nimport sklearn.metrics as skmetrics\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndata = skdata.load_lfw_people()\n\n# Split the data into training and testing sets\ntrain_data, test_data, train_labels, test_labels = skms.train_test_split(\n    data.data, data.target, test_size=0.2, random_state=42)\n\n# Create the model\nmodel = sksvm.SVC(kernel='linear', C=1)\n\n# Train the model\nmodel.fit(train_data, train_labels)\n\n# Make predictions on the testing set\npredictions = model.predict(test_data)\n\n# Calculate the accuracy\naccuracy = skmetrics.accuracy_score(test_labels, predictions)\nprint('Accuracy: %.2f' % (accuracy * 100))\n\n# Create a confusion matrix\ncm = skmetrics.confusion_matrix(test_labels, predictions)\nprint(cm)\n\n# Visualize the confusion matrix\nplt.matshow(cm)\nplt.title('Confusion Matrix')\nplt.colorbar()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n\n# Create a precision and recall curve\nprecision, recall, thresholds = skmetrics.precision_recall_curve(\n    test_labels, predictions)\n\n# Create a precision-recall curve\nplt.plot(recall, precision, 'g-', linewidth=2)\nplt.title('Precision-Recall Curve')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.show()\n\n# Create a ROC curve\nfpr, tpr, thresholds = skmetrics.roc_curve(test_labels, predictions)\n\n# Create a ROC curve\nplt.plot(fpr, tpr, 'b-', linewidth=2)\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()\n\n# Create a precision-recall curve for each class\nprecision = dict()\nrecall = dict()\nfor i in range(6):\n    precision[i], recall[i], _ = skmetrics.precision_recall_curve(\n        test_labels, predictions)\n\n# Create a precision-recall curve for each class\nplt.plot(recall[0], precision[0], 'b-', linewidth=2)\nplt.plot(recall[1], precision[1], 'g-', linewidth=2)\nplt.plot(recall[2], precision[2], 'r-', linewidth=2)\nplt.plot(recall[3], precision[3], 'c-', linewidth=2)\nplt.plot(recall[4], precision[4],'m-', linewidth=2)\nplt.plot(recall[5], precision[5], 'y-', linewidth=2)\nplt.title('Precision-Recall Curve')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.show()\n\n# Create a precision-recall curve for each class\nplt.plot(recall[0], precision[0], 'b-', linewidth=2)\nplt.plot(recall[1], precision[1], 'g-', linewidth=2)\nplt.plot(recall[2], precision[2], 'r-', linewidth=2)\nplt.plot(recall[3], precision[3], 'c-', linewidth=2)\nplt.plot(recall[4], precision[4],'m-', linewidth=2)\nplt.plot(recall[5], precision[5], 'y-', linewidth=2)\nplt.title('Precision-Recall Curve')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\n",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that uses the 'scikit-learn' API to load the LFW (Labeled Faces in the Wild) dataset, split it into training and testing sets, train a Random Forest classifier, make predictions, calculate accuracy, and save the trained model.\n\n### Code:"
    },
    {
        "api": "scikit-learn",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install scikit-learn\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Load the Breast Cancer dataset\nbreast_cancer = load_breast_cancer()\nX, y = breast_cancer.data, breast_cancer.target\n\n#Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n#Initialize the K-Nearest Neighbors classifier\nknn_classifier = KNeighborsClassifier(n_neighbors=3)\n\n#Train the classifier\nknn_classifier.fit(X_train, y_train)\n\n#Make predictions\ny_pred = knn_classifier.predict(X_test)\n\n#Calculate accuracy\naccuracy = (y_pred == y_test).mean()\nprint(f'Accuracy: {accuracy:.2f}')\n\n#Serialize and save the model\njoblib.dump(knn_classifier, 'breast_cancer_classifier.pkl')\n",
        "instruction": "Create a Python program that uses the 'scikit-learn' API to load the Breast Cancer dataset, split it into training and testing sets, train a K-Nearest Neighbors classifier, make predictions, calculate accuracy, and save the trained model.",
        "prediction": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Load the data\ndata = pd.read_csv('breast-cancer-data.csv')\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 2:32], data.iloc[:, 1],\n                                                    test_size=0.25, random_state=42)\n\n# Create the model\nknn = KNeighborsClassifier()\n\n# Fit the model\nknn.fit(X_train, y_train)\n\n# Make predictions\npredictions = knn.predict(X_test)\n\n# Evaluate the model\nprint(knn.score(X_test, y_test))\n\n# Create a confusion matrix\nconfusion_matrix = pd.crosstab(y_test, predictions, rownames=['Actual'], colnames=['Predicted'])\nprint(confusion_matrix)\n\n# Create a classification report\nclassification_report = pd.classification_report(y_test, predictions)\nprint(classification_report)\n\n# Create a model accuracy score\nmodel_accuracy = knn.score(X_test, y_test)\nprint(model_accuracy)\n\n# Create a model accuracy score\nmodel_accuracy = knn.score(X_test, y_test)\nprint(model_accuracy)\n\n# Create a model accuracy score\nmodel_accuracy = knn.score(X_test, y_test)\nprint(model_accuracy)\n\n# Create a model accuracy score\nmodel_accuracy = knn.score(X_test, y_test)\nprint(model_accuracy)\n\n# Create a model accuracy score\nmodel_accuracy = knn.score(X_test, y_test)\nprint(model_accuracy)\n\n# Create a model accuracy score\nmodel_accuracy = knn.score(X_test, y_test)\nprint(model_accuracy)\n\n# Create a model accuracy score\nmodel_accuracy = knn.score(X_test, y_test)\nprint(model_accuracy)\n\n# Create a model accuracy score\nmodel_accuracy = knn.score(X_test, y_test)\nprint(model_accuracy)\n\n# Create a model accuracy score\nmodel_accuracy = knn.score(X_test, y_test)\nprint(model_accuracy)\n\n# Create a model accuracy score\nmodel_accuracy = knn.score(X_test, y_test)\nprint(model_accuracy)\n\n# Create a model accuracy score\nmodel_accuracy = knn.score(X_test, y_test)\nprint(model_accuracy)\n\n# Create a model accuracy score\nmodel_accuracy = knn.score(X_test, y_test)\nprint(model_accuracy)\n\n# Create a model accuracy score\nmodel_accuracy = knn.score(X_test, y_test)\nprint(model_accuracy)\n\n# Create a model accuracy score\nmodel_accuracy = knn.score(X_test, y_test)\nprint(model_accuracy)\n\n# Create a model accuracy score\nmodel_accuracy = knn.score(X_test, y_test)\nprint(model_accuracy)\n\n# Create a model accuracy score\nmodel_accuracy = knn.score(X_test, y_test)\nprint(model_accuracy)\n\n# Create a model accuracy score\nmodel_accuracy = knn.score(X_test, y_test)\nprint(model_accuracy)\n\n# Create a model accuracy score\nmodel_accuracy = knn.score(X_test, y_test)\nprint(model_accuracy)\n\n# Create a model accuracy score\nmodel_accuracy = knn.score(X_test, y_test)\nprint(model_accuracy)\n\n# Create a model accuracy score\nmodel_accuracy = knn.score(X_test, y_test)\nprint(model_accuracy)\n\n# Create a model accuracy score\nmodel_accuracy",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that uses the 'scikit-learn' API to load the Breast Cancer dataset, split it into training and testing sets, train a K-Nearest Neighbors classifier, make predictions, calculate accuracy, and save the trained model.\n\n### Code:"
    },
    {
        "api": "scikit-learn",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install scikit-learn\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.cluster import KMeans\n\n#Load the Breast Cancer dataset\nbreast_cancer = load_breast_cancer()\nX, y = breast_cancer.data, breast_cancer.target\n\n#Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n#Initialize the K-Means clustering model\nkmeans = KMeans(n_clusters=2)\n\n#Train the model\nkmeans.fit(X_train)\n\n#Make predictions\ny_pred = kmeans.predict(X_test)\n\n#Serialize and save the model\njoblib.dump(kmeans, 'breast_cancer_kmeans_model.pkl')\n",
        "instruction": "Create a Python program that uses the 'scikit-learn' API to load the Breast Cancer dataset, split it into training and testing sets, train a K-Means clustering model, make predictions, and save the trained model.",
        "prediction": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Load the data\ndata = pd.read_csv('breast-cancer-data.csv')\n\n# Split the data into training and testing sets\nX = data.drop(['Class'], axis=1)\ny = data['Class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale the data\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Create the model\nmodel = KNeighborsClassifier(n_neighbors=5)\nmodel.fit(X_train, y_train)\n\n# Make predictions\npredictions = model.predict(X_test)\n\n# Evaluate the model\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))\n\n# Create a K-NN model\nmodel = KNeighborsClassifier(n_neighbors=5)\n\n# Fit the model\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n# Create a K-NN model\nmodel = KNeighborsClassifier(n_neighbors=5)\n\n# Fit the model\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n# Create a K-NN model\nmodel = KNeighborsClassifier(n_neighbors=5)\n\n# Fit the model\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n# Create a K-NN model\nmodel = KNeighborsClassifier(n_neighbors=5)\n\n# Fit the model\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n# Create a K-NN model\nmodel = KNeighborsClassifier(n_neighbors=5)\n\n# Fit the model\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n# Create a K-NN model\nmodel = KNeighborsClassifier(n_neighbors=5)\n\n# Fit the model\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n# Create a K-NN model\nmodel = KNeighborsClassifier(n_neighbors=5)\n\n# Fit the model\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that uses the 'scikit-learn' API to load the Breast Cancer dataset, split it into training and testing sets, train a K-Means clustering model, make predictions, and save the trained model.\n\n### Code:"
    },
    {
        "api": "scipy",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install scipy matplotlib\nimport scipy as sp\nimport matplotlib.pyplot as plt\n\nface = sp.datasets.face(gray=True)\n\nshifted_face = sp.ndimage.shift(face, (50, 50))\nrotated_face = sp.ndimage.rotate(face, 30)\n\nplt.figure(figsize=(15, 3))\nplt.subplot(151)\nplt.imshow(face, cmap=plt.cm.gray)\nplt.axis(\"off\")\n\nplt.subplot(152)\nplt.imshow(shifted_face, cmap=plt.cm.gray)\nplt.axis(\"off\")\n\nplt.subplot(153)\nplt.imshow(rotated_face, cmap=plt.cm.gray)\nplt.axis(\"off\")\n\nplt.subplots_adjust(wspace=0.05, left=0.01, bottom=0.01, right=0.99, top=0.99)\n\nplt.show()",
        "instruction": "Create a Python program using the 'scipy' API to apply various image transformations to a sample image. The program should perform operations like shifting and rotating the image and display the original and transformed images.",
        "prediction": "import scipy.io as sio\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef displayData(X):\n    num_plots = int(np.size(X, 0) / 5)\n    f, plots = plt.subplots(num_plots, 5, figsize=(5, num_plots * 1.2))\n    for i in range(num_plots):\n        for j in range(5):\n            plots[i, j].imshow(X[i * 5 + j].reshape(32, 32).T, cmap='gray')\n            plots[i, j].axis('off')\n    plt.show()\n\ndef displayImage(im):\n    plt.imshow(im.reshape(32, 32).T, cmap='gray')\n    plt.axis('off')\n    plt.show()\n\ndef displayImage(im):\n    plt.imshow(im.reshape(32, 32).T, cmap='gray')\n    plt.axis('off')\n    plt.show()\n\ndef displayImage(im):\n    plt.imshow(im.reshape(32, 32).T, cmap='gray')\n    plt.axis('off')\n    plt.show()\n\ndef displayImage(im):\n    plt.imshow(im.reshape(32, 32).T, cmap='gray')\n    plt.axis('off')\n    plt.show()\n\ndef displayImage(im):\n    plt.imshow(im.reshape(32, 32).T, cmap='gray')\n    plt.axis('off')\n    plt.show()\n\ndef displayImage(im):\n    plt.imshow(im.reshape(32, 32).T, cmap='gray')\n    plt.axis('off')\n    plt.show()\n\ndef displayImage(im):\n    plt.imshow(im.reshape(32, 32).T, cmap='gray')\n    plt.axis('off')\n    plt.show()\n\ndef displayImage(im):\n    plt.imshow(im.reshape(32, 32).T, cmap='gray')\n    plt.axis('off')\n    plt.show()\n\ndef displayImage(im):\n    plt.imshow(im.reshape(32, 32).T, cmap='gray')\n    plt.axis('off')\n    plt.show()\n\ndef displayImage(im):\n    plt.imshow(im.reshape(32, 32).T, cmap='gray')\n    plt.axis('off')\n    plt.show()\n\ndef displayImage(im):\n    plt.imshow(im.reshape(32, 32).T, cmap='gray')\n    plt.axis('off')\n    plt.show()\n\ndef displayImage(im):\n    plt.imshow(im.reshape(32, 32).T, cmap='gray')\n    plt.axis('off')\n    plt.show()\n\ndef displayImage(im):\n    plt.imshow(im.reshape(32, 32).T, cmap='gray')\n    plt.axis('off')\n    plt.show()\n\ndef displayImage(im):\n    plt.imshow(im.reshape(32, 32).T, cmap='gray')\n    plt.axis('off')\n    plt.show()\n\ndef displayImage(im):\n    plt.imshow(im.reshape(32, 32).T, cmap='gray')\n    plt.axis('off')\n    plt.show()\n\ndef displayImage(im):\n    plt.imshow(im.reshape(32, 32).T, cmap='gray')\n    plt.axis('off')\n    plt.show()\n\ndef displayImage(im):\n    plt.imshow(im.reshape(32, 32).T, cmap='gray')\n    plt.axis('off')\n    plt.show()\n\ndef displayImage(im):\n    plt.imshow(im.reshape(32, 32).T, cmap='gray')\n    plt.axis('off')\n    plt",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'scipy' API to apply various image transformations to a sample image. The program should perform operations like shifting and rotating the image and display the original and transformed images.\n\n### Code:"
    },
    {
        "api": "scipy",
        "decoder_input_ids": false,
        "ground-truth": "import scipy as sp\nfrom scipy.optimize import minimize\n\n# Define the objective function\ndef objective(x):\n    return x[0]**2 + x[1]**2\n\n# Define the nonlinear inequality constraints\ndef constraint(x):\n    return x[0]**2 + x[1]**2 - 1\n\n# Specify the optimization method\nmethod = 'SLSQP'\n\n# Specify the initial guess for the variables\nx0 = [0, 0]\n\n# Specify the constraints\nconstraints = [{'type': 'ineq', 'fun': constraint}]\n\n# Find the optimal solution with nonlinear inequality constraints\nresult = minimize(objective, x0, method=method, constraints=constraints)\n\n# Display the optimal solution\nprint(\"Optimal solution:\", result.x)",
        "instruction": "Create a Python program using the 'scipy' API to perform numerical optimization with nonlinear inequality constraints. The program should define an objective function to be minimized or maximized, specify the nonlinear inequality constraints, and use the 'scipy.optimize.minimize' function with the 'constraints' parameter to find the optimal solution.",
        "prediction": "import scipy.optimize as opt\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef objective(x):\n    return x[0]**2 + x[1]**2\n\ndef constraint1(x):\n    return x[0]**2 + x[1]**2 - 1.0\n\ndef constraint2(x):\n    return x[0]**2 + x[1]**2 - 0.5\n\ndef constraint3(x):\n    return x[0]**2 + x[1]**2 - 0.1\n\ndef constraint4(x):\n    return x[0]**2 + x[1]**2 - 0.01\n\ndef constraint5(x):\n    return x[0]**2 + x[1]**2 - 0.001\n\ndef constraint6(x):\n    return x[0]**2 + x[1]**2 - 0.0001\n\ndef constraint7(x):\n    return x[0]**2 + x[1]**2 - 0.00001\n\ndef constraint8(x):\n    return x[0]**2 + x[1]**2 - 0.000001\n\ndef constraint9(x):\n    return x[0]**2 + x[1]**2 - 0.0000001\n\ndef constraint10(x):\n    return x[0]**2 + x[1]**2 - 0.00000001\n\ndef constraint11(x):\n    return x[0]**2 + x[1]**2 - 0.000000001\n\ndef constraint12(x):\n    return x[0]**2 + x[1]**2 - 0.0000000001\n\ndef constraint13(x):\n    return x[0]**2 + x[1]**2 - 0.00000000001\n\ndef constraint14(x):\n    return x[0]**2 + x[1]**2 - 0.000000000001\n\ndef constraint15(x):\n    return x[0]**2 + x[1]**2 - 0.0000000000001\n\ndef constraint16(x):\n    return x[0]**2 + x[1]**2 - 0.00000000000001\n\ndef constraint17(x):\n    return x[0]**2 + x[1]**2 - 0.000000000000001\n\ndef constraint18(x):\n    return x[0]**2 + x[1]**2 - 0.0000000000000001\n\ndef constraint19(x):\n    return x[0]**2 + x[1]**2 - 0.0000000000000011\n\ndef constraint20(x):\n    return x[0]**2 + x[1]**2 - 0.0000000000000001\n\ndef constraint21(x):\n    return x[0]**2 + x[1]**2 - 0.0000000000000011\n\ndef constraint22(x):\n    return x[0]**2 + x[1]**2 - 0.0000000000000001\n\ndef constraint23(x):\n    return x[0]**2 + x[1]**2 - 0.0000000000000011\n\ndef constraint24(x):\n    return x[0]**2 + x[1]**2 - 0.0000000000000001\n\ndef constraint25(x):\n    return x[0]**2 + x[1]**2 - 0.0000000000000011\n\ndef constraint26(x):\n    return x[0]**2 + x[1]**2 - 0.0000000000000001\n\ndef constraint27(x):\n    return x[0]**2 + x[1]**2 - 0.0000000000000011\n\ndef constraint28(x):\n    return x[0]**2 + x[1]**2 - 0.0000000000000001\n\ndef constraint29(x):\n    return x[0]**2 + x[1]**2 - 0.0000000000000011\n\ndef constraint30(x):\n    return x[0]**2 + x[1]**2 - 0.0000000000000001\n\ndef constraint31(x):\n    return x[0]**2 + x[1]**2 - 0.0000000000000011\n\ndef constraint32(x):\n    return x[0]**2 + x[1]**2 - 0.0000000000000001\n\ndef constraint33(x):\n    return x[0]**2 + x[1]**2 - 0.0000000000000011\n\ndef constraint34",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'scipy' API to perform numerical optimization with nonlinear inequality constraints. The program should define an objective function to be minimized or maximized, specify the nonlinear inequality constraints, and use the 'scipy.optimize.minimize' function with the 'constraints' parameter to find the optimal solution.\n\n### Code:"
    },
    {
        "api": "spacy",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install spacy\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef extract_named_entities(text):\n    doc = nlp(text)\n    entities = []\n    for ent in doc.ents:\n        entities.append((ent.text, ent.label_))\n    return entities\n\ntext = \"Apple Inc. is a technology company headquartered in Cupertino, California. Tim Cook is the CEO of Apple.\"\nentities = extract_named_entities(text)\nfor entity in entities:\n    print(entity)",
        "instruction": "Create a Python program using the 'spacy' API to analyze text and extract named entities. The program should identify and label different types of named entities such as persons, organizations, and locations.",
        "prediction": "import spacy\nnlp = spacy.load('en_core_web_sm')\n\ntext = \"It's official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n\n# Process the text\ndoc = nlp(text)\n\nfor token in doc:\n    # Get the token text, part-of-speech tag and dependency label\n    token_text = token.text\n    token_pos = token.pos_\n    token_dep = token.dep_\n    # This is for formatting only\n    print(f\"{token_text:<12}{token_pos:<10}{token_dep:<10}\")\n\n# Get the span for \"Apple\"\n# Note: The span excludes the period, so the period \".\" does not appear\ntoken1 = doc[1]\ntoken2 = doc[3]\n\n# Create a span for \"Apple\"\napple_span = doc[1:3]\n\n# Overwrite the doc.ents with a list containing the span\ndoc.ents = [apple_span]\n\n# Iterate over the entities in the doc\nfor ent in doc.ents:\n    # Get the entity text\n    ent_text = ent.text\n    # Get the entity label\n    ent_label = ent.label_\n    print(f\"{ent_text:>10}{ent_label:<10}\")\n\n# Get the span for \"Apple\"\ntoken1 = doc[1]\ntoken2 = doc[3]\n\n# Create a span for \"Apple\"\napple_span = doc[1:3]\n\n# Overwrite the doc.ents with a list containing the span\ndoc.ents = [apple_span]\n\n# Iterate over the entities in the doc\nfor ent in doc.ents:\n    # Get the entity text\n    ent_text = ent.text\n    # Get the entity label\n    ent_label = ent.label_\n    print(f\"{ent_text:>10}{ent_label:<10}\")",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'spacy' API to analyze text and extract named entities. The program should identify and label different types of named entities such as persons, organizations, and locations.\n\n### Code:"
    },
    {
        "api": "spacy",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install spacy\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef sentiment_analysis(text):\n    doc = nlp(text)\n    sentiment = \"Neutral\"\n    # Perform sentiment analysis based on the content of the text\n    # Add your sentiment analysis logic here\n    return sentiment\n\ntext = \"I love this movie!\"\nsentiment = sentiment_analysis(text)\nprint(sentiment)",
        "instruction": "Create a Python program using the 'spacy' API to perform sentiment analysis on a given text. The program should analyze the sentiment of the text and classify it as positive, negative, or neutral.",
        "prediction": "import spacy\n\nnlp = spacy.load('en_core_web_sm')\n\ntext = \"It's official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n\n# Process the text\ndoc = nlp(text)\n\nfor token in doc:\n    # Get the token text, part-of-speech tag and dependency label\n    token_text = token.text\n    token_pos = token.pos_\n    token_dep = token.dep_\n    # This is for formatting only\n    print('{:<12}{:<10}{:<10}'.format(token_text, token_pos, token_dep))\n\n# Get the span for \"Apple\"\n# Note: the span excludes the period, so the actual string is longer\nspan = doc[7:10]\n\n# Get the span for \"U.S.\"\nspan = doc[29:32]\n\n# Get the span for \"public company\"\nspan = doc[41:45]\n\n# Get the span for \"first\"\nspan = doc[49:52]\n\n# Get the span for \"reach\"\nspan = doc[59:62]\n\n# Get the span for \"a $1 trillion market value\"\nspan = doc[75:79]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"Apple\"\nspan = doc[7:10]\n\n# Get the span for \"",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'spacy' API to perform sentiment analysis on a given text. The program should analyze the sentiment of the text and classify it as positive, negative, or neutral.\n\n### Code:"
    },
    {
        "api": "stumpy",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install stumpy\nimport stumpy\nimport numpy as np\n\n# Generate a random time series\nyour_time_series = np.random.rand(1000)\n\n# Specify the motif length\nmotif_length = 30\n\n# Perform motif discovery\nmotif_indices = stumpy.motif(your_time_series, motif_length)\n\n# Print the indices of identified motifs\nprint(f\"Motif Indices: {motif_indices}\")",
        "instruction": "Develop a Python program using the 'stumpy' API to perform motif discovery in a given time series. The program should allow the user to specify the desired motif length and find the motifs within the time series. It should return the indices of all identified motifs.",
        "prediction": "import stumpy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef get_data(N, M, sigma, seed=0):\n    np.random.seed(seed)\n    X = np.random.randn(N, M)\n    beta = np.random.randn(M)\n    e = np.random.randn(N)\n    y = X @ beta + e\n    X = stumpy.Stumpy(X)\n    return X, y, beta, e\n\ndef get_data_with_stump(N, M, sigma, seed=0):\n    np.random.seed(seed)\n    X = np.random.randn(N, M)\n    beta = np.random.randn(M)\n    e = np.random.randn(N)\n    y = X @ beta + e\n    X = stumpy.Stumpy(X)\n    return X, y, beta, e\n\ndef get_data_with_stump_and_stumpy(N, M, sigma, seed=0):\n    np.random.seed(seed)\n    X = np.random.randn(N, M)\n    beta = np.random.randn(M)\n    e = np.random.randn(N)\n    y = X @ beta + e\n    X = stumpy.Stumpy(X)\n    return X, y, beta, e\n\ndef get_data_with_stump_and_stumpy_and_stumpy_and_stumpy(N, M, sigma, seed=0):\n    np.random.seed(seed)\n    X = np.random.randn(N, M)\n    beta = np.random.randn(M)\n    e = np.random.randn(N)\n    y = X @ beta + e\n    X = stumpy.Stumpy(X)\n    return X, y, beta, e\n\ndef get_data_with_stump_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_stumpy_and_",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program using the 'stumpy' API to perform motif discovery in a given time series. The program should allow the user to specify the desired motif length and find the motifs within the time series. It should return the indices of all identified motifs.\n\n### Code:"
    },
    {
        "api": "stumpy",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install stumpy\nimport stumpy\nimport numpy as np\n\n# Generate a random time series\nyour_time_series = np.random.rand(10000)\nwindow_size = 50  \n\n# Compute the e z-normalized matrix profile \nmatrix_profile = stumpy.stump(your_time_series, m=window_size)\n\noutput_file_path = \"matrix_profile.txt\"\nnp.savetxt(output_file_path, matrix_profile)",
        "instruction": "Develop a Python program using the 'stumpy' API to generate a matrix profile for a given random time series data. The program should compute the e z-normalized matrix profile, and the resulting profile should be saved to a text file.",
        "prediction": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef generate_data(n, m, sigma, sigma_b, sigma_e, sigma_w):\n    X = np.random.normal(0, sigma, n)\n    X_b = np.random.normal(0, sigma_b, n)\n    X_e = np.random.normal(0, sigma_e, n)\n    X_w = np.random.normal(0, sigma_w, n)\n    Y = X + X_b + X_e + X_w\n    return X, Y\n\ndef generate_data_with_noise(n, m, sigma, sigma_b, sigma_e, sigma_w):\n    X = np.random.normal(0, sigma, n)\n    X_b = np.random.normal(0, sigma_b, n)\n    X_e = np.random.normal(0, sigma_e, n)\n    X_w = np.random.normal(0, sigma_w, n)\n    Y = X + X_b + X_e + X_w\n    Y += np.random.normal(0, 0.1, n)\n    return X, Y\n\ndef generate_data_with_noise_and_outliers(n, m, sigma, sigma_b, sigma_e, sigma_w):\n    X = np.random.normal(0, sigma, n)\n    X_b = np.random.normal(0, sigma_b, n)\n    X_e = np.random.normal(0, sigma_e, n)\n    X_w = np.random.normal(0, sigma_w, n)\n    Y = X + X_b + X_e + X_w\n    Y += np.random.normal(0, 0.1, n)\n    Y[Y > 0.5] = 0.5\n    return X, Y\n\ndef generate_data_with_noise_and_outliers_and_noise(n, m, sigma, sigma_b, sigma_e, sigma_w):\n    X = np.random.normal(0, sigma, n)\n    X_b = np.random.normal(0, sigma_b, n)\n    X_e = np.random.normal(0, sigma_e, n)\n    X_w = np.random.normal(0, sigma_w, n)\n    Y = X + X_b + X_e + X_w\n    Y += np.random.normal(0, 0.1, n)\n    Y[Y > 0.5] = 0.5\n    Y += np.random.normal(0, 0.1, n)\n    return X, Y\n\ndef generate_data_with_noise_and_outliers_and_noise_and_outliers(n, m, sigma, sigma_b, sigma_e, sigma_w):\n    X = np.random.normal(0, sigma, n)\n    X_b = np.random.normal(0, sigma_b, n)\n    X_e = np.random.normal(0, sigma_e, n)\n    X_w = np.random.normal(0, sigma_w, n)\n    Y = X + X_b + X_e + X_w\n    Y += np.random.normal(0, 0.1, n)\n    Y[Y > 0.5] = 0.5\n    Y += np.random.normal(0, 0.1, n)\n    Y[Y > 0.5] = 0.5\n    return X, Y\n\ndef generate_data_with_noise_and_outliers_and_noise_and_outliers_and_noise(n, m, sigma, sigma_b, sigma_e, sigma_w):\n    X = np.random.normal(0, sigma, n)\n    X_b = np.random.normal(0, sigma_b, n)\n    X_e = np.random.normal(0, sigma_e, n)\n    X_w = np.random.normal(0, s",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program using the 'stumpy' API to generate a matrix profile for a given random time series data. The program should compute the e z-normalized matrix profile, and the resulting profile should be saved to a text file.\n\n### Code:"
    },
    {
        "api": "supervision",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install ultralytics supervision opencv-python\nimport cv2\nfrom ultralytics import YOLO\nimport supervision as sv\nimport numpy as np\n\nZONE_POLYGON = np.array([\n    [0, 0],\n    [0.5, 0],\n    [0.5, 1],\n    [0, 1]\n])\nwebcam_resolution = (1280, 720)\n\nframe_width, frame_height = webcam_resolution\n\ncap = cv2.VideoCapture(0)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, frame_width)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, frame_height)\n\nmodel = YOLO(\"yolov8s.pt\")\n\nbox_annotator = sv.BoxAnnotator(\n        thickness=2,\n        text_thickness=2,\n        text_scale=1\n    )\n\nzone_polygon = (ZONE_POLYGON * np.array(webcam_resolution)).astype(int)\nzone = sv.PolygonZone(polygon=zone_polygon, frame_resolution_wh=tuple(webcam_resolution))\nzone_annotator = sv.PolygonZoneAnnotator(\n        zone=zone, \n        color=sv.Color.red(),\n        thickness=2,\n        text_thickness=4,\n        text_scale=2\n    )\n\nwhile True:\n  _, frame = cap.read()\n\n  result = model(frame, agnostic_nms=True)[0]\n  detections = sv.Detections.from_ultralytics(result)\n  print(detections)\n  labels = [\n            f\"{model.model.names[class_id]} {confidence:0.2f}\"\n            for _, _, confidence, class_id, _\n            in detections\n        ]\n  frame = box_annotator.annotate(\n            scene=frame, \n            detections=detections, \n            labels=labels\n        )\n\n  zone.trigger(detections=detections)\n  frame = zone_annotator.annotate(scene=frame)      \n        \n  cv2.imshow(\"yolov8\", frame)\n\n  if (cv2.waitKey(30) == 27):\n      break",
        "instruction": "Develop a Python program that utilizes the 'supervision' API for object detection using a YOLO model on a webcam feed. Annotate detected objects with labels and draw a red polygon zone on the video frame. Use the Ultralytics YOLO model for object detection.",
        "prediction": "import cv2\nimport numpy as np\n\ndef main():\n    \n    windowName = ['Binary', 'Binary Inv', 'Zero', 'Zero Inv', 'Trunc']\n    \n    cap = cv2.VideoCapture(0)\n    \n    if cap.isOpened():\n        ret, frame = cap.read()\n    else:\n        ret = False\n\n\n    while ret:\n    \n        ret, frame = cap.read()\n        \n        th = 127\n        max_val = 255\n    \n        ret, o1 = cv2.threshold(frame, th, max_val, cv2.THRESH_BINARY)\n        ret, o2 = cv2.threshold(frame, th, max_val, cv2.THRESH_BINARY_INV)\n        ret, o3 = cv2.threshold(frame, th, max_val, cv2.THRESH_TOZERO)\n        ret, o4 = cv2.threshold(frame, th, max_val, cv2.THRESH_TOZERO_INV)\n        ret, o5 = cv2.threshold(frame, th, max_val, cv2.THRESH_TRUNC)\n        \n        cv2.imshow(windowName[0], o1)\n        cv2.imshow(windowName[1], o2)\n        cv2.imshow(windowName[2], o3)\n        cv2.imshow(windowName[3], o4)\n        cv2.imshow(windowName[4], o5)\n        if cv2.waitKey(1) == 27:\n            break\n\n    cv2.destroyAllWindows()\n    cap.release()\n\nif __name__ == \"__main__\":\n    main()",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program that utilizes the 'supervision' API for object detection using a YOLO model on a webcam feed. Annotate detected objects with labels and draw a red polygon zone on the video frame. Use the Ultralytics YOLO model for object detection.\n\n### Code:"
    },
    {
        "api": "supervision",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install supervision torchvision\nimport torch\nimport torchvision.transforms as transforms\nimport supervision as sv\n\n# Load the pre-trained ResNet model\nmodel = torch.hub.load('pytorch/vision:v0.9.0', 'resnet18', pretrained=True)\nmodel.eval()\n\n# Define the image transformation pipeline\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Load the image from file\nimage_path = 'image.jpg'\nimage = sv.Image.from_file(image_path)\n\n# Apply the transformation pipeline to the image\ninput_tensor = transform(image)\n\n# Add a batch dimension to the input tensor\ninput_tensor = input_tensor.unsqueeze(0)\n\n# Perform the classification\nwith torch.no_grad():\n    output = model(input_tensor)\n\n# Get the predicted class label and confidence score\n_, predicted_class = torch.max(output, 1)\nconfidence = torch.nn.functional.softmax(output, dim=1)[0, predicted_class]\n\n# Display the predicted class label and confidence score\nprint(f\"Predicted class: {predicted_class.item()}\")\nprint(f\"Confidence: {confidence.item()}\")",
        "instruction": "Develop a Python program that utilizes the 'supervision' API to perform image classification using a pre-trained ResNet model. Load an image from a file, classify it using the ResNet model, and display the predicted class label along with the confidence score.",
        "prediction": "import tensorflow as tf\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nmodel = load_model('model.h5')\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntest_dir = './data/test'\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(150, 150),\n    batch_size=20,\n    class_mode='binary'\n)\n\nfilenames = test_generator.filenames\nnb_samples = len(filenames)\n\npredict = model.predict_generator(test_generator, steps=np.ceil(nb_samples/32))\n\npredict_labels = np.where(predict > 0.5, 1, 0)\n\nplt.figure(figsize=(20, 20))\nfor i in range(len(predict_labels)):\n    plt.subplot(4, 4, i+1)\n    plt.imshow(test_generator[i][0])\n    plt.title('{}'.format(predict_labels[i]))\n    plt.axis('off')\nplt.show()\n\nprint('Confusion Matrix')\nprint(tf.math.confusion_matrix(test_generator.classes, predict_labels))\nprint('Classification Report')\ntarget_names = ['with_mask', 'without_mask']\nprint(tf.math.confusion_matrix(test_generator.classes, predict_labels, target_names=target_names))\nprint('Accuracy: {}'.format(np.sum(test_generator.classes == predict_labels) / len(test_generator.classes)))\nprint('Precision: {}'.format(tf.math.confusion_matrix(test_generator.classes, predict_labels)[1, 1] / (tf.math.confusion_matrix(test_generator.classes, predict_labels)[1, 1] + tf.math.confusion_matrix(test_generator.classes, predict_labels)[0, 1])))\nprint('Recall: {}'.format(tf.math.confusion_matrix(test_generator.classes, predict_labels)[1, 1] / (tf.math.confusion_matrix(test_generator.classes, predict_labels)[1, 1] + tf.math.confusion_matrix(test_generator.classes, predict_labels)[1, 0])))\nprint('F1 Score: {}'.format(2 * (tf.math.confusion_matrix(test_generator.classes, predict_labels)[1, 1] / (tf.math.confusion_matrix(test_generator.classes, predict_labels)[1, 1] + tf.math.confusion_matrix(test_generator.classes, predict_labels)[0, 1])) * (tf.math.confusion_matrix(test_generator.classes, predict_labels)[1, 1] / (tf.math.confusion_matrix(test_generator.classes, predict_labels)[1, 1] + tf.math.confusion_matrix(test_generator.classes, predict_labels)[1, 0]))))\nprint('Specificity: {}'.format(tf.math.confusion_matrix(test_generator.classes, predict_labels)[0, 0] / (tf.math.confusion_matrix(test_generator.classes, predict_labels)[0, 0] + tf.math.confusion_matrix(test_generator.classes, predict_labels)[0, 1])))\nprint('Sensitivity: {}'.format(tf.math.confusion_matrix(test_generator.classes, predict_labels)[1, 1] / (tf.math.confusion_matrix(test_generator.classes, predict_labels)[1, 1] + tf.math.confusion_matrix(test_generator.classes, predict_labels)[0, 1])))",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nDevelop a Python program that utilizes the 'supervision' API to perform image classification using a pre-trained ResNet model. Load an image from a file, classify it using the ResNet model, and display the predicted class label along with the confidence score.\n\n### Code:"
    },
    {
        "api": "tensorflow",
        "decoder_input_ids": false,
        "ground-truth": "#pip install tensorflow\nimport tensorflow as tf\nimport numpy as np\n\n# Generate synthetic data for binary classification\ndata_size = 1000\ndata_x = np.random.rand(data_size, 2)\ndata_y = (data_x[:, 0] + data_x[:, 1] > 1).astype(int)\n\n# Build a simple feedforward neural network\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(2,)),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model on the synthetic data\nprint(\"Training............\")\nmodel.fit(data_x, data_y, epochs=10)\n\n\nprint(\"\\nEvaluation............\")\nloss, accuracy = model.evaluate(data_x, data_y)\nprint(\"Accuracy: \",accuracy)\nprint(\"Loss: \",loss)\n\npredictions = model.predict(data_x)\nprint(\"Are all predictions between 0 and 1?\",np.all(predictions >= 0) and np.all(predictions <= 1))",
        "instruction": "Create a Python program that uses the 'tensorflow' API to build a simple feedforward neural network for binary classification. The program should generate synthetic data, compile the model, train it on the data, and evaluate its performance.",
        "prediction": "import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport logging\nlogger = tf.get_logger()\nlogger.setLevel(logging.ERROR)\n\n_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\nzip_dir = tf.keras.utils.get_file('cats_and_dogs_filterted.zip', origin=_URL, extract=True)\n\nzip_dir_base = os.path.dirname(zip_dir)\n\nbase_dir = os.path.join(os.path.dirname(zip_dir), 'cats_and_dogs_filtered')\ntrain_dir = os.path.join(base_dir, 'train')\nvalidation_dir = os.path.join(base_dir, 'validation')\n\ntrain_cats_dir = os.path.join(train_dir, 'cats')  # directory with our training cat pictures\ntrain_dogs_dir = os.path.join(train_dir, 'dogs')  # directory with our training dog pictures\nvalidation_cats_dir = os.path.join(validation_dir, 'cats')  # directory with our validation cat pictures\nvalidation_dogs_dir = os.path.join(validation_dir, 'dogs')  # directory with our validation dog pictures\n\nnum_cats_tr = len(os.listdir(train_cats_dir))\nnum_dogs_tr = len(os.listdir(train_dogs_dir))\n\nnum_cats_val = len(os.listdir(validation_cats_dir))\nnum_dogs_val = len(os.listdir(validation_dogs_dir))\n\ntotal_train = num_cats_tr + num_dogs_tr\ntotal_val = num_cats_val + num_dogs_val\n\nprint('total training cat images:', num_cats_tr)\nprint('total training dog images:', num_dogs_tr)\n\nprint('total validation cat images:', num_cats_val)\nprint('total validation dog images:', num_dogs_val)\nprint(\"--\")\nprint(\"Total training images:\", total_train)\nprint(\"Total validation images:\", total_val)\n\nbatch_size = 128\nepochs = 15\nIMG_HEIGHT = 150\nIMG_WIDTH = 150\n\ntrain_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data\nvalidation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data\n\ntrain_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n                                                     directory=train_dir,\n                                                     shuffle=True,\n                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                                     class_mode='binary')\n\nval_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n                                                     directory=validation_dir,\n                                                     shuffle=False,\n                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                                     class_mode='binary')\n\nsample_training_images, _ = next(train_data_gen)\n\n# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\ndef plotImages(images_arr):\n    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n    axes = axes.flatten()\n    for img, ax in zip( images_arr, axes):\n        ax.imshow(img)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\nplotImages(sample_training_images[:5])\n\nmodel = Sequential([\n    Conv2D(16, 3, padding",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that uses the 'tensorflow' API to build a simple feedforward neural network for binary classification. The program should generate synthetic data, compile the model, train it on the data, and evaluate its performance.\n\n### Code:"
    },
    {
        "api": "tensorflow",
        "decoder_input_ids": false,
        "ground-truth": "#pip install tensorflow\nimport tensorflow as tf\nimport numpy as np\n\n# Generate a synthetic sequence dataset\nseq_length = 100\nsequences = [np.random.randint(0, 10, size=seq_length) for _ in range(1000)]\ntargets = [np.sum(seq) for seq in sequences]\n\n# Preprocess the data\nsequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post')\ntargets = np.array(targets)\n\n# Build a simple recurrent neural network\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=10, output_dim=16, input_length=seq_length),\n    tf.keras.layers.LSTM(64, activation='relu', return_sequences=False),\n    tf.keras.layers.Dense(1)\n])\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='mean_squared_error')\n\n# Train the model on the synthetic sequence data\nmodel.fit(sequences, targets, epochs=5)\n\n# Generate predictions on new sequences\nnew_sequence = np.array([np.random.randint(0, 10, size=seq_length)])\nprediction = model.predict(new_sequence)\nprint(\"Predicted sum of the new sequence:\", prediction[0][0])",
        "instruction": "Create a Python program that uses the 'tensorflow' API to implement a recurrent neural network (RNN) for sequence prediction. The program should use a sequence dataset, preprocess it, build an RNN model, train it, and generate predictions on new sequences.",
        "prediction": "import tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n# MNIST data set\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n\n# hyper parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\n\n# input place holders\nX = tf.placeholder(tf.float32, [None, 784])\nY = tf.placeholder(tf.float32, [None, 10])\n\n# weights & bias for nn layers\nW1 = tf.Variable(tf.random_normal([784, 256]))\nb1 = tf.Variable(tf.random_normal([256]))\nL1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n\nW2 = tf.Variable(tf.random_normal([256, 256]))\nb2 = tf.Variable(tf.random_normal([256]))\nL2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n\nW3 = tf.Variable(tf.random_normal([256, 10]))\nb3 = tf.Variable(tf.random_normal([10]))\nhypothesis = tf.matmul(L2, W3) + b3\n\n# define cost/loss & optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    logits=hypothesis, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# initialize\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n# train my model\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = int(mnist.train.num_examples / batch_size)\n\n    for i in range(total_batch):\n        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n        feed_dict = {X: batch_xs, Y: batch_ys}\n        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n        avg_cost += c / total_batch\n\n    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n\nprint('Learning Finished!')\n\n# Test model and check accuracy\ncorrect_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint('Accuracy:', sess.run(accuracy, feed_dict={\n      X: mnist.test.images, Y: mnist.test.labels}))\n\n# Get one and predict\nr = random.randint(0, mnist.test.num_examples - 1)\nprint(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\nprint(\"Prediction: \", sess.run(\n    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that uses the 'tensorflow' API to implement a recurrent neural network (RNN) for sequence prediction. The program should use a sequence dataset, preprocess it, build an RNN model, train it, and generate predictions on new sequences.\n\n### Code:"
    },
    {
        "api": "validators",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install validators\nimport validators\nimport requests\n\ndef validate_url(url):\n    if validators.url(url):\n        try:\n            response = requests.get(url)\n            if response.status_code == 200:\n                if validators.url(url, public=True, schemes=['https']):\n                    return \"Valid URL, reachable, and secure (HTTPS). Response status code: \" + str(response.status_code)\n                else:\n                    return \"Valid URL, reachable, but not secure (HTTP). Response status code: \" + str(response.status_code)\n            else:\n                return \"Valid URL but unreachable. Response status code: \" + str(response.status_code)\n        except requests.exceptions.RequestException:\n            return \"Valid URL but unreachable.\"\n    else:\n        return \"Invalid URL.\"\n\n# Get user inputs\nuser_url = input(\"Enter a URL: \")\n\n# Validate and display results\nprint(validate_url(user_url))\n",
        "instruction": "Create a Python program using the 'validators' API to validate user-provided URLs, checking if they adhere to the standard URL format. Additionally, the program should also check if the URL is reachable by making a request to it and display the response status code. If the URL is not reachable, the program should display an appropriate error message. Finally, the program should also check if the URL is secure (HTTPS) and display a message indicating its security status.",
        "prediction": "import urllib.request\nimport json\nimport ssl\n\n# Ignore SSL certificate errors\nctx = ssl.create_default_context()\nctx.check_hostname = False\nctx.verify_mode = ssl.CERT_NONE\n\nurl = input('Enter URL: ')\ndata = urllib.request.urlopen(url, context=ctx).read()\n\ninfo = json.loads(data)\n\n# print(json.dumps(info, indent=2))\n\nsum = 0\nfor item in info['comments']:\n    sum += int(item['count'])\n\nprint(sum)\n",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'validators' API to validate user-provided URLs, checking if they adhere to the standard URL format. Additionally, the program should also check if the URL is reachable by making a request to it and display the response status code. If the URL is not reachable, the program should display an appropriate error message. Finally, the program should also check if the URL is secure (HTTPS) and display a message indicating its security status.\n\n### Code:"
    },
    {
        "api": "validators",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install validators\nimport validators\nimport smtplib\n\ndef validate_email(email):\n    if validators.email(email):\n        try:\n            server = smtplib.SMTP('smtp.gmail.com', 587)\n            server.starttls()\n            server.login(\"your_email_address\", \"your_email_password\")\n            response = server.sendmail(\"your_email_address\", email, \"Test message\")\n            server.quit()\n            if response == {}:\n                return \"Valid email address and deliverable.\"\n            else:\n                return \"Valid email address but undeliverable.\"\n        except smtplib.SMTPException:\n            return \"Valid email address but undeliverable.\"\n    else:\n        return \"Invalid email address.\"\n\n# Get user inputs\nuser_email = input(\"Enter an email address: \")\n\n# Validate and display results\nprint(validate_email(user_email))\n",
        "instruction": "Create a Python program using the 'validators' API to validate user-provided email addresses, checking if they adhere to the standard email format. Additionally, the program should also check if the email address is deliverable by making a request to the email server.",
        "prediction": "import urllib.request\nimport json\nimport sys\nimport os\n\ndef validate_email(email):\n    if email.count('@')!= 1:\n        return False\n    if email.count('.') < 1:\n        return False\n    return True\n\ndef validate_email_format(email):\n    if email.count('@')!= 1:\n        return False\n    if email.count('.') < 1:\n        return False\n    return True\n\ndef validate_email_format_and_validate(email):\n    if validate_email_format(email) == False:\n        return False\n    if validate_email(email) == False:\n        return False\n    return True\n\ndef validate_email_format_and_validate_and_get_domain(email):\n    if validate_email_format_and_validate(email) == False:\n        return False\n    return email.split('@')[1]\n\ndef validate_email_format_and_validate_and_get_domain_and_name(email):\n    if validate_email_format_and_validate(email) == False:\n        return False\n    return email.split('@')[1].split('.')[0], email.split('@')[0]\n\ndef validate_email_format_and_validate_and_get_domain_and_name_and_extension(email):\n    if validate_email_format_and_validate(email) == False:\n        return False\n    return email.split('@')[1].split('.')[0], email.split('@')[0], email.split('.')[1]\n\ndef validate_email_format_and_validate_and_get_domain_and_name_and_extension_and_username(email):\n    if validate_email_format_and_validate(email) == False:\n        return False\n    return email.split('@')[1].split('.')[0], email.split('@')[0], email.split('.')[1], email.split('@')[0]\n\ndef validate_email_format_and_validate_and_get_domain_and_name_and_extension_and_username_and_domain(email):\n    if validate_email_format_and_validate(email) == False:\n        return False\n    return email.split('@')[1].split('.')[0], email.split('@')[0], email.split('.')[1], email.split('@')[0], email.split('@')[1].split('.')[0]\n\ndef validate_email_format_and_validate_and_get_domain_and_name_and_extension_and_username_and_domain_and_extension(email):\n    if validate_email_format_and_validate(email) == False:\n        return False\n    return email.split('@')[1].split('.')[0], email.split('@')[0], email.split('.')[1], email.split('@')[0], email.split('@')[1].split('.')[0], email.split('.')[1]\n\ndef validate_email_format_and_validate_and_get_domain_and_name_and_extension_and_username_and_domain_and_extension_and_username(email):\n    if validate_email_format_and_validate(email) == False:\n        return False\n    return email.split('@')[1].split('.')[0], email.split('@')[0], email.split('.')[1], email.split('@')[0], email.split('@')[1].split('.')[0], email.split('@')[0], email.split('.')[1]\n\ndef validate_email_format_and_validate_and_get_domain_and_name_and_extension_and_username_and_domain_and_extension_and_username_and_domain(email):\n    if validate_email_format_and_validate(email) == False:\n        return False\n    return email.split('@')[1].split('.')[0], email.split('@')[0], email.split('.')[1], email.split('@')[0], email.split('@')[1].split('.')[0], email.split",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'validators' API to validate user-provided email addresses, checking if they adhere to the standard email format. Additionally, the program should also check if the email address is deliverable by making a request to the email server.\n\n### Code:"
    },
    {
        "api": "xarray",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install xarray\nimport xarray as xr\n\n# Read the NetCDF file\nds = xr.open_dataset('input.nc')\n\n# Extract the variable\nvar = ds['temperature']\n\n# Perform a mathematical operation on the variable\nvar_modified = var * 2\n\n# Save the modified variable to a new NetCDF file\nvar_modified.to_netcdf('output.nc')",
        "instruction": "Create a Python program using the 'xarray' API to read and manipulate a NetCDF file. The program should read a NetCDF file, extract a specific variable, perform a mathematical operation on the variable, and save the modified variable back to a new NetCDF file.",
        "prediction": "import xarray as xr\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom matplotlib.dates import DateFormatter\nimport pandas as pd\nimport numpy as np\nimport openaq\nfrom datetime import datetime\nfrom matplotlib.backends.backend_pdf import PdfPages\n\n#defining emission to be observed and conversion (can be found in conversion file)\nemission='no2'\nEmission='NO2'\nconv = 1.88*10**9\n\n#types of environment: Background Urban, Traffic Urban, Industrial Urban, Background Rural, Industrial Suburban, Background Suburban.\n\nenvironment_type='Background Rural'\ndata_area='Greater London'\n\nmetadata_csv='/users/mtj507/scratch/defra_data/defra_site_metadata.csv'\nmetadata=pd.read_csv(metadata_csv, low_memory=False)\n#metadata=metadata.loc[metadata['Environment Type']==environment_type]\n#metadata=metadata[metadata['Site Name'].str.match(data_area)]\nmetadata=metadata.loc[metadata['Site Name']=='London Westminster']\nmetadata=metadata.reset_index(drop=False)\narea=metadata['Zone']\nlocation=metadata['Site Name']\nlatitude=metadata['Latitude']\nlongitude=metadata['Longitude']\nenvironment=metadata['Environment Type']\nno_locations=len(metadata.index)\n\n#change to UTF csv before moving across to Viking and edit doc so its easy to import by deleting first 3 rowns and moving time and date column headers into same row as locations. Delete empty rows up to 'end' at bottom and format time cells to time.\nno2_defra_csv='/users/mtj507/scratch/defra_data/defra_no2_uk_2019.csv'\nddf=pd.read_csv(no2_defra_csv, low_memory=False)\nddf.index=pd.to_datetime(ddf['Date'], dayfirst=True)+pd.to_timedelta(ddf['Time'])\nddf=ddf.loc[:, ~ddf.columns.str.contains('^Unnamed')]\nddf=ddf.dropna(axis=0)\nddf=ddf.replace('No data', np.nan)\nddf['hour']=ddf.index.hour\nddf['weekday']=ddf.index.weekday\nddf['month']=ddf.index.month.astype(str)\nddf['month']=ddf['month'].str.zfill(2)\nddf['day']=ddf.index.day.astype(str)\nddf['day']=ddf['day'].str.zfill(2)\nddf['day and month']=ddf['month']+ddf['day']\n\nddf=ddf.loc[str(2019):str(2020)]\nddf=ddf.loc[~ddf['day and month'].isin(['2902','2903','2904'])]\n\nddf=ddf.loc[str(2019):str(2020)]\nddf=ddf.loc[~ddf['day and month'].isin(['2902','2903','2904'])]\nddf=ddf.loc[:, ~ddf.columns.str.contains('^Unnamed')]\nddf=ddf.dropna(axis=0)\nddf=ddf.astype(float)\nddf['no2']=ddf['no2']*conv\n\nddf_median=ddf.groupby('day and month').median()\nddf_median['median']=ddf_median.mean(axis=1)\nddf_median=ddf_median.astype(float)\nddf_median['median']=ddf_median['median']*conv\n\nddf_median.index=pd.to_datetime(ddf_median.index, format='%d%b%y')\n\nddf_median.to_csv('/users/mtj507/scratch/defra_data/defra_no2_uk_2019_median.csv')",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'xarray' API to read and manipulate a NetCDF file. The program should read a NetCDF file, extract a specific variable, perform a mathematical operation on the variable, and save the modified variable back to a new NetCDF file.\n\n### Code:"
    },
    {
        "api": "xarray",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install xarray\nimport xarray as xr\n\n# Read the NetCDF file\nds = xr.open_dataset('input.nc')\n\n# Select the variable\nvar = ds['temperature']\n\n# Slice the data along one or more dimensions\nsliced_data = var.sel(time=slice('2000-01-01', '2000-12-31'), lat=slice(0, 90))\n\n# Apply a mask based on a condition\nmask = sliced_data > 25\nmasked_data = sliced_data.where(mask)\n\n# Calculate the mean, standard deviation, and maximum value of the masked and sliced data along another specified dimension\nmean = masked_data.mean(dim='time')\nstd_dev = masked_data.std(dim='time')\nmax_value = masked_data.max(dim='time')\n\n# Print the calculated statistics\nprint(\"Mean: \", mean)\nprint(\"Standard Deviation: \", std_dev)\nprint(\"Maximum Value: \", max_value)",
        "instruction": "Create a Python program using the 'xarray' API to perform data slicing, masking, and aggregation on a multi-dimensional data array. The program should read a NetCDF file, select a specific variable, slice the data along one or more dimensions, apply a mask based on a condition, and then calculate the mean, standard deviation, and maximum value of the masked and sliced data along another specified dimension. Finally, the program should print the calculated statistics.",
        "prediction": "import xarray as xr\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom matplotlib.dates import DateFormatter\nimport pandas as pd\nimport numpy as np\nimport openaq\nfrom datetime import datetime\nfrom matplotlib.backends.backend_pdf import PdfPages\n\n#defining emission to be observed and conversion (can be found in conversion file)\nemission='no2'\nEmission='NO2'\nconv = 1.88*10**9\n\n#types of environment: Background Urban, Traffic Urban, Industrial Urban, Background Rural, Industrial Suburban, Background Suburban.\n\nenvironment_type='Background Rural'\ndata_area='Greater London'\n\nmetadata_csv='/users/mtj507/scratch/defra_data/defra_site_metadata.csv'\nmetadata=pd.read_csv(metadata_csv, low_memory=False)\n#metadata=metadata.loc[metadata['Environment Type']==environment_type]\n#metadata=metadata[metadata['Site Name'].str.match(data_area)]\nmetadata=metadata.loc[metadata['Site Name']=='London Westminster']\nmetadata=metadata.reset_index(drop=False)\narea=metadata['Zone']\nlocation=metadata['Site Name']\nlatitude=metadata['Latitude']\nlongitude=metadata['Longitude']\nenvironment=metadata['Environment Type']\nno_locations=len(metadata.index)\n\n#change to UTF csv before moving across to Viking and edit doc so its easy to import by deleting first 3 rowns and moving time and date column headers into same row as locations. Delete empty rows up to 'end' at bottom and format time cells to time.\nno2_defra_csv='/users/mtj507/scratch/defra_data/defra_no2_uk_2019.csv'\nddf=pd.read_csv(no2_defra_csv, low_memory=False)\nddf.index=pd.to_datetime(ddf['Date'], dayfirst=True)+pd.to_timedelta(ddf['Time'])\nddf=ddf.loc[:, ~ddf.columns.str.contains('^Unnamed')]\nddf=ddf.dropna(axis=0)\nddf=ddf.replace('No data', np.nan)\nddf['hour']=ddf.index.hour\nddf['weekday']=ddf.index.weekday\nddf['month']=ddf.index.month.astype(str)\nddf['month']=ddf['month'].str.zfill(2)\nddf['day']=ddf.index.day.astype(str)\nddf['day']=ddf['day'].str.zfill(2)\nddf['day and month']=ddf['month']+ddf['day']\n\nddf=ddf.loc[str(2019):str(2020)]\nddf=ddf.loc[~ddf['day and month'].isin(['2902','2903','2904'])]\n\nddf=ddf.loc[str(2019):str(2020)]\nddf=ddf.loc[:, ~ddf.columns.str.contains('^Unnamed')]\nddf=ddf.dropna(axis=0)\nddf=ddf.astype(float)\n\nno_defra_time_periods=len(ddf.index)\n\nno_locations=len(metadata.index)\n\nno_emissions=no_defra_time_periods*no_locations\n\nddf.insert(0, 'Date', ddf.index.date)\nddf.insert(1, 'Time', ddf.index.time)\nddf.insert(2, 'Emission', Emission)\nddf.insert(3, 'Hour', ddf.index.hour)\nddf.insert(4, 'Weekday', ddf.index.weekday)\nddf.insert(5, 'Month', ddf.index.month)\nddf.insert(6, 'Day', ddf.index.day)\nddf.insert(7, 'Day and month', ddf.index.day.astype(str)+ddf.index.month.astype(str))\n\nddf.reset_index(inplace",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program using the 'xarray' API to perform data slicing, masking, and aggregation on a multi-dimensional data array. The program should read a NetCDF file, select a specific variable, slice the data along one or more dimensions, apply a mask based on a condition, and then calculate the mean, standard deviation, and maximum value of the masked and sliced data along another specified dimension. Finally, the program should print the calculated statistics.\n\n### Code:"
    },
    {
        "api": "ydata-profiling",
        "decoder_input_ids": false,
        "ground-truth": "#! pip install ydata-profiling\nimport pandas as pd\nfrom ydata_profiling import ProfileReport\n\n# Load the JSONL data\ndata = pd.read_json('data.jsonl', lines=True)\n\n# Perform data profiling\nprofiling_report = ProfileReport(data)\n\n# Save the report as an HTML file\nprofiling_report.to_file('jsonl_data_profile.html')",
        "instruction": "Create a Python program that employs the 'ydata-profiling' API to profile a JSONL file named 'data.jsonl'. The profiling should include a count of records, data types of columns, and a table of unique values for a column named 'user_id'. Save the profiling report as an HTML file named 'jsonl_data_profile.html'.",
        "prediction": "// https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py\nimport os\nimport sys\nimport json\nimport glob\nimport requests\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\nfrom model import LSTM\nfrom utils import train, evaluate, preprocess, collate_fn, get_data_from_file, get_data_from_url\n\ndef main():\n    # check if cuda is available\n    train_on_gpu = torch.cuda.is_available()\n    if not train_on_gpu:\n        print('CUDA is not available.  Training on CPU...')\n    else:\n        print('CUDA is available!  Training on GPU...')\n\n    # get data from file\n    file_name = 'data.jsonl'\n    data = get_data_from_file(file_name)\n\n    # get data from url\n    url = 'https://raw.githubusercontent.com/kolaveridi/kaggle-Twitter-US-Airline-Sentiment-/master/data/Tweets.json'\n    data_url = get_data_from_url(url)\n    data = data + data_url\n\n    # preprocess the data\n    data = preprocess(data)\n\n    # create a dataframe with all the data\n    df = pd.DataFrame(data, columns=['target', 'ids', 'date', 'flag', 'user', 'text'])\n    print(df.head())\n\n    # split the data into training and validation set\n    train_df = df[df['date'] < '2015-01-01']\n    val_df = df[df['date'] > '2014-12-31']\n\n    # get the training data\n    train_data = train_df[['text', 'target']].rename(columns={'text': 'tweet', 'target': 'airline_sentiment'})\n    train_data['tweet'] = train_data['tweet'].apply(lambda x: x.lower())\n    train_data['tweet'] = train_data['tweet'].str.replace('[^\\w\\s]','')\n\n    # get the validation data\n    val_data = val_df[['text', 'target']].rename(columns={'text': 'tweet', 'target': 'airline_sentiment'})\n    val_data['tweet'] = val_data['tweet'].apply(lambda x: x.lower())\n    val_data['tweet'] = val_data['tweet'].str.replace('[^\\w\\s]','')\n\n    # get the testing data\n    test_data = df[df['date'] > '2015-01-01'][['text', 'target']].rename(columns={'text': 'tweet', 'target': 'airline_sentiment'})\n    test_data['tweet'] = test_data['tweet'].apply(lambda x: x.lower())\n    test_data['tweet'] = test_data['tweet'].str.replace('[^\\w\\s]','')\n\n    # get the number of unique words\n    words = set()\n    for line in train_data['tweet']:\n        words.update(line.split())\n    n_words = len(words)\n    print(n_words)\n\n    # get the maximum length of a tweet\n    max_len = max([len(s.split()) for s in train_data['tweet']])\n    print(max_len)\n\n    # get the maximum length of a tweet\n    max_len = max([len(s.split()) for s in train_data['tweet']])\n    print(max_len)\n\n    # get the maximum length of a tweet\n    max_len = max([len(s.split()) for s in train_data['tweet']])\n    print(max_len)\n\n    # get the maximum length of a tweet\n    max_len = max([len(s.split()) for s in train_data['tweet']])\n    print(",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that employs the 'ydata-profiling' API to profile a JSONL file named 'data.jsonl'. The profiling should include a count of records, data types of columns, and a table of unique values for a column named 'user_id'. Save the profiling report as an HTML file named 'jsonl_data_profile.html'.\n\n### Code:"
    },
    {
        "api": "ydata-profiling",
        "decoder_input_ids": false,
        "ground-truth": "#! pip install ydata-profiling\nimport pandas as pd\nfrom ydata_profiling import ProfileReport\n\n# Load the CSV data\ndata = pd.read_csv(\"data.csv\")\n\n# Perform data profiling\nprofiling_report = ProfileReport(data)\n\n# Save the report as an HTML file\nprofiling_report.to_file(\"data_profile.html\")",
        "instruction": "Create a Python program that uses the \"ydata-profiling\" API to perform data profiling on a CSV file named \"data.csv.\" The profiling should include statistics like mean, median, and histograms for numeric columns. Save the profiling report as an HTML file named \"data_profile.html.\"",
        "prediction": "import csv\nimport yaml\nimport requests\nimport json\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom datetime import timedelta\n\ndef get_data(url):\n    response = requests.get(url)\n    return response.json()\n\ndef get_data_from_csv(file_name):\n    with open(file_name, 'r') as f:\n        reader = csv.reader(f)\n        data = list(reader)\n    return data\n\ndef get_data_from_yaml(file_name):\n    with open(file_name, 'r') as f:\n        data = yaml.load(f, Loader=yaml.FullLoader)\n    return data\n\ndef get_data_from_json(file_name):\n    with open(file_name, 'r') as f:\n        data = json.load(f)\n    return data\n\ndef get_data_from_pandas(file_name):\n    data = pd.read_csv(file_name)\n    return data\n\ndef get_data_from_numpy(file_name):\n    data = np.load(file_name)\n    return data\n\ndef get_data_from_plot(file_name):\n    data = np.load(file_name)\n    return data\n\ndef get_data_from_plot_2(file_name):\n    data = np.load(file_name)\n    return data\n\ndef get_data_from_plot_3(file_name):\n    data = np.load(file_name)\n    return data\n\ndef get_data_from_plot_4(file_name):\n    data = np.load(file_name)\n    return data\n\ndef get_data_from_plot_5(file_name):\n    data = np.load(file_name)\n    return data\n\ndef get_data_from_plot_6(file_name):\n    data = np.load(file_name)\n    return data\n\ndef get_data_from_plot_7(file_name):\n    data = np.load(file_name)\n    return data\n\ndef get_data_from_plot_8(file_name):\n    data = np.load(file_name)\n    return data\n\ndef get_data_from_plot_9(file_name):\n    data = np.load(file_name)\n    return data\n\ndef get_data_from_plot_10(file_name):\n    data = np.load(file_name)\n    return data\n\ndef get_data_from_plot_11(file_name):\n    data = np.load(file_name)\n    return data\n\ndef get_data_from_plot_12(file_name):\n    data = np.load(file_name)\n    return data\n\ndef get_data_from_plot_13(file_name):\n    data = np.load(file_name)\n    return data\n\ndef get_data_from_plot_14(file_name):\n    data = np.load(file_name)\n    return data\n\ndef get_data_from_plot_15(file_name):\n    data = np.load(file_name)\n    return data\n\ndef get_data_from_plot_16(file_name):\n    data = np.load(file_name)\n    return data\n\ndef get_data_from_plot_17(file_name):\n    data = np.load(file_name)\n    return data\n\ndef get_data_from_plot_18(file_name):\n    data = np.load(file_name)\n    return data\n\ndef get_data_from_plot_19(file_name):\n    data = np.load(file_name)\n    return data\n\ndef get_data_from_plot_20(file_name):\n    data = np.load(file_name)\n    return data\n\ndef get_data_from_plot_21(file_name):\n    data = np.load(file_name)\n    return data\n\ndef get_data_from_plot_22(file_name):\n    data = np.load(file_name)\n    return data\n\ndef",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that uses the \"ydata-profiling\" API to perform data profiling on a CSV file named \"data.csv.\" The profiling should include statistics like mean, median, and histograms for numeric columns. Save the profiling report as an HTML file named \"data_profile.html.\"\n\n### Code:"
    },
    {
        "api": "langchain",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install langchain\n# Import necessary libraries\nfrom langchain.prompts import PromptTemplate\n\n# define the template\nprompt_template = PromptTemplate(\n    input_variables=[\"adjective\", \"content\"],\n    template = \"Tell me a {adjective} joke about {content}.\"\n)\n\n# can also use invoke syntax (applied to following examples as well)\nprompt_value = prompt_template.invoke({\"adjective\": \"funny\", \"content\": \"AI\"})\nprint(prompt_value)",
        "instruction": "Create a Python program that utilizes the \"langchain\" API to generate a prompt template that take to inputs adjective and content to till the model to generate a joke about the content with the given adjective. Then invoke the template with the inputs \"funny\" and \"AI\" to generate a joke about AI.",
        "prediction": "import urllib.request\nimport json\nimport os\n\ndef get_prompt(prompt):\n    prompt = prompt.replace(' ', '+')\n    url = 'https://api.languagetool.org/v2/convert?text=' + prompt + '&to=french&format=json'\n    response = urllib.request.urlopen(url)\n    data = json.loads(response.read())\n    return data['text']\n\ndef get_prompt_from_file(prompt_file):\n    prompts = []\n    with open(prompt_file, 'r') as f:\n        for line in f:\n            prompts.append(line.strip())\n    return prompts\n\ndef get_prompt_from_url(prompt_url):\n    prompts = []\n    with urllib.request.urlopen(prompt_url) as f:\n        for line in f:\n            prompts.append(line.decode('utf-8').strip())\n    return prompts\n\ndef get_prompts(prompt_file, prompt_url):\n    if prompt_file:\n        return get_prompt_from_file(prompt_file)\n    elif prompt_url:\n        return get_prompt_from_url(prompt_url)\n    else:\n        raise Exception('No prompt file or url specified')\n\ndef get_prompt_text(prompt_file, prompt_url):\n    prompts = get_prompts(prompt_file, prompt_url)\n    prompt_text = ''\n    for prompt in prompts:\n        prompt_text += get_prompt(prompt) + '\\n'\n    return prompt_text\n\ndef get_prompt_text_from_file(prompt_file):\n    prompts = get_prompts(prompt_file, None)\n    prompt_text = ''\n    for prompt in prompts:\n        prompt_text += get_prompt(prompt) + '\\n'\n    return prompt_text\n\ndef get_prompt_text_from_url(prompt_url):\n    prompts = get_prompts(None, prompt_url)\n    prompt_text = ''\n    for prompt in prompts:\n        prompt_text += get_prompt(prompt) + '\\n'\n    return prompt_text\n\ndef get_prompt_text_from_prompts(prompt_file, prompt_url):\n    if prompt_file:\n        return get_prompt_text_from_file(prompt_file)\n    elif prompt_url:\n        return get_prompt_text_from_url(prompt_url)\n    else:\n        raise Exception('No prompt file or url specified')\n\ndef get_prompt_text_from_prompts_from_file(prompt_file):\n    return get_prompt_text_from_file(prompt_file)\n\ndef get_prompt_text_from_prompts_from_url(prompt_url):\n    return get_prompt_text_from_url(prompt_url)\n\ndef get_prompt_text_from_prompts_from_prompts(prompt_file, prompt_url):\n    if prompt_file:\n        return get_prompt_text_from_file(prompt_file)\n    elif prompt_url:\n        return get_prompt_text_from_url(prompt_url)\n    else:\n        raise Exception('No prompt file or url specified')\n\ndef get_prompt_text_from_prompts_from_prompts_from_file(prompt_file):\n    return get_prompt_text_from_file(prompt_file)\n\ndef get_prompt_text_from_prompts_from_prompts_from_url(prompt_url):\n    return get_prompt_text_from_url(prompt_url)\n\ndef get_prompt_text_from_prompts_from_prompts_from_prompts(prompt_file, prompt_url):\n    if prompt_file:\n        return get_prompt_text_from_file(prompt_file)\n    elif prompt_url:\n        return get",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that utilizes the \"langchain\" API to generate a prompt template that take to inputs adjective and content to till the model to generate a joke about the content with the given adjective. Then invoke the template with the inputs \"funny\" and \"AI\" to generate a joke about AI.\n\n### Code:"
    },
    {
        "api": "langchain",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install langchain\n# Import necessary libraries\nfrom langchain.document_loaders import WebBaseLoader\n\n# Declare the loader\nloader = WebBaseLoader(\n    web_paths=(\"https://en.wikipedia.org/wiki/Germany\",),\n)\n# Use it to load the text data\ndocs = loader.load()\n\n# Let's look at a piece of text from the first document\ndocs[0].page_content[:1000]",
        "instruction": "Create a Python program that utilizes the \"langchain\" API to load the text from the Wikipedia page on Germany and print the first 1000 characters of the page content.",
        "prediction": "import urllib.request\nimport urllib.parse\nimport json\n\ndef get_data(page):\n    url = 'https://api.github.com/search/repositories?q=language:python&sort=stars&order=desc&page='+str(page)\n    data = urllib.request.urlopen(url).read()\n    data = json.loads(data)\n    return data\n\ndef get_total_pages():\n    url = 'https://api.github.com/search/repositories?q=language:python&sort=stars&order=desc'\n    data = urllib.request.urlopen(url).read()\n    data = json.loads(data)\n    return data['total_count']/100\n\ndef get_data_from_page(page):\n    data = get_data(page)\n    for item in data['items']:\n        print(item['name'])\n        print(item['owner']['login'])\n        print(item['stargazers_count'])\n        print(item['html_url'])\n        print('\\n')\n\ndef get_data_from_all_pages():\n    total_pages = get_total_pages()\n    for page in range(1, int(total_pages)+1):\n        get_data_from_page(page)\n\nget_data_from_all_pages()",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that utilizes the \"langchain\" API to load the text from the Wikipedia page on Germany and print the first 1000 characters of the page content.\n\n### Code:"
    },
    {
        "api": "langchain",
        "decoder_input_ids": false,
        "ground-truth": "#!pip install langchain\n# Import necessary libraries\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.document_loaders import WebBaseLoader\n\n# Declare the loader\nloader = WebBaseLoader(\n    web_paths=(\"https://en.wikipedia.org/wiki/Sinai_Peninsula\",),\n)\n# Use it to load the text data\ndocs = loader.load()\n\n# Declare the loader\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,\n    add_start_index=True\n)\n\n# Use it to split the data\nall_splits = text_splitter.split_documents(docs)",
        "instruction": "Create a Python program that utilizes the \"langchain\" API to load the text from the Wikipedia page on the Sinai Peninsula and split the text into chunks of 1000 characters with an overlap of 200 characters using Recursive Character Text Splitter.",
        "prediction": "import urllib.request\nimport json\nimport os\n\ndef get_data(url):\n    with urllib.request.urlopen(url) as url:\n        data = json.loads(url.read().decode())\n    return data\n\ndef get_data_from_file(file_name):\n    with open(file_name) as file:\n        data = json.load(file)\n    return data\n\ndef get_data_from_url(url):\n    with urllib.request.urlopen(url) as url:\n        data = json.loads(url.read().decode())\n    return data\n\ndef get_data_from_file_and_url(file_name, url):\n    with open(file_name) as file:\n        data = json.load(file)\n    with urllib.request.urlopen(url) as url:\n        data = json.loads(url.read().decode())\n    return data\n\ndef get_data_from_file_and_url_and_save_to_file(file_name, url, file_name_to_save):\n    with open(file_name) as file:\n        data = json.load(file)\n    with urllib.request.urlopen(url) as url:\n        data = json.loads(url.read().decode())\n    with open(file_name_to_save, 'w') as file:\n        json.dump(data, file)\n    return data\n\ndef get_data_from_file_and_url_and_save_to_file_and_print(file_name, url, file_name_to_save):\n    with open(file_name) as file:\n        data = json.load(file)\n    with urllib.request.urlopen(url) as url:\n        data = json.loads(url.read().decode())\n    with open(file_name_to_save, 'w') as file:\n        json.dump(data, file)\n    print(data)\n    return data\n\ndef get_data_from_file_and_url_and_save_to_file_and_print_and_return(file_name, url, file_name_to_save):\n    with open(file_name) as file:\n        data = json.load(file)\n    with urllib.request.urlopen(url) as url:\n        data = json.loads(url.read().decode())\n    with open(file_name_to_save, 'w') as file:\n        json.dump(data, file)\n    print(data)\n    return data\n\ndef get_data_from_file_and_url_and_save_to_file_and_print_and_return_and_delete_file(file_name, url, file_name_to_save):\n    with open(file_name) as file:\n        data = json.load(file)\n    with urllib.request.urlopen(url) as url:\n        data = json.loads(url.read().decode())\n    with open(file_name_to_save, 'w') as file:\n        json.dump(data, file)\n    print(data)\n    os.remove(file_name)\n    return data",
        "source": "Below is an instruction that describes a coding task. Write a full code that appropriately fulfils the request.\n\n### Instruction:\nCreate a Python program that utilizes the \"langchain\" API to load the text from the Wikipedia page on the Sinai Peninsula and split the text into chunks of 1000 characters with an overlap of 200 characters using Recursive Character Text Splitter.\n\n### Code:"
    }
]