[
    {
        "instruction": "Create a Python program that retrieves the metadata of a file on Google Drive using the PyDrive2 API. The program should take the file ID as input and display the metadata information of the file, including its title, size, creation date, and last modified date. Make sure the program is capable of handling cases where the file ID provided is invalid or when there is an authentication issue.",
        "api": "PyDrive2",
        "output": "#!pip install pydrive2\nfrom pydrive2.auth import GoogleAuth\nfrom pydrive2.drive import GoogleDrive\n\n# Function to authenticate and get the GoogleDrive instance\ndef authenticate_drive():\n    gauth = GoogleAuth()\n    gauth.LocalWebserverAuth()  # Authenticate using local webserver\n    return GoogleDrive(gauth)\n\ndef get_file_metadata(file_id):\n    drive = authenticate_drive()\n    \n    try:\n        file = drive.CreateFile({'id': file_id})\n        if not file:\n            print(f\"File with ID '{file_id}' does not exist on Google Drive.\")\n            return\n        \n        print(\"File Metadata:\")\n        print(f\"Title: {file['title']}\")\n        print(f\"Size: {file['fileSize']} bytes\")\n        print(f\"Creation Date: {file['createdDate']}\")\n        print(f\"Last Modified Date: {file['modifiedDate']}\")\n    except Exception as e:\n        print(f\"Error retrieving file metadata: {e}\")\n\nif __name__ == \"__main__\":\n    file_id = input(\"Enter the file ID on Google Drive to retrieve metadata: \")\n    \n    get_file_metadata(file_id)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n# pydrive2.drive module#\n\n\n*class* pydrive2.drive.GoogleDrive(*auth=None*)#\nBases: `ApiAttributeMixin`\n\n\nMain Google Drive class.\n\n\nCreateFile(*metadata=None*)#\nCreate an instance of GoogleDriveFile with auth of this instance.\n\n\nThis method would not upload a file to GoogleDrive.\n\nParameters:\n**metadata** (*dict.*) – file resource to initialize GoogleDriveFile with.\n\nReturns:\npydrive2.files.GoogleDriveFile – initialized with auth of this\ninstance.\n\nGetAbout()#\nReturn information about the Google Drive of the auth instance.\n\nReturns:\nA dictionary of Google Drive information like user, usage, quota etc.\n\nListFile(*param=None*)#\nCreate an instance of GoogleDriveFileList with auth of this instance.\n\n\nThis method will not fetch from Files.List().\n\nParameters:\n**param** (*dict.*) – parameter to be sent to Files.List().\n\nReturns:\npydrive2.files.GoogleDriveFileList – initialized with auth of\nthis instance.\n\n## pydrive2.files module#\n\n\n*exception* pydrive2.files.ApiRequestError(*http\\_error*)#\nBases: `OSError`\n\n\nGetField(*field*)#\nReturns the field from the first error\n\n\n*exception* pydrive2.files.FileNotDownloadableError#\nBases: `RuntimeError`\n\n\nError trying to download file that is not downloadable.\n\n*exception* pydrive2.files.FileNotUploadedError#\nBases: `RuntimeError`\n\n\nError trying to access metadata of file that is not uploaded.\n\n*class* pydrive2.files.GoogleDriveFile(*auth=None*, *metadata=None*, *uploaded=False*)#\nBases: `ApiAttributeMixin`, `ApiResource`\n\n\nGoogle Drive File\n\n==================\n Document 1 \n----------------\n# pydrive2.files module#\n\n\n*exception* pydrive2.files.ApiRequestError(*http\\_error*)#\nBases: `OSError`\n\n\nGetField(*field*)#\nReturns the field from the first error\n\n\n*exception* pydrive2.files.FileNotDownloadableError#\nBases: `RuntimeError`\n\n\nError trying to download file that is not downloadable.\n\n*exception* pydrive2.files.FileNotUploadedError#\nBases: `RuntimeError`\n\n\nError trying to access metadata of file that is not uploaded.\n\n*class* pydrive2.files.GoogleDriveFile(*auth=None*, *metadata=None*, *uploaded=False*)#\nBases: `ApiAttributeMixin`, `ApiResource`\n\n\nGoogle Drive File instance.\n\n\nInherits ApiResource which inherits dict.\nCan access and modify metadata like dictionary.\n\n\nCopy(*target\\_folder=None*, *new\\_title=None*, *param=None*)#\nCreates a copy of this file. Folders cannot be copied.\n\nParameters:\n* **target\\_folder** (*GoogleDriveFile**,* *optional*) – Folder where the file will be copied.\n* **new\\_title** (*str**,* *optional*) – Name of the new file.\n* **param** (*dict**,* *optional*) – addition parameters to pass.\n\nRaises:\nApiRequestError\n\nReturns:\nthe copied file\n\nReturn type:\nGoogleDriveFile\n\nDelete(*param=None*)#\nHard-delete a file.\n\nParameters:\n**param** (*dict.*) – additional parameter to file.\n\nDeletePermission(*permission\\_id*)#\nDeletes the permission specified by the permission\\_id.\n\nParameters:\n**permission\\_id** (*str*) – The permission id.\n\nReturns:\nTrue if it succeeds.\n\nReturn type:\nbool\n\nFetchContent(*mimetype=None*, *remove\\_bom=False*)#\nDownload file’s content from download\\_url.\n\nRaises:\nApiRequestError, FileNotUploadedError, FileNotDownloadableError\n\nFetchMetadata(*fields=None*, *fetch\\_all=False*)#\nDownload file’s metadata from id using Files.get().\n\nParameters:\n* **fields** (*str*) – The fields to include, as one string, each entry separated\nby commas, e.g. ‘fields,labels’.\n* **fetch\\_all** (*bool*) – Whether to fetch all fields.\n\nRaises:\nApiRequestError, FileNotUploadedError\n\nGetContentFile(*filename*, *mimetype=None*, *remove\\_bom=False*, *callback=None*, *chunksize=104857600*, *acknowledge\\_abuse=False*)#\nSave content of this file as a local file.\n\nParameters:\n* **filename** (*str*) – name of the file to write to.\n* **mimetype** (*str*) – mimeType of the file.\n* **remove\\_bom** (*bool*) – Whether to remove the byte order marking.\n* **callback** – passed two arguments: (total transferred, file size).\n* **chunksize** (*int*) – chunksize in bytes (standard 100 MB(1024\\*1024\\*100))\n* **acknowledge\\_abuse** (*bool*) – Acknowledging the risk and download file\nidentified as abusive.\n\nGetContentIOBuffer(*mimetype=None*, *encoding=None*, *remove\\_bom=False*, *chunksize=104857600*, *acknowledge\\_abuse=False*)#\nGet a file-like object which has a buffered read() method.\n\nParameters:\n* **mimetype** (*str*) – mimeType of the file.\n* **encoding** (*str*) – The encoding to use when decoding the byte string.\n* **remove\\_bom** (*bool*) – Whether to remove the byte order marking.\n* **chunksize** (*int*) – default read()/iter() chunksize.\n* **acknowledge\\_abuse** (*bool*) – Acknowledging the risk and download file\nidentified as abusive.\n\nReturns:\nMediaIoReadable – file-like object.\n\nGetContentString(*mimetype=None*, *encoding='utf-8'*, *remove\\_bom=False*)#\nGet content of this file as a string.\n\nParameters:\n* **mimetype** (*str*) – The mimetype of the content string.\n* **encoding** (*str*) – The encoding to use when decoding the byte string.\n* **remove\\_bom** (*bool*) – Whether to strip a known BOM.\n\nReturns:\nstr – utf-8 decoded content of the file\n\nGetPermissions()#\nGet file’s or shared drive’s permissions.\n\n\nFor files in a shared drive, at most 100 results will be returned.\nIt doesn’t paginate and collect all results.\n\nReturns:\nA list of the permission objects.\n\nReturn type:\nobject[]\n\nGetRevisions()#\nGet file’s or shared drive’s revisions.\n\nReturns:\nA list of the revision objects.\n\nInsertPermission(*new\\_permission*, *param=None*)#\nInsert a new permission. Re-fetches all permissions after call.\n\nParameters:\n* **new\\_permission** (*object*) – The new permission to insert, please see the\nofficial Google Drive API guide on permissions.insert\nfor details.\n* **param** (*dict*) – addition parameters to pass\n\nReturns:\nThe permission object.\n\nReturn type:\nobject\n\nSetContentFile(*filename*)#\nSet content of this file from a file.\n\n\nOpens the file specified by this method.\nWill be read, uploaded, and closed by Upload() method.\nSets metadata ‘title’ and ‘mimeType’ automatically if not specified and\nthe file is uploaded for the first time (id is not set).\n\nParameters:\n**filename** (*str.*) – name of the file to be uploaded.\n\nSetContentString(*content*, *encoding='utf-8'*)#\nSet content of this file to be a string.\n\n\nCreates io.BytesIO instance of utf-8 encoded string.\nSets mimeType to be ‘text/plain’ if not specified and file id is not\nset (means that we are uploading this file for the first time).\n\nParameters:\n* **encoding** (*str*) – The encoding to use when setting the content of this file.\n* **content** (*str*) – content of the file in string.\n\nTrash(*param=None*)#\nMove a file to the trash.\n\nUnTrash(*param=None*)#\nMove a file out of the trash.\n:param param: Additional parameter to file.\n:type param: dict.\n:raises: ApiRequestError\n\nUpload(*param=None*)#\nUpload/update file by choosing the most efficient method.\n\nParameters:\n**param** (*dict.*) – additional parameter to upload file.\n\ncontent#\nA data descriptor that sets and returns values.\n\nuploaded#\nA data descriptor that sets and returns values.\n\n\n*class* pydrive2.files.GoogleDriveFileList(*auth=None*, *param=None*)#\nBases: `ApiResourceList`\n\n\nGoogle Drive FileList instance.\n\n\nEquivalent to Files.list() in Drive APIs.\n\n*class* pydrive2.files.IoBuffer(*encoding*)#\nBases: `object`\n\n\nLightweight retention of one chunk.\n\n\nread()#\n\nwrite(*chunk*)#\n\n\npydrive2.files.LoadMetadata(*decoratee*)#\nDecorator to check if the file has metadata and fetches it if not.\n\n*class* pydrive2.files.MediaIoReadable(*request*, *encoding=None*, *pre\\_buffer=True*, *remove\\_prefix=b''*, *chunksize=104857600*)#\nBases: `object`\n\nReturns:\nbytes or str – chunk (or None if done)\n\n\n## pydrive2.settings module#\n\n\n*exception* pydrive2.settings.InvalidConfigError#\nBases: `OSError`\n\n\nError trying to read client configuration.\n\npydrive2.settings.LoadSettingsFile(*filename='settings.yaml'*)#\nLoads settings file in yaml format given file name.\n\nParameters:\n**filename** (*str.*) – path for settings file. ‘settings.yaml’ by default.\n\nRaises:\nSettingsError\n\n*exception* pydrive2.settings.SettingsError#\nBases: `OSError`\n\n\nError while loading/saving settings\n\npydrive2.settings.ValidateSettings(*data*)#\nValidates if current settings is valid.\n\nParameters:\n**data** (*dict.*) – dictionary containing all settings.\n\n\n## pydrive2.fs module#\n\n\n*class* pydrive2.fs.GDriveFileSystem(*\\*args*, *\\*\\*kwargs*)#\nBases: `AbstractFileSystem`\n\n\nAccess to gdrive as an fsspec filesystem\n\n==================\n Document 2 \n----------------\n# pydrive2.auth module#\n\n\n*exception* pydrive2.auth.AuthError#\nBases: `Exception`\n\n\nBase error for authentication/authorization errors.\n\n*exception* pydrive2.auth.AuthenticationError#\nBases: `AuthError`\n\n\nGeneral authentication error.\n\n*exception* pydrive2.auth.AuthenticationRejected#\nBases: `AuthError`\n\n\nUser rejected authentication.\n\npydrive2.auth.CheckAuth(*decoratee*)#\nDecorator to check if it requires OAuth2 flow request.\n\npydrive2.auth.CheckServiceAuth(*decoratee*)#\nDecorator to authorize service account.\n\n*class* pydrive2.auth.GoogleAuth(*settings\\_file='settings.yaml'*, *http\\_timeout=None*, *settings=None*)#\nBases: `ApiAttributeMixin`\n\n\nWrapper class for oauth2client library in google-api-python-client.\n\n\nLoads all settings and credentials from one ‘settings.yaml’ file\nand performs common OAuth2.0 related functionality such as authentication\nand authorization.\n\n\nAuth(*code*)#\nAuthenticate, authorize, and build service.\n\nParameters:\n**code** (*str.*) – Code for authentication.\n\nRaises:\nAuthenticationError\n\nAuthenticate(*code*)#\nAuthenticates given authentication code back from user.\n\nAuthorize()#\nAuthorizes and builds service.\n\nCLIENT\\_CONFIGS\\_LIST *= ['client\\_id', 'client\\_secret', 'auth\\_uri', 'token\\_uri', 'revoke\\_uri', 'redirect\\_uri']*#\n\nCommandLineAuth()#\nAuthenticate and authorize from user by printing authentication url\nretrieving authentication code from command-line.\n\nReturns:\nstr – code returned from commandline.\n\nDEFAULT\\_SETTINGS *= {'client\\_config\\_backend': 'file', 'client\\_config\\_file': 'client\\_secrets.json', 'oauth\\_scope': ['https://www.googleapis.com/auth/drive'], 'save\\_credentials': False}*#\n\nGetAuthUrl()#\nCreates authentication url where user visits to grant access.\n\nReturns:\nstr – Authentication url.\n\nGetFlow()#\nGets Flow object from client configuration.\n\nRaises:\nInvalidConfigError\n\nGet\\_Http\\_Object()#\nCreate and authorize an httplib2.Http object. Necessary for\nthread-safety.\n:return: The http object to be used in each call.\n:rtype: httplib2.Http\n\nLoadClientConfig(*backend=None*)#\nLoads client configuration according to specified backend.\n\n\nIf you have any specific backend to load client configuration from in mind,\ndon’t use this function and use the corresponding function you want.\n\nParameters:\n**backend** (*str.*) – backend to load client configuration from.\n\nLoadClientConfigFile(*client\\_config\\_file=None*)#\nLoads client configuration file downloaded from APIs console.\n\n\nLoads client config file from path in settings if not specified.\n\nParameters:\n**client\\_config\\_file** (*str.*) – path of client config file to read.\n\nLoadClientConfigSettings()#\nLoads client configuration from settings file.\n\nLoadCredentials(*backend=None*)#\nLoads credentials or create empty credentials if it doesn’t exist.\n\nParameters:\n**backend** (*str.*) – target backend to save credential to.\n\nLoadCredentialsFile(*credentials\\_file=None*)#\nLoads credentials or create empty credentials if it doesn’t exist.\n\n\nLoads credentials file from path in settings if not specified.\n\nParameters:\n**credentials\\_file** (*str.*) – path of credentials file to read.\n\nRaises:\nInvalidConfigError, InvalidCredentialsError\n\nLoadServiceConfigSettings()#\nLoads client configuration from settings.\n:raises: InvalidConfigError\n\nLocalWebserverAuth(*host\\_name='localhost'*, *port\\_numbers=None*, *launch\\_browser=True*, *bind\\_addr=None*)#\nAuthenticate and authorize from user by creating local web server and\nretrieving authentication code.\n\n\nThis function is not for web server application. It creates local web\nserver for user from standalone application.\n\nParameters:\n* **host\\_name** (*str.*) – host name of the local web server.\n* **port\\_numbers** (*list.*) – list of port numbers to be tried to used.\n* **launch\\_browser** (*bool*) – should browser be launched automatically\n* **bind\\_addr** (*str.*) – optional IP address for the local web server to listen on.\nIf not specified, it will listen on the address specified in the\nhost\\_name parameter.\n\nReturns:\nstr – code returned from local web server\n\nRaises:\nAuthenticationRejected, AuthenticationError\n\nRefresh()#\nRefreshes the access\\_token.\n\nRaises:\nRefreshError\n\nSERVICE\\_CONFIGS\\_LIST *= ['client\\_user\\_email']*#\n\nSaveCredentials(*backend=None*)#\nSaves credentials according to specified backend.\n\n\nIf you have any specific credentials backend in mind, don’t use this\nfunction and use the corresponding function you want.\n\nParameters:\n**backend** (*str.*) – backend to save credentials.\n\nSaveCredentialsFile(*credentials\\_file=None*)#\nSaves credentials to the file in JSON format.\n\nParameters:\n**credentials\\_file** (*str.*) – destination to save file to.\n\nServiceAuth()#\nAuthenticate and authorize using P12 private key, client id\nand client email for a Service account.\n:raises: AuthError, InvalidConfigError\n\n*property* access\\_token\\_expired#\nChecks if access token doesn’t exist or is expired.\n\nReturns:\nbool – True if access token doesn’t exist or is expired.\n\nauth\\_method#\nA data descriptor that sets and returns values.\n\nclient\\_config#\nA data descriptor that sets and returns values.\n\ncredentials#\nA data descriptor that sets and returns values.\n\nflow#\nA data descriptor that sets and returns values.\n\nhttp#\nA data descriptor that sets and returns values.\n\nservice#\nA data descriptor that sets and returns values.\n\nsettings#\nA data descriptor that sets and returns values.\n\n\n*exception* pydrive2.auth.InvalidCredentialsError#\nBases: `OSError`\n\n\nError trying to read credentials file.\n\npydrive2.auth.LoadAuth(*decoratee*)#\nDecorator to check if the auth is valid and loads auth if not.\n\n*exception* pydrive2.auth.RefreshError#\nBases: `AuthError`\n\n\nAccess token refresh error.\n## pydrive2.drive module#\n\n\n*class* pydrive2.drive.GoogleDrive(*auth=None*)#\nBases: `ApiAttributeMixin`\n\n\nMain Google Drive class.\n\n\nCreateFile(*metadata=None*)#\nCreate an instance of GoogleDriveFile with auth of this instance.\n\n\nThis method would not upload a file to GoogleDrive.\n\nParameters:\n**metadata** (*dict.*) – file resource to initialize GoogleDriveFile with.\n\nReturns:\npydrive2.files.GoogleDriveFile – initialized with auth of this\ninstance.\n\nGetAbout()#\nReturn information"
    },
    {
        "instruction": "Create a Python program that retrieves the list of permissions for a file or folder on Google Drive using the PyDrive2 API. The program should take the file or folder ID as input and display the list of permissions, including the email addresses and roles of the users or groups that have access to the file or folder. Make sure the program is capable of handling cases where the file or folder ID provided is invalid or when there is an authentication issue.",
        "api": "PyDrive2",
        "output": "#!pip install pydrive2\nfrom pydrive2.auth import GoogleAuth\nfrom pydrive2.drive import GoogleDrive\n\n# Function to authenticate and get the GoogleDrive instance\ndef authenticate_drive():\n    gauth = GoogleAuth()\n    gauth.LocalWebserverAuth()  # Authenticate using local webserver\n    return GoogleDrive(gauth)\n\ndef get_permissions(file_or_folder_id):\n    drive = authenticate_drive()\n    \n    try:\n        file_or_folder = drive.CreateFile({'id': file_or_folder_id})\n        if not file_or_folder:\n            print(f\"File or folder with ID '{file_or_folder_id}' does not exist on Google Drive.\")\n            return\n        \n        permissions = file_or_folder.GetPermissions()\n        \n        print(f\"Permissions for file or folder with ID '{file_or_folder_id}':\")\n        \n        for permission in permissions:\n            print(f\"Email Address: {permission['emailAddress']}, Role: {permission['role']}\")\n    except Exception as e:\n        print(f\"Error retrieving permissions: {e}\")\n\nif __name__ == \"__main__\":\n    file_or_folder_id = input(\"Enter the file or folder ID on Google Drive to retrieve permissions: \")\n    \n    get_permissions(file_or_folder_id)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n# pydrive2.drive module#\n\n\n*class* pydrive2.drive.GoogleDrive(*auth=None*)#\nBases: `ApiAttributeMixin`\n\n\nMain Google Drive class.\n\n\nCreateFile(*metadata=None*)#\nCreate an instance of GoogleDriveFile with auth of this instance.\n\n\nThis method would not upload a file to GoogleDrive.\n\nParameters:\n**metadata** (*dict.*) – file resource to initialize GoogleDriveFile with.\n\nReturns:\npydrive2.files.GoogleDriveFile – initialized with auth of this\ninstance.\n\nGetAbout()#\nReturn information about the Google Drive of the auth instance.\n\nReturns:\nA dictionary of Google Drive information like user, usage, quota etc.\n\nListFile(*param=None*)#\nCreate an instance of GoogleDriveFileList with auth of this instance.\n\n\nThis method will not fetch from Files.List().\n\nParameters:\n**param** (*dict.*) – parameter to be sent to Files.List().\n\nReturns:\npydrive2.files.GoogleDriveFileList – initialized with auth of\nthis instance.\n\n## pydrive2.files module#\n\n\n*exception* pydrive2.files.ApiRequestError(*http\\_error*)#\nBases: `OSError`\n\n\nGetField(*field*)#\nReturns the field from the first error\n\n\n*exception* pydrive2.files.FileNotDownloadableError#\nBases: `RuntimeError`\n\n\nError trying to download file that is not downloadable.\n\n*exception* pydrive2.files.FileNotUploadedError#\nBases: `RuntimeError`\n\n\nError trying to access metadata of file that is not uploaded.\n\n*class* pydrive2.files.GoogleDriveFile(*auth=None*, *metadata=None*, *uploaded=False*)#\nBases: `ApiAttributeMixin`, `ApiResource`\n\n\nGoogle Drive File\n\n==================\n Document 1 \n----------------\n# pydrive2.files module#\n\n\n*exception* pydrive2.files.ApiRequestError(*http\\_error*)#\nBases: `OSError`\n\n\nGetField(*field*)#\nReturns the field from the first error\n\n\n*exception* pydrive2.files.FileNotDownloadableError#\nBases: `RuntimeError`\n\n\nError trying to download file that is not downloadable.\n\n*exception* pydrive2.files.FileNotUploadedError#\nBases: `RuntimeError`\n\n\nError trying to access metadata of file that is not uploaded.\n\n*class* pydrive2.files.GoogleDriveFile(*auth=None*, *metadata=None*, *uploaded=False*)#\nBases: `ApiAttributeMixin`, `ApiResource`\n\n\nGoogle Drive File instance.\n\n\nInherits ApiResource which inherits dict.\nCan access and modify metadata like dictionary.\n\n\nCopy(*target\\_folder=None*, *new\\_title=None*, *param=None*)#\nCreates a copy of this file. Folders cannot be copied.\n\nParameters:\n* **target\\_folder** (*GoogleDriveFile**,* *optional*) – Folder where the file will be copied.\n* **new\\_title** (*str**,* *optional*) – Name of the new file.\n* **param** (*dict**,* *optional*) – addition parameters to pass.\n\nRaises:\nApiRequestError\n\nReturns:\nthe copied file\n\nReturn type:\nGoogleDriveFile\n\nDelete(*param=None*)#\nHard-delete a file.\n\nParameters:\n**param** (*dict.*) – additional parameter to file.\n\nDeletePermission(*permission\\_id*)#\nDeletes the permission specified by the permission\\_id.\n\nParameters:\n**permission\\_id** (*str*) – The permission id.\n\nReturns:\nTrue if it succeeds.\n\nReturn type:\nbool\n\nFetchContent(*mimetype=None*, *remove\\_bom=False*)#\nDownload file’s content from download\\_url.\n\nRaises:\nApiRequestError, FileNotUploadedError, FileNotDownloadableError\n\nFetchMetadata(*fields=None*, *fetch\\_all=False*)#\nDownload file’s metadata from id using Files.get().\n\nParameters:\n* **fields** (*str*) – The fields to include, as one string, each entry separated\nby commas, e.g. ‘fields,labels’.\n* **fetch\\_all** (*bool*) – Whether to fetch all fields.\n\nRaises:\nApiRequestError, FileNotUploadedError\n\nGetContentFile(*filename*, *mimetype=None*, *remove\\_bom=False*, *callback=None*, *chunksize=104857600*, *acknowledge\\_abuse=False*)#\nSave content of this file as a local file.\n\nParameters:\n* **filename** (*str*) – name of the file to write to.\n* **mimetype** (*str*) – mimeType of the file.\n* **remove\\_bom** (*bool*) – Whether to remove the byte order marking.\n* **callback** – passed two arguments: (total transferred, file size).\n* **chunksize** (*int*) – chunksize in bytes (standard 100 MB(1024\\*1024\\*100))\n* **acknowledge\\_abuse** (*bool*) – Acknowledging the risk and download file\nidentified as abusive.\n\nGetContentIOBuffer(*mimetype=None*, *encoding=None*, *remove\\_bom=False*, *chunksize=104857600*, *acknowledge\\_abuse=False*)#\nGet a file-like object which has a buffered read() method.\n\nParameters:\n* **mimetype** (*str*) – mimeType of the file.\n* **encoding** (*str*) – The encoding to use when decoding the byte string.\n* **remove\\_bom** (*bool*) – Whether to remove the byte order marking.\n* **chunksize** (*int*) – default read()/iter() chunksize.\n* **acknowledge\\_abuse** (*bool*) – Acknowledging the risk and download file\nidentified as abusive.\n\nReturns:\nMediaIoReadable – file-like object.\n\nGetContentString(*mimetype=None*, *encoding='utf-8'*, *remove\\_bom=False*)#\nGet content of this file as a string.\n\nParameters:\n* **mimetype** (*str*) – The mimetype of the content string.\n* **encoding** (*str*) – The encoding to use when decoding the byte string.\n* **remove\\_bom** (*bool*) – Whether to strip a known BOM.\n\nReturns:\nstr – utf-8 decoded content of the file\n\nGetPermissions()#\nGet file’s or shared drive’s permissions.\n\n\nFor files in a shared drive, at most 100 results will be returned.\nIt doesn’t paginate and collect all results.\n\nReturns:\nA list of the permission objects.\n\nReturn type:\nobject[]\n\nGetRevisions()#\nGet file’s or shared drive’s revisions.\n\nReturns:\nA list of the revision objects.\n\nInsertPermission(*new\\_permission*, *param=None*)#\nInsert a new permission. Re-fetches all permissions after call.\n\nParameters:\n* **new\\_permission** (*object*) – The new permission to insert, please see the\nofficial Google Drive API guide on permissions.insert\nfor details.\n* **param** (*dict*) – addition parameters to pass\n\nReturns:\nThe permission object.\n\nReturn type:\nobject\n\nSetContentFile(*filename*)#\nSet content of this file from a file.\n\n\nOpens the file specified by this method.\nWill be read, uploaded, and closed by Upload() method.\nSets metadata ‘title’ and ‘mimeType’ automatically if not specified and\nthe file is uploaded for the first time (id is not set).\n\nParameters:\n**filename** (*str.*) – name of the file to be uploaded.\n\nSetContentString(*content*, *encoding='utf-8'*)#\nSet content of this file to be a string.\n\n\nCreates io.BytesIO instance of utf-8 encoded string.\nSets mimeType to be ‘text/plain’ if not specified and file id is not\nset (means that we are uploading this file for the first time).\n\nParameters:\n* **encoding** (*str*) – The encoding to use when setting the content of this file.\n* **content** (*str*) – content of the file in string.\n\nTrash(*param=None*)#\nMove a file to the trash.\n\nUnTrash(*param=None*)#\nMove a file out of the trash.\n:param param: Additional parameter to file.\n:type param: dict.\n:raises: ApiRequestError\n\nUpload(*param=None*)#\nUpload/update file by choosing the most efficient method.\n\nParameters:\n**param** (*dict.*) – additional parameter to upload file.\n\ncontent#\nA data descriptor that sets and returns values.\n\nuploaded#\nA data descriptor that sets and returns values.\n\n\n*class* pydrive2.files.GoogleDriveFileList(*auth=None*, *param=None*)#\nBases: `ApiResourceList`\n\n\nGoogle Drive FileList instance.\n\n\nEquivalent to Files.list() in Drive APIs.\n\n*class* pydrive2.files.IoBuffer(*encoding*)#\nBases: `object`\n\n\nLightweight retention of one chunk.\n\n\nread()#\n\nwrite(*chunk*)#\n\n\npydrive2.files.LoadMetadata(*decoratee*)#\nDecorator to check if the file has metadata and fetches it if not.\n\n*class* pydrive2.files.MediaIoReadable(*request*, *encoding=None*, *pre\\_buffer=True*, *remove\\_prefix=b''*, *chunksize=104857600*)#\nBases: `object`\n\nReturns:\nbytes or str – chunk (or None if done)\n\n\n## pydrive2.settings module#\n\n\n*exception* pydrive2.settings.InvalidConfigError#\nBases: `OSError`\n\n\nError trying to read client configuration.\n\npydrive2.settings.LoadSettingsFile(*filename='settings.yaml'*)#\nLoads settings file in yaml format given file name.\n\nParameters:\n**filename** (*str.*) – path for settings file. ‘settings.yaml’ by default.\n\nRaises:\nSettingsError\n\n*exception* pydrive2.settings.SettingsError#\nBases: `OSError`\n\n\nError while loading/saving settings\n\npydrive2.settings.ValidateSettings(*data*)#\nValidates if current settings is valid.\n\nParameters:\n**data** (*dict.*) – dictionary containing all settings.\n\n\n## pydrive2.fs module#\n\n\n*class* pydrive2.fs.GDriveFileSystem(*\\*args*, *\\*\\*kwargs*)#\nBases: `AbstractFileSystem`\n\n\nAccess to gdrive as an fsspec filesystem"
    },
    {
        "instruction": "Create a Python program that lists all files in a specified folder on Google Drive using the PyDrive2 API. The program should take the folder's ID as input and display a list of file names along with their corresponding IDs. Make sure the program is capable of handling cases where the folder ID provided does not exist or when there is an authentication issue.",
        "api": "PyDrive2",
        "output": "#!pip install pydrive2\nfrom pydrive2.auth import GoogleAuth\nfrom pydrive2.drive import GoogleDrive\n\n# Function to authenticate and get the GoogleDrive instance\ndef authenticate_drive():\n    gauth = GoogleAuth()\n    gauth.LocalWebserverAuth()  # Authenticate using local webserver\n    return GoogleDrive(gauth)\n\ndef list_files_in_drive_folder(folder_id):\n    drive = authenticate_drive()\n    \n    try:\n        folder = drive.CreateFile({'id': folder_id})\n        if not folder:\n            print(f\"Folder with ID '{folder_id}' does not exist on Google Drive.\")\n            return\n        \n        folder_title = folder['title']\n        print(f\"Listing files in '{folder_title}' folder (ID: {folder_id}):\")\n        \n        file_list = drive.ListFile({'q': f\"'{folder_id}' in parents and trashed=false\"}).GetList()\n        if not file_list:\n            print(\"No files found in this folder.\")\n            return\n        \n        for file in file_list:\n            print(f\"File Name: {file['title']}, File ID: {file['id']}\")\n    except Exception as e:\n        print(f\"Error listing files: {e}\")\n\nif __name__ == \"__main__\":\n    folder_id = input(\"Enter the folder ID on Google Drive to list files from: \")\n    \n    list_files_in_drive_folder(folder_id)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n# pydrive2.drive module#\n\n\n*class* pydrive2.drive.GoogleDrive(*auth=None*)#\nBases: `ApiAttributeMixin`\n\n\nMain Google Drive class.\n\n\nCreateFile(*metadata=None*)#\nCreate an instance of GoogleDriveFile with auth of this instance.\n\n\nThis method would not upload a file to GoogleDrive.\n\nParameters:\n**metadata** (*dict.*) – file resource to initialize GoogleDriveFile with.\n\nReturns:\npydrive2.files.GoogleDriveFile – initialized with auth of this\ninstance.\n\nGetAbout()#\nReturn information about the Google Drive of the auth instance.\n\nReturns:\nA dictionary of Google Drive information like user, usage, quota etc.\n\nListFile(*param=None*)#\nCreate an instance of GoogleDriveFileList with auth of this instance.\n\n\nThis method will not fetch from Files.List().\n\nParameters:\n**param** (*dict.*) – parameter to be sent to Files.List().\n\nReturns:\npydrive2.files.GoogleDriveFileList – initialized with auth of\nthis instance.\n\n## pydrive2.files module#\n\n\n*exception* pydrive2.files.ApiRequestError(*http\\_error*)#\nBases: `OSError`\n\n\nGetField(*field*)#\nReturns the field from the first error\n\n\n*exception* pydrive2.files.FileNotDownloadableError#\nBases: `RuntimeError`\n\n\nError trying to download file that is not downloadable.\n\n*exception* pydrive2.files.FileNotUploadedError#\nBases: `RuntimeError`\n\n\nError trying to access metadata of file that is not uploaded.\n\n*class* pydrive2.files.GoogleDriveFile(*auth=None*, *metadata=None*, *uploaded=False*)#\nBases: `ApiAttributeMixin`, `ApiResource`\n\n\nGoogle Drive File\n\n==================\n Document 1 \n----------------\n# pydrive2.files module#\n\n\n*exception* pydrive2.files.ApiRequestError(*http\\_error*)#\nBases: `OSError`\n\n\nGetField(*field*)#\nReturns the field from the first error\n\n\n*exception* pydrive2.files.FileNotDownloadableError#\nBases: `RuntimeError`\n\n\nError trying to download file that is not downloadable.\n\n*exception* pydrive2.files.FileNotUploadedError#\nBases: `RuntimeError`\n\n\nError trying to access metadata of file that is not uploaded.\n\n*class* pydrive2.files.GoogleDriveFile(*auth=None*, *metadata=None*, *uploaded=False*)#\nBases: `ApiAttributeMixin`, `ApiResource`\n\n\nGoogle Drive File instance.\n\n\nInherits ApiResource which inherits dict.\nCan access and modify metadata like dictionary.\n\n\nCopy(*target\\_folder=None*, *new\\_title=None*, *param=None*)#\nCreates a copy of this file. Folders cannot be copied.\n\nParameters:\n* **target\\_folder** (*GoogleDriveFile**,* *optional*) – Folder where the file will be copied.\n* **new\\_title** (*str**,* *optional*) – Name of the new file.\n* **param** (*dict**,* *optional*) – addition parameters to pass.\n\nRaises:\nApiRequestError\n\nReturns:\nthe copied file\n\nReturn type:\nGoogleDriveFile\n\nDelete(*param=None*)#\nHard-delete a file.\n\nParameters:\n**param** (*dict.*) – additional parameter to file.\n\nDeletePermission(*permission\\_id*)#\nDeletes the permission specified by the permission\\_id.\n\nParameters:\n**permission\\_id** (*str*) – The permission id.\n\nReturns:\nTrue if it succeeds.\n\nReturn type:\nbool\n\nFetchContent(*mimetype=None*, *remove\\_bom=False*)#\nDownload file’s content from download\\_url.\n\nRaises:\nApiRequestError, FileNotUploadedError, FileNotDownloadableError\n\nFetchMetadata(*fields=None*, *fetch\\_all=False*)#\nDownload file’s metadata from id using Files.get().\n\nParameters:\n* **fields** (*str*) – The fields to include, as one string, each entry separated\nby commas, e.g. ‘fields,labels’.\n* **fetch\\_all** (*bool*) – Whether to fetch all fields.\n\nRaises:\nApiRequestError, FileNotUploadedError\n\nGetContentFile(*filename*, *mimetype=None*, *remove\\_bom=False*, *callback=None*, *chunksize=104857600*, *acknowledge\\_abuse=False*)#\nSave content of this file as a local file.\n\nParameters:\n* **filename** (*str*) – name of the file to write to.\n* **mimetype** (*str*) – mimeType of the file.\n* **remove\\_bom** (*bool*) – Whether to remove the byte order marking.\n* **callback** – passed two arguments: (total transferred, file size).\n* **chunksize** (*int*) – chunksize in bytes (standard 100 MB(1024\\*1024\\*100))\n* **acknowledge\\_abuse** (*bool*) – Acknowledging the risk and download file\nidentified as abusive.\n\nGetContentIOBuffer(*mimetype=None*, *encoding=None*, *remove\\_bom=False*, *chunksize=104857600*, *acknowledge\\_abuse=False*)#\nGet a file-like object which has a buffered read() method.\n\nParameters:\n* **mimetype** (*str*) – mimeType of the file.\n* **encoding** (*str*) – The encoding to use when decoding the byte string.\n* **remove\\_bom** (*bool*) – Whether to remove the byte order marking.\n* **chunksize** (*int*) – default read()/iter() chunksize.\n* **acknowledge\\_abuse** (*bool*) – Acknowledging the risk and download file\nidentified as abusive.\n\nReturns:\nMediaIoReadable – file-like object.\n\nGetContentString(*mimetype=None*, *encoding='utf-8'*, *remove\\_bom=False*)#\nGet content of this file as a string.\n\nParameters:\n* **mimetype** (*str*) – The mimetype of the content string.\n* **encoding** (*str*) – The encoding to use when decoding the byte string.\n* **remove\\_bom** (*bool*) – Whether to strip a known BOM.\n\nReturns:\nstr – utf-8 decoded content of the file\n\nGetPermissions()#\nGet file’s or shared drive’s permissions.\n\n\nFor files in a shared drive, at most 100 results will be returned.\nIt doesn’t paginate and collect all results.\n\nReturns:\nA list of the permission objects.\n\nReturn type:\nobject[]\n\nGetRevisions()#\nGet file’s or shared drive’s revisions.\n\nReturns:\nA list of the revision objects.\n\nInsertPermission(*new\\_permission*, *param=None*)#\nInsert a new permission. Re-fetches all permissions after call.\n\nParameters:\n* **new\\_permission** (*object*) – The new permission to insert, please see the\nofficial Google Drive API guide on permissions.insert\nfor details.\n* **param** (*dict*) – addition parameters to pass\n\nReturns:\nThe permission object.\n\nReturn type:\nobject\n\nSetContentFile(*filename*)#\nSet content of this file from a file.\n\n\nOpens the file specified by this method.\nWill be read, uploaded, and closed by Upload() method.\nSets metadata ‘title’ and ‘mimeType’ automatically if not specified and\nthe file is uploaded for the first time (id is not set).\n\nParameters:\n**filename** (*str.*) – name of the file to be uploaded.\n\nSetContentString(*content*, *encoding='utf-8'*)#\nSet content of this file to be a string.\n\n\nCreates io.BytesIO instance of utf-8 encoded string.\nSets mimeType to be ‘text/plain’ if not specified and file id is not\nset (means that we are uploading this file for the first time).\n\nParameters:\n* **encoding** (*str*) – The encoding to use when setting the content of this file.\n* **content** (*str*) – content of the file in string.\n\nTrash(*param=None*)#\nMove a file to the trash.\n\nUnTrash(*param=None*)#\nMove a file out of the trash.\n:param param: Additional parameter to file.\n:type param: dict.\n:raises: ApiRequestError\n\nUpload(*param=None*)#\nUpload/update file by choosing the most efficient method.\n\nParameters:\n**param** (*dict.*) – additional parameter to upload file.\n\ncontent#\nA data descriptor that sets and returns values.\n\nuploaded#\nA data descriptor that sets and returns values.\n\n\n*class* pydrive2.files.GoogleDriveFileList(*auth=None*, *param=None*)#\nBases: `ApiResourceList`\n\n\nGoogle Drive FileList instance.\n\n\nEquivalent to Files.list() in Drive APIs.\n\n*class* pydrive2.files.IoBuffer(*encoding*)#\nBases: `object`\n\n\nLightweight retention of one chunk.\n\n\nread()#\n\nwrite(*chunk*)#\n\n\npydrive2.files.LoadMetadata(*decoratee*)#\nDecorator to check if the file has metadata and fetches it if not.\n\n*class* pydrive2.files.MediaIoReadable(*request*, *encoding=None*, *pre\\_buffer=True*, *remove\\_prefix=b''*, *chunksize=104857600*)#\nBases: `object`\n\nReturns:\nbytes or str – chunk (or None if done)\n\n\n## pydrive2.settings module#\n\n\n*exception* pydrive2.settings.InvalidConfigError#\nBases: `OSError`\n\n\nError trying to read client configuration.\n\npydrive2.settings.LoadSettingsFile(*filename='settings.yaml'*)#\nLoads settings file in yaml format given file name.\n\nParameters:\n**filename** (*str.*) – path for settings file. ‘settings.yaml’ by default.\n\nRaises:\nSettingsError\n\n*exception* pydrive2.settings.SettingsError#\nBases: `OSError`\n\n\nError while loading/saving settings\n\npydrive2.settings.ValidateSettings(*data*)#\nValidates if current settings is valid.\n\nParameters:\n**data** (*dict.*) – dictionary containing all settings.\n\n\n## pydrive2.fs module#\n\n\n*class* pydrive2.fs.GDriveFileSystem(*\\*args*, *\\*\\*kwargs*)#\nBases: `AbstractFileSystem`\n\n\nAccess to gdrive as an fsspec filesystem"
    },
    {
        "instruction": "Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as an Excel file (XLSX) with a custom color scheme and line numbers.",
        "api": "Pygments",
        "output": "# !pip install Pygments\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import ExcelFormatter\n\n# Read the code file\nwith open(\"code.py\", \"r\") as file:\n    code = file.read()\n\n# Determine the lexer based on the file extension\nlexer = get_lexer_by_name(\"python\")\n\n# Define a custom formatter with a custom color scheme and line numbers\nformatter = ExcelFormatter(style=\"friendly\", linenos=True)\n\n# Apply syntax highlighting to the code with the custom color scheme and line numbers\nhighlighted_code = highlight(code, lexer, formatter)\n\n# Save the highlighted code to an Excel file (XLSX) with the custom color scheme and line numbers\nwith open(\"highlighted_code.xlsx\", \"wb\") as xlsx_file:\n    xlsx_file.write(highlighted_code)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# The full Pygments API¶\n\n\nThis page describes the Pygments API.\n\n## High-level API¶\n\n\nFunctions from the `pygments` module:\n\n\npygments.lex(*code*, *lexer*)¶\nLex code with the lexer (must be a Lexer instance)\nand return an iterable of tokens. Currently, this only calls\nlexer.get\\_tokens().\n\npygments.format(*tokens*, *formatter*, *outfile=None*)¶\nFormat `tokens` (an iterable of tokens) with the formatter `formatter`\n(a Formatter instance).\n\n\nIf `outfile`\n\n==================\n Document 1 \n----------------\n# High-level API¶\n\n\nFunctions from the `pygments` module:\n\n\npygments.lex(*code*, *lexer*)¶\nLex code with the lexer (must be a Lexer instance)\nand return an iterable of tokens. Currently, this only calls\nlexer.get\\_tokens().\n\npygments.format(*tokens*, *formatter*, *outfile=None*)¶\nFormat `tokens` (an iterable of tokens) with the formatter `formatter`\n(a Formatter instance).\n\n\nIf `outfile` is given and a valid file object (an object with a\n`write` method), the result will be written to it, otherwise it\nis returned as a string.\n\npygments.highlight(*code*, *lexer*, *formatter*, *outfile=None*)¶\nThis is the most high-level highlighting function. It combines lex and\nformat in one function.\n\nFunctions from `pygments.lexers`:\n\n\npygments.lexers.get\\_lexer\\_by\\_name(*\\_alias*, *\\*\\*options*)¶\nReturn an instance of a Lexer subclass that has alias in its\naliases list. The lexer is given the options at its\ninstantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if no lexer with that alias is\nfound.\n\npygments.lexers.get\\_lexer\\_for\\_filename(*\\_fn*, *code=None*, *\\*\\*options*)¶\nGet a lexer for a filename.\n\n\nReturn a Lexer subclass instance that has a filename pattern\nmatching fn. The lexer is given the options at its\ninstantiation.\n\n\nRaise `pygments.util.ClassNotFound` if no lexer for that filename\nis found.\n\n\nIf multiple lexers match the filename pattern, use their `analyse\\_text()`\nmethods to figure out which one is more appropriate.\n\npygments.lexers.get\\_lexer\\_for\\_mimetype(*\\_mime*, *\\*\\*options*)¶\nReturn a Lexer subclass instance that has mime in its mimetype\nlist. The lexer is given the options at its instantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if not lexer for that mimetype\nis found.\n\npygments.lexers.load\\_lexer\\_from\\_file(*filename*, *lexername='CustomLexer'*, *\\*\\*options*)¶\nLoad a lexer from a file.\n\n\nThis method expects a file located relative to the current working\ndirectory, which contains a Lexer class. By default, it expects the\nLexer to be name CustomLexer; you can specify your own class name\nas the second argument to this function.\n\n\nUsers should be very careful with the input, because this method\nis equivalent to running eval on the input file.\n\n\nRaises ClassNotFound if there are any problems importing the Lexer.\n\nNew in version 2.2.\n\n\npygments.lexers.guess\\_lexer(*\\_text*, *\\*\\*options*)¶\nReturn a Lexer subclass instance that’s guessed from the text in\ntext. For that, the `analyse\\_text()` method of every known lexer\nclass is called with the text as argument, and the lexer which returned the\nhighest value will be instantiated and returned.\n\n\n`pygments.util.ClassNotFound` is raised if no lexer thinks it can\nhandle the content.\n\npygments.lexers.guess\\_lexer\\_for\\_filename(*\\_fn*, *\\_text*, *\\*\\*options*)¶\nAs `guess\\_lexer()`, but only lexers which have a pattern in filenames\nor alias\\_filenames that matches filename are taken into consideration.\n\npygments.lexers.get\\_all\\_lexers(*plugins=True*)¶\nReturn a generator of tuples in the form `(name, aliases,\nfilenames, mimetypes)` of all know lexers.\n\n\nIf *plugins* is true (the default), plugin lexers supplied by entrypoints\nare also returned. Otherwise, only builtin ones are considered.\n\npygments.lexers.find\\_lexer\\_class\\_by\\_name(*\\_alias*)¶\nReturn the Lexer subclass that has alias in its aliases list, without\ninstantiating it.\n\n\nLike get\\_lexer\\_by\\_name, but does not instantiate the class.\n\n\npygments.lexers.find\\_lexer\\_class(*name*)¶\nReturn the Lexer subclass that with the *name* attribute as given by\nthe *name* argument.\n\nFunctions from `pygments.formatters`:\n\n\npygments.formatters.get\\_formatter\\_by\\_name(*\\_alias*, *\\*\\*options*)¶\nReturn an instance of a `Formatter` subclass that has alias in its\naliases list. The formatter is given the options at its instantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if no formatter with that\nalias is found.\n\npygments.formatters.get\\_formatter\\_for\\_filename(*fn*, *\\*\\*options*)¶\nReturn a `Formatter` subclass instance that has a filename pattern\nmatching fn. The formatter is given the options at its instantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if no formatter for that filename\nis found.\n\npygments.formatters.load\\_formatter\\_from\\_file(*filename*, *formattername='CustomFormatter'*, *\\*\\*options*)¶\nReturn a Formatter subclass instance loaded from the provided file, relative\nto the current directory.\n\n\nThe file is expected to contain a Formatter class named `formattername`\n(by default, CustomFormatter). Users should be very careful with the input, because\nthis method is equivalent to running `eval()` on the input file. The formatter is\ngiven the options at its instantiation.\n\n\n`pygments.util.ClassNotFound` is raised if there are any errors loading\nthe formatter.\n\n\nFunctions from `pygments.styles`:\n\n\npygments.styles.get\\_style\\_by\\_name(*name*)¶\nReturn a style class by its short name. The names of the builtin styles\nare listed in `pygments.styles.STYLE\\_MAP`.\n\n\nWill raise `pygments.util.ClassNotFound` if no style of that name is\nfound.\n\npygments.styles.get\\_all\\_styles()¶\nReturn a generator for all styles by name, both builtin and plugin.\n\npygments.styles.STYLE\\_MAP *= {'abap': 'abap::AbapStyle', 'algol': 'algol::AlgolStyle', 'algol\\_nu': 'algol\\_nu::Algol\\_NuStyle', 'arduino': 'arduino::ArduinoStyle', 'autumn': 'autumn::AutumnStyle', 'borland': 'borland::BorlandStyle', 'bw': 'bw::BlackWhiteStyle', 'colorful': 'colorful::ColorfulStyle', 'default': 'default::DefaultStyle', 'dracula': 'dracula::DraculaStyle', 'emacs': 'emacs::EmacsStyle', 'friendly': 'friendly::FriendlyStyle', 'friendly\\_grayscale': 'friendly\\_grayscale::FriendlyGrayscaleStyle', 'fruity': 'fruity::FruityStyle', 'github-dark': 'gh\\_dark::GhDarkStyle', 'gruvbox-dark': 'gruvbox::GruvboxDarkStyle', 'gruvbox-light': 'gruvbox::GruvboxLightStyle', 'igor': 'igor::IgorStyle', 'inkpot': 'inkpot::InkPotStyle', 'lightbulb': 'lightbulb::LightbulbStyle', 'lilypond': 'lilypond::LilyPondStyle', 'lovelace': 'lovelace::LovelaceStyle', 'manni': 'manni::ManniStyle', 'material': 'material::MaterialStyle', 'monokai': 'monokai::MonokaiStyle', 'murphy': 'murphy::MurphyStyle', 'native': 'native::NativeStyle', 'nord': 'nord::NordStyle', 'nord-darker': 'nord::NordDarkerStyle', 'one-dark': 'onedark::OneDarkStyle', 'paraiso-dark': 'paraiso\\_dark::ParaisoDarkStyle', 'paraiso-light': 'paraiso\\_light::ParaisoLightStyle', 'pastie': 'pastie::PastieStyle', 'perldoc': 'perldoc::PerldocStyle', 'rainbow\\_dash': 'rainbow\\_dash::RainbowDashStyle', 'rrt': 'rrt::RrtStyle', 'sas': 'sas::SasStyle', 'solarized-dark': 'solarized::SolarizedDarkStyle', 'solarized-light': 'solarized::SolarizedLightStyle', 'staroffice': 'staroffice::StarofficeStyle', 'stata-dark': 'stata\\_dark::StataDarkStyle', 'stata-light': 'stata\\_light::StataLightStyle', 'tango': 'tango::TangoStyle', 'trac': 'trac::TracStyle', 'vim': 'vim::VimStyle', 'vs': 'vs::VisualStudioStyle', 'xcode': 'xcode::XcodeStyle', 'zenburn': 'zenburn::ZenburnStyle'}*¶\nA dictionary of built-in styles, mapping style names to\n`'submodule::classname'` strings.\nThis list is deprecated. Use pygments.styles.STYLES instead\n## Lexers¶\n\n\nThe base lexer class from which all lexers are derived is:\n\n\n*class* pygments.lexer.Lexer(*\\*\\*options*)¶\nLexer for a specific language.\n\n\nSee also Write your own lexer, a high-level guide to writing\nlexers.\n\n\nLexer classes have attributes used for choosing the most appropriate\nlexer based on various criteria.\n\n\nname¶\nFull\n\n==================\n Document 2 \n----------------\n# Lexers¶\n\n\nThe base lexer class from which all lexers are derived is:\n\n\n*class* pygments.lexer.Lexer(*\\*\\*options*)¶\nLexer for a specific language.\n\n\nSee also Write your own lexer, a high-level guide to writing\nlexers.\n\n\nLexer classes have attributes used for choosing the most appropriate\nlexer based on various criteria.\n\n\nname¶\nFull name of the lexer, in human-readable form\n\naliases¶\nA list of short, unique identifiers that can be used to look\nup the lexer from a list, e.g., using get\\_lexer\\_by\\_name().\n\nfilenames¶\nA list of fnmatch patterns that match filenames which contain\ncontent for this lexer. The patterns in this list should be unique among\nall lexers.\n\nalias\\_filenames *= []*¶\nA list of fnmatch patterns that match filenames which may or may not\ncontain content for this lexer. This list is used by the\n`guess\\_lexer\\_for\\_filename()` function, to determine which lexers\nare then included in guessing the correct one. That means that\ne.g. every lexer for HTML and a template language should include\n`\\\\*.html` in this list.\n\nmimetypes¶\nA list of MIME types for content that can be lexed with this lexer.\n\npriority *= 0*¶\nPriority, should multiple lexers match and no content is provided\n\nLexers included in Pygments should have an additional attribute:\n\n\nurl¶\nURL of the language specification/definition. Used in the Pygments\ndocumentation.\n\nLexers included in Pygments may have additional attributes:\n\n\n\\_example¶\nExample file name. Relative to the `tests/examplefiles` directory.\nThis is used by the documentation generator to show an example.\n\nYou can pass options to the constructor. The basic options recognized\nby all lexers and processed by the base Lexer class are:\n\n`stripnl`Strip leading and trailing newlines from the input (default: True).\n\n`stripall`Strip all leading and trailing whitespace from the input\n(default: False).\n\n`ensurenl`Make sure that the input ends with a newline (default: True). This\nis required for some lexers that consume input linewise.\n\nNew in version 1.3.\n\n\n`tabsize`If given and greater than 0, expand tabs in the input (default: 0).\n\n`encoding`If given, must be an encoding name. This encoding will be used to\nconvert the input string to Unicode, if it is not already a Unicode\nstring (default: `'guess'`, which uses a simple UTF-8 / Locale /\nLatin1 detection. Can also be `'chardet'` to use the chardet\nlibrary, if it is installed.\n\n`inencoding`Overrides the `encoding` if given.\n\n\n\\_\\_init\\_\\_(*\\*\\*options*)¶\nThis constructor takes arbitrary options as keyword arguments.\nEvery subclass must first process its own options and then call\nthe Lexer constructor, since it processes the basic\noptions like stripnl.\n\n\nAn example looks like this:\n\n```\ndef \\_\\_init\\_\\_(self, \\*\\*options):\n    self.compress = options.get('compress', '')\n    Lexer.\\_\\_init\\_\\_(self, \\*\\*options)\n\n```\n\n\nAs these options must all be specifiable as strings (due to the\ncommand line usage), there are various utility functions\navailable to help with that, see Utilities.\n\nadd\\_filter(*filter\\_*, *\\*\\*options*)¶\nAdd a new stream filter to this lexer.\n\n*static* analyse\\_text(*text*)¶\nA static method which is called for lexer guessing.\n\n\nIt should analyse the text and return a float in the range\nfrom `0.0` to `1.0`. If it returns `0.0`, the lexer\nwill not be selected as the most probable one, if it returns\n`1.0`, it will be selected immediately. This is used by\nguess\\_lexer.\n\n\nThe LexerMeta metaclass automatically wraps this function so\nthat it works like a static method (no `self` or `cls`\nparameter) and the return value is automatically converted to\nfloat. If the return value is an object that is boolean False\nit’s the same as if the return values was `0.0`.\n\nget\\_tokens(*text*, *unfiltered=False*)¶\nThis method is the basic interface of a lexer. It is called by\nthe highlight() function. It must process the text and return an\niterable of `(tokentype, value)` pairs from text.\n\n\nNormally, you don’t need to override this method. The default\nimplementation processes the options recognized by all lexers\n(stripnl, stripall and so on), and then yields all tokens\nfrom get\\_tokens\\_unprocessed(), with the `index` dropped.\n\n\nIf unfiltered is set to True, the filtering mechanism is\nbypassed even if filters are defined.\n\nget\\_tokens\\_unprocessed(*text*)¶\nThis method should process the text and return an iterable of\n`(index, tokentype, value)` tuples where `index` is the starting\nposition of the token within the input text.\n\n\nIt must be overridden by subclasses. It is recommended to\nimplement it as a generator to maximize effectiveness.\n\n\nThere are several base class derived from `Lexer` you can use to build your lexer from:\n\n\n*class* pygments.lexer.RegexLexer(*\\*args*, *\\*\\*kwds*)¶\nBase for simple stateful regular expression-based lexers.\nSimplifies the lexing process so that you need only\nprovide a list of states and regular expressions.\n\n*class* pygments.lexer.ExtendedRegexLexer(*\\*args*, *\\*\\*kwds*)¶\nA RegexLexer that uses a context object to store its state.\n\n*class* pygments.lexer.DelegatingLexer(*\\_root\\_lexer*, *\\_language\\_lexer*, *\\_needle=('Other',)*, *\\*\\*options*)¶\nThis lexer takes two lexer as arguments. A root lexer and\na language lexer. First everything is scanned using the language\nlexer, afterwards all `Other` tokens are lexed using the root\nlexer.\n\n\nThe lexers from the `template` lexer package use this base lexer.\n## Formatters¶\n\n\nA formatter is derived from this class:\n\n\n*class* pygments.formatter.Formatter(*\\*\\*options*)¶\nConverts a token stream to text.\n\n\nFormatters should have attributes to help selecting them. These\nare similar to the corresponding `Lexer`\nattributes.\n\n\nname¶\nFull name for the formatter, in human-readable form.\n\naliases¶\nA list of short, unique identifiers that"
    },
    {
        "instruction": "Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as an Excel file (XLSX).",
        "api": "Pygments",
        "output": "# !pip install Pygments\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import ExcelFormatter\n\n# Read the code file\nwith open(\"code.py\", \"r\") as file:\n    code = file.read()\n\n# Determine the lexer based on the file extension\nlexer = get_lexer_by_name(\"python\")\n\n# Apply syntax highlighting to the code\nhighlighted_code = highlight(code, lexer, ExcelFormatter())\n\n# Save the highlighted code to an Excel file (XLSX)\nwith open(\"highlighted_code.xlsx\", \"wb\") as xlsx_file:\n    xlsx_file.write(highlighted_code)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# The full Pygments API¶\n\n\nThis page describes the Pygments API.\n\n## High-level API¶\n\n\nFunctions from the `pygments` module:\n\n\npygments.lex(*code*, *lexer*)¶\nLex code with the lexer (must be a Lexer instance)\nand return an iterable of tokens. Currently, this only calls\nlexer.get\\_tokens().\n\npygments.format(*tokens*, *formatter*, *outfile=None*)¶\nFormat `tokens` (an iterable of tokens) with the formatter `formatter`\n(a Formatter instance).\n\n\nIf `outfile`\n\n==================\n Document 1 \n----------------\n# High-level API¶\n\n\nFunctions from the `pygments` module:\n\n\npygments.lex(*code*, *lexer*)¶\nLex code with the lexer (must be a Lexer instance)\nand return an iterable of tokens. Currently, this only calls\nlexer.get\\_tokens().\n\npygments.format(*tokens*, *formatter*, *outfile=None*)¶\nFormat `tokens` (an iterable of tokens) with the formatter `formatter`\n(a Formatter instance).\n\n\nIf `outfile` is given and a valid file object (an object with a\n`write` method), the result will be written to it, otherwise it\nis returned as a string.\n\npygments.highlight(*code*, *lexer*, *formatter*, *outfile=None*)¶\nThis is the most high-level highlighting function. It combines lex and\nformat in one function.\n\nFunctions from `pygments.lexers`:\n\n\npygments.lexers.get\\_lexer\\_by\\_name(*\\_alias*, *\\*\\*options*)¶\nReturn an instance of a Lexer subclass that has alias in its\naliases list. The lexer is given the options at its\ninstantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if no lexer with that alias is\nfound.\n\npygments.lexers.get\\_lexer\\_for\\_filename(*\\_fn*, *code=None*, *\\*\\*options*)¶\nGet a lexer for a filename.\n\n\nReturn a Lexer subclass instance that has a filename pattern\nmatching fn. The lexer is given the options at its\ninstantiation.\n\n\nRaise `pygments.util.ClassNotFound` if no lexer for that filename\nis found.\n\n\nIf multiple lexers match the filename pattern, use their `analyse\\_text()`\nmethods to figure out which one is more appropriate.\n\npygments.lexers.get\\_lexer\\_for\\_mimetype(*\\_mime*, *\\*\\*options*)¶\nReturn a Lexer subclass instance that has mime in its mimetype\nlist. The lexer is given the options at its instantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if not lexer for that mimetype\nis found.\n\npygments.lexers.load\\_lexer\\_from\\_file(*filename*, *lexername='CustomLexer'*, *\\*\\*options*)¶\nLoad a lexer from a file.\n\n\nThis method expects a file located relative to the current working\ndirectory, which contains a Lexer class. By default, it expects the\nLexer to be name CustomLexer; you can specify your own class name\nas the second argument to this function.\n\n\nUsers should be very careful with the input, because this method\nis equivalent to running eval on the input file.\n\n\nRaises ClassNotFound if there are any problems importing the Lexer.\n\nNew in version 2.2.\n\n\npygments.lexers.guess\\_lexer(*\\_text*, *\\*\\*options*)¶\nReturn a Lexer subclass instance that’s guessed from the text in\ntext. For that, the `analyse\\_text()` method of every known lexer\nclass is called with the text as argument, and the lexer which returned the\nhighest value will be instantiated and returned.\n\n\n`pygments.util.ClassNotFound` is raised if no lexer thinks it can\nhandle the content.\n\npygments.lexers.guess\\_lexer\\_for\\_filename(*\\_fn*, *\\_text*, *\\*\\*options*)¶\nAs `guess\\_lexer()`, but only lexers which have a pattern in filenames\nor alias\\_filenames that matches filename are taken into consideration.\n\npygments.lexers.get\\_all\\_lexers(*plugins=True*)¶\nReturn a generator of tuples in the form `(name, aliases,\nfilenames, mimetypes)` of all know lexers.\n\n\nIf *plugins* is true (the default), plugin lexers supplied by entrypoints\nare also returned. Otherwise, only builtin ones are considered.\n\npygments.lexers.find\\_lexer\\_class\\_by\\_name(*\\_alias*)¶\nReturn the Lexer subclass that has alias in its aliases list, without\ninstantiating it.\n\n\nLike get\\_lexer\\_by\\_name, but does not instantiate the class.\n\n\npygments.lexers.find\\_lexer\\_class(*name*)¶\nReturn the Lexer subclass that with the *name* attribute as given by\nthe *name* argument.\n\nFunctions from `pygments.formatters`:\n\n\npygments.formatters.get\\_formatter\\_by\\_name(*\\_alias*, *\\*\\*options*)¶\nReturn an instance of a `Formatter` subclass that has alias in its\naliases list. The formatter is given the options at its instantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if no formatter with that\nalias is found.\n\npygments.formatters.get\\_formatter\\_for\\_filename(*fn*, *\\*\\*options*)¶\nReturn a `Formatter` subclass instance that has a filename pattern\nmatching fn. The formatter is given the options at its instantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if no formatter for that filename\nis found.\n\npygments.formatters.load\\_formatter\\_from\\_file(*filename*, *formattername='CustomFormatter'*, *\\*\\*options*)¶\nReturn a Formatter subclass instance loaded from the provided file, relative\nto the current directory.\n\n\nThe file is expected to contain a Formatter class named `formattername`\n(by default, CustomFormatter). Users should be very careful with the input, because\nthis method is equivalent to running `eval()` on the input file. The formatter is\ngiven the options at its instantiation.\n\n\n`pygments.util.ClassNotFound` is raised if there are any errors loading\nthe formatter.\n\n\nFunctions from `pygments.styles`:\n\n\npygments.styles.get\\_style\\_by\\_name(*name*)¶\nReturn a style class by its short name. The names of the builtin styles\nare listed in `pygments.styles.STYLE\\_MAP`.\n\n\nWill raise `pygments.util.ClassNotFound` if no style of that name is\nfound.\n\npygments.styles.get\\_all\\_styles()¶\nReturn a generator for all styles by name, both builtin and plugin.\n\npygments.styles.STYLE\\_MAP *= {'abap': 'abap::AbapStyle', 'algol': 'algol::AlgolStyle', 'algol\\_nu': 'algol\\_nu::Algol\\_NuStyle', 'arduino': 'arduino::ArduinoStyle', 'autumn': 'autumn::AutumnStyle', 'borland': 'borland::BorlandStyle', 'bw': 'bw::BlackWhiteStyle', 'colorful': 'colorful::ColorfulStyle', 'default': 'default::DefaultStyle', 'dracula': 'dracula::DraculaStyle', 'emacs': 'emacs::EmacsStyle', 'friendly': 'friendly::FriendlyStyle', 'friendly\\_grayscale': 'friendly\\_grayscale::FriendlyGrayscaleStyle', 'fruity': 'fruity::FruityStyle', 'github-dark': 'gh\\_dark::GhDarkStyle', 'gruvbox-dark': 'gruvbox::GruvboxDarkStyle', 'gruvbox-light': 'gruvbox::GruvboxLightStyle', 'igor': 'igor::IgorStyle', 'inkpot': 'inkpot::InkPotStyle', 'lightbulb': 'lightbulb::LightbulbStyle', 'lilypond': 'lilypond::LilyPondStyle', 'lovelace': 'lovelace::LovelaceStyle', 'manni': 'manni::ManniStyle', 'material': 'material::MaterialStyle', 'monokai': 'monokai::MonokaiStyle', 'murphy': 'murphy::MurphyStyle', 'native': 'native::NativeStyle', 'nord': 'nord::NordStyle', 'nord-darker': 'nord::NordDarkerStyle', 'one-dark': 'onedark::OneDarkStyle', 'paraiso-dark': 'paraiso\\_dark::ParaisoDarkStyle', 'paraiso-light': 'paraiso\\_light::ParaisoLightStyle', 'pastie': 'pastie::PastieStyle', 'perldoc': 'perldoc::PerldocStyle', 'rainbow\\_dash': 'rainbow\\_dash::RainbowDashStyle', 'rrt': 'rrt::RrtStyle', 'sas': 'sas::SasStyle', 'solarized-dark': 'solarized::SolarizedDarkStyle', 'solarized-light': 'solarized::SolarizedLightStyle', 'staroffice': 'staroffice::StarofficeStyle', 'stata-dark': 'stata\\_dark::StataDarkStyle', 'stata-light': 'stata\\_light::StataLightStyle', 'tango': 'tango::TangoStyle', 'trac': 'trac::TracStyle', 'vim': 'vim::VimStyle', 'vs': 'vs::VisualStudioStyle', 'xcode': 'xcode::XcodeStyle', 'zenburn': 'zenburn::ZenburnStyle'}*¶\nA dictionary of built-in styles, mapping style names to\n`'submodule::classname'` strings.\nThis list is deprecated. Use pygments.styles.STYLES instead\n## Lexers¶\n\n\nThe base lexer class from which all lexers are derived is:\n\n\n*class* pygments.lexer.Lexer(*\\*\\*options*)¶\nLexer for a specific language.\n\n\nSee also Write your own lexer, a high-level guide to writing\nlexers.\n\n\nLexer classes have attributes used for choosing the most appropriate\nlexer based on various criteria.\n\n\nname¶\nFull\n\n==================\n Document 2 \n----------------\n# Lexers¶\n\n\nThe base lexer class from which all lexers are derived is:\n\n\n*class* pygments.lexer.Lexer(*\\*\\*options*)¶\nLexer for a specific language.\n\n\nSee also Write your own lexer, a high-level guide to writing\nlexers.\n\n\nLexer classes have attributes used for choosing the most appropriate\nlexer based on various criteria.\n\n\nname¶\nFull name of the lexer, in human-readable form\n\naliases¶\nA list of short, unique identifiers that can be used to look\nup the lexer from a list, e.g., using get\\_lexer\\_by\\_name().\n\nfilenames¶\nA list of fnmatch patterns that match filenames which contain\ncontent for this lexer. The patterns in this list should be unique among\nall lexers.\n\nalias\\_filenames *= []*¶\nA list of fnmatch patterns that match filenames which may or may not\ncontain content for this lexer. This list is used by the\n`guess\\_lexer\\_for\\_filename()` function, to determine which lexers\nare then included in guessing the correct one. That means that\ne.g. every lexer for HTML and a template language should include\n`\\\\*.html` in this list.\n\nmimetypes¶\nA list of MIME types for content that can be lexed with this lexer.\n\npriority *= 0*¶\nPriority, should multiple lexers match and no content is provided\n\nLexers included in Pygments should have an additional attribute:\n\n\nurl¶\nURL of the language specification/definition. Used in the Pygments\ndocumentation.\n\nLexers included in Pygments may have additional attributes:\n\n\n\\_example¶\nExample file name. Relative to the `tests/examplefiles` directory.\nThis is used by the documentation generator to show an example.\n\nYou can pass options to the constructor. The basic options recognized\nby all lexers and processed by the base Lexer class are:\n\n`stripnl`Strip leading and trailing newlines from the input (default: True).\n\n`stripall`Strip all leading and trailing whitespace from the input\n(default: False).\n\n`ensurenl`Make sure that the input ends with a newline (default: True). This\nis required for some lexers that consume input linewise.\n\nNew in version 1.3.\n\n\n`tabsize`If given and greater than 0, expand tabs in the input (default: 0).\n\n`encoding`If given, must be an encoding name. This encoding will be used to\nconvert the input string to Unicode, if it is not already a Unicode\nstring (default: `'guess'`, which uses a simple UTF-8 / Locale /\nLatin1 detection. Can also be `'chardet'` to use the chardet\nlibrary, if it is installed.\n\n`inencoding`Overrides the `encoding` if given.\n\n\n\\_\\_init\\_\\_(*\\*\\*options*)¶\nThis constructor takes arbitrary options as keyword arguments.\nEvery subclass must first process its own options and then call\nthe Lexer constructor, since it processes the basic\noptions like stripnl.\n\n\nAn example looks like this:\n\n```\ndef \\_\\_init\\_\\_(self, \\*\\*options):\n    self.compress = options.get('compress', '')\n    Lexer.\\_\\_init\\_\\_(self, \\*\\*options)\n\n```\n\n\nAs these options must all be specifiable as strings (due to the\ncommand line usage), there are various utility functions\navailable to help with that, see Utilities.\n\nadd\\_filter(*filter\\_*, *\\*\\*options*)¶\nAdd a new stream filter to this lexer.\n\n*static* analyse\\_text(*text*)¶\nA static method which is called for lexer guessing.\n\n\nIt should analyse the text and return a float in the range\nfrom `0.0` to `1.0`. If it returns `0.0`, the lexer\nwill not be selected as the most probable one, if it returns\n`1.0`, it will be selected immediately. This is used by\nguess\\_lexer.\n\n\nThe LexerMeta metaclass automatically wraps this function so\nthat it works like a static method (no `self` or `cls`\nparameter) and the return value is automatically converted to\nfloat. If the return value is an object that is boolean False\nit’s the same as if the return values was `0.0`.\n\nget\\_tokens(*text*, *unfiltered=False*)¶\nThis method is the basic interface of a lexer. It is called by\nthe highlight() function. It must process the text and return an\niterable of `(tokentype, value)` pairs from text.\n\n\nNormally, you don’t need to override this method. The default\nimplementation processes the options recognized by all lexers\n(stripnl, stripall and so on), and then yields all tokens\nfrom get\\_tokens\\_unprocessed(), with the `index` dropped.\n\n\nIf unfiltered is set to True, the filtering mechanism is\nbypassed even if filters are defined.\n\nget\\_tokens\\_unprocessed(*text*)¶\nThis method should process the text and return an iterable of\n`(index, tokentype, value)` tuples where `index` is the starting\nposition of the token within the input text.\n\n\nIt must be overridden by subclasses. It is recommended to\nimplement it as a generator to maximize effectiveness.\n\n\nThere are several base class derived from `Lexer` you can use to build your lexer from:\n\n\n*class* pygments.lexer.RegexLexer(*\\*args*, *\\*\\*kwds*)¶\nBase for simple stateful regular expression-based lexers.\nSimplifies the lexing process so that you need only\nprovide a list of states and regular expressions.\n\n*class* pygments.lexer.ExtendedRegexLexer(*\\*args*, *\\*\\*kwds*)¶\nA RegexLexer that uses a context object to store its state.\n\n*class* pygments.lexer.DelegatingLexer(*\\_root\\_lexer*, *\\_language\\_lexer*, *\\_needle=('Other',)*, *\\*\\*options*)¶\nThis lexer takes two lexer as arguments. A root lexer and\na language lexer. First everything is scanned using the language\nlexer, afterwards all `Other` tokens are lexed using the root\nlexer.\n\n\nThe lexers from the `template` lexer package use this base lexer.\n## Formatters¶\n\n\nA formatter is derived from this class:\n\n\n*class* pygments.formatter.Formatter(*\\*\\*options*)¶\nConverts a token stream to text.\n\n\nFormatters should have attributes to help selecting them. These\nare similar to the corresponding `Lexer`\nattributes.\n\n\nname¶\nFull name for the formatter, in human-readable form.\n\naliases¶\nA list of short, unique identifiers that"
    },
    {
        "instruction": "Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a CSV file.",
        "api": "Pygments",
        "output": "# !pip install Pygments\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import CsvFormatter\n\n# Read the code file\nwith open(\"code.py\", \"r\") as file:\n    code = file.read()\n\n# Determine the lexer based on the file extension\nlexer = get_lexer_by_name(\"python\")\n\n# Apply syntax highlighting to the code\nhighlighted_code = highlight(code, lexer, CsvFormatter())\n\n# Save the highlighted code to a CSV file\nwith open(\"highlighted_code.csv\", \"w\") as csv_file:\n    csv_file.write(highlighted_code)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# The full Pygments API¶\n\n\nThis page describes the Pygments API.\n\n## High-level API¶\n\n\nFunctions from the `pygments` module:\n\n\npygments.lex(*code*, *lexer*)¶\nLex code with the lexer (must be a Lexer instance)\nand return an iterable of tokens. Currently, this only calls\nlexer.get\\_tokens().\n\npygments.format(*tokens*, *formatter*, *outfile=None*)¶\nFormat `tokens` (an iterable of tokens) with the formatter `formatter`\n(a Formatter instance).\n\n\nIf `outfile`\n\n==================\n Document 1 \n----------------\n# High-level API¶\n\n\nFunctions from the `pygments` module:\n\n\npygments.lex(*code*, *lexer*)¶\nLex code with the lexer (must be a Lexer instance)\nand return an iterable of tokens. Currently, this only calls\nlexer.get\\_tokens().\n\npygments.format(*tokens*, *formatter*, *outfile=None*)¶\nFormat `tokens` (an iterable of tokens) with the formatter `formatter`\n(a Formatter instance).\n\n\nIf `outfile` is given and a valid file object (an object with a\n`write` method), the result will be written to it, otherwise it\nis returned as a string.\n\npygments.highlight(*code*, *lexer*, *formatter*, *outfile=None*)¶\nThis is the most high-level highlighting function. It combines lex and\nformat in one function.\n\nFunctions from `pygments.lexers`:\n\n\npygments.lexers.get\\_lexer\\_by\\_name(*\\_alias*, *\\*\\*options*)¶\nReturn an instance of a Lexer subclass that has alias in its\naliases list. The lexer is given the options at its\ninstantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if no lexer with that alias is\nfound.\n\npygments.lexers.get\\_lexer\\_for\\_filename(*\\_fn*, *code=None*, *\\*\\*options*)¶\nGet a lexer for a filename.\n\n\nReturn a Lexer subclass instance that has a filename pattern\nmatching fn. The lexer is given the options at its\ninstantiation.\n\n\nRaise `pygments.util.ClassNotFound` if no lexer for that filename\nis found.\n\n\nIf multiple lexers match the filename pattern, use their `analyse\\_text()`\nmethods to figure out which one is more appropriate.\n\npygments.lexers.get\\_lexer\\_for\\_mimetype(*\\_mime*, *\\*\\*options*)¶\nReturn a Lexer subclass instance that has mime in its mimetype\nlist. The lexer is given the options at its instantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if not lexer for that mimetype\nis found.\n\npygments.lexers.load\\_lexer\\_from\\_file(*filename*, *lexername='CustomLexer'*, *\\*\\*options*)¶\nLoad a lexer from a file.\n\n\nThis method expects a file located relative to the current working\ndirectory, which contains a Lexer class. By default, it expects the\nLexer to be name CustomLexer; you can specify your own class name\nas the second argument to this function.\n\n\nUsers should be very careful with the input, because this method\nis equivalent to running eval on the input file.\n\n\nRaises ClassNotFound if there are any problems importing the Lexer.\n\nNew in version 2.2.\n\n\npygments.lexers.guess\\_lexer(*\\_text*, *\\*\\*options*)¶\nReturn a Lexer subclass instance that’s guessed from the text in\ntext. For that, the `analyse\\_text()` method of every known lexer\nclass is called with the text as argument, and the lexer which returned the\nhighest value will be instantiated and returned.\n\n\n`pygments.util.ClassNotFound` is raised if no lexer thinks it can\nhandle the content.\n\npygments.lexers.guess\\_lexer\\_for\\_filename(*\\_fn*, *\\_text*, *\\*\\*options*)¶\nAs `guess\\_lexer()`, but only lexers which have a pattern in filenames\nor alias\\_filenames that matches filename are taken into consideration.\n\npygments.lexers.get\\_all\\_lexers(*plugins=True*)¶\nReturn a generator of tuples in the form `(name, aliases,\nfilenames, mimetypes)` of all know lexers.\n\n\nIf *plugins* is true (the default), plugin lexers supplied by entrypoints\nare also returned. Otherwise, only builtin ones are considered.\n\npygments.lexers.find\\_lexer\\_class\\_by\\_name(*\\_alias*)¶\nReturn the Lexer subclass that has alias in its aliases list, without\ninstantiating it.\n\n\nLike get\\_lexer\\_by\\_name, but does not instantiate the class.\n\n\npygments.lexers.find\\_lexer\\_class(*name*)¶\nReturn the Lexer subclass that with the *name* attribute as given by\nthe *name* argument.\n\nFunctions from `pygments.formatters`:\n\n\npygments.formatters.get\\_formatter\\_by\\_name(*\\_alias*, *\\*\\*options*)¶\nReturn an instance of a `Formatter` subclass that has alias in its\naliases list. The formatter is given the options at its instantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if no formatter with that\nalias is found.\n\npygments.formatters.get\\_formatter\\_for\\_filename(*fn*, *\\*\\*options*)¶\nReturn a `Formatter` subclass instance that has a filename pattern\nmatching fn. The formatter is given the options at its instantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if no formatter for that filename\nis found.\n\npygments.formatters.load\\_formatter\\_from\\_file(*filename*, *formattername='CustomFormatter'*, *\\*\\*options*)¶\nReturn a Formatter subclass instance loaded from the provided file, relative\nto the current directory.\n\n\nThe file is expected to contain a Formatter class named `formattername`\n(by default, CustomFormatter). Users should be very careful with the input, because\nthis method is equivalent to running `eval()` on the input file. The formatter is\ngiven the options at its instantiation.\n\n\n`pygments.util.ClassNotFound` is raised if there are any errors loading\nthe formatter.\n\n\nFunctions from `pygments.styles`:\n\n\npygments.styles.get\\_style\\_by\\_name(*name*)¶\nReturn a style class by its short name. The names of the builtin styles\nare listed in `pygments.styles.STYLE\\_MAP`.\n\n\nWill raise `pygments.util.ClassNotFound` if no style of that name is\nfound.\n\npygments.styles.get\\_all\\_styles()¶\nReturn a generator for all styles by name, both builtin and plugin.\n\npygments.styles.STYLE\\_MAP *= {'abap': 'abap::AbapStyle', 'algol': 'algol::AlgolStyle', 'algol\\_nu': 'algol\\_nu::Algol\\_NuStyle', 'arduino': 'arduino::ArduinoStyle', 'autumn': 'autumn::AutumnStyle', 'borland': 'borland::BorlandStyle', 'bw': 'bw::BlackWhiteStyle', 'colorful': 'colorful::ColorfulStyle', 'default': 'default::DefaultStyle', 'dracula': 'dracula::DraculaStyle', 'emacs': 'emacs::EmacsStyle', 'friendly': 'friendly::FriendlyStyle', 'friendly\\_grayscale': 'friendly\\_grayscale::FriendlyGrayscaleStyle', 'fruity': 'fruity::FruityStyle', 'github-dark': 'gh\\_dark::GhDarkStyle', 'gruvbox-dark': 'gruvbox::GruvboxDarkStyle', 'gruvbox-light': 'gruvbox::GruvboxLightStyle', 'igor': 'igor::IgorStyle', 'inkpot': 'inkpot::InkPotStyle', 'lightbulb': 'lightbulb::LightbulbStyle', 'lilypond': 'lilypond::LilyPondStyle', 'lovelace': 'lovelace::LovelaceStyle', 'manni': 'manni::ManniStyle', 'material': 'material::MaterialStyle', 'monokai': 'monokai::MonokaiStyle', 'murphy': 'murphy::MurphyStyle', 'native': 'native::NativeStyle', 'nord': 'nord::NordStyle', 'nord-darker': 'nord::NordDarkerStyle', 'one-dark': 'onedark::OneDarkStyle', 'paraiso-dark': 'paraiso\\_dark::ParaisoDarkStyle', 'paraiso-light': 'paraiso\\_light::ParaisoLightStyle', 'pastie': 'pastie::PastieStyle', 'perldoc': 'perldoc::PerldocStyle', 'rainbow\\_dash': 'rainbow\\_dash::RainbowDashStyle', 'rrt': 'rrt::RrtStyle', 'sas': 'sas::SasStyle', 'solarized-dark': 'solarized::SolarizedDarkStyle', 'solarized-light': 'solarized::SolarizedLightStyle', 'staroffice': 'staroffice::StarofficeStyle', 'stata-dark': 'stata\\_dark::StataDarkStyle', 'stata-light': 'stata\\_light::StataLightStyle', 'tango': 'tango::TangoStyle', 'trac': 'trac::TracStyle', 'vim': 'vim::VimStyle', 'vs': 'vs::VisualStudioStyle', 'xcode': 'xcode::XcodeStyle', 'zenburn': 'zenburn::ZenburnStyle'}*¶\nA dictionary of built-in styles, mapping style names to\n`'submodule::classname'` strings.\nThis list is deprecated. Use pygments.styles.STYLES instead\n## Lexers¶\n\n\nThe base lexer class from which all lexers are derived is:\n\n\n*class* pygments.lexer.Lexer(*\\*\\*options*)¶\nLexer for a specific language.\n\n\nSee also Write your own lexer, a high-level guide to writing\nlexers.\n\n\nLexer classes have attributes used for choosing the most appropriate\nlexer based on various criteria.\n\n\nname¶\nFull\n\n==================\n Document 2 \n----------------\n# Lexers¶\n\n\nThe base lexer class from which all lexers are derived is:\n\n\n*class* pygments.lexer.Lexer(*\\*\\*options*)¶\nLexer for a specific language.\n\n\nSee also Write your own lexer, a high-level guide to writing\nlexers.\n\n\nLexer classes have attributes used for choosing the most appropriate\nlexer based on various criteria.\n\n\nname¶\nFull name of the lexer, in human-readable form\n\naliases¶\nA list of short, unique identifiers that can be used to look\nup the lexer from a list, e.g., using get\\_lexer\\_by\\_name().\n\nfilenames¶\nA list of fnmatch patterns that match filenames which contain\ncontent for this lexer. The patterns in this list should be unique among\nall lexers.\n\nalias\\_filenames *= []*¶\nA list of fnmatch patterns that match filenames which may or may not\ncontain content for this lexer. This list is used by the\n`guess\\_lexer\\_for\\_filename()` function, to determine which lexers\nare then included in guessing the correct one. That means that\ne.g. every lexer for HTML and a template language should include\n`\\\\*.html` in this list.\n\nmimetypes¶\nA list of MIME types for content that can be lexed with this lexer.\n\npriority *= 0*¶\nPriority, should multiple lexers match and no content is provided\n\nLexers included in Pygments should have an additional attribute:\n\n\nurl¶\nURL of the language specification/definition. Used in the Pygments\ndocumentation.\n\nLexers included in Pygments may have additional attributes:\n\n\n\\_example¶\nExample file name. Relative to the `tests/examplefiles` directory.\nThis is used by the documentation generator to show an example.\n\nYou can pass options to the constructor. The basic options recognized\nby all lexers and processed by the base Lexer class are:\n\n`stripnl`Strip leading and trailing newlines from the input (default: True).\n\n`stripall`Strip all leading and trailing whitespace from the input\n(default: False).\n\n`ensurenl`Make sure that the input ends with a newline (default: True). This\nis required for some lexers that consume input linewise.\n\nNew in version 1.3.\n\n\n`tabsize`If given and greater than 0, expand tabs in the input (default: 0).\n\n`encoding`If given, must be an encoding name. This encoding will be used to\nconvert the input string to Unicode, if it is not already a Unicode\nstring (default: `'guess'`, which uses a simple UTF-8 / Locale /\nLatin1 detection. Can also be `'chardet'` to use the chardet\nlibrary, if it is installed.\n\n`inencoding`Overrides the `encoding` if given.\n\n\n\\_\\_init\\_\\_(*\\*\\*options*)¶\nThis constructor takes arbitrary options as keyword arguments.\nEvery subclass must first process its own options and then call\nthe Lexer constructor, since it processes the basic\noptions like stripnl.\n\n\nAn example looks like this:\n\n```\ndef \\_\\_init\\_\\_(self, \\*\\*options):\n    self.compress = options.get('compress', '')\n    Lexer.\\_\\_init\\_\\_(self, \\*\\*options)\n\n```\n\n\nAs these options must all be specifiable as strings (due to the\ncommand line usage), there are various utility functions\navailable to help with that, see Utilities.\n\nadd\\_filter(*filter\\_*, *\\*\\*options*)¶\nAdd a new stream filter to this lexer.\n\n*static* analyse\\_text(*text*)¶\nA static method which is called for lexer guessing.\n\n\nIt should analyse the text and return a float in the range\nfrom `0.0` to `1.0`. If it returns `0.0`, the lexer\nwill not be selected as the most probable one, if it returns\n`1.0`, it will be selected immediately. This is used by\nguess\\_lexer.\n\n\nThe LexerMeta metaclass automatically wraps this function so\nthat it works like a static method (no `self` or `cls`\nparameter) and the return value is automatically converted to\nfloat. If the return value is an object that is boolean False\nit’s the same as if the return values was `0.0`.\n\nget\\_tokens(*text*, *unfiltered=False*)¶\nThis method is the basic interface of a lexer. It is called by\nthe highlight() function. It must process the text and return an\niterable of `(tokentype, value)` pairs from text.\n\n\nNormally, you don’t need to override this method. The default\nimplementation processes the options recognized by all lexers\n(stripnl, stripall and so on), and then yields all tokens\nfrom get\\_tokens\\_unprocessed(), with the `index` dropped.\n\n\nIf unfiltered is set to True, the filtering mechanism is\nbypassed even if filters are defined.\n\nget\\_tokens\\_unprocessed(*text*)¶\nThis method should process the text and return an iterable of\n`(index, tokentype, value)` tuples where `index` is the starting\nposition of the token within the input text.\n\n\nIt must be overridden by subclasses. It is recommended to\nimplement it as a generator to maximize effectiveness.\n\n\nThere are several base class derived from `Lexer` you can use to build your lexer from:\n\n\n*class* pygments.lexer.RegexLexer(*\\*args*, *\\*\\*kwds*)¶\nBase for simple stateful regular expression-based lexers.\nSimplifies the lexing process so that you need only\nprovide a list of states and regular expressions.\n\n*class* pygments.lexer.ExtendedRegexLexer(*\\*args*, *\\*\\*kwds*)¶\nA RegexLexer that uses a context object to store its state.\n\n*class* pygments.lexer.DelegatingLexer(*\\_root\\_lexer*, *\\_language\\_lexer*, *\\_needle=('Other',)*, *\\*\\*options*)¶\nThis lexer takes two lexer as arguments. A root lexer and\na language lexer. First everything is scanned using the language\nlexer, afterwards all `Other` tokens are lexed using the root\nlexer.\n\n\nThe lexers from the `template` lexer package use this base lexer.\n## Formatters¶\n\n\nA formatter is derived from this class:\n\n\n*class* pygments.formatter.Formatter(*\\*\\*options*)¶\nConverts a token stream to text.\n\n\nFormatters should have attributes to help selecting them. These\nare similar to the corresponding `Lexer`\nattributes.\n\n\nname¶\nFull name for the formatter, in human-readable form.\n\naliases¶\nA list of short, unique identifiers that"
    },
    {
        "instruction": "Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a CSV file with a custom color scheme and line numbers.",
        "api": "Pygments",
        "output": "# !pip install Pygments\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import CsvFormatter\n\n# Read the code file\nwith open(\"code.py\", \"r\") as file:\n    code = file.read()\n\n# Determine the lexer based on the file extension\nlexer = get_lexer_by_name(\"python\")\n\n# Define a custom formatter with a custom color scheme and line numbers\nformatter = CsvFormatter(style=\"fruity\", linenos=True)\n\n# Apply syntax highlighting to the code with the custom color scheme and line numbers\nhighlighted_code = highlight(code, lexer, formatter)\n\n# Save the highlighted code to a CSV file with the custom color scheme and line numbers\nwith open(\"highlighted_code.csv\", \"w\") as csv_file:\n    csv_file.write(highlighted_code)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# The full Pygments API¶\n\n\nThis page describes the Pygments API.\n\n## High-level API¶\n\n\nFunctions from the `pygments` module:\n\n\npygments.lex(*code*, *lexer*)¶\nLex code with the lexer (must be a Lexer instance)\nand return an iterable of tokens. Currently, this only calls\nlexer.get\\_tokens().\n\npygments.format(*tokens*, *formatter*, *outfile=None*)¶\nFormat `tokens` (an iterable of tokens) with the formatter `formatter`\n(a Formatter instance).\n\n\nIf `outfile`\n\n==================\n Document 1 \n----------------\n# High-level API¶\n\n\nFunctions from the `pygments` module:\n\n\npygments.lex(*code*, *lexer*)¶\nLex code with the lexer (must be a Lexer instance)\nand return an iterable of tokens. Currently, this only calls\nlexer.get\\_tokens().\n\npygments.format(*tokens*, *formatter*, *outfile=None*)¶\nFormat `tokens` (an iterable of tokens) with the formatter `formatter`\n(a Formatter instance).\n\n\nIf `outfile` is given and a valid file object (an object with a\n`write` method), the result will be written to it, otherwise it\nis returned as a string.\n\npygments.highlight(*code*, *lexer*, *formatter*, *outfile=None*)¶\nThis is the most high-level highlighting function. It combines lex and\nformat in one function.\n\nFunctions from `pygments.lexers`:\n\n\npygments.lexers.get\\_lexer\\_by\\_name(*\\_alias*, *\\*\\*options*)¶\nReturn an instance of a Lexer subclass that has alias in its\naliases list. The lexer is given the options at its\ninstantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if no lexer with that alias is\nfound.\n\npygments.lexers.get\\_lexer\\_for\\_filename(*\\_fn*, *code=None*, *\\*\\*options*)¶\nGet a lexer for a filename.\n\n\nReturn a Lexer subclass instance that has a filename pattern\nmatching fn. The lexer is given the options at its\ninstantiation.\n\n\nRaise `pygments.util.ClassNotFound` if no lexer for that filename\nis found.\n\n\nIf multiple lexers match the filename pattern, use their `analyse\\_text()`\nmethods to figure out which one is more appropriate.\n\npygments.lexers.get\\_lexer\\_for\\_mimetype(*\\_mime*, *\\*\\*options*)¶\nReturn a Lexer subclass instance that has mime in its mimetype\nlist. The lexer is given the options at its instantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if not lexer for that mimetype\nis found.\n\npygments.lexers.load\\_lexer\\_from\\_file(*filename*, *lexername='CustomLexer'*, *\\*\\*options*)¶\nLoad a lexer from a file.\n\n\nThis method expects a file located relative to the current working\ndirectory, which contains a Lexer class. By default, it expects the\nLexer to be name CustomLexer; you can specify your own class name\nas the second argument to this function.\n\n\nUsers should be very careful with the input, because this method\nis equivalent to running eval on the input file.\n\n\nRaises ClassNotFound if there are any problems importing the Lexer.\n\nNew in version 2.2.\n\n\npygments.lexers.guess\\_lexer(*\\_text*, *\\*\\*options*)¶\nReturn a Lexer subclass instance that’s guessed from the text in\ntext. For that, the `analyse\\_text()` method of every known lexer\nclass is called with the text as argument, and the lexer which returned the\nhighest value will be instantiated and returned.\n\n\n`pygments.util.ClassNotFound` is raised if no lexer thinks it can\nhandle the content.\n\npygments.lexers.guess\\_lexer\\_for\\_filename(*\\_fn*, *\\_text*, *\\*\\*options*)¶\nAs `guess\\_lexer()`, but only lexers which have a pattern in filenames\nor alias\\_filenames that matches filename are taken into consideration.\n\npygments.lexers.get\\_all\\_lexers(*plugins=True*)¶\nReturn a generator of tuples in the form `(name, aliases,\nfilenames, mimetypes)` of all know lexers.\n\n\nIf *plugins* is true (the default), plugin lexers supplied by entrypoints\nare also returned. Otherwise, only builtin ones are considered.\n\npygments.lexers.find\\_lexer\\_class\\_by\\_name(*\\_alias*)¶\nReturn the Lexer subclass that has alias in its aliases list, without\ninstantiating it.\n\n\nLike get\\_lexer\\_by\\_name, but does not instantiate the class.\n\n\npygments.lexers.find\\_lexer\\_class(*name*)¶\nReturn the Lexer subclass that with the *name* attribute as given by\nthe *name* argument.\n\nFunctions from `pygments.formatters`:\n\n\npygments.formatters.get\\_formatter\\_by\\_name(*\\_alias*, *\\*\\*options*)¶\nReturn an instance of a `Formatter` subclass that has alias in its\naliases list. The formatter is given the options at its instantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if no formatter with that\nalias is found.\n\npygments.formatters.get\\_formatter\\_for\\_filename(*fn*, *\\*\\*options*)¶\nReturn a `Formatter` subclass instance that has a filename pattern\nmatching fn. The formatter is given the options at its instantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if no formatter for that filename\nis found.\n\npygments.formatters.load\\_formatter\\_from\\_file(*filename*, *formattername='CustomFormatter'*, *\\*\\*options*)¶\nReturn a Formatter subclass instance loaded from the provided file, relative\nto the current directory.\n\n\nThe file is expected to contain a Formatter class named `formattername`\n(by default, CustomFormatter). Users should be very careful with the input, because\nthis method is equivalent to running `eval()` on the input file. The formatter is\ngiven the options at its instantiation.\n\n\n`pygments.util.ClassNotFound` is raised if there are any errors loading\nthe formatter.\n\n\nFunctions from `pygments.styles`:\n\n\npygments.styles.get\\_style\\_by\\_name(*name*)¶\nReturn a style class by its short name. The names of the builtin styles\nare listed in `pygments.styles.STYLE\\_MAP`.\n\n\nWill raise `pygments.util.ClassNotFound` if no style of that name is\nfound.\n\npygments.styles.get\\_all\\_styles()¶\nReturn a generator for all styles by name, both builtin and plugin.\n\npygments.styles.STYLE\\_MAP *= {'abap': 'abap::AbapStyle', 'algol': 'algol::AlgolStyle', 'algol\\_nu': 'algol\\_nu::Algol\\_NuStyle', 'arduino': 'arduino::ArduinoStyle', 'autumn': 'autumn::AutumnStyle', 'borland': 'borland::BorlandStyle', 'bw': 'bw::BlackWhiteStyle', 'colorful': 'colorful::ColorfulStyle', 'default': 'default::DefaultStyle', 'dracula': 'dracula::DraculaStyle', 'emacs': 'emacs::EmacsStyle', 'friendly': 'friendly::FriendlyStyle', 'friendly\\_grayscale': 'friendly\\_grayscale::FriendlyGrayscaleStyle', 'fruity': 'fruity::FruityStyle', 'github-dark': 'gh\\_dark::GhDarkStyle', 'gruvbox-dark': 'gruvbox::GruvboxDarkStyle', 'gruvbox-light': 'gruvbox::GruvboxLightStyle', 'igor': 'igor::IgorStyle', 'inkpot': 'inkpot::InkPotStyle', 'lightbulb': 'lightbulb::LightbulbStyle', 'lilypond': 'lilypond::LilyPondStyle', 'lovelace': 'lovelace::LovelaceStyle', 'manni': 'manni::ManniStyle', 'material': 'material::MaterialStyle', 'monokai': 'monokai::MonokaiStyle', 'murphy': 'murphy::MurphyStyle', 'native': 'native::NativeStyle', 'nord': 'nord::NordStyle', 'nord-darker': 'nord::NordDarkerStyle', 'one-dark': 'onedark::OneDarkStyle', 'paraiso-dark': 'paraiso\\_dark::ParaisoDarkStyle', 'paraiso-light': 'paraiso\\_light::ParaisoLightStyle', 'pastie': 'pastie::PastieStyle', 'perldoc': 'perldoc::PerldocStyle', 'rainbow\\_dash': 'rainbow\\_dash::RainbowDashStyle', 'rrt': 'rrt::RrtStyle', 'sas': 'sas::SasStyle', 'solarized-dark': 'solarized::SolarizedDarkStyle', 'solarized-light': 'solarized::SolarizedLightStyle', 'staroffice': 'staroffice::StarofficeStyle', 'stata-dark': 'stata\\_dark::StataDarkStyle', 'stata-light': 'stata\\_light::StataLightStyle', 'tango': 'tango::TangoStyle', 'trac': 'trac::TracStyle', 'vim': 'vim::VimStyle', 'vs': 'vs::VisualStudioStyle', 'xcode': 'xcode::XcodeStyle', 'zenburn': 'zenburn::ZenburnStyle'}*¶\nA dictionary of built-in styles, mapping style names to\n`'submodule::classname'` strings.\nThis list is deprecated. Use pygments.styles.STYLES instead\n## Lexers¶\n\n\nThe base lexer class from which all lexers are derived is:\n\n\n*class* pygments.lexer.Lexer(*\\*\\*options*)¶\nLexer for a specific language.\n\n\nSee also Write your own lexer, a high-level guide to writing\nlexers.\n\n\nLexer classes have attributes used for choosing the most appropriate\nlexer based on various criteria.\n\n\nname¶\nFull\n\n==================\n Document 2 \n----------------\n# Lexers¶\n\n\nThe base lexer class from which all lexers are derived is:\n\n\n*class* pygments.lexer.Lexer(*\\*\\*options*)¶\nLexer for a specific language.\n\n\nSee also Write your own lexer, a high-level guide to writing\nlexers.\n\n\nLexer classes have attributes used for choosing the most appropriate\nlexer based on various criteria.\n\n\nname¶\nFull name of the lexer, in human-readable form\n\naliases¶\nA list of short, unique identifiers that can be used to look\nup the lexer from a list, e.g., using get\\_lexer\\_by\\_name().\n\nfilenames¶\nA list of fnmatch patterns that match filenames which contain\ncontent for this lexer. The patterns in this list should be unique among\nall lexers.\n\nalias\\_filenames *= []*¶\nA list of fnmatch patterns that match filenames which may or may not\ncontain content for this lexer. This list is used by the\n`guess\\_lexer\\_for\\_filename()` function, to determine which lexers\nare then included in guessing the correct one. That means that\ne.g. every lexer for HTML and a template language should include\n`\\\\*.html` in this list.\n\nmimetypes¶\nA list of MIME types for content that can be lexed with this lexer.\n\npriority *= 0*¶\nPriority, should multiple lexers match and no content is provided\n\nLexers included in Pygments should have an additional attribute:\n\n\nurl¶\nURL of the language specification/definition. Used in the Pygments\ndocumentation.\n\nLexers included in Pygments may have additional attributes:\n\n\n\\_example¶\nExample file name. Relative to the `tests/examplefiles` directory.\nThis is used by the documentation generator to show an example.\n\nYou can pass options to the constructor. The basic options recognized\nby all lexers and processed by the base Lexer class are:\n\n`stripnl`Strip leading and trailing newlines from the input (default: True).\n\n`stripall`Strip all leading and trailing whitespace from the input\n(default: False).\n\n`ensurenl`Make sure that the input ends with a newline (default: True). This\nis required for some lexers that consume input linewise.\n\nNew in version 1.3.\n\n\n`tabsize`If given and greater than 0, expand tabs in the input (default: 0).\n\n`encoding`If given, must be an encoding name. This encoding will be used to\nconvert the input string to Unicode, if it is not already a Unicode\nstring (default: `'guess'`, which uses a simple UTF-8 / Locale /\nLatin1 detection. Can also be `'chardet'` to use the chardet\nlibrary, if it is installed.\n\n`inencoding`Overrides the `encoding` if given.\n\n\n\\_\\_init\\_\\_(*\\*\\*options*)¶\nThis constructor takes arbitrary options as keyword arguments.\nEvery subclass must first process its own options and then call\nthe Lexer constructor, since it processes the basic\noptions like stripnl.\n\n\nAn example looks like this:\n\n```\ndef \\_\\_init\\_\\_(self, \\*\\*options):\n    self.compress = options.get('compress', '')\n    Lexer.\\_\\_init\\_\\_(self, \\*\\*options)\n\n```\n\n\nAs these options must all be specifiable as strings (due to the\ncommand line usage), there are various utility functions\navailable to help with that, see Utilities.\n\nadd\\_filter(*filter\\_*, *\\*\\*options*)¶\nAdd a new stream filter to this lexer.\n\n*static* analyse\\_text(*text*)¶\nA static method which is called for lexer guessing.\n\n\nIt should analyse the text and return a float in the range\nfrom `0.0` to `1.0`. If it returns `0.0`, the lexer\nwill not be selected as the most probable one, if it returns\n`1.0`, it will be selected immediately. This is used by\nguess\\_lexer.\n\n\nThe LexerMeta metaclass automatically wraps this function so\nthat it works like a static method (no `self` or `cls`\nparameter) and the return value is automatically converted to\nfloat. If the return value is an object that is boolean False\nit’s the same as if the return values was `0.0`.\n\nget\\_tokens(*text*, *unfiltered=False*)¶\nThis method is the basic interface of a lexer. It is called by\nthe highlight() function. It must process the text and return an\niterable of `(tokentype, value)` pairs from text.\n\n\nNormally, you don’t need to override this method. The default\nimplementation processes the options recognized by all lexers\n(stripnl, stripall and so on), and then yields all tokens\nfrom get\\_tokens\\_unprocessed(), with the `index` dropped.\n\n\nIf unfiltered is set to True, the filtering mechanism is\nbypassed even if filters are defined.\n\nget\\_tokens\\_unprocessed(*text*)¶\nThis method should process the text and return an iterable of\n`(index, tokentype, value)` tuples where `index` is the starting\nposition of the token within the input text.\n\n\nIt must be overridden by subclasses. It is recommended to\nimplement it as a generator to maximize effectiveness.\n\n\nThere are several base class derived from `Lexer` you can use to build your lexer from:\n\n\n*class* pygments.lexer.RegexLexer(*\\*args*, *\\*\\*kwds*)¶\nBase for simple stateful regular expression-based lexers.\nSimplifies the lexing process so that you need only\nprovide a list of states and regular expressions.\n\n*class* pygments.lexer.ExtendedRegexLexer(*\\*args*, *\\*\\*kwds*)¶\nA RegexLexer that uses a context object to store its state.\n\n*class* pygments.lexer.DelegatingLexer(*\\_root\\_lexer*, *\\_language\\_lexer*, *\\_needle=('Other',)*, *\\*\\*options*)¶\nThis lexer takes two lexer as arguments. A root lexer and\na language lexer. First everything is scanned using the language\nlexer, afterwards all `Other` tokens are lexed using the root\nlexer.\n\n\nThe lexers from the `template` lexer package use this base lexer.\n## Formatters¶\n\n\nA formatter is derived from this class:\n\n\n*class* pygments.formatter.Formatter(*\\*\\*options*)¶\nConverts a token stream to text.\n\n\nFormatters should have attributes to help selecting them. These\nare similar to the corresponding `Lexer`\nattributes.\n\n\nname¶\nFull name for the formatter, in human-readable form.\n\naliases¶\nA list of short, unique identifiers that"
    },
    {
        "instruction": "Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a RTF file.",
        "api": "Pygments",
        "output": "# !pip install Pygments\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import RtfFormatter\n\n# Read the code file\nwith open(\"code.py\", \"r\") as file:\n    code = file.read()\n\n# Determine the lexer based on the file extension\nlexer = get_lexer_by_name(\"python\")\n\n# Apply syntax highlighting to the code\nhighlighted_code = highlight(code, lexer, RtfFormatter())\n\n# Save the highlighted code to a RTF file\nwith open(\"highlighted_code.rtf\", \"w\") as rtf_file:\n    rtf_file.write(highlighted_code)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# The full Pygments API¶\n\n\nThis page describes the Pygments API.\n\n## High-level API¶\n\n\nFunctions from the `pygments` module:\n\n\npygments.lex(*code*, *lexer*)¶\nLex code with the lexer (must be a Lexer instance)\nand return an iterable of tokens. Currently, this only calls\nlexer.get\\_tokens().\n\npygments.format(*tokens*, *formatter*, *outfile=None*)¶\nFormat `tokens` (an iterable of tokens) with the formatter `formatter`\n(a Formatter instance).\n\n\nIf `outfile`\n\n==================\n Document 1 \n----------------\n# High-level API¶\n\n\nFunctions from the `pygments` module:\n\n\npygments.lex(*code*, *lexer*)¶\nLex code with the lexer (must be a Lexer instance)\nand return an iterable of tokens. Currently, this only calls\nlexer.get\\_tokens().\n\npygments.format(*tokens*, *formatter*, *outfile=None*)¶\nFormat `tokens` (an iterable of tokens) with the formatter `formatter`\n(a Formatter instance).\n\n\nIf `outfile` is given and a valid file object (an object with a\n`write` method), the result will be written to it, otherwise it\nis returned as a string.\n\npygments.highlight(*code*, *lexer*, *formatter*, *outfile=None*)¶\nThis is the most high-level highlighting function. It combines lex and\nformat in one function.\n\nFunctions from `pygments.lexers`:\n\n\npygments.lexers.get\\_lexer\\_by\\_name(*\\_alias*, *\\*\\*options*)¶\nReturn an instance of a Lexer subclass that has alias in its\naliases list. The lexer is given the options at its\ninstantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if no lexer with that alias is\nfound.\n\npygments.lexers.get\\_lexer\\_for\\_filename(*\\_fn*, *code=None*, *\\*\\*options*)¶\nGet a lexer for a filename.\n\n\nReturn a Lexer subclass instance that has a filename pattern\nmatching fn. The lexer is given the options at its\ninstantiation.\n\n\nRaise `pygments.util.ClassNotFound` if no lexer for that filename\nis found.\n\n\nIf multiple lexers match the filename pattern, use their `analyse\\_text()`\nmethods to figure out which one is more appropriate.\n\npygments.lexers.get\\_lexer\\_for\\_mimetype(*\\_mime*, *\\*\\*options*)¶\nReturn a Lexer subclass instance that has mime in its mimetype\nlist. The lexer is given the options at its instantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if not lexer for that mimetype\nis found.\n\npygments.lexers.load\\_lexer\\_from\\_file(*filename*, *lexername='CustomLexer'*, *\\*\\*options*)¶\nLoad a lexer from a file.\n\n\nThis method expects a file located relative to the current working\ndirectory, which contains a Lexer class. By default, it expects the\nLexer to be name CustomLexer; you can specify your own class name\nas the second argument to this function.\n\n\nUsers should be very careful with the input, because this method\nis equivalent to running eval on the input file.\n\n\nRaises ClassNotFound if there are any problems importing the Lexer.\n\nNew in version 2.2.\n\n\npygments.lexers.guess\\_lexer(*\\_text*, *\\*\\*options*)¶\nReturn a Lexer subclass instance that’s guessed from the text in\ntext. For that, the `analyse\\_text()` method of every known lexer\nclass is called with the text as argument, and the lexer which returned the\nhighest value will be instantiated and returned.\n\n\n`pygments.util.ClassNotFound` is raised if no lexer thinks it can\nhandle the content.\n\npygments.lexers.guess\\_lexer\\_for\\_filename(*\\_fn*, *\\_text*, *\\*\\*options*)¶\nAs `guess\\_lexer()`, but only lexers which have a pattern in filenames\nor alias\\_filenames that matches filename are taken into consideration.\n\npygments.lexers.get\\_all\\_lexers(*plugins=True*)¶\nReturn a generator of tuples in the form `(name, aliases,\nfilenames, mimetypes)` of all know lexers.\n\n\nIf *plugins* is true (the default), plugin lexers supplied by entrypoints\nare also returned. Otherwise, only builtin ones are considered.\n\npygments.lexers.find\\_lexer\\_class\\_by\\_name(*\\_alias*)¶\nReturn the Lexer subclass that has alias in its aliases list, without\ninstantiating it.\n\n\nLike get\\_lexer\\_by\\_name, but does not instantiate the class.\n\n\npygments.lexers.find\\_lexer\\_class(*name*)¶\nReturn the Lexer subclass that with the *name* attribute as given by\nthe *name* argument.\n\nFunctions from `pygments.formatters`:\n\n\npygments.formatters.get\\_formatter\\_by\\_name(*\\_alias*, *\\*\\*options*)¶\nReturn an instance of a `Formatter` subclass that has alias in its\naliases list. The formatter is given the options at its instantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if no formatter with that\nalias is found.\n\npygments.formatters.get\\_formatter\\_for\\_filename(*fn*, *\\*\\*options*)¶\nReturn a `Formatter` subclass instance that has a filename pattern\nmatching fn. The formatter is given the options at its instantiation.\n\n\nWill raise `pygments.util.ClassNotFound` if no formatter for that filename\nis found.\n\npygments.formatters.load\\_formatter\\_from\\_file(*filename*, *formattername='CustomFormatter'*, *\\*\\*options*)¶\nReturn a Formatter subclass instance loaded from the provided file, relative\nto the current directory.\n\n\nThe file is expected to contain a Formatter class named `formattername`\n(by default, CustomFormatter). Users should be very careful with the input, because\nthis method is equivalent to running `eval()` on the input file. The formatter is\ngiven the options at its instantiation.\n\n\n`pygments.util.ClassNotFound` is raised if there are any errors loading\nthe formatter.\n\n\nFunctions from `pygments.styles`:\n\n\npygments.styles.get\\_style\\_by\\_name(*name*)¶\nReturn a style class by its short name. The names of the builtin styles\nare listed in `pygments.styles.STYLE\\_MAP`.\n\n\nWill raise `pygments.util.ClassNotFound` if no style of that name is\nfound.\n\npygments.styles.get\\_all\\_styles()¶\nReturn a generator for all styles by name, both builtin and plugin.\n\npygments.styles.STYLE\\_MAP *= {'abap': 'abap::AbapStyle', 'algol': 'algol::AlgolStyle', 'algol\\_nu': 'algol\\_nu::Algol\\_NuStyle', 'arduino': 'arduino::ArduinoStyle', 'autumn': 'autumn::AutumnStyle', 'borland': 'borland::BorlandStyle', 'bw': 'bw::BlackWhiteStyle', 'colorful': 'colorful::ColorfulStyle', 'default': 'default::DefaultStyle', 'dracula': 'dracula::DraculaStyle', 'emacs': 'emacs::EmacsStyle', 'friendly': 'friendly::FriendlyStyle', 'friendly\\_grayscale': 'friendly\\_grayscale::FriendlyGrayscaleStyle', 'fruity': 'fruity::FruityStyle', 'github-dark': 'gh\\_dark::GhDarkStyle', 'gruvbox-dark': 'gruvbox::GruvboxDarkStyle', 'gruvbox-light': 'gruvbox::GruvboxLightStyle', 'igor': 'igor::IgorStyle', 'inkpot': 'inkpot::InkPotStyle', 'lightbulb': 'lightbulb::LightbulbStyle', 'lilypond': 'lilypond::LilyPondStyle', 'lovelace': 'lovelace::LovelaceStyle', 'manni': 'manni::ManniStyle', 'material': 'material::MaterialStyle', 'monokai': 'monokai::MonokaiStyle', 'murphy': 'murphy::MurphyStyle', 'native': 'native::NativeStyle', 'nord': 'nord::NordStyle', 'nord-darker': 'nord::NordDarkerStyle', 'one-dark': 'onedark::OneDarkStyle', 'paraiso-dark': 'paraiso\\_dark::ParaisoDarkStyle', 'paraiso-light': 'paraiso\\_light::ParaisoLightStyle', 'pastie': 'pastie::PastieStyle', 'perldoc': 'perldoc::PerldocStyle', 'rainbow\\_dash': 'rainbow\\_dash::RainbowDashStyle', 'rrt': 'rrt::RrtStyle', 'sas': 'sas::SasStyle', 'solarized-dark': 'solarized::SolarizedDarkStyle', 'solarized-light': 'solarized::SolarizedLightStyle', 'staroffice': 'staroffice::StarofficeStyle', 'stata-dark': 'stata\\_dark::StataDarkStyle', 'stata-light': 'stata\\_light::StataLightStyle', 'tango': 'tango::TangoStyle', 'trac': 'trac::TracStyle', 'vim': 'vim::VimStyle', 'vs': 'vs::VisualStudioStyle', 'xcode': 'xcode::XcodeStyle', 'zenburn': 'zenburn::ZenburnStyle'}*¶\nA dictionary of built-in styles, mapping style names to\n`'submodule::classname'` strings.\nThis list is deprecated. Use pygments.styles.STYLES instead\n## Lexers¶\n\n\nThe base lexer class from which all lexers are derived is:\n\n\n*class* pygments.lexer.Lexer(*\\*\\*options*)¶\nLexer for a specific language.\n\n\nSee also Write your own lexer, a high-level guide to writing\nlexers.\n\n\nLexer classes have attributes used for choosing the most appropriate\nlexer based on various criteria.\n\n\nname¶\nFull\n\n==================\n Document 2 \n----------------\n# Lexers¶\n\n\nThe base lexer class from which all lexers are derived is:\n\n\n*class* pygments.lexer.Lexer(*\\*\\*options*)¶\nLexer for a specific language.\n\n\nSee also Write your own lexer, a high-level guide to writing\nlexers.\n\n\nLexer classes have attributes used for choosing the most appropriate\nlexer based on various criteria.\n\n\nname¶\nFull name of the lexer, in human-readable form\n\naliases¶\nA list of short, unique identifiers that can be used to look\nup the lexer from a list, e.g., using get\\_lexer\\_by\\_name().\n\nfilenames¶\nA list of fnmatch patterns that match filenames which contain\ncontent for this lexer. The patterns in this list should be unique among\nall lexers.\n\nalias\\_filenames *= []*¶\nA list of fnmatch patterns that match filenames which may or may not\ncontain content for this lexer. This list is used by the\n`guess\\_lexer\\_for\\_filename()` function, to determine which lexers\nare then included in guessing the correct one. That means that\ne.g. every lexer for HTML and a template language should include\n`\\\\*.html` in this list.\n\nmimetypes¶\nA list of MIME types for content that can be lexed with this lexer.\n\npriority *= 0*¶\nPriority, should multiple lexers match and no content is provided\n\nLexers included in Pygments should have an additional attribute:\n\n\nurl¶\nURL of the language specification/definition. Used in the Pygments\ndocumentation.\n\nLexers included in Pygments may have additional attributes:\n\n\n\\_example¶\nExample file name. Relative to the `tests/examplefiles` directory.\nThis is used by the documentation generator to show an example.\n\nYou can pass options to the constructor. The basic options recognized\nby all lexers and processed by the base Lexer class are:\n\n`stripnl`Strip leading and trailing newlines from the input (default: True).\n\n`stripall`Strip all leading and trailing whitespace from the input\n(default: False).\n\n`ensurenl`Make sure that the input ends with a newline (default: True). This\nis required for some lexers that consume input linewise.\n\nNew in version 1.3.\n\n\n`tabsize`If given and greater than 0, expand tabs in the input (default: 0).\n\n`encoding`If given, must be an encoding name. This encoding will be used to\nconvert the input string to Unicode, if it is not already a Unicode\nstring (default: `'guess'`, which uses a simple UTF-8 / Locale /\nLatin1 detection. Can also be `'chardet'` to use the chardet\nlibrary, if it is installed.\n\n`inencoding`Overrides the `encoding` if given.\n\n\n\\_\\_init\\_\\_(*\\*\\*options*)¶\nThis constructor takes arbitrary options as keyword arguments.\nEvery subclass must first process its own options and then call\nthe Lexer constructor, since it processes the basic\noptions like stripnl.\n\n\nAn example looks like this:\n\n```\ndef \\_\\_init\\_\\_(self, \\*\\*options):\n    self.compress = options.get('compress', '')\n    Lexer.\\_\\_init\\_\\_(self, \\*\\*options)\n\n```\n\n\nAs these options must all be specifiable as strings (due to the\ncommand line usage), there are various utility functions\navailable to help with that, see Utilities.\n\nadd\\_filter(*filter\\_*, *\\*\\*options*)¶\nAdd a new stream filter to this lexer.\n\n*static* analyse\\_text(*text*)¶\nA static method which is called for lexer guessing.\n\n\nIt should analyse the text and return a float in the range\nfrom `0.0` to `1.0`. If it returns `0.0`, the lexer\nwill not be selected as the most probable one, if it returns\n`1.0`, it will be selected immediately. This is used by\nguess\\_lexer.\n\n\nThe LexerMeta metaclass automatically wraps this function so\nthat it works like a static method (no `self` or `cls`\nparameter) and the return value is automatically converted to\nfloat. If the return value is an object that is boolean False\nit’s the same as if the return values was `0.0`.\n\nget\\_tokens(*text*, *unfiltered=False*)¶\nThis method is the basic interface of a lexer. It is called by\nthe highlight() function. It must process the text and return an\niterable of `(tokentype, value)` pairs from text.\n\n\nNormally, you don’t need to override this method. The default\nimplementation processes the options recognized by all lexers\n(stripnl, stripall and so on), and then yields all tokens\nfrom get\\_tokens\\_unprocessed(), with the `index` dropped.\n\n\nIf unfiltered is set to True, the filtering mechanism is\nbypassed even if filters are defined.\n\nget\\_tokens\\_unprocessed(*text*)¶\nThis method should process the text and return an iterable of\n`(index, tokentype, value)` tuples where `index` is the starting\nposition of the token within the input text.\n\n\nIt must be overridden by subclasses. It is recommended to\nimplement it as a generator to maximize effectiveness.\n\n\nThere are several base class derived from `Lexer` you can use to build your lexer from:\n\n\n*class* pygments.lexer.RegexLexer(*\\*args*, *\\*\\*kwds*)¶\nBase for simple stateful regular expression-based lexers.\nSimplifies the lexing process so that you need only\nprovide a list of states and regular expressions.\n\n*class* pygments.lexer.ExtendedRegexLexer(*\\*args*, *\\*\\*kwds*)¶\nA RegexLexer that uses a context object to store its state.\n\n*class* pygments.lexer.DelegatingLexer(*\\_root\\_lexer*, *\\_language\\_lexer*, *\\_needle=('Other',)*, *\\*\\*options*)¶\nThis lexer takes two lexer as arguments. A root lexer and\na language lexer. First everything is scanned using the language\nlexer, afterwards all `Other` tokens are lexed using the root\nlexer.\n\n\nThe lexers from the `template` lexer package use this base lexer.\n## Formatters¶\n\n\nA formatter is derived from this class:\n\n\n*class* pygments.formatter.Formatter(*\\*\\*options*)¶\nConverts a token stream to text.\n\n\nFormatters should have attributes to help selecting them. These\nare similar to the corresponding `Lexer`\nattributes.\n\n\nname¶\nFull name for the formatter, in human-readable form.\n\naliases¶\nA list of short, unique identifiers that"
    },
    {
        "instruction": "Create a Python program that uses the 'SQLAlchemy' API to connect to a MySQL database. Define a 'Employee' entity with attributes like 'id,' 'first_name,' 'last_name,' 'position,' and 'salary.' The program should establish a connection to the MySQL database, create the 'employees' table and add a new employee. Then, retrieve employee details from the database.",
        "api": "SQLAlchemy",
        "output": "#!pip install SQLAlchemy\nfrom sqlalchemy import create_engine, Column, Integer, String, Float\nfrom sqlalchemy.orm import sessionmaker, declarative_base\n\n# Create a MySQL database connection\nengine = create_engine('mysql://username:password@localhost/mydatabase')\n\n# Define the Employee class to represent the \"Employee\" entity\nBase = declarative_base()\n\nclass Employee(Base):\n    __tablename__ = 'employees'\n    id = Column(Integer, primary_key=True)\n    first_name = Column(String)\n    last_name = Column(String)\n    position = Column(String)\n    salary = Column(Float)\n\n# Create the table in the database\nBase.metadata.create_all(engine)\n\n# Function to add a new employee to the database\ndef add_employee(first_name, last_name, position, salary):\n    Session = sessionmaker(bind=engine)\n    session = Session()\n\n    employee = Employee(first_name=first_name, last_name=last_name, position=position, salary=salary)\n    session.add(employee)\n    session.commit()\n    session.close()\n\nadd_employee(\"John\", \"Smith\", \"Manager\", 60000.00)\n\nSession = sessionmaker(bind=engine)\nsession = Session()\nemployee = session.query(Employee).filter_by(first_name=\"John\").first()\nsession.close()\n\nprint(\"Employee data: \")\nprint(employee.first_name)\nprint(employee.last_name)\nprint(employee.position)\nprint(employee.salary)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n# 2.0 Migration - ORM Usage¶\n\n\nThe biggest visible change in SQLAlchemy 2.0 is the use of\n`Session.execute()` in conjunction with `select()` to run ORM\nqueries, instead of using `Session.query()`. As mentioned elsewhere,\nthere is no plan to actually remove the `Session.query()` API itself,\nas it is now implemented by using the new API internally it will remain as a\nlegacy API, and both APIs can be used freely.\n\n\nThe table below provides an introduction to the general change in\ncalling form with links to documentation for each technique\npresented. The individual migration notes are in the embedded sections\nfollowing the table, and may include additional notes not summarized here.\n\n**Overview of Major ORM Querying Patterns**¶| 1.x style form | 2.0 style form | See Also |\n| --- | --- | --- |\n| \n```\nsession.query(User).get(42)\n```\n\n | \n```\nsession.get(User, 42)\n```\n\n | ORM Query - get() method moves to Session |\n| \n```\nsession.query(User).all()\n```\n\n | \n```\nsession.execute(\n  select(User)\n).scalars().all()\n# or\n\nsession.scalars(\n  select(User)\n).all()\n```\n\n | ORM Query Unified with Core Select\n`Session.scalars()`\n`Result.scalars()` |\n| \n```\nsession.query(User).\\\n  filter\\_by(name=\"some user\").\\\n  one()\n```\n\n | \n```\nsession.execute(\n  select(User).\n  filter\\_by(name=\"some user\")\n).scalar\\_one()\n```\n\n | ORM Query Unified with Core Select\n`Result.scalar\\_one()` |\n| \n```\nsession.query(User).\\\n  filter\\_by(name=\"some user\").\\\n  first()\n```\n\n |\n\n==================\n Document 1 \n----------------\nSQLAlchemy Documentation¶\n\n\nGetting Started\n\n\nNew to SQLAlchemy? Start here:\n\n\n* **For Python Beginners:** Installation Guide - basic guidance on installing with pip and similar\n* **For Python Veterans:** SQLAlchemy Overview - brief architectural overview\n\n\nTutorials\n\n\nNew users of SQLAlchemy, as well as veterans of older SQLAlchemy\nrelease series, should start with the\nSQLAlchemy Unified Tutorial, which covers everything an Alchemist needs\nto know when using the ORM or just Core.\n\n\n* **For a quick glance:** ORM Quick Start - a glimpse at what working with the ORM looks like\n* **For all users:** SQLAlchemy Unified Tutorial - In depth tutorial for Core and ORM\n\n\nMigration Notes\n\n\nUsers coming from older versions of SQLAlchemy, especially those transitioning\nfrom the 1.x style of working, will want to review this documentation.\n\n\n* Migrating to SQLAlchemy 2.0 - Complete background on migrating from 1.3 or 1.4 to 2.0\n* What’s New in SQLAlchemy 2.0? - New 2.0 features and behaviors beyond the 1.x migration\n* Changelog catalog - Detailed changelogs for all SQLAlchemy Versions\n\n\nReference and How To\n\n\n**SQLAlchemy ORM** - Detailed guides and API reference for using the ORM\n\n\n* **Mapping Classes:**\nMapping Python Classes |\nRelationship Configuration\n* **Using the ORM:**\nUsing the ORM Session |\nORM Querying Guide |\nUsing AsyncIO\n* **Configuration Extensions:**\nAssociation Proxy |\nHybrid Attributes |\nMutable Scalars |\nAutomap |\nAll extensions\n* **Extending the ORM:**\nORM Events and Internals\n* **Other:**\nIntroduction to Examples\n\n\n**SQLAlchemy Core** - Detailed guides and API reference for working with Core\n\n\n* **Engines, Connections, Pools:**\nEngine Configuration |\nConnections, Transactions, Results |\nAsyncIO Support |\nConnection Pooling\n* **Schema Definition:**\nOverview |\nTables and Columns |\nDatabase Introspection (Reflection) |\nInsert/Update Defaults |\nConstraints and Indexes |\nUsing Data Definition Language (DDL)\n* **SQL Statements:**\nSQL Expression Elements |\nOperator Reference |\nSELECT and related constructs |\nINSERT, UPDATE, DELETE |\nSQL Functions |\nTable of Contents\n* **Datatypes:**\nOverview |\nBuilding Custom Types |\nType API Reference\n* **Core Basics:**\nOverview |\nRuntime Inspection API |\nEvent System |\nCore Event Interfaces |\nCreating Custom SQL Constructs\n\n\nDialect Documentation\n\n\nThe **dialect** is the system SQLAlchemy uses to communicate with various types of DBAPIs and databases.\nThis section describes notes, options, and usage patterns regarding individual dialects.\n\n\nPostgreSQL |\nMySQL |\nSQLite |\nOracle |\nMicrosoft SQL Server\n\n\nMore Dialects …\n\n\nSupplementary\n\n\n* Frequently Asked Questions - A collection of common problems and solutions\n* Glossary - Terms used in SQLAlchemy’s documentation\n* Error Message Guide - Explainations of many SQLAlchemy Errors\n* Complete table of of contents\n* Index\n\n Documentation last generated: Sun 01 Oct 2023 10:33:57 AM \n\n\nThe new approach makes use of the `aliased()` construct so that the\nORM internals don’t need to guess which entities and columns should be adapted\nand in what way; in the example above, the `ua` and `aa` objects, both\nof which are `AliasedClass` instances, provide to the internals\nan unambiguous marker as to where the subquery should be referred towards\nas well as what entity column or relationship is being considered for a given\ncomponent of the query.\n\n Documentation last generated: Tue 19 Sep 2023 04:03:48 PM \n\n Documentation last generated: Tue 19 Sep 2023 04:04:37 PM\n\n==================\n Document 2 \n----------------\n## First Prerequisite, step two - A Working 1.4 Application¶\n\n\nOnce the application is good to go on SQLAlchemy 1.3, the next step is to get\nit running on SQLAlchemy 1.4. In the vast majority of cases, applications\nshould run without problems from SQLAlchemy 1.3 to 1.4. However, it’s always\nthe case between any 1.x and 1.y release, APIs and behaviors have changed\neither subtly or in some cases a little less subtly, and the SQLAlchemy\nproject always gets a good deal of regression reports for the first few\nmonths.\n\n\nThe 1.x->1.y release process usually has a few changes around the margins\nthat are a little bit more dramatic and are based around use cases that are\nexpected to be very seldom if at all used. For 1.4, the changes identified\nas being in this realm are as follows:\n\n\n* The URL object is now immutable - this impacts code that would be manipulating the\n`URL` object and may impact code that makes use of the\n`CreateEnginePlugin` extension point. This is an uncommon\ncase but may affect in particular some test suites that are making use of\nspecial database provisioning logic. A github search for code that uses\nthe relatively new and little-known `CreateEnginePlugin`\nclass found two projects that were unaffected by the change.\n* A SELECT statement is no longer implicitly considered to be a FROM clause - this change may impact code that was somehow relying\nupon behavior that was mostly unusable in the `Select` construct,\nwhere it would create unnamed subqueries that were usually confusing and\nnon-working. These subqueries would be rejected by most databases in any\ncase as a name is usually required except on SQLite, however it is possible\nsome applications will need to adjust some queries that are inadvertently\nrelying upon this.\n* select().join() and outerjoin() add JOIN criteria to the current query, rather than creating a subquery - somewhat related, the `Select` class\nfeatured `.join()` and `.outerjoin()` methods that implicitly created a\nsubquery and then returned a `Join` construct, which again would\nbe mostly useless and produced lots of confusion. The decision was made to\nmove forward with the vastly more useful 2.0-style join-building approach\nwhere these methods now work the same way as the ORM `Query.join()`\nmethod.\n* Many Core and ORM statement objects now perform much of their construction and validation in the compile phase - some error messages related to\nconstruction of a `Query` or `Select` may not be\nemitted until compilation / execution, rather than at construction time.\nThis might impact some test suites that are testing against failure modes.\n\n\nFor the full overview of SQLAlchemy 1.4 changes, see the\nWhat’s New in SQLAlchemy 1.4? document.\n\n### Migration to 2.0 Step One - Python 3 only (Python 3.7 minimum for 2.0 compatibility)¶\n\n\nSQLAlchemy 2.0 was first inspired by the fact that Python 2’s EOL was in 2020.\nSQLAlchemy is taking a longer period of time than other major"
    },
    {
        "instruction": "Develop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the cookies, user agent, and referer information from the URL request.",
        "api": "apeye",
        "output": "#!pip install apeye\nimport apeye\n\nbase_url = \"https://example.com\"\n\nclient = apeye.URL(base_url)\n\ncookies = client.cookies\nuser_agent = client.user_agent\nreferer = client.referer\n\nprint(\"Cookies:\")\nprint(cookies)\n\nprint(\"\\nUser Agent:\")\nprint(user_agent)\n\nprint(\"\\nReferer:\")\nprint(referer)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# `apeye.url`¶\n\n\n**Source code:** apeye\\_core/\\_\\_init\\_\\_.py\n\n\n---\n\n\n`pathlib`-like approach to URLs.\n\nChanged in version 0.2.0: `SlumberURL` and `RequestsURL`\nmoved to `apeye.slumber\\_url` and `apeye.requests\\_url` respectively.\n\n\nNote\n\n\nThe classes in this module can instead be imported from the\napeye\\_core module instead.\n\n**Classes:**\n\n| `Domain`(subdomain, domain, suffix) | `typing.NamedTuple` of a URL’s subdomain, domain, and suffix.\n\n==================\n Document 1 \n----------------\n `apeye.url`¶\n\n\n**Source code:** apeye\\_core/\\_\\_init\\_\\_.py\n\n\n---\n\n\n`pathlib`-like approach to URLs.\n\nChanged in version 0.2.0: `SlumberURL` and `RequestsURL`\nmoved to `apeye.slumber\\_url` and `apeye.requests\\_url` respectively.\n\n\nNote\n\n\nThe classes in this module can instead be imported from the\napeye\\_core module instead.\n\n**Classes:**\n\n| `Domain`(subdomain, domain, suffix) | `typing.NamedTuple` of a URL’s subdomain, domain, and suffix. |\n| `URL`([url]) | `pathlib`-like class for URLs. |\n| `URLPath`(\\*args) | Represents the path part of a URL. |\n\n\n**Data:**\n\n| `URLPathType` | Invariant `TypeVar` bound to `apeye.url.URLPath`. |\n| `URLType` | Invariant `TypeVar` bound to `apeye.url.URL`. |\n\n\nURLType *= TypeVar(URLType, bound=URL)*¶\n**Type:**    `TypeVar`\n\n\nInvariant `TypeVar` bound to `apeye.url.URL`.\n\nURLPathType *= TypeVar(URLPathType, bound=URLPath)*¶\n**Type:**    `TypeVar`\n\n\nInvariant `TypeVar` bound to `apeye.url.URLPath`.\n\n*class* URL(*url=''*)[source]¶\nBases: `PathLike`\n\n\n`pathlib`-like class for URLs.\n\nParameters\n**url** (`Union`[`str`, `URL`]) – The URL to construct the `URL` object from.\nDefault `''`.\n\nChanged in version 0.3.0: The `url` parameter can now be a string or a `URL`.\n\n\nChanged in version 1.1.0: Added support for sorting and rich comparisons (`<`, `<=`, `>` and `>=`).\n\n**Methods:**\n\n| `\\_\\_eq\\_\\_`(other) | Return `self == other`. |\n| `\\_\\_fspath\\_\\_`() | Returns the file system path representation of the `URL`. |\n| `\\_\\_repr\\_\\_`() | Returns the string representation of the `URL`. |\n| `\\_\\_str\\_\\_`() | Returns the `URL` as a string. |\n| `\\_\\_truediv\\_\\_`(key) | Construct a new `URL` object for the given child of this `URL`. |\n| `from\\_parts`(scheme, netloc, path[, query, …]) | Construct a `URL` from a scheme, netloc and path. |\n| `joinurl`(\\*args) | Construct a new `URL` object by combining the given arguments with this instance’s path part. |\n| `relative\\_to`(other) | Returns a version of this URL’s path relative to `other`. |\n| `strict\\_compare`(other) | Return `self ≡ other`, comparing the scheme, netloc, path, fragment and query parameters. |\n| `with\\_name`(name[, inherit]) | Return a new `URL` with the file name changed. |\n| `with\\_suffix`(suffix[, inherit]) | Returns a new `URL` with the file suffix changed. |\n\n\n**Attributes:**\n\n| `base\\_url` | Returns a `apeye.url.URL` object representing the URL without query strings or URL fragments. |\n| `domain` | Returns a `apeye.url.Domain` object representing the domain part of the URL. |\n| `fqdn` | Returns the Fully Qualified Domain Name of the `URL` . |\n| `fragment` | The URL fragment, used to identify a part of the document. |\n| `name` | The final path component, if any. |\n| `netloc` | Network location part of the URL |\n| `parent` | The logical parent of the `URL`. |\n| `parents` | An immutable sequence providing access to the logical ancestors of the `URL`. |\n| `parts` | An object providing sequence-like access to the components in the URL. |\n| `path` | The hierarchical path of the URL |\n| `port` | The port of number of the URL as an integer, if present. |\n| `query` | The query parameters of the URL, if present. |\n| `scheme` | URL scheme specifier |\n| `stem` | The final path component, minus its last suffix. |\n| `suffix` | The final component’s last suffix, if any. |\n| `suffixes` | A list of the final component’s suffixes, if any. |\n\n\n\\_\\_class\\_getitem\\_\\_ *= <bound method GenericAlias of <class 'apeye.url.URL'>>*¶\n**Type:**    `MethodType`\n\n\\_\\_eq\\_\\_(*other*)[source]¶\nReturn `self == other`.\n\nAttention\n\n\nURL fragments and query parameters are not compared.\n\nSee also\n\n\n`URL.strict\\_compare()`, which *does* consider those attributes.\n\nReturn type\n`bool`\n\n\\_\\_fspath\\_\\_()[source]¶\nReturns the file system path representation of the `URL`.\n\n\nThis is comprised of the `netloc` and `path` attributes.\n\nReturn type\n`str`\n\n\\_\\_repr\\_\\_()[source]¶\nReturns the string representation of the `URL`.\n\n\\_\\_str\\_\\_()[source]¶\nReturns the `URL` as a string.\n\n\\_\\_truediv\\_\\_(*key*)[source]¶\nConstruct a new `URL` object for the given child of this `URL`.\n\nReturn type\n`~URLType`\n\nChanged in version 0.7.0: \n* Added support for division by integers.\n* Now officially supports the new path having a URL fragment and/or query parameters.\nAny URL fragment or query parameters from the parent URL are not inherited by its children.\n\n\n*property* base\\_url*: apeye.url.URLType*¶\nReturns a `apeye.url.URL` object representing the URL without query strings or URL fragments.\n\nNew in version 0.7.0.\n\n\nReturn type\n`~URLType`\n\n*property* domain*: apeye.url.Domain*¶\nReturns a `apeye.url.Domain` object representing the domain part of the URL.\n\nReturn type\n`Domain`\n\n*property* fqdn*: str*¶\nReturns the Fully Qualified Domain Name of the `URL` .\n\nfragment¶\n**Type:**    `Optional`[`str`]\n\n\nThe URL fragment, used to identify a part of the document. `None` if absent from the URL.\n\n\n*classmethod* from\\_parts(*scheme*, *netloc*, *path*, *query=None*, *fragment=None*)[source]¶\nConstruct a `URL` from a scheme, netloc and path.\n\nParameters\n* **scheme** (`str`) – The scheme of the URL, e.g `'http'`.\n* **netloc** (`str`) – The netloc of the URl, e.g. `'bbc.co.uk:80'`.\n* **path** (`Union`[`str`, `Path`, `PathLike`]) – The path of the URL, e.g. `'/news'`.\n* **query** (`Optional`[`Mapping`[`Any`, `List`]]) – The query parameters of the URL, if present.\nDefault `None`.\n* **fragment** (`Optional`[`str`]) – The URL fragment, used to identify a part of the document.\n`None` if absent from the URL.\nDefault `None`.\n\n\nPut together, the resulting path would be `'http://bbc.co.uk:80/news'`\n\nChanged in version 0.7.0: Added the `query` and `fragment` arguments.\n\n\njoinurl(*\\*args*)[source]¶\nConstruct a new `URL` object by combining the given arguments with this instance’s path part.\n\nNew in version 1.1.0.\n\nExcept for the final path element any queries and fragments are ignored.\n\nReturns\nA new `URL` representing either a subpath\n(if all arguments are relative paths) or a totally different path\n(if one of the arguments is absolute).\n\n*property* name*: str*¶\nThe final path component, if any.\n\nnetloc¶\n**Type:**    `str`\n\n\nNetwork location part of the URL\n\n*property* parent*: apeye.url.URLType*¶\nThe logical parent of the `URL`.\n\n*property* parents*: Tuple[apeye.url.URLType, ...]*¶\nAn immutable sequence providing access to the logical ancestors of the `URL`.\n\nReturn type\n`Tuple`[`~URLType`, …]\n\n*property* parts*: Tuple[str, ...]*¶\nAn object providing sequence-like access to the components in the URL.\n\n\nTo retrieve only the parts of the path, use `URL.path.parts`.\n\nReturn type\n`Tuple`[`str`, …]\n\npath¶\n**Type:**    `URLPath`\n\n\nThe hierarchical path of the URL\n\n*property* port*: Optional[int]*¶\nThe port of number of the URL as an integer, if present. Default `None`.\n\n\nReturn type\n`Optional`[`int`]\n\nquery¶\n**Type:**    `Dict`[`str`, `List`[`str`]]\n\n\nThe query parameters of the URL, if present.\n\n\nrelative\\_to(*other*)[source]¶\nReturns a version of this URL’s path relative to `other`.\n\n\nParameters\n**other** (`Union`[`str`, `URL`, `URLPath`]) – Either a `URL`, or a string or `URLPath` representing an *absolute* path.\nIf a `URL`, the `netloc` must match this URL’s.\n\nRaises\n**ValueError** – if the operation is not possible\n(i.e. because this URL’s path is not a subpath of the other path)\n\nReturn type\n`URLPath`\n\nscheme¶\n**Type:**    `str`\n\n\nURL scheme specifier\n\n*property* stem*: str*¶\nThe final path component, minus its last suffix.\n\nstrict\\_compare(*other*)[source]¶\nReturn `self ≡ other`, comparing the scheme, netloc, path, fragment and query parameters.\n\n\nReturn type\n`bool`\n\n*property* suffix*: str*¶\nThe final component’s last suffix, if any.\n\n\nThis includes the leading period. For example: `'.txt'`.\n\n*property* suffixes*: List[str]*¶\nA list of the final component’s suffixes, if any.\n\n\nThese include the leading periods. For example: `['.tar', '.gz']`.\n\nReturn type\n`List`[`str`]\n\nwith\\_name(*name*, *inherit=True*)[source]¶\nReturn a new `URL` with the file name changed.\n\nParameters\n* **name** (`str`)\n* **inherit** (`bool`) – Whether the new `URL` should inherit the query string\nand fragment from this `URL`.\nDefault `True`.\n\nChanged in version 0.7.0: Added the `inherit` parameter.\n\n\nwith\\_suffix(*suffix*, *inherit=True*)[source]¶\nReturns a new `URL` with the file suffix changed.\n\n\nIf the `URL` has no suffix, add the given suffix.\n\n\nIf the given suffix is an empty string, remove the suffix from the `URL`.\n\nParameters\n* **suffix** (`str`)\n* **inherit** (`bool`) – Whether the new `URL` should inherit the query string\nand fragment from this `URL`.\nDefault `True`.\n\n*class* URLPath(*\\*args*)¶\nBases: `PurePosixPath`\n\n\nRepresents the path part of a URL.\n\n\nSubclass of `pathlib.PurePosixPath` that provides a subset of its methods.\n\nChanged in version 1.1.0: Implemented `is\\_absolute()`, `joinpath()`,\n`relative\\_to()`, `match()`,\n`anchor`, `drive`, and support for rich comparisons (`<`, `<=`, `>` and `>=`),\nwhich previously raised `NotImplementedError`.\n\n| `\\_\\_bytes\\_\\_`() | Return the bytes representation of the path. |\n| `\\_\\_eq\\_\\_`(other) | Return `self == other`. |\n| `\\_\\_repr\\_\\_`() | Return a string representation of the `URLPath`. |\n| `\\_\\_rtruediv\\_\\_`(key) | Return `value / self`. |\n| `\\_\\_str\\_\\_`() | Return the string representation of the path, suitable for passing to system calls. |\n| `\\_\\_truediv\\_\\_`(key) | Return `self / value`. |\n| `is\\_absolute`() | Returns whether the path is absolute (i.e. |\n| `is\\_relative\\_to`(\\*other) | Return True if the path is relative to another path or False. |\n| `is\\_reserved`() | Return True if the path contains one of the special names reserved by the system, if any. |\n| `joinpath`(\\*args) | Combine this `URLPath` with one or several arguments. |\n| `relative\\_to`(\\*other) | Returns the relative path to another path identified by the passed arguments. |\n| `with\\_name`(name) | Return a new path with the file name changed. |\n| `with\\_stem`(stem) | Return a new path with the stem changed. |\n| `with\\_suffix`(suffix) | Return a new path with the file suffix changed. |\n\n| `name` | The final path component, if any. |\n| `parent` | The logical parent of the path. |\n| `parents` | A sequence of this path’s logical parents. |\n| `parts` | An object providing sequence-like access to the components in the filesystem path. |\n| `root` | The root of the path, if any. |\n| `stem` | The final path component, minus its last suffix. |\n| `suffix` | The final component’s last suffix, if any. |\n| `suffixes` | A list of the final component’s suffixes, if any. |\n\n\n\\_\\_bytes\\_\\_()¶\nReturn the bytes representation of the path. This is only\nrecommended to use under Unix.\n\n\\_\\_eq\\_\\_(*other*)¶\nReturn `self == other`.\n\n\\_\\_repr\\_\\_()[source]¶\nReturn a string representation of the `URLPath`.\n\n\\_\\_rtruediv\\_\\_(*key*)¶\nReturn `value / self`.\n\n\\_\\_str\\_\\_()[source]¶\nReturn the string representation of the path, suitable for passing to system calls.\n\n\\_\\_truediv\\_\\_(*key*)¶\nReturn `self / value`.\n\nis\\_absolute()[source]¶\nReturns whether the path is absolute (i.e. starts with `/`).\n\nNew in version 1.1.0: previously raised `NotImplementedError`.\n\nis\\_relative\\_to(*\\*other*)¶\nReturn True if the path is relative to another path or False.\n\nis\\_reserved()¶\nReturn True if the path contains one of the special names reserved\nby the system, if any.\n\njoinpath(*\\*args*)[source]¶\nCombine this `URLPath` with one or several arguments.\n\n\nReturn type\n`~URLPathType`\n\nReturns\nA new `URLPath` representing either a subpath\n(if all arguments are relative paths) or a totally different path\n(if one of the arguments is absolute).\n\n*property* name¶\nThe final path component, if any.\n\n*property* parent¶\nThe logical parent of the path.\n\n*property* parents¶\nA sequence of this path’s logical parents.\n\n*property* parts¶\nAn object providing sequence-like access to the\ncomponents in the filesystem path.\n\nrelative\\_to(*\\*other*)[source]¶\nReturns the relative path to another path identified by the passed arguments.\n\n\nThe arguments are joined together to form a single path, and therefore the following behave identically:\n\n```\n>>> URLPath(\"/news/sport\").relative\\_to(\"/\", \"news\")\nURLPath('sport')\n>>> URLPath(\"/news/sport\").relative\\_to(\"/news\")\nURLPath('sport')\n\n```\n\n\nParameters\n**\\*other** (`Union`[`str`, `Path`, `PathLike`])\n\nRaises\n**ValueError** – if the operation is not possible (because this is not a subpath of the other path)\n\n\n`relative\\_to()`, which is recommended when constructing a relative path from a `URL`.\nThis method cannot correctly handle some cases, such as:\n\n```\n>>> URL(\"https://github.com/domdfcoding\").path.relative\\_to(URL(\"https://github.com\").path)\nTraceback (most recent call last):\nValueError: '/domdfcoding' does not start with ''\n\n\nSince `URL(\"https://github.com\").path` is `URLPath('')`.\n\n\nInstead, use:\n\n```\n>>> URL(\"https://github.com/domdfcoding\").relative\\_to(URL(\"https://github.com\"))\nURLPath('domdfcoding')\n\n*property* root¶\nThe root of the path, if any.\n\n*property* stem¶\nThe final path component, minus its last suffix.\n\n*property* suffix¶\nThe final component’s last suffix, if any.\n\n\nThis includes the leading period. For example: ‘.txt’\n\n*property* suffixes¶\nA list of the final component’s suffixes, if any.\n\n\nThese include the leading periods. For example: [‘.tar’, ‘.gz’]\n\nwith\\_name(*name*)¶\nReturn a new path with the file name changed.\n\nwith\\_stem(*stem*)¶\nReturn a new path with the stem changed.\n\nwith\\_suffix(*suffix*)¶\nReturn a new path with the file suffix changed. If the path\nhas no suffix, add given suffix. If the given suffix is an empty\nstring, remove the suffix from the path.\n\n\n*namedtuple* Domain(*subdomain*, *domain*, *suffix*)¶\n`typing.NamedTuple` of a URL’s subdomain, domain, and suffix.\n\nFields\n0. **subdomain** (`str`) – Alias for field number 0\n1. **domain** (`str`) – Alias for field number 1\n2. **suffix** (`str`) – Alias for field number 2\n\n\n\\_\\_repr\\_\\_()[source]¶\nReturn a string representation of the `Domain`.\n\n*property* fqdn¶\nReturns a Fully Qualified Domain Name, if there is a proper domain/suffix.\n\n```\n>>> URL('https://forums.bbc.co.uk/path/to/file').domain.fqdn\n'forums.bbc.co.uk'\n>>> URL('https://localhost:8080').domain.fqdn\n''\n\n*property* ipv4*: Optional[ipaddress.IPv4Address]*¶\nReturns the ipv4 if that is what the presented domain/url is.\n\n```\n>>> URL('https://127.0.0.1/path/to/file').domain.ipv4\nIPv4Address('127.0.0.1')\n>>> URL('https://127.0.0.1.1/path/to/file').domain.ipv4\n>>> URL('https://256.1.1.1').domain.ipv4\n\nReturn type\n`Optional`[`IPv4Address`]\n\n*property* registered\\_domain¶\nJoins the domain and suffix fields with a dot, if they’re both set.\n\n```\n>>> URL('https://forums.bbc.co.uk').domain.registered\\_domain\n'bbc.co.uk'\n>>> URL('https://localhost:8080').domain.registered\\_domain\n''\n\n# `apeye.requests\\_url`¶\n\n\nExtension of `URL` with support for interacting with the website using the Requests library.\n\nNew in version 0.2.0.\n\n| `RequestsURL`([url]) | Extension of `URL` with support for interacting with the website using the Requests library. |\n| `TrailingRequestsURL`([url]) | Extension of `RequestsURL`\n\n==================\n Document 2 \n----------------\n`apeye.rate\\_limiter`¶\n\n\nRate limiters for making calls to external APIs in a polite manner.\n\n\nThis module has the following additional requirements:\n\n> \n> \n> ```\n> cachecontrol[filecache]>=0.12.6\n> filelock>=3.8.0; python_version >= \"3.7\"\n> lockfile>=0.12.2; python_version < \"3.7\"\n> \n> ```\n> \n> \n> \n\n\nThese can be installed as follows:\n\n> \n> \n> ```\n> python -m pip install apeye[limiter]\n> \n> ```\n> \n\n| `HTTPCache`(app\\_name[, expires\\_after]) | Cache HTTP requests for up to 28 days and limit the rate of requests to no more than 5/second. |\n| `RateLimitAdapter`([cache, cache\\_etags, …]) | Custom `cachecontrol.adapter.CacheControlAdapter` to limit the rate of requests to 5 per second. |\n\n| `rate\\_limit`([min\\_time, logger]) | Decorator to force a function to run no less than `min\\_time` seconds after it last ran. |\n\n\n*class* HTTPCache(*app\\_name*, *expires\\_after=datetime.timedelta(days=28)*)[source]¶\nCache HTTP requests for up to 28 days and limit the rate of requests to no more than 5/second.\n\nParameters\n* **app\\_name** (`str`) – The name of the app. This dictates the name of the cache directory.\n* **expires\\_after** (`timedelta`) – The maximum time to cache responses for.\nDefault `datetime.timedelta(days=28)`.\n\n| `clear`() | Clear the cache. |\n\n\napp\\_name¶\n**Type:**    `str`\n\nclear()[source]¶\nClear the cache.\n\n\nrate\\_limit(*min\\_time=0.2*, *logger=None*)[source]¶\nDecorator to force a function to run no less than `min\\_time` seconds after it last ran.\nUsed for rate limiting.\n\nParameters\n* **min\\_time** (`float`) – The minimum interval between subsequent runs of the decorated function.\nDefault `0.2`, which gives a maximum rate of 5 calls per second.\n* **logger** (`Optional`[`Logger`]) – Optional logger to log information about requests to. Defaults to the root logger.\n\nReturn type\n`Callable`[[`Callable`], `Any`]\n\n*class* RateLimitAdapter(*cache=None*, *cache\\_etags=True*, *controller\\_class=None*, *serializer=None*, *heuristic=None*, *cacheable\\_methods=None*, *\\*args*, *\\*\\*kw*)[source]¶\nBases: `CacheControlAdapter`\n\n\nCustom `cachecontrol.adapter.CacheControlAdapter` to limit the rate of requests to 5 per second.\n\nParameters\n* **cache** (`BaseCache | None`) – Default `None`.\n* **cache\\_etags** (`bool`) – Default `True`.\n* **controller\\_class** (`type[CacheController] | None`) – Default `None`.\n* **serializer** (`Serializer | None`) – Default `None`.\n* **heuristic** (`BaseHeuristic | None`) – Default `None`.\n* **cacheable\\_methods** (`Collection[str] | None`) – Default `None`.\n\n| `rate\\_limited\\_send`(\\*args, \\*\\*kwargs) | Wrapper around `CacheControlAdapter.send` to limit the rate of requests. |\n| `send`(request[, cacheable\\_methods]) | Send a request. |\n\n\nrate\\_limited\\_send(*\\*args*, *\\*\\*kwargs*)[source]¶\nWrapper around `CacheControlAdapter.send`\nto limit the rate of requests.\n\nsend(*request*, *cacheable\\_methods=None*, *\\*\\*kwargs*)[source]¶\nSend a request.\n\n\nUse the request information to see if it exists in the cache and cache the response if we need to and can.\n\nParameters\n* **request** (`PreparedRequest`) – The `requests.PreparedRequest` being sent.\n* **cacheable\\_methods** (`Optional`[`Collection`[`str`]]) – Default `None`.\n* **\\*\\*kwargs** – Additional arguments taken by `requests.adapters.HTTPAdapter.send()` (e.g. `timeout`).\n\n# `apeye.url`¶\n\n\n\n# `apeye.slumber\\_url`¶\n\n\n\n# `apeye.cache`¶\n\n\n# `apeye.email\\_validator`¶\n\n\n\n# `apeye.rate\\_limiter`¶"
    },
    {
        "instruction": "Develop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the status code, headers, and content of the URL response. Additionally, the program should save the content of the response to a file named 'output.txt'.",
        "api": "apeye",
        "output": "#!pip install apeye\nimport apeye\n\nbase_url = \"https://example.com\"\n\nclient = apeye.URL(base_url)\n\nresponse = client.get()\n\nprint(\"Status Code:\")\nprint(response.status_code)\n\nprint(\"\\nHeaders:\")\nprint(response.headers)\n\nprint(\"\\nContent:\")\nprint(response.content)\n\n# Save content to file\nwith open('output.txt', 'w') as file:\n    file.write(response.content)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# `apeye.url`¶\n\n\n**Source code:** apeye\\_core/\\_\\_init\\_\\_.py\n\n\n---\n\n\n`pathlib`-like approach to URLs.\n\nChanged in version 0.2.0: `SlumberURL` and `RequestsURL`\nmoved to `apeye.slumber\\_url` and `apeye.requests\\_url` respectively.\n\n\nNote\n\n\nThe classes in this module can instead be imported from the\napeye\\_core module instead.\n\n**Classes:**\n\n| `Domain`(subdomain, domain, suffix) | `typing.NamedTuple` of a URL’s subdomain, domain, and suffix.\n\n==================\n Document 1 \n----------------\n `apeye.url`¶\n\n\n**Source code:** apeye\\_core/\\_\\_init\\_\\_.py\n\n\n---\n\n\n`pathlib`-like approach to URLs.\n\nChanged in version 0.2.0: `SlumberURL` and `RequestsURL`\nmoved to `apeye.slumber\\_url` and `apeye.requests\\_url` respectively.\n\n\nNote\n\n\nThe classes in this module can instead be imported from the\napeye\\_core module instead.\n\n**Classes:**\n\n| `Domain`(subdomain, domain, suffix) | `typing.NamedTuple` of a URL’s subdomain, domain, and suffix. |\n| `URL`([url]) | `pathlib`-like class for URLs. |\n| `URLPath`(\\*args) | Represents the path part of a URL. |\n\n\n**Data:**\n\n| `URLPathType` | Invariant `TypeVar` bound to `apeye.url.URLPath`. |\n| `URLType` | Invariant `TypeVar` bound to `apeye.url.URL`. |\n\n\nURLType *= TypeVar(URLType, bound=URL)*¶\n**Type:**    `TypeVar`\n\n\nInvariant `TypeVar` bound to `apeye.url.URL`.\n\nURLPathType *= TypeVar(URLPathType, bound=URLPath)*¶\n**Type:**    `TypeVar`\n\n\nInvariant `TypeVar` bound to `apeye.url.URLPath`.\n\n*class* URL(*url=''*)[source]¶\nBases: `PathLike`\n\n\n`pathlib`-like class for URLs.\n\nParameters\n**url** (`Union`[`str`, `URL`]) – The URL to construct the `URL` object from.\nDefault `''`.\n\nChanged in version 0.3.0: The `url` parameter can now be a string or a `URL`.\n\n\nChanged in version 1.1.0: Added support for sorting and rich comparisons (`<`, `<=`, `>` and `>=`).\n\n**Methods:**\n\n| `\\_\\_eq\\_\\_`(other) | Return `self == other`. |\n| `\\_\\_fspath\\_\\_`() | Returns the file system path representation of the `URL`. |\n| `\\_\\_repr\\_\\_`() | Returns the string representation of the `URL`. |\n| `\\_\\_str\\_\\_`() | Returns the `URL` as a string. |\n| `\\_\\_truediv\\_\\_`(key) | Construct a new `URL` object for the given child of this `URL`. |\n| `from\\_parts`(scheme, netloc, path[, query, …]) | Construct a `URL` from a scheme, netloc and path. |\n| `joinurl`(\\*args) | Construct a new `URL` object by combining the given arguments with this instance’s path part. |\n| `relative\\_to`(other) | Returns a version of this URL’s path relative to `other`. |\n| `strict\\_compare`(other) | Return `self ≡ other`, comparing the scheme, netloc, path, fragment and query parameters. |\n| `with\\_name`(name[, inherit]) | Return a new `URL` with the file name changed. |\n| `with\\_suffix`(suffix[, inherit]) | Returns a new `URL` with the file suffix changed. |\n\n\n**Attributes:**\n\n| `base\\_url` | Returns a `apeye.url.URL` object representing the URL without query strings or URL fragments. |\n| `domain` | Returns a `apeye.url.Domain` object representing the domain part of the URL. |\n| `fqdn` | Returns the Fully Qualified Domain Name of the `URL` . |\n| `fragment` | The URL fragment, used to identify a part of the document. |\n| `name` | The final path component, if any. |\n| `netloc` | Network location part of the URL |\n| `parent` | The logical parent of the `URL`. |\n| `parents` | An immutable sequence providing access to the logical ancestors of the `URL`. |\n| `parts` | An object providing sequence-like access to the components in the URL. |\n| `path` | The hierarchical path of the URL |\n| `port` | The port of number of the URL as an integer, if present. |\n| `query` | The query parameters of the URL, if present. |\n| `scheme` | URL scheme specifier |\n| `stem` | The final path component, minus its last suffix. |\n| `suffix` | The final component’s last suffix, if any. |\n| `suffixes` | A list of the final component’s suffixes, if any. |\n\n\n\\_\\_class\\_getitem\\_\\_ *= <bound method GenericAlias of <class 'apeye.url.URL'>>*¶\n**Type:**    `MethodType`\n\n\\_\\_eq\\_\\_(*other*)[source]¶\nReturn `self == other`.\n\nAttention\n\n\nURL fragments and query parameters are not compared.\n\nSee also\n\n\n`URL.strict\\_compare()`, which *does* consider those attributes.\n\nReturn type\n`bool`\n\n\\_\\_fspath\\_\\_()[source]¶\nReturns the file system path representation of the `URL`.\n\n\nThis is comprised of the `netloc` and `path` attributes.\n\nReturn type\n`str`\n\n\\_\\_repr\\_\\_()[source]¶\nReturns the string representation of the `URL`.\n\n\\_\\_str\\_\\_()[source]¶\nReturns the `URL` as a string.\n\n\\_\\_truediv\\_\\_(*key*)[source]¶\nConstruct a new `URL` object for the given child of this `URL`.\n\nReturn type\n`~URLType`\n\nChanged in version 0.7.0: \n* Added support for division by integers.\n* Now officially supports the new path having a URL fragment and/or query parameters.\nAny URL fragment or query parameters from the parent URL are not inherited by its children.\n\n\n*property* base\\_url*: apeye.url.URLType*¶\nReturns a `apeye.url.URL` object representing the URL without query strings or URL fragments.\n\nNew in version 0.7.0.\n\n\nReturn type\n`~URLType`\n\n*property* domain*: apeye.url.Domain*¶\nReturns a `apeye.url.Domain` object representing the domain part of the URL.\n\nReturn type\n`Domain`\n\n*property* fqdn*: str*¶\nReturns the Fully Qualified Domain Name of the `URL` .\n\nfragment¶\n**Type:**    `Optional`[`str`]\n\n\nThe URL fragment, used to identify a part of the document. `None` if absent from the URL.\n\n\n*classmethod* from\\_parts(*scheme*, *netloc*, *path*, *query=None*, *fragment=None*)[source]¶\nConstruct a `URL` from a scheme, netloc and path.\n\nParameters\n* **scheme** (`str`) – The scheme of the URL, e.g `'http'`.\n* **netloc** (`str`) – The netloc of the URl, e.g. `'bbc.co.uk:80'`.\n* **path** (`Union`[`str`, `Path`, `PathLike`]) – The path of the URL, e.g. `'/news'`.\n* **query** (`Optional`[`Mapping`[`Any`, `List`]]) – The query parameters of the URL, if present.\nDefault `None`.\n* **fragment** (`Optional`[`str`]) – The URL fragment, used to identify a part of the document.\n`None` if absent from the URL.\nDefault `None`.\n\n\nPut together, the resulting path would be `'http://bbc.co.uk:80/news'`\n\nChanged in version 0.7.0: Added the `query` and `fragment` arguments.\n\n\njoinurl(*\\*args*)[source]¶\nConstruct a new `URL` object by combining the given arguments with this instance’s path part.\n\nNew in version 1.1.0.\n\nExcept for the final path element any queries and fragments are ignored.\n\nReturns\nA new `URL` representing either a subpath\n(if all arguments are relative paths) or a totally different path\n(if one of the arguments is absolute).\n\n*property* name*: str*¶\nThe final path component, if any.\n\nnetloc¶\n**Type:**    `str`\n\n\nNetwork location part of the URL\n\n*property* parent*: apeye.url.URLType*¶\nThe logical parent of the `URL`.\n\n*property* parents*: Tuple[apeye.url.URLType, ...]*¶\nAn immutable sequence providing access to the logical ancestors of the `URL`.\n\nReturn type\n`Tuple`[`~URLType`, …]\n\n*property* parts*: Tuple[str, ...]*¶\nAn object providing sequence-like access to the components in the URL.\n\n\nTo retrieve only the parts of the path, use `URL.path.parts`.\n\nReturn type\n`Tuple`[`str`, …]\n\npath¶\n**Type:**    `URLPath`\n\n\nThe hierarchical path of the URL\n\n*property* port*: Optional[int]*¶\nThe port of number of the URL as an integer, if present. Default `None`.\n\n\nReturn type\n`Optional`[`int`]\n\nquery¶\n**Type:**    `Dict`[`str`, `List`[`str`]]\n\n\nThe query parameters of the URL, if present.\n\n\nrelative\\_to(*other*)[source]¶\nReturns a version of this URL’s path relative to `other`.\n\n\nParameters\n**other** (`Union`[`str`, `URL`, `URLPath`]) – Either a `URL`, or a string or `URLPath` representing an *absolute* path.\nIf a `URL`, the `netloc` must match this URL’s.\n\nRaises\n**ValueError** – if the operation is not possible\n(i.e. because this URL’s path is not a subpath of the other path)\n\nReturn type\n`URLPath`\n\nscheme¶\n**Type:**    `str`\n\n\nURL scheme specifier\n\n*property* stem*: str*¶\nThe final path component, minus its last suffix.\n\nstrict\\_compare(*other*)[source]¶\nReturn `self ≡ other`, comparing the scheme, netloc, path, fragment and query parameters.\n\n\nReturn type\n`bool`\n\n*property* suffix*: str*¶\nThe final component’s last suffix, if any.\n\n\nThis includes the leading period. For example: `'.txt'`.\n\n*property* suffixes*: List[str]*¶\nA list of the final component’s suffixes, if any.\n\n\nThese include the leading periods. For example: `['.tar', '.gz']`.\n\nReturn type\n`List`[`str`]\n\nwith\\_name(*name*, *inherit=True*)[source]¶\nReturn a new `URL` with the file name changed.\n\nParameters\n* **name** (`str`)\n* **inherit** (`bool`) – Whether the new `URL` should inherit the query string\nand fragment from this `URL`.\nDefault `True`.\n\nChanged in version 0.7.0: Added the `inherit` parameter.\n\n\nwith\\_suffix(*suffix*, *inherit=True*)[source]¶\nReturns a new `URL` with the file suffix changed.\n\n\nIf the `URL` has no suffix, add the given suffix.\n\n\nIf the given suffix is an empty string, remove the suffix from the `URL`.\n\nParameters\n* **suffix** (`str`)\n* **inherit** (`bool`) – Whether the new `URL` should inherit the query string\nand fragment from this `URL`.\nDefault `True`.\n\n*class* URLPath(*\\*args*)¶\nBases: `PurePosixPath`\n\n\nRepresents the path part of a URL.\n\n\nSubclass of `pathlib.PurePosixPath` that provides a subset of its methods.\n\nChanged in version 1.1.0: Implemented `is\\_absolute()`, `joinpath()`,\n`relative\\_to()`, `match()`,\n`anchor`, `drive`, and support for rich comparisons (`<`, `<=`, `>` and `>=`),\nwhich previously raised `NotImplementedError`.\n\n| `\\_\\_bytes\\_\\_`() | Return the bytes representation of the path. |\n| `\\_\\_eq\\_\\_`(other) | Return `self == other`. |\n| `\\_\\_repr\\_\\_`() | Return a string representation of the `URLPath`. |\n| `\\_\\_rtruediv\\_\\_`(key) | Return `value / self`. |\n| `\\_\\_str\\_\\_`() | Return the string representation of the path, suitable for passing to system calls. |\n| `\\_\\_truediv\\_\\_`(key) | Return `self / value`. |\n| `is\\_absolute`() | Returns whether the path is absolute (i.e. |\n| `is\\_relative\\_to`(\\*other) | Return True if the path is relative to another path or False. |\n| `is\\_reserved`() | Return True if the path contains one of the special names reserved by the system, if any. |\n| `joinpath`(\\*args) | Combine this `URLPath` with one or several arguments. |\n| `relative\\_to`(\\*other) | Returns the relative path to another path identified by the passed arguments. |\n| `with\\_name`(name) | Return a new path with the file name changed. |\n| `with\\_stem`(stem) | Return a new path with the stem changed. |\n| `with\\_suffix`(suffix) | Return a new path with the file suffix changed. |\n\n| `name` | The final path component, if any. |\n| `parent` | The logical parent of the path. |\n| `parents` | A sequence of this path’s logical parents. |\n| `parts` | An object providing sequence-like access to the components in the filesystem path. |\n| `root` | The root of the path, if any. |\n| `stem` | The final path component, minus its last suffix. |\n| `suffix` | The final component’s last suffix, if any. |\n| `suffixes` | A list of the final component’s suffixes, if any. |\n\n\n\\_\\_bytes\\_\\_()¶\nReturn the bytes representation of the path. This is only\nrecommended to use under Unix.\n\n\\_\\_eq\\_\\_(*other*)¶\nReturn `self == other`.\n\n\\_\\_repr\\_\\_()[source]¶\nReturn a string representation of the `URLPath`.\n\n\\_\\_rtruediv\\_\\_(*key*)¶\nReturn `value / self`.\n\n\\_\\_str\\_\\_()[source]¶\nReturn the string representation of the path, suitable for passing to system calls.\n\n\\_\\_truediv\\_\\_(*key*)¶\nReturn `self / value`.\n\nis\\_absolute()[source]¶\nReturns whether the path is absolute (i.e. starts with `/`).\n\nNew in version 1.1.0: previously raised `NotImplementedError`.\n\nis\\_relative\\_to(*\\*other*)¶\nReturn True if the path is relative to another path or False.\n\nis\\_reserved()¶\nReturn True if the path contains one of the special names reserved\nby the system, if any.\n\njoinpath(*\\*args*)[source]¶\nCombine this `URLPath` with one or several arguments.\n\n\nReturn type\n`~URLPathType`\n\nReturns\nA new `URLPath` representing either a subpath\n(if all arguments are relative paths) or a totally different path\n(if one of the arguments is absolute).\n\n*property* name¶\nThe final path component, if any.\n\n*property* parent¶\nThe logical parent of the path.\n\n*property* parents¶\nA sequence of this path’s logical parents.\n\n*property* parts¶\nAn object providing sequence-like access to the\ncomponents in the filesystem path.\n\nrelative\\_to(*\\*other*)[source]¶\nReturns the relative path to another path identified by the passed arguments.\n\n\nThe arguments are joined together to form a single path, and therefore the following behave identically:\n\n```\n>>> URLPath(\"/news/sport\").relative\\_to(\"/\", \"news\")\nURLPath('sport')\n>>> URLPath(\"/news/sport\").relative\\_to(\"/news\")\nURLPath('sport')\n\n```\n\n\nParameters\n**\\*other** (`Union`[`str`, `Path`, `PathLike`])\n\nRaises\n**ValueError** – if the operation is not possible (because this is not a subpath of the other path)\n\n\n`relative\\_to()`, which is recommended when constructing a relative path from a `URL`.\nThis method cannot correctly handle some cases, such as:\n\n```\n>>> URL(\"https://github.com/domdfcoding\").path.relative\\_to(URL(\"https://github.com\").path)\nTraceback (most recent call last):\nValueError: '/domdfcoding' does not start with ''\n\n\nSince `URL(\"https://github.com\").path` is `URLPath('')`.\n\n\nInstead, use:\n\n```\n>>> URL(\"https://github.com/domdfcoding\").relative\\_to(URL(\"https://github.com\"))\nURLPath('domdfcoding')\n\n*property* root¶\nThe root of the path, if any.\n\n*property* stem¶\nThe final path component, minus its last suffix.\n\n*property* suffix¶\nThe final component’s last suffix, if any.\n\n\nThis includes the leading period. For example: ‘.txt’\n\n*property* suffixes¶\nA list of the final component’s suffixes, if any.\n\n\nThese include the leading periods. For example: [‘.tar’, ‘.gz’]\n\nwith\\_name(*name*)¶\nReturn a new path with the file name changed.\n\nwith\\_stem(*stem*)¶\nReturn a new path with the stem changed.\n\nwith\\_suffix(*suffix*)¶\nReturn a new path with the file suffix changed. If the path\nhas no suffix, add given suffix. If the given suffix is an empty\nstring, remove the suffix from the path.\n\n\n*namedtuple* Domain(*subdomain*, *domain*, *suffix*)¶\n`typing.NamedTuple` of a URL’s subdomain, domain, and suffix.\n\nFields\n0. **subdomain** (`str`) – Alias for field number 0\n1. **domain** (`str`) – Alias for field number 1\n2. **suffix** (`str`) – Alias for field number 2\n\n\n\\_\\_repr\\_\\_()[source]¶\nReturn a string representation of the `Domain`.\n\n*property* fqdn¶\nReturns a Fully Qualified Domain Name, if there is a proper domain/suffix.\n\n```\n>>> URL('https://forums.bbc.co.uk/path/to/file').domain.fqdn\n'forums.bbc.co.uk'\n>>> URL('https://localhost:8080').domain.fqdn\n''\n\n*property* ipv4*: Optional[ipaddress.IPv4Address]*¶\nReturns the ipv4 if that is what the presented domain/url is.\n\n```\n>>> URL('https://127.0.0.1/path/to/file').domain.ipv4\nIPv4Address('127.0.0.1')\n>>> URL('https://127.0.0.1.1/path/to/file').domain.ipv4\n>>> URL('https://256.1.1.1').domain.ipv4\n\nReturn type\n`Optional`[`IPv4Address`]\n\n*property* registered\\_domain¶\nJoins the domain and suffix fields with a dot, if they’re both set.\n\n```\n>>> URL('https://forums.bbc.co.uk').domain.registered\\_domain\n'bbc.co.uk'\n>>> URL('https://localhost:8080').domain.registered\\_domain\n''\n\n# `apeye.requests\\_url`¶\n\n\nExtension of `URL` with support for interacting with the website using the Requests library.\n\nNew in version 0.2.0.\n\n| `RequestsURL`([url]) | Extension of `URL` with support for interacting with the website using the Requests library. |\n| `TrailingRequestsURL`([url]) | Extension of `RequestsURL`\n\n==================\n Document 2 \n----------------\n`apeye.rate\\_limiter`¶\n\n\nRate limiters for making calls to external APIs in a polite manner.\n\n\nThis module has the following additional requirements:\n\n> \n> \n> ```\n> cachecontrol[filecache]>=0.12.6\n> filelock>=3.8.0; python_version >= \"3.7\"\n> lockfile>=0.12.2; python_version < \"3.7\"\n> \n> ```\n> \n> \n> \n\n\nThese can be installed as follows:\n\n> \n> \n> ```\n> python -m pip install apeye[limiter]\n> \n> ```\n> \n\n| `HTTPCache`(app\\_name[, expires\\_after]) | Cache HTTP requests for up to 28 days and limit the rate of requests to no more than 5/second. |\n| `RateLimitAdapter`([cache, cache\\_etags, …]) | Custom `cachecontrol.adapter.CacheControlAdapter` to limit the rate of requests to 5 per second. |\n\n| `rate\\_limit`([min\\_time, logger]) | Decorator to force a function to run no less than `min\\_time` seconds after it last ran. |\n\n\n*class* HTTPCache(*app\\_name*, *expires\\_after=datetime.timedelta(days=28)*)[source]¶\nCache HTTP requests for up to 28 days and limit the rate of requests to no more than 5/second.\n\nParameters\n* **app\\_name** (`str`) – The name of the app. This dictates the name of the cache directory.\n* **expires\\_after** (`timedelta`) – The maximum time to cache responses for.\nDefault `datetime.timedelta(days=28)`.\n\n| `clear`() | Clear the cache. |\n\n\napp\\_name¶\n**Type:**    `str`\n\nclear()[source]¶\nClear the cache.\n\n\nrate\\_limit(*min\\_time=0.2*, *logger=None*)[source]¶\nDecorator to force a function to run no less than `min\\_time` seconds after it last ran.\nUsed for rate limiting.\n\nParameters\n* **min\\_time** (`float`) – The minimum interval between subsequent runs of the decorated function.\nDefault `0.2`, which gives a maximum rate of 5 calls per second.\n* **logger** (`Optional`[`Logger`]) – Optional logger to log information about requests to. Defaults to the root logger.\n\nReturn type\n`Callable`[[`Callable`], `Any`]\n\n*class* RateLimitAdapter(*cache=None*, *cache\\_etags=True*, *controller\\_class=None*, *serializer=None*, *heuristic=None*, *cacheable\\_methods=None*, *\\*args*, *\\*\\*kw*)[source]¶\nBases: `CacheControlAdapter`\n\n\nCustom `cachecontrol.adapter.CacheControlAdapter` to limit the rate of requests to 5 per second.\n\nParameters\n* **cache** (`BaseCache | None`) – Default `None`.\n* **cache\\_etags** (`bool`) – Default `True`.\n* **controller\\_class** (`type[CacheController] | None`) – Default `None`.\n* **serializer** (`Serializer | None`) – Default `None`.\n* **heuristic** (`BaseHeuristic | None`) – Default `None`.\n* **cacheable\\_methods** (`Collection[str] | None`) – Default `None`.\n\n| `rate\\_limited\\_send`(\\*args, \\*\\*kwargs) | Wrapper around `CacheControlAdapter.send` to limit the rate of requests. |\n| `send`(request[, cacheable\\_methods]) | Send a request. |\n\n\nrate\\_limited\\_send(*\\*args*, *\\*\\*kwargs*)[source]¶\nWrapper around `CacheControlAdapter.send`\nto limit the rate of requests.\n\nsend(*request*, *cacheable\\_methods=None*, *\\*\\*kwargs*)[source]¶\nSend a request.\n\n\nUse the request information to see if it exists in the cache and cache the response if we need to and can.\n\nParameters\n* **request** (`PreparedRequest`) – The `requests.PreparedRequest` being sent.\n* **cacheable\\_methods** (`Optional`[`Collection`[`str`]]) – Default `None`.\n* **\\*\\*kwargs** – Additional arguments taken by `requests.adapters.HTTPAdapter.send()` (e.g. `timeout`).\n\n# `apeye.url`¶\n\n\n\n# `apeye.slumber\\_url`¶\n\n\n\n# `apeye.cache`¶\n\n\n# `apeye.email\\_validator`¶\n\n\n\n# `apeye.rate\\_limiter`¶"
    },
    {
        "instruction": "Create a Python program using the 'bitstring' API to perform a bitwise concatenation operation on two binary strings. The program should take two binary strings as input and perform the bitwise concatenation operation on them. The program should then print the result of the bitwise concatenation operation.",
        "api": "bitstring",
        "output": "#!pip install bitstring\nfrom bitstring import BitArray\n\nbinary1 = input(\"Enter the first binary string: \")\nbinary2 = input(\"Enter the second binary string: \")\n\nbitarray1 = BitArray(bin=binary1)\nbitarray2 = BitArray(bin=binary2)\n\nresult = bitarray1 + bitarray2\n\nprint(\"Result of bitwise concatenation operation:\", result.bin)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n\n\nbitstring is a Python module that makes the creation and analysis of binary data as simple and efficient as possible.\n\n\nIt has been maintained since 2006 and now has many millions of downloads per year.\nYou can try out an interactive walkthrough notebook on binder (or the non-interactive version here).\n\n\n\n\n\n\n\n\n---\n# Overview¶\n\n\n* Create bitstrings from hex, octal, binary, files, formatted strings, bytes, integers and floats of different endiannesses.\n* Powerful binary packing and unpacking functions.\n* Bit level slicing, joining, searching, replacing and more.\n* Read from and interpret bitstrings as streams of\n\n==================\n Document 1 \n----------------\n Overview¶\n\n\n* Create bitstrings from hex, octal, binary, files, formatted strings, bytes, integers and floats of different endiannesses.\n* Powerful binary packing and unpacking functions.\n* Bit level slicing, joining, searching, replacing and more.\n* Read from and interpret bitstrings as streams of binary data.\n* Create arrays of any fixed-length format.\n* Rich API - chances are that whatever you want to do there’s a simple and elegant way of doing it.\n* Supports Python 3.7 and later. Use bitstring version 3 for Python 2.7 and 3.x support.\n* Open source software, released under the MIT licence.\n\n\nIt is not difficult to manipulate binary data in Python, for example using the `struct` and `array` modules, but it can be quite fiddly and time consuming even for quite small tasks, especially if you are not dealing with whole-byte data.\n\n\nThe bitstring module provides support many different bit formats, allowing easy and efficient storage, interpretation and construction.\n\n## Documentation¶\n\n\nThe Quick Reference provides a basic list of the classes and their methods.\n\n\nThe Reference section has a complete list of all the classes, methods, properties and functions of the bitstring module, together with short examples for many items.\n\n\n## Mixed format bitstrings¶\n\n\nIf you have binary data (or want to construct it) from multiple types then you could use the `BitArray` class.\nThe example below constructs a 28 bit bitstring from a hexadecimal string, then unpacks it into multiple bit\n\n==================\n Document 2 \n----------------\n# Mixed format bitstrings¶\n\n\nIf you have binary data (or want to construct it) from multiple types then you could use the `BitArray` class.\nThe example below constructs a 28 bit bitstring from a hexadecimal string, then unpacks it into multiple bit interpretations.\nIt also demonstrates how it can be flexibly modified and sliced using standard notation, and how properties such as `bin` and `float` can be used to interpret the data.\n\n```\n>>> s = bitstring.BitArray('0x4f8e220')\n>>> s.unpack('uint12, hex8, bin')\n[1272, 'e2', '00100000']\n>>> '0b11000' in s\nTrue\n>>> s += 'f32=0.001'\n>>> s.bin\n'010011111000111000100010000000111010100000110001001001101111'\n>>> s[-32:].float\n0.0010000000474974513\n\n```\n\n\nThe module also supplies the `BitStream` class, which adds a bit position so that objects can also be read from, searched in, and navigated in, similar to a file or stream.\n\n\nBitstrings are designed to be as lightweight as possible and can be considered to be just a list of binary digits. They are however stored efficiently - although there are a variety of ways of creating and viewing the binary data, the bitstring itself just stores the byte data, and all views are calculated as needed, and are not stored as part of the object.\n\n\nThe different views or interpretations on the data are accessed through properties such as `hex`, `bin` and `int`, and an extensive set of functions is supplied for modifying, navigating and analysing the binary data.\n\n\nThere are also a companion classes called `Bits` and `ConstBitStream` which are immutable versions of `BitArray` and `BitStream` respectively.\nSee the reference documentation for full details.\n\n## Arrays of bitstrings¶\n\nNote\n\n\nThis class was introduced in version 4.1 of bitstring, and is a ‘beta’ feature that may have some small changes in future point releases.\n\nIf you are dealing with just one type of data but perhaps it’s not\n\n==================\n Document 3 \n----------------\n Appendices¶\n\n\nGathered together here are a few odds and ends that didn’t fit well into either the user manual or the reference section. The only unifying theme is that none of them provide any vital knowledge about `bitstring`, and so they can all be safely ignored.\n\n* Exponential-Golomb Codes\n\t+ Interleaved exponential-Golomb codes\n* Exotic Floating Point Formats\n\t+ 8-bit Floating Point Types\n* Optimisation Techniques\n\t+ Use combined read and interpretation\n\t+ Choose the simplest class you can\n\t+ Use dedicated functions for bit setting and checking\n\n�PNG\n\u001a\n\u0000\u0000\u0000\nIHDR\u0000\u0000\u0004�\u0000\u0000\u0001�\b\u0006\u0000\u0000\u0000|>�\u0000\u0000\u0012�zTXtRaw profile type exif\u0000\u0000xڭ�iv#9����\u0014s\u0004�\u001f\u0007�{}�9�|\u0016��%���j�SO\"E\u0005#\u0000ws[��������\\_��].�W���/\u001b/\n�t��7��������~��\\_^w+��F\u001e\u0013����^\\_�����\u001b>\u001e��Y�t����\\_�`���ؿ��}��\u0015E�����}�\u0014\\_\b�\u0013�׶|��>oa������2����.�l$�����ܨ�.\\'�xRH��1��\u0002���K�'��!U\u000e���<�J��P����?��}�ʯgߺR�%��\u0014.�\u001c�x�k1��\u001f\\_\u000f��⻧ğ���/8|y��\u0010�o�����ݽ絻�+%��M}l�yƁ����m���w�y{����@���/?�Z�B�-7��\b7��q��\u0012s<��\u0018��y��\u0016-��\u001d���\n7�diӵ�\u0016�M�ݯ����\\n�΅w��\u00188Y\u0010\u0014�~�7�~{�{�o\nܟև�?!\n�,C��O��!�~�<\u0005����O}Mt�,Z�\u001c�Uk��l\f.:8��݃\u0003Ƙq��g�u�٧ͱ��ʫ����n�\u001a;��]w�}�\u001e'\u001c�t�)��v��3.P���[n���kw��Zp�����w-|t->�ҁ�W�xkk\u001f�\b����ѱ�\u0003\u001do�\u0000����!���:��\u001b4�Jd�E��A\u001d����Xn�ջw�K�\\���o�sN��otΩu���\u001f��C׶�f��\u000ei\fUT�\u0018?\u000e\u001a�\u000f�P-;ҷ\u0012��6���1�N�`�)B��#r\u0018�Ǔ��\u001fO�$�vW�{��[���B\u0011B\u0003J�\u0006��h��k�j��v�\b����>�\u0002�n���\u000e��u�r�2T�SR�s�S�\u00056wu�y�O�\u0016]=5q�\u000b���:���v�Z[O۟ڙ�4;\u0007�xf�`��ۺ9!1oH$�c��,\u0016�?74z:oKi��W\ncM�k��t vM�3�x�rg���4�1�Up�j�җ\u001dg\u0006\u0013,�4X������d\u0013̳&\\*�\\*��@q�b�9��\u0001������gվCa��\\�w�4��\u001f��\u001dp��YJn'L�\u0004 �;�vc�Q�e5�qzJZFd�k07c��m�\u0012�vW��9\u000bjq�Լ���E��K\u001e��\u0005�NS�d�Oh�+2��]&���:z[�\u0011�qR�sW\u001b�\u0019�m\u0012��m�p.8\u0018~oJ]ﮫ�3Ga|�lyڞBW(=�^\u0018�i::�\\���`Hh�mrk1w\u0006��t�cc\n���c�s2��b��А2�ӈ\\Fީ��G����u�\\���9v��\u001bl\u001f\u0004`y�5��U\u0013�Ŧ�� �\u000b�`�����Y\u001f��\u0005^m�\u000eQ���.p¤�k�\u0006����sŶ\u0004\nu�R�\n3\u0010v�\n\u001b6���T��l�^g�C���\u0004�\u000f��A��I�\u0002\u0004��1�X\u000e�8iՓ�F�‹rl��ml\n�b�m��ǂ�2&��Rس�\b՞�\\i(2�z��\u0003t.\b��P\\_��+�~�\u0006\u000f��c���\\m�9�Z�\u001d\u0016q�^\\^�dP�\u0006CN�\n�l�9l\n\u0012�\u0018��ӳ�ƂT���y���\u00172i�J�\u0017Þ�Q�\u001bE݋�ھ�.Vy��og��\u0005���Y+;\u0003#\\*u�\nz\u0007���t\\21��T����\u0006�R��԰�C�\fe4x%vA\u001d9�3e0\u0013ZQN\u0002�\u0018-K�5�α\u0007�2pt4��9oN |�^���\u000e�UI[����&����\u0006N.�s4\u0019%P��L\u0015r�\u0006g\u001b\u001cAI��~f�p�Eg\u00188�E��y���b�/�Ƥܡ�\u0002�g+�\u0002eĮA\u0003=Nf�\u0016����\u0016\u0005ݹ\"P\nܬ�L���:'����\u0014\u0016�\n\u0017\u0016X8��Sf\u001e\u000f���LJ�[\u001d�@iE�M1L��+<�\u0012��J�3�z�>��\u00039�GNͬ�2$\u000f�ځ�L���)UȜ hh˜p\u001eJ��N�}��໲$x\u0002\u0002D��4�o����Ƀ��\u0013G\u000fa�s\u0010\u000f7a�3�\n����2\u001au��\u0019�:��\u0007p\u0004��4+\u0005��·g�\u000b�\fmp�\n�F@�1&D�UR[�m\\*��\u000e�\u0002g�\u0000\"\\*�)�\u0011�\u000f�ˉ\u00025Ol��\u0015�2F�!�\f�\n�\u0010����<-���Sׯ�\u000fXS�\u0006��U��2\u001d\n^\u001ab\u0002��p\u001f\u001ďB��v@x���\u001e\u0003�\u0005`��Sl�@��9��5B>��\b\u0005\u001b\u001c�&�S�� )\u001b0)\u000f\u001b�3R��H\u000f�)�nz\\*��\u001e�'/���[�w\u0015����Y�\u0010�\f�`�ދ�\u001f#\u0004|�Z&�\u0006\u001f�\u0001�!\u0013W�a2/��2�\u001a�ؐ\u0006C0fNF�`\u001e�XvYD��P)\u0013�\f����\u0013��B-\u0018�Foz�G)�\u0001�\u0013��`j�����\u0014�=Q��!�x���O\ba���KR�2��[wl�w�K\u0019<5��Y�v���+�\u0003Z�X����\u0015�|\u0007�\u000e��C�CpY�)s\u001b\u001d\u0016.c\\\u001e2��\"�u2�l\u0010U��{\u0005����-V�\u0007\n&�\u001b�-\u001beƲ�A�\u001bn\f%��=�K���b�LfF���\f>3�q儖�lbTT�o\\_2��\u0010M��u\\*�l\bK�1�.�6\f�|?#��4F\u00026S\f\u0016�a�p��N�+q.��\u0017�\u0007�\u00141�q%I\u001d�6]�χ��驗m��7\u0019?�\u001b�淈�\u0015]���F\u001f��f����X��ɢ]\u0005UeJla\u0004yQ��w��\u0003r�j2�\u0002��d��\u0006ե\u001d͸\f�\u001d�\u0006J�,��\u0012n�f�ClI\u000b�/�-��x���\u0002�>��UiA~\n\u0000�\u001e\b��\n[!e�+�Uf�uZ\n�3f\u000b�|h=�Ʒ�F�%\u0015�u(6�\u0019(\\*R\b�P�\u001d�(\fV�\u0003n1D\u000e\u0014��\u001a���� �T�/�sF�P\u0019�,�g\u0015QDB~1Nj(S��X�\u0012C\u000e�� /ߘ�<3���\b/o\u0001��!yC�v\u00145��Y\u0010I \\_\u0001�/�\u0000Z��D�gJ�y3pD�!Y:U2\u001c\\�\u001cL�\n\"\u0018��%,��\u0019^Q�m\u0010\u001cRY��l6���߯�1��m�\u0002P\"n\u0010��%�\u001eD6H�\u0007�0F5=a��m���j�z�\n��4��W��\u0013�\u0005@\u0003l{��C�àG�\u0000�a�&N���f޺?9���I1˻D�������I�)�:E�$��\n)|m\u0014�1�'�\u0000����Q\u00167oD� Jv�B讠�\u001d\u0004\u0006�<ލ&�\b���\u001b\u0010\u0007�b\u000b\n���p��\u0019�\u000bC�I\b4�Pw��\\_=z\u0014 /^O�Pʖ\u0019\u001c�+\\*\u0011��{�\u0003��o��\u0018څ�����xY\u000b�� ��M!����=\\_MZic�Kd��Ҭ\u001fa��Op\u0015��q,\u0019\\*\u001fb��o\u000e\u001b\u0017ҔӐ ��\\*\\�\u0007�~�yH�d@\u0018\u001d�F�\u0015�\u000e�\bXg���\u0012\u001b1\u0012�\u001d�j'Tr\u0014��d���ȉ\fǷCB�oF\\\u0011\nܫ��y<\u0016<��R��\u001a��X�Q����J$v\u0011��{\u0015����8\u0011z��K0[�\u000e�\u0006��m��i�\\*]\f��Ɛ��ն(U�\u0013H$�\u0006<&�%��#��#;\u000eg�\n,\\_�h� \"\u0000�w2٤Yo�n�\n\u001d�\u0006�\u0001&�\u0004@W�\u0000ye\u001a�\u001c�7\\\u0007�[�Z��\u001a\u0016��\u0016\u000b\u0012�&�$\u0018e\u0002Hh�3ʫRQO�)�?�\\P\u0010�J��'b\u000b(ꐷ\u00018�:0 $h\u001b\u0006���8��`\u000f��}���OU�����uM�3i�$@��UC����B�;�SQ(�L |ۙM��)�\u001f�HL&���'�W�\u0004?�}�o�\u0004�:�v�\u0019>\u000eZH�A\f~\u0019Ck��\u0006eY^y�ǯ\"]�G��8ȋ����$�x�^�$MF��q.c�%8枪\u001d�\u00047Ln����F�V���u��\b�\u0013\n\u0011�!�\u001d#�p\u001c,\u0017ׁ�@�6dmJ��]Ĳ��\u000e\f[�N���-\\*aK\u001aTI�r#���k�\nM�\fw\u000f�Ȅ���&ÕAԍ�v�\u0005b�\u0003�&�5��9\u0010nƐ�K�<\u0011�Bm�\b\u0016Q�\u0016�J\u001c�3\\*d\u0012\\�EK�\u0015a�\u001f=� #�\u0013\u0013�0��>E�k�y�I\u0000�e3\u0010\u0002�d���g�\u0019�t�S��V1\u0012�`��,�i����/�ԩi��\u000f���@���\u0018\u0019\u000e\u0001��\u00179�L����+\u0014\u0011jY��D\u001e0]�Ƌ����҄gk\u0015��\u00156jq��\u0010�j������^i������\u001b���H\u0007\u000eh�A\\}vD|A3\u0010][�\u0001�\u00160\u0015#\u0017R\u001d�\u0015�C�\u0006�� �+V�.b8B\u0010aPx{�W\u001b�\u0002�\u0016��$��P\u0003]�h\u001bgK��{ƸgN|�a\u001e�\u0018LQ�9�\f�IK8\u0015�f\u0011�����'k\u0005\u0004ȵ�LY�\u000f�=�Q!$��m\nݤ�e�]\u0014\nˇCd6��8���a8V\u0010On\f-z�\u0001�S�L��N�\u0007\u001fX��D}p[n\b��\u0010o�\\_��œ�\f�����K�C�\u001e�\n]/R2Q�RP\u0007|�ת����\n�NXV�\u001a�\u0000�!P�\u0002��2^G�#3\u0011\u0018\\_{�J}���beE>�74�/j�)eh�sS\nb\u0001�ȸn�ܤ�\u0002�\u001f|g �!�\u001a\b�\u001e�%�#ǎ�\u000f��nE!���\u0007JP��\u0006\u0011\u0017y}\u0019\u0003}\n�=�'Lϧ<�\u0007�T\u000f�N��\bn��Y��M3���\u0000���M�\u0001�IÄ ��1H\u001e��`tX\u0014y��59 ��\u0017Һ�'\u0003�\n\u00173n�\u0019\u0011�a+=�<�b\f��\u0010a��w|\u0012�p\u0005���1\u0015d�{�� �\u0011�\u001e���y\u0002�]\\*�\u0010#���!c�k��I$�^P\u0007��\u0005��\u0013��qϜ����\n�?d��0��?��\u0002 �t\u001e\b����0/Dw%6\u0016�6e�\u0012�s\u0003+fX\u0001\u000b8?�B��vK|>1�Ua6%\bPw͜����D;��MF)0��<��N���N\u0013��cgPW�>Y�Y�$~w�\b'�\u000f-<�n#�Oڃ�޻��{2\u0010ԪO��%��x\u0003b\\�ໃӯ8g\u0012-L\b� �vKYb�gT�^T�]^��Y���\u001d\u0003�\u0018�\b���\u000e�\u0002\u0019H�����$��Ơ��ſ�V��0�~|q&���^x�\u0019\"�8ƺ�[����>V�L\u0017r�ǭ�w���L\\*�u�<�Ӯ\\_�N�ʥP�\u0014�\\GU �t\u0010�t�9��:���\\_�?��G�$�\u001f\\*T�\u0019��g�\u001d�'\u0006�6�\u0015?�|��RK����Hd��{#\u0010�X@�gt�up�?GǻN��\u0011��\f\u0015�\u0013.�n0�\u0000�O�yp���NY����ğ�H��\u0005\u0015\\*񳆯��� �Sm�0�������o����A���\\�'4|��'$�\u0017\u0014~\u0007����;\n�\u0003\u0003��\u000b\n��?��\"\\_0�B��\u0004������\u0005�{b�{��~O\f�\u0017������=1�U^ ���\\T:����~\u000b\u0003\u001c 19ҭ�Iɤ^C\\_\u0017����|z\u001cW���Y��e��AyJ��\n3x�P����)8&��6�\f��My=��\u0002JiD!�h�S�^����U\u0004�&���[uG��8=\u000f��+�\u001f�᧍\f?��G�=ʣ���Z����B\u0002جu�\u0000\bu+�:'��ݺ珷����ArdM�kb�s�~�no�%��ø�n��rc}7K&W!�\u0015\u001fS����\\*��t3�\f���S�yO��b�\u0003\u0019\u000b+��\\���c����ˌq�>\u0003ı���\u000f��ߏ�\u001f���os�\u0007�#` �>\u001f�\u0000\u0000\u0001�iCCPICC profile\u0000\u0000x�}�;H�@\u001cƿ���T:4��C��dA|�U(B�P+��`\u001e}A��$��Qp-8�X�:�8���\\*\b�\u000f\u0010W\u0017'E\u0017)�I�E�\u0007�����\u0000�YU4+4\u0006h�mfRI!�\\_\u0015¯\b!�\u0018xLI�ẻb\u001a���\u001e\u0001��%X���?G�Z�\u0014 \u0010�\\*�i\u0013o\u0010Oo�\u0006�}b^)K\\*�9�I\u0017$~d���\u001b��\u001c���lf��'\u0016J],w�R65�I⸪����A�/�K&W\u0005\n9\u0016P�\u0006������n��ĸ�\u0014I\u0002=/��1\f�w�V�q��\u001d�u\u0002\u0004��+��5��O�\u001b\u001d-~\u0004D����&�\u0001�;���!��+\u0005ir�\"�~Fߔ\u0007b�@ߚ�[{\u001f�\u000f@��J�\u0000\u0007��H���}����ۿg���\u0000��r� ���\u0000\u0000\u0010�iTXtXML:com.adobe.xmp\u0000\u0000\u0000\u0000\u0000xpacket begin=\"﻿\" id=\"W5M0MpCehiHzreSzNTczkc9d\"?\n\n\nScreenshot\n\nxpacket end=\"w\"?���%\u0000\u0000\u0000 pHYs\u0000\u0000\u0016%\u0000\u0000\u0016%\u0001IR$�\u0000\u0000\u0000\u0007tIME\u0007�\u0004\u0010\u0007\\*8 \\*\u0012�\u0000\u0000 \u0000IDATx���}|U՝���Ivr�D\bID��Td�(\u001a���\u0003\f\u001c�:���Ѷ8���\nV,&&��4���X{8�X�����\u0006m�#��L���5(�\\*`x�O\u0014�b}F�\u0003��$'�ɹ$(\b\" 9g�����z�kf|i�>ߵ��k}��k�\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000H�\u0010!\u0018����x2˳s��U�W-#�\u0000\u0000\u0000\u0000\u0000\u0000��F\b��D�]J\u0014\u0000\u0000\u0000\u0000\u0000\u0000��$\u001c\u0000\u0000\u0000\u0000\u0000\u0000�`$�\u0000\u0000\u0000\u0000\u0000\u0000�\u0004# \u0007\u0000\u0000\u0000\u0000\u0000\u0000$\u0018I8\u0000\u0000\u0000\u0000\u0000\u0000 �H�\u0001\u0000\u0000\u0000\u0000\u0000\u0000 F\u0012\u000e\u0000\u0000\u0000\u0000\u0000\u0000H0�p\u0000\u0000\u0000\u0000\u0000\u0000@���\u0003\u0000\u0000\u0000\u0000\u0000\u0000\u0012�$\u001c\u0000\u0000\u0000\u0000\u0000\u0000�`$�\u0000\u0000\u0000\u0000\u0000\u0000�\u0004# \u0007\u0000\u0000\u0000\u0000\u0000\u0000$\u0018I8\u0000\u0000\u0000\u0000\u0000\u0000 �H�\u0001\u0000\u0000\u0000\u0000\u0000\u0000 f\u0011\u0002\u0000�Kk}�d��͛t�c\u001f�����]�̬�rF��\u000f�\u001b���\\�\bQ\u0003\u0000 @c���\u000b%-�{�I˟�\u0018w���v\u0019���j��?���B�\u0000\u0000�&D\b����9��2��J�'�7��:C�c�t��8�����6��|q���x�h\u0002\u0000�˱�Y\u0012.���5����v�u\u0012k��Rj'�\u0004\u0000�\u0002�;�D\u0012\u000e\u0010�u���Dg�\u001c��u��-��ٙJ�mD\u0016\u0000\u0000\u001f�\u0003�� c'7:�\u0013���߳Ox�-iߵP)���\u0002\u0000����0��C�\u000f��r��K���v������eD\u0019\u0000\u0000��\u0002��Z�t�kH�X {�ru��{�2\u0000 �8�\u0001��\n�k�,Q 8\u0011\u0011'z��E�m�k�'\u0010m\u0000\u0000\f\u001c\u000b�?��D%�DD��9k�}�y�H\u0003\u0000��$\u001c��\u001ft�Zs�\u0013;��D��\u0011��q#�i��\u0012u\u0000\u0000̱����H�~\u001b\u000e�u��e5�\u0013q\u0000@\u0010��\u0003pT�f��t�Y��2\u001d7��z\n�\u0007\u0000�{��o�?�R���^�����&\u0012q\u0000��! \u0007�3i��vb�?�Eَ\u001b�E\n\u0000\u0000��X����u{��^�\u001bn�T߿�D\u001c\u0000 PH�\u0001�L�\u001by�Ӂ?��\u0000\u0000��s��~�\u0013=w�W��[$z՚� \u0000@P��\u0003p�w�\u0003���\u001a�։SuM�bj\u0003\u0000\u0000\u000f�ñ\u000b\u001e��\u001a���k�s�\n\u0000@\u0010��\u0003p\u0018�uF26\\_��\u0004\u0000\u0000��\u001b\u000b���Ϧ\\�[8��\u001a\u0001\u0000\u0004\u0001I8\u0000�\u001b;�٨�@m�B\\*\u0005\u0000��q��n1�Z��-\u0012V�\u0001\u0000��$\u001c��\u0007ޭ\u0013��t={3��@�\u0000\u0000�\u001c���ˌ����U�\f\u0000��H�\u00018t��٦]�\u0013\u001d�Dk=��\u0001\u0000 2�k�%yy@\u0004\u0000\u0000��$\u001c�C�Ol0��H�\u0001\u0000�\u0004��i\u001c�\u0004\u0000@\u0002��\u0003p���mr��\u0017��7��\u0001\u0000 ���S��6\u0013?�\u0005\u0000`\u0018H�\u0001����@\u0010\u0000\u0000H0�:��k��\u001c\u0000��H�\u0001���ڋN%\n\u0000\u0000$X�2w�����T\u0010\u0000��H�\u0001��7�l�\u0000\u0000@�g\u0007iac�-�h&\u0015\u0004\u0000��c�\u0010\u0000\u0000\u0000\u0000\u0000\u0000\u0000�E\u0012\u000e�/LɈ\u0012\u0004\u0000\u0000\u0012m ���k��`#\u0015\u0004\u0000�3�p\u0000|�t�{MD\u0001\u0000�\u0004��~�܋s�D\u0005\u0001\u0000��$\u001c�C\u0014���ya�\u001f��v\u0000\u0000H�������'�\u001e\u0000����\u0003p�����F^X\\_��p\u0000\u0000$�R�}S��P\u0015\u0015ORC\u0000\u0000?# \u0007�P��~j���r\u0000\u0000H���W�{!�)\u0003T\f\u0000��H�\u00018�R�ʹk�Ox�-�T\u0017�\u0003\u0000@\u0012�>zȴK�Z�֛�\u0018\u0000�ߑ�\u0003p\u0018;kS�Q\u0017Ծ�\u0012j\u0005\u0000��PJm[�g�!���;?�f\u0000\u0000~G\u0012\u000e��\u0003݊kkM��s��z�Z\u0001\u0000 yNv�\u0017�r-v����\b\u0000 \b,B\u0000��\u0003��\u001f���.x���ֿ�� �Sk=ID�Ų� /�u?PJ�J+\u0006��ޜ\"\"\u0005 �7]7&\"-J)���/���'�}���N�����\\_��\u001a\u0001\u0000\u0004\u0001I8\u0000G\u001e�V�������[��5�9�ߤ��?a�5w.qb�\n��&���U�Gh��Q�Uw-u��5$����\\_Q��wD\u001f���k\u0017�Ll��\u0012쬍�8\u001e\u001d\u0000\u0010\u0014|�\n�3U]�(�U�KƷ�\\*�έA��\u0019g&}���\u000fӂ��krK\u001a�]�[8�Q\"\u000f�PJ��V�t�ʷs��YUT<IM\u0000\u0000��$\u001c��OR�J�������i\u0002\u0010\u0000\u0000\fRJ5�Y\u001b��?���u\n�1\u0000\u0000��$\u001c�ϕ�D�}RK]��$�\u0000\u00000���xҶ\u001a�$m,���ͪ|ٵD\u001e\u0000\u00104$�\u0000\u001c����Ђ�}�\u001etߤ�\u0015a�J\u0000\u0000\f���b[� ?,���8�\u0015p\u0000��\" \u0007���~݂�m��+ \u0019t[�EA�\u0003\u000e\u0000� RJ�n�\\* ٹ;F}�ы�ۥ��$�\\*\\*�!�\u0000��\" \u0007`�\u0003��\n\u000e��F%af[�3��JBJ�6�\u000b\u0000�\u000f�\u0002e��jSUI�����[����m5\u0016��w���(\u0000\u0000��g\\_\u0000F8\u0000��&\u0011�Ik}�sr�Ko�����ڹ[�Pe˔�H\u0013�\u0004\u0000��\u000e$�t͝K��y�: ��~n����k�D�? %\u0000 E��\u0003p\\�R/��!o��օ\"2Y,+W\\w�п�1\u0012o\u0000\u0000\u0004h,P�b�����X`�X��\"\"�\u001f(�^e,\u0000\u0000Hu$�\u0000��`|���-D\u0002\u0000��\u001d\u000b�\u0016��D\u0002\u0000�O�'\u001c\u0000\u0000\u0000\u0000\u0000\u0000�`$�\u0000\u0000\u0000\u0000\u0000\u0000�\u0004# \u0007\u0000\u0000\u0000\u0000\u0000\u0000$\u0018I8\u0000\u0000\u0000\u0000\u0000\u0000 �H�\u0001\u0000\u0000\u0000\u0000\u0000\u0000 F\u0012\u000e\u0000\u0000\u0000\u0000\u0000\u0000H0�p\u0000\u0000\u0000\u0000\u0000\u0000@���\u0003\u0000\u0000\u0000\u0000\u0000\u0000\u0012�$\u001c\u0000\u0000\u0000\u0000\u0000\u0000�`$�\u0000\u0000\u0000\u0000\u0000\u0000�\u0004# \u0007\u0000\u0000\u0000\u0000\u0000\u0000$\u0018I8\u0000\u0000\u0000\u0000\u0000\u0000 �H�\u0001\u0000\u0000\u0000\u0000\u0000\u0000 F\u0012\u000e\u0000\u0000\u0000\u0000\u0000\u0000H0�p\u0000\u0000\u0000\u0000\u0000\u0000@���\u0003\u0000\u0000\u0000\u0000\u0000\u0000\u0012�$\u001c\u0000\u0000\u0000\u0000\u0000\u0000�`$�\u0000\u0000\u0000\u0000\u0000\u0000�\u0004# \u0007\u0000\u0000\u0000\u0000\u0000\u0000$\u0018I8\u0000\u0000\u0000\u0000\u0000\u0000 �H�\u0001\u0000\u0000\u0000\u0000\u0000\u0000 F\u0012\u000e\u0000\u0000\u0000\u0000\u0000\u0000H0�p\u0000\u0000\u0000\u0000\u0000\u0000@���\u0003\u0000\u0000\u0000\u0000\u0000\u0000\u0012�$\u001c\u0000\u0000\u0000\u0000\u0000\u0000�`$�\u0000\u0000\u0000\u0000\u0000\u0000�\u0004�\b\u0001F��z��\u0014�e\u0015��>F���D▤��EDd` &\u0012rE�OB}-��D�K)���\u0001H�~R$[,+W�\u0019\u0005\"��q\\_y���\u001f�\u0014��P?٢�z��\u0001��\u00072Dd���\u0011 \u0017\u001f2^:�\u000f�X���\u0017�הR}D\u000e�o���\u0002��\\�x���\u0007�\u0012��'��&\"�\"�R���a��`���\\*\"ْ\u001e.\u001el�\u0007�^V�Ȁ+\u0003�.\u0011�\u0013\u0019���\u0007C}����\u0018M$�0�N�,I�\\*\u0011+\\\"�E��\u0016�n�,\u0010\u0011\u0011�\u001d��>��?Giu������/g�w�J���@l�Hl����J�6\"\u000f�\\_�d�YbeΔp���nQ�\u0013\u001d�\u000e�'���t\u000f�'\u0017�퓓3[�$��3��4IO�R�e�\u000e\u0018�\u001f�HF��\u0012ʘ�nڤ+7v\u0016\u001c�\u000f\u001c�x���Y.�o���\u001b�����J�#L\u000ea\\�òfHZ�\\ e�}7���\u000fk������f9'�)����%�\u000f~�3\u000f������'Kf�TB\u0019%�٧^�L�\u0018i\u0019�\u0018q\u001b<�\u000e��tȤ������\u0018�\u0010�#D\b�}\u0003Ɠ]fSU����k�\u0017��}q{�\u001b~�>γ�\u000b\\_k�h�/��zR)�\"-\u0012~��\u001b�\u0007���rQ~���w��>>�����RIϹ�����xr\u001d3���\u0019i����X��w?�ʹ�y���\\<�u�d|�T]��{3\u0015������1����-JX9�;���|�\u0013�mﵵ\u0017J���o���u��S/+n����7Ko�)������gsE�\u000b$�}��sԿ\u0019�.�zw�!����\\*\\*��f�yxh\u000fS�$��\u0013�������5�\u001c<\u001e�\u0004��&���\u000e��y��r0Uy�3�M����� o��~�\u0002��D��5��%=�Ntƕ�^c�\f��r�����g`x���M�7�줖yYq�ܴl!�f0'������M����LD�V�\u0017H�Óg`�\\_���:q�1�m����:��\u0016j(����\u0012\u001e����SnL���X�͸v\u0019׽�Rܮ߲�\u0012��A�0\u0005= �kV��9g>��j����z��L����ӌQn\u0017��\u000b%-g��B� /l�������dL(t͝K$=�K\u0012\n�\u0013Y��Uz�7���k%�o��,>�Oz�����j]s�e�w�J�&\u001d�j\u0013Eo�η�AU��7;Ϋm��g%�\u001c���ܦ[� �@���׹1���h�gH昈��\u0012�D8\u001e�Iߋ�r�#I�\u0014f��3g���^Ta�Ɖ������,M��\"\u001e���?��\u0015�|�(Ȑ���'�\u001f�w?\u0010���(�v&�wM�̂p�3�\u001b����R%����%}\u0000�!Z�%Ⴋ���7��ڧdD�4��Jq�~�̶��.���%I),��:V�\u000e\u000fI�a\nb\u0012Nk=I�\u0005�~�؎:���o\\*���j��}��{�\u0013��2h�~eÓ�GZ\n��dߛV�t�=���S%<���q]��\"w��\u0012k��i��{����EP\u0000��9�X=�kV�N��\u0007�7\u0011��\u000bU��[ �\u001b�ޛ3�\u0003{=z%\u0019I��;7�7��J��3�G�W���\\%�+�m���D���Z�ޛ3c��m}�}�����\u0015I>�h�!\u0019y���gf}G<\u0018�����o��W���/\u0013���0a�,?�`���ܦ��ɿvZ'N\u001d�攦\u001b\\�2qkiu��ٛ����C��P�k%�I�,L�\u001b#\u0012p\u001eI�:KD|��ӵ�\u0017��<�i�c�\u0004��\u0014\u0011q��\\_.\"�\u000f��e��y� ��I�y$#�DD\u0012� �\u0015>KbIl烟��j\u0012Nk=E�K^q�\u0014�KGj4�d'�\\*s�$�3����\"2�I&][��9w�\u0006��\u0016����>�J]��:���\u0003�n�`~\u001a.���5}I��{'\u0014�Lؐ����:�q�^o\u0013���1\u0014\"\u0004)ٹM���\u000f9m�K�5���鞳���y����M��q\u001b��\u0002��>rvSh�f�'G�'5~��=�^D�u���\u0012k��\u0019H!a\u0006���u�Pߠ���7�\u001e�\u0012��8�OZ\u00062�8�\u000b���3����wg�z�1tϭ\u0011=约/\u0011�%��ޛU�w~B+5n�u�\u0014�lw�\u0014�\u0007���Q�7Koǿ0?M-i� �:���~긑ם�ɥ��۝謕�\u001b��u�|��\u0000�H\u0003��g�\u001b�����ORf\\縑�t�ڟk�'�\\*0�\u0006���i�0���9\u001e��;�8\u0017\u0016�O�.k�(��\f]���\u001byaS�\u0012p��\u000f�RZ�\u001c�ZϦ�\u001aѷ����ɸ�F^p�\u0014�����\u0016槩�$\\�tp��\u000b\u0007\u0007��U�\u001e\u000b':����9�k�\\B�\u0000 \"���o\b�c���+\u001d7�o�Z�\f\"�Q\u0013�k�`�7������\u001d7���ñ��~+��Jt�������K\u001c7�k�ߣ��s#���\u000f>Nk�hܥ�\u0004}烏;n�%��9x~Z���\u0011|$�����7�Nς\nD�S�]켇����q���D\u0003H�>�������\u0013=w)�8B?�5�\u0006Ǎ���6���p]/v��\u001b�$��5�M> 2S\u0006hSFJ�g�/t|��\n&C.x4�q��-\u001eZ\u0015w\u0016m6�c��{ot��{^�(m���ǿ�zc\\k=�h\u0004\u0017I�`O.\u0017:n�3\u0015��\u001e�w���q#/��\u001b�\u0006�b}d�\u0003\u000f9=\u000b�&\u0012�2(<���g\u0018\u0014b4t'���I��\u001f����������\b�O�E�(>��\u0015�#�BAk}vius<Փ!�\u001byI����q���\u000e�����h|���\u0005⸑�i��E\u0012.���\\_��m\u0018\u000f��K�\u001f\u0002H��Q���Ǉ6i�1�\u0014�\u001f\u001a\u0014���h�8$�`������G�Zs�\\_>?=1����4��i��p�]K\u001d7�\u0002�54\u0017�נ�\u001fx�H$lnʋ�\u0011�ɕ\nO��\u0016$xH�\u0005or9ip��\u0013�\u0011tvnd��o�D\u0002\bl\u001fY⸑�^\u001e�1(�S����H`$�R]I/4�v±O\u0014�[�t�Y�x��tiT&\n��M�<]ߠ��y|\u0005���[��%?��8�\u0018�q�С6��\u001c�GZ\n�q#�Z�D#8��\u0004������q#�\u0013��}\u0000ϼa�m[��\n\b�@��\u0012Ǎ� \u0012��O�P4�z�A!�\u0017J+>�>���n���F?���,��3Ҁ�e�鵏��)��퉎q�趭$�F��\n��ơ6�1�r#���څD\"\u0018H�\u0005��[��:������5��\u0015�\u001d\u0010�>R�\\_���N3(\u0004D$�V���ņ�M>��X��;�`\u0013��\u0012�{�7Ɲ\u000fO9����y@iu3��\u0011��t��y���F{�ճ`\u0003[�\u0004\u0003I� ttu�78�s�D�:�ڻ+�\u0004��>����H�%����T\f�)�=�-0��{����V�1\u0001'\"\"=�$�4`�J��g�\u001b;\u000b��a��gM$�;���,Ǎt��}\u001c�HĘ�{N����D��H������7�8�s99%�\u0013̹5�n�ω\u0004��>���B'v��D\"��d��\u0007y;�c���\u0004�\n���}Ī5���\u0013�C��ٟ�D�~�V���⛢���0�\fd�ʆ'I�\u001d��k��F^\"\u0012 \u001esu�V���7�p>V}����:��x���Eg\\Y}�\u0006\u001e�\u0006�Z\u0017�~\u0012���9�$�p,����熲O��>b��\u000ea8�P|\u001f-�@�\u001b���w�\u0013\u0014��G�|��UI#�HK�躆���紿�;�8=\u000b6\u0010�$��\u0006\u0013q|��S$�������]���@$Ѻ�E���$�\u0000�\fP�H+QHr̻���;9\u0001\nG���\u0004�z\u000e\u001f�j�s\u0003�ax|��pf�yP�\\_\u001c����ֳ\u0007�(<ۢ3�k] ���yi��\u0015N켇�D��eW�z�]|\u0011�C$�����?�kgOq:�H�\u0017by�w�o&\u0011\u0007\u0018�G�<�}�ՠ0v�CZ��D\u0002�%�\u001aH�R��^��R���@\u00043�J8Cyqlm�!�Uw-u�f�:y��m�\u0005�j�\u000b�ħ�u����̭!\u0012\u001e���y\nZ�\u0019D�\\_H�����g�k��D�;��g�}�sL�\u0001S��Uw-eC`�'+��Z�ID\u0002G���n2����{K]�O\\_��\u0005#�\u0003� g(׃2�\u001f�q��:��\u001a��Q����ྴ��\nN@5�]n%A�/$�|���.t��n!\u0012�{�/G��!\u0011\u0007�8\u0018a�a����N\u0014pD�={�Y\\t�����,���\\*0�\f���3��'�D�\u001a{��\\?0�\u0015� A��\u0006M\u0014�\u000e�a\u0005�9c\\_\u0012ľB\u0012�/\u001d��\u0013���,������\n�\u001c� |�j�w����\u001aG\u0016�|+��u\f|�9�����:��u�hPf�L�K�g\u0014�\f��l\u0019Ƞ\u0006\u0012��yC��:Һ�\u0012�\u001fj\u0013�zY��D�\u001f,B���NOu�\b�\u001e\u0013\u001fĭ\u0013���\u0006�ʖ)�\u0001x�W�v\f�\f���\u001e�om�����\u0019�o�!�g�w�N�Չ<��Y�r�T\u001f-\u0014\u0007Ij��#>8��u�|߉\u0006.9�Ms�^DD$-T��>;P+=M}�\n�:\n��Xk��^�(���v��)�����\u001f{�h��$�?:z#\u0013p�i}�8��V�}�VD�VJŎ�a��H�XV��\\��M.\nL�tͼA׬n\u001a�\u001a����Ɲth\u0017��S>z��@\u0012�ID�s\u0004G)�MD�\u000f�W��֛�鶝��L]O\u0002\u000e�\u0012�'}���:Éή\u000e`4?�A\u0019)����eLޝ9�����>�p�L�k�)=\u001f����f�P���\n$-�K�{��~{�t�>��~��,[�J�Zk=�q#[���.+n��]ˤ/����\u001c����:CDN����owhj��h�yc��\u0005��V�t�`����19%�G��x�������ħǎ}\u00029 \"m\"��:�C���Nς�};�������e�T3�\u00058�PB�\u0007���%Ň\\_x��-���<�TWS\u0002�QeK�D�LDD�?��5}�I5mgm����x�6�û�x�W ���݄�Ĩ�\u0003J�Ѡ���'�K�n���\u0011�j����r�?��g�\u000f��\u001c����O{�9�\u000b\u001e�\u0014[\n�+w���z�s��!�}�\\*�Z���~���\u0013��Ed���\u001c�˦��ɒ?�a\u0013\u0016�8nd\u0007]��H��;��m����\n�Ծ���\u0018F��́\u0007�ֺ@�Ew�6�<�\u000eOk���b\u0016\u001f�\u000f�\u001f��dN��>L�`�����i\u000b�����-\b5��${\"^��+D��d������3T�z�\u0006�#I=e3��\u0004��'˟\u0013�i�z��w�v��i���/�垺�3O����z�]`�\u0012p\u0017�˿~��B#Ib\u001c�9Vqm���\u000e�\u0005�/��'�z՚k����I�\u0006��|�qg�������s�R/�f[=�\\v@\u0006�s���\u0019��}�\u0013;�a�B\u001c 3P\u001c��:])��ɣ�R�DdpB��\u0017\u001d7�\\_b�>fF���\u001e\u0011~si������;�Y\u0004#Y}Kζ\\_��Qߧbp\u0012���3�uj��1V�+։�:�ꮥ^�\u0012�TU\u0012:�燝�q��z.|������~s��Ve+\u001eI^�����.|�I:��[�����+#��\u001f�\u0010���zG\u0013��HJ����`}\u001ca���BU�Xה����NDB��=t/\u0006> �kV���i��3�|�f����R\\*����/\"�\f��\\�>�a?�\u000f�G�`�J��\u0003y�^qZ�,N{�����\u001a��k]�\u0014���j\\_��\u001d���Az+��>����)�z��(\n�g6�[�1��� �e~�Mu�y\u0011ߏ����[�U\u0017\u0017s���'���oE|�rϾ�x��+KƷJ�U�Rz�����g':떠��#}���wd��JB�������rh��w}ƪ5�9�sꍌ�Pr2����S\u001c7��\u0017m5��o��G��:C2��)��>�j�^6��x�\f���S\u001f��Ґ� 8���q�\n�B��x���t���ZO�e\u0001��Y\u0002.w�\u001d&'�DD�үL���s\u0012\u001f��j�%�����f�l]�TU\u0012j�\\* \u0005-\u0001�\u0014�\u001f����TU\u0012�:\u0001'\"��\\_��Ԅ� [@$�����'��-U�՘n�!yJ�>U����Vc���m5=Lj# ��\u000fW�q�m5�+����5+��m�\\* �V�4���W��Wha@��ն'�g�ժl��I��bSUa\u0000�(\u0000\u0000 \u0000IDATI�oƵ'&\u0016�g/WeW��ֈc\u0017\u000f̉�v��MU%!U��Z�\u0015�i�Y\u001b��b���$d��0nՙ�zJ �Xu�|���)��7w�Vc�\\*�涃\u000f\u000e4|L\u0016SeK�l�1�b��IU$�R��:��V�j�RJ��\u000e�\b\u001d�˶՘i������4ѫ�\\Gk\u0003\u0012ko��\u0007�]��\u0007N&���u�\\_��-U��<�\u001a�Q���K\f�00\u0010��O���!MU%!?�\u0005H�L��\u0010{�]W�{��\\*��\u0015v�f���13v\u0005��j��8�����g��3��{���k~��vg�۞n��\u0012$\u0006I�\u0014c[���U\u0017\u0017+�|?\u0000VJ���..����&^��=�^k=�V\u0007$�\u0013\u001d�^�ն�ȷ�f�5��Y\u001b\u0017������c���B��\u001cն\u001a�ו\\_Ⱦ�8���~\\_]}�كO]�|�\u001d��7L��߷�\u000b\\��e�62�h��xض\u001a3��� �J����%Yv�\u000eNRM!$�R�m5�)�^\u000e��RJ5�Vc����\u0016N��\u0007$��:��H�ٛ����\u0010UQ����\u00137%#\\*C�^�\u001d\u0005R�̬�\u0007�~3����\n�' g��x���\u0016�\\*��\\*�s��o6gLRI`�W5�����1�����eU����u��Q�1��~ݶ\u001a��s�\u0006�p)�>᝷��\u0018t\u0005�7\\*��l�1sf�~��k��\"�Z��\n�\u0004���f�����|��zq$�8{�K�\u001b�\u0002\b�7���Ruw�\u0005�}\u0004�m��xX�}���|���'���֛p-/�O4(m]�q��j᝷�2\u0000>CUT�VJ5�9[�x~!����ׁ\f�5j\u0015�P\u0002.����R\\*vr�֬ˊ�~7�H�\u0005qИ��2���U�&�>�V|��KƷ\u001as=��~1�\u0013���hw`�үL��\u0001p N \u0003�=�\n?{\u0005 8\u0004�}[�g\u0004�G����ח�Dg\\���i�\u000b���\u001b\nj�)��;h�\u0019�޷=�\u001e�~� PH�\u0005�����M�\u0017} �\u0013p\u0007T]�(�\u001fr��\u0016\u001379\u0005pl� w�¤l�����\u0003�@�Q�Hű��\\��H ��{KU\u0010\\_�(�޷s���\u001a\u001e����\u00194\u001e\u0019�� ���uL�^9yJF�`\u0004\bI�\u0000�(�]���6\u0012p��4}�1;�j�gP#���>�7\u0015\u0006zۚ�JBMU%���-�\u0014H�$���~U��[�\u0004��p�\u0004z�ގ�\f\u0018��-lZ�\fSV��V�iJ�\u000e��C�g���syD\"8H�\u0005���,V-\u001c��겭�3M���\u0003�R#��\f\u0018\u000b�\u0002�Q\u001f3\u0004d�z�H��o.\n������2\u000b+}\u0017���o�p\u0019v�\u000f\\_VJ��;�3�'\u0010�` \u0017\u0010��x\u0002�v��q��s�������\f�ZO�F��V���^�(��\u0003A\u001fKem\\D\u0014\u0010����}�R\\*������7/���/��-dN��\u0006��gUe�z�ԣ�g[l��/�����\u000b�Cu���\u0016\"�9\u001dW��?0�B2��Oj\u00038n�d\u0016���X\u0001$\u0002�)�\b�8���e)�;��y���Y�/���ՠ1���Z������i�n��0�\u001bI8����!\"��H\u001c� �j<��k���I�A�=\u0018��E؁�b\u0002� �S���y�ۯ�֏�[8}�\u0001�il�tl�R}\u0012��\u0001$>G\u0012��&�>?�NkX\u001d�����<\\_�kk\u0017R\u001b�qٟ�\u0012ӳI�\u0001\u0001d�6��X\n�\u001e���a#��/z:��:�\u000fa�Z\u0017����6�� Ｅ�z��tX��n�j<�H�\u0017I8?\u000f\u001a�?|ED:��0up�ח�tߜ\nT\u0004p\\\u0003���^h��?&�@\u0000����YU���\u0017R��5��NO/ =�+�\bTz�\u0002ϯ�}ח�KGd����̈́��H��Z��(�\u0006�ð\u001f�}v�s���N�h�3�\n�?�։S��S�\u0004\u0010\u001c���\u001eRp�[�L���]���Z��\u0018�\\*6�aO�(w�j�T7w�����q;��'�p~}�X�ɧ\u0013ǡ/���א�})\u0015\u0001�L���F\u0010�\u0000MdR�3=���\u0019מ�?~ ��A+�\u0013�ߓNk=���T@�]q�\u001e�n;��/\u0013\u0006�! �CC'��I$�cЭԀ�����kX�7�aj\u0002�\u0017g��ź��ˈ\u0004�vζ\\_\u0010\u0005\u0004ٸ�\u0017��w�\u0016��~�������g����\u001d܋3�]z|�Y�=C$��$�\u000f͓g��\\*�Qл��%�-\u0003|�\n\u001c��\u0017\u0012I���{��R� �\u0003�L\u0010\u0010�IzEE�Nн�\u0017�\n7���<����\u0001b\u001f���N;ﶭ�3������Y�ED> \u0012��i\nع�V{y\nZ�\u0012j\u0002\u0018�y�]�\n \u001d7�\u0012�/�\\_�;E)��H ��7w�z\f\u0016���!\u001cy���e�p��VaAɨ�3!�\u0017�p>s���t\u000ec\u0018E�vo7d\u001e;�S �\b�ч�,�q#;����R\u0011��LK�Ɂ\f\b��?\\_��!89�Z5\n�&{Y�ն�4�`tg�v��+\b���� ��QJy���i��I��H���\u0015뼾\u0006�{^Ï�y\\*���B�\u0000>��q\u0017A@���)�)��b=\u001bi G\u0010\u001e����ן��Q���w?J$��$����?~CD���h���˽,\\_k�K-\u0000����q⸑]�����֓�\b`�)\u0019QQJ�\u0011 \u0004U���̠X\u000b18��5�:��]\u001e\u001f�\u0017`�\u001c6�\u001f$�����?�\u00145\u0011q��/���\f\\*\u0001\u0018�`.w�1�\f;]ӗ8n�u���7�/\u000e0WiƮ�D\u0001A���f���.Bp(�\\_��Z\u0018}J�>�mWD�\u001fH���e�m\"��KT�5�\u001fr���p\u0011��\u0000#\u0015k���]���)�:ndGius\\�Zs�ֺ��\u0002\f2��j\u0001\u0004[o\u001b�[\u000f�&\u0004�����.N�H\u001c��>A\u0012�'��m�KV�%ΥY[�<��wM��\u001a\u0000F<�3��7�{�\u001aǍ��7�Z�\\_���\u0001�FEœD\u0001\u00016n!\n\"\"�\u0001!�����^\u0015m��\u000b\u0001\u0015�s�n%\f�# �\u001fo\u0012�\u0004��t\b\u0002�Ov��2ӯ��S��\u0017<긑N��FW�ܹDk=��\u0003�kJF� \u0000)B)�ʠOٛ�\u0006�\n����\u001aHh{\u001f��Gw\u0010 ��Y�E�\u00145ѝ��K�Y\u001d\u0003\u001c��{���~�^gOq�\u0013;�!Ǎ��u�)����g��lj\u0012H��1�5\u0011\u0005\u0004��[�\u0018�8�� \u001c䉎q^η^�\u0006\u0012�O�}�$�\u000f�!�\u0016�)j�\n���/R\u0003���a5ܑ��f�\u0013�u��F6�V7ǫ��\u0010׵wWp�\u0003� =m�\b\u0002����=o\u0011�OL��$\bCx�\u0012ܡ\u0005<0�E\b�p+��\bB�ew�Z&2ϛ%�\u0019y\u000bD�ej\u0001\u0018\u0019u���\u0017����e �׿c��\"\u0011)�\u0011\u0011)�n�ˊ�$;�Z����(\"/�i\np�\u0006�% ��\u001f�� |b|F�N�qS������U����[Y��\u0014}g�\\_�b���\u0010�0\u0017I8`]yR����<+{��\"��J\u0000Fnq��g8\u0003�]A�M��\u0014����\u0003�y�32Iv�$�\u001dO��v�\u0014��\u0000����\b���\u0017 �AzZ�\u0014�H\u0012n�w+�>z�:�g܀�z\u0003�0\u001b��\u001a�p�\b��%����\u0004%gOq:5\u0000\u001c�=����}m��h�8�Y+\u001d7��q#���o��Uk��Z3�\u0000\u0000��E� �A�� ��ɞ���Mb(yz ��X g�i�T��\u0003�c�ʯZ�趭K;��x\u001b�|u����v��;$��J�m�\u0006�PlX�\u0014�J����l�p@z�b��gJ�P\u0001I�W\u0012�c%��z��;�2$���C\u0000>���3C��۝��78ndkius\\�������'�̎\u0012\u0004\u0004\u001e�\u0014|ʀ�� \f�kMZN\u0014R�՟��C\u0014\fF\u0012�\u0007�%!H�i�k\u000f{U��:�\u001a\u0000F�m5�L�\u00188�s�\u001e8uU�5ܭ�>���T6)���(\u0000��%�\u0007<�1� ����Y�$p�����MD\b�\u0019��c\u001e�>�\n\u0000F�Rj�m��+Db�\u0013���q#/�V7�u��\u0015Z�B�����\u0001'�\u0002����\u001e���\\*�$�\u0017��\b��H�\u0001�<�;����0\u0015\u0000�\u001e��~g����H\u001c��[㸑V}�o^�Z�\u0010\u0011���\u0000{C\u0001@�����($s��\u0006�����\nF\u0012�|\u001cː\\�zXv6�\u0007Fy R�b\u001d��#sZ'Nu�ȎE�m���څD\u0004�\u0017��\u0004\u0001@\\*�tۛ�M�@ҵ\u0011\u0002s��3\u0018�\u0004x0aW���[s�\u0001 \u0001�u�u��x\u000e�8���%Nς\n�����C�\n0�\u0002��2=+9��!�O:�B4\u0018I8�M\u001b��\n�S�e\u0015\u0013\u0004 1�R/6U�����9=\u000b6,�mk��T\u0011H�8��\u0002HU�m{��K��O��3Y�\u0007�\u0004!�&\u0007\u0019\u0005\u0004\u0001H��������a\"��:�8nd�����k�Y�� ae\u0000\u0000$}���B\u0010�O��3Z?o\nR� Şp@\u0012��o��S?��6��q#�z�]K�\u0006\u0000\u0000���\u0001p�n�\u000f|�$���tX)u7Z�8\u0001����v��D���y\n?�穸��:\u0003\u0000\u0000��iYh\u0000\u001c<�'\u0004�)\u0006�L\u0006H2U�����$t��;���}�8q�H�����h\u0000\u0000�c\u0016Jg�k� $���xk�J\u0006�0\u001a����!�j�,N�#\u0018G��,xZ��{#�\u0000\u0000�Wb���n�H��O��3Y<�[�T�?�I\u0010\u0000�(��\u001e���PSUI��6\u0002�\u0019�謕���\\_\u0013 \u0000\u0000��s�� \u0004!���P9\u0018���\u000b\bB\\*�qr\u0010`���-\f5U����\u001f�L4\u000e�tM���w=\u0015'\u0012\u0000\u0000�c��Y�مs �h��Dd\f�0\u0017I8����?�(x�iye?5\u0000�EU��o�\\*\u0019L�em�$\"�x�c�|o�F\u0012q\u0000\u0000��׫��:N�J��̲�H\u0010�E\u0012�`���\u0004!��ܳ F�\u0001s��kk?Nȅ�����l�,\u0010��F\u000e�\u0001\u0000��1�R�mz�j\\_\u000e\u0015�\\钞{)a0\u0017I8?�DH\u001e˚�a�\u001c�\b�e0[�b�� 9�qN\\*���S������V\u0001\u0000\u0000๴��IU���\n\"\u0004�A�s/��tV�\u0001>���r !�TU\u0012�s�nM�\u00188�\u0019W�Uw-�5\u0000\u0000`�)\u0019Q���x˟T��HK!Q0��\b\u0001u���|����'\u001c\u0010\u0000���\u000eZ%Wd���[���y\nZk�}\u0001\u0000�@�c�k�x�B�C��p\u0016!0�\u0018a�T��j�E�M�=ܫ\u0001@���6\u0011�x�?�uk�Dg\\\u0019�>�]�����y\u0000\u0000\f�ݺVdr�'e�g-\u0016\u0011�JH�\fB`6���K�^������\u0000�`�|�\u000e^)�d|k��P]�Jj\u0019\u0000\u0000��un�hw�\u001e�\u0002\u0012o(o��CÑq0\\{�ԇ�B�:-��\u001a\u0014��\b\u000eHEJ����\u0016�\u000e:u�r��&���F��\u0004j\u0017\u0000\u0000�tzU�ED?9�%��\u0007��l$�\f���q�7\u0013��4�\n^���Ä\u001f��\\��A��������Hm\u0002\u0000`�\u0016/\u000b�r�C\n������l$�|r3\u0011�$\b\u0017|׳���/�\u0000\u0000\u0007SJ��ׄ��:q�ֺ�Z\u0004\u0000��qE�Ǘ0�ZH8\u0012�>@\u0012�\u001f�Ⱦp���h�\n�\u0015���L\n\u00008�����\\�����E�O���\u0003\u0000��Gs�����37R\u0003�3�/8�H��Ď\u001f\u0014��(|���N�#���\u000f�\u0005\u0000�BU^�\f���<'�i�u:m�K�֓�1\u0000\u0000�piN�\u001d��\u000b�N(baIBe���\\_!\f��&�\u0001gOq:u�p��5PJuQ\u0005\u0000��o�5T�?�:.w�:#/2s�W�)\u0000\u0000\f\u0011����Wp\"��0\u0016\u0007`�\u0003�\u001d�( \u0004 \u0014.��WE�fu\u0013\u0000�E�}���d�Q���/US;\u0000\u0000\u0018c���g\u0016��\nF\u001f���\u000bI8�\b\u0017ղ|7q���5}�W��\u000b�|+�\u0000`4��oն\u001a3M��M�|�Z�\u00113\u0000\u0000&�\u0013�z����K9%5!2vg��E\u0018����O\f%��\u0012#���c\u001f=@\u0015\u0000\u0018�\u0001v�Q��f�O�\u0000\u0000`\u0006���~�/�Ljaԍ�\u0014�'\n>A\u0012�\\_&�\u001a.\u00012\u000b��\u0013�W�\u0004\u0000�޷�}�\n;���=��Px!�\u0001\u0000�!����^\u0016�~`�\u000e洣Gk�!�\u001b����}�q#/ ���z��D�]J$\u0000\u0004�\\*��O�ܭwxy\n�e��\u0004\u0000\u0000�p�?yY|�@��H!\u00151jr���7\u0010\u0006� �?c ���\u000b/\u000b�zr\f �T�257�ó���c;\u0005\u0000\u0000L\u0018\u0017x�/���\u0014��'V�\u001d��\u0018��#\u0012�B��������\u001a�Nk�5�uO/\"��6j\u0002@�Օ\\_\u0018��\u00128�\u0001\u0000\u0000CعM�\u001e\f��P$��\u001b\n\u0005�{����\\_H����6�TD\n�Ĩ(��3��\u000b0�M\u0014��\u0018p�l\\_�Y��ᓩ\u0001\u0000\u0000\f\u0011��>�/aoΌ\u000fY\\2rZ�\f�\u001cs5��\u001f\u001a�\u001f�O|�O{��[8�C/˿���J\u0000�4���eޕ\u001e\\*�\u0006\u0000\u00000dL��n��቎q\"\"\u0013��\u0011\u001b�DgW\u0013\u0006�! �CC��X�{\u001c��\u0005��\u0016yz\n���O�&�(}\u0003!���LɈzSp(�$\u001c�&\u0000� &��w��ajc�sٰ��ꉄ?���93��\u001anĝV��;����PJ5S\u001bH��>� �0���x��4-=��\u000f��' \u0007\u00000@o�=F\\GF�Me\fs.+2�难�h�\u0013I8�\u001aZ�{\u001a�я�\u0004��SN��\u0002��o�\u0016�Pm.I8\u001cA�\\_{T2\nrH��$\u001c\u0000�{��M�t�k�Z�H�\u001c�BǍ�B\u0018��\u0004��9nd����`\u0018��aǍ���uXm;�Q\u001b�##-��2���\"�8Ҁ{�'\u0005�BF~j��6��2Y�\n\u00000���q� ����\u0005{�������e%��\u0007����������aHϾĐ���TF�(�\u001a�\\�\u0006\u0018#\u001e��xY9�ɿ7\\_���\f\u0000`\u0006UQ� ��\u0011�D�c������4�����i�������9'z�R��c��\n��y\u000f{}\u001d&l�����oyT�\u0018�\u000f3ĻM��<���\u0001\u0000�4;w�\u001dF�k�J�Ĳ�2�=��8bǕ!���o\u0019�~�}Q~�(����\u0014����G%���`\u001f0���9�DgW;�Y��f�\u0012�$\u001bp��xY�րWK���F\u0001\u00000�R��.|�ɔ�q��\u001b\u001c���\u00186MDN0aOs�\u001e�p\u0001�Nց���D�0���[L����m�S\u001d)(4���S�\u0018��\u0019�\u001by����g�����)\u001a\u000f�V`�[�\fH�힔��WB�\b\u00000FǛ\\_6�r\u001c7�'�\u0013q\u0007\u0012pO���C\u0003\n\u0016�p\u0001�D�]\\*�ٗ�ա\u0013N��pVJ��VR�@�7��f�KS��\\_u�RǍl=b�5�:]��N���������\u0007F���m��\u001bʜG�\b\u00000h��f\u0017��Ө�m\n'�\u000e$�~�.��N\u0016\n4`H�\u0005�\u0013;�a��\u000bHĉh��?�\u0002�K��x\u000e-4E�E=��?�;햔����op��5\u001c��<�A���\nLf�����\fH�k^��[N\\_J�\b\u00000�G��2nn;��;9�\u000ek8h\u000f�=��k\u0002�Z\n �g��TO�i�\u000b\u001d7�Iפ�z�֙��R�^����-Z�I�\u0016�\u001f��T|���c\u0019�m�Z��\\*�y����\u001e�\u0003mF\u0006���$ �)�/�,\u0003\u0000\u00186^��s��¸���~hSS�9�\u001b'�\u0007\\���\u000b�TN�\n��k5�l�q\u001a�\u0012�H\u000fON��[Z�\u001c�c�p\u0007w/h��J����&���%�[�\nH�w[\u0004�g}�\u000e\n\u0000`\u0012U��[F�m��K��}��:���i��%=��SP��$\\�9=\u000b6HF�7��Y\u001d��:Ѵ\u0015p�!W�R/�\"�ܓV�Ƚ?���9~<�;���@Ǩ��\u001b�(��zs��ayͳ��N���\u0002\u0000��\u000e?{��s��y\u000fI�����.\u000e��Z�4�u�d��ۉ]�8-0�H�\u0005��=�A�E�k�\u000b\u0002>\u0001O�ZOr܈q��\\���\u000b�Dx�\u0007�M.�ZO\n��\\_[{��Fv\u001dw���V��/ l[���&!;\u0010k25&J�.��c�\"�u!�\u0014\u0000��gc�uƎe�O[<��b��:\u001c�9l���帑\u000f��+i}��$\\\\*L»�/Y?0�=��Z\u000eu^��\u001byݴk�Ox�-����B\\���]��W\u00055����\n�g�ӣ�\\_�\u0017<�Պ��Ʃ���zVx\\_t�ɱ��\u0011�����;\u0002\u0000���X�F/\"p��K�9�\u001e���~�~Ik���>Y�N�긑\u0017hq��$\\�h\u0019�8��e��:70\u0013K�\u000b%\\�KǍl6���үL��ADd|��z6P�\\*�\n�g�������5�\u001e�謕+\u001b��\u0007&NZOq�J��(;S\u0006��\u001c�t�{���s��.�ZO��\u0004\u0000\u00185�Q�};��V��щθ�q#{���+��b����\u0004�rW8n�\u001d�u�TZ[�\bAjq��f��͝Z�\"�Rj���ʰ�Lt��.qͼF;��\u0015M49\u001c\u0010�h��x��jw欗D$\u0014�p��=\u0013w��\u0013��\u001fi)����m5�QJm�s�F�Sݑ�ZN��gM\u000fPO�:\u0011)���w�f�\u0017�{\u0013\u0000\u0010\f���J����:�������m5��Z�\\*\"\u001d��q�V�\u0015KF�eN��zS�H\u000eV¥ �u�TǍ�\u0011�Z��.��'�\u001f�=\b��^N,?O�\f\u0018��\u0002<\u0018�(��e�������o�{\u001c��\u0019����M����縑ͺn���\u001a��9�bT�w���A�ݿ���7v\u0016\u0004��\u0004\u0000\u0004�m5\u0016�f��F6;n�}h�{��:��\\_��jK�=��q/x��n�,�C<G)����Lk=ۄ}2�ܭw���ˊ��\u0016o\u000f\\*u�3��𮧖��n�T\\*��\u0010��ْ�7W�=����'�\u0001\u0000G�\\*W������~ۿ��y���p�\u0003��\u0000\u0000\u0015�IDAT`L\"�}��Z��H��t�������}?��.]D�Ed��\u0014HFN�o{f5t�-\u0012o�L$� \"\"o���D^ʔ\u0001�Z�3󇾧�'\"}�N�\nula\u0011)���\u000b���퀙Y�EU\\�ТpD�\u001fT�x��\u0013\u0019�4u�DZuݽ7Io�m�=��ZO���\u000b��y\u000f9�|<\bѵ��UE�3��{u�|߉ή6�\n���\u0012G\"/��d|t�4�\u000e\u001d�Zg�����\f�c���˭�\u001d}�J��6�D�8\u0011���ڻ+Uŵ� h#\u0013$#�R�{ΚC�M������N�\u0003�T����3=���8\u001c��\u0013\u0011�sw�\u00137�������ti�c\"\u0012\u001bz2\u000e�H�A�y�\fn�e\n�Q�\"�+\u0012.�p�EC{\u0010�7\u001c\u0003�p8D���ӳ�i\u0011\u0011;g�Z�m��Z���\\Ldxo\n�qRp��ݱ\"r�\u0014�lq�\u0014���#���\u00026��g\u000f`\f���\u0001It�J\u0011Y��|p�t�#\u0011ym$���SD�@2�/v��n9xr��\u001c]��CN��%���P��E�m�K37}���.�օ�Y�/Nt�ri5#>���[�Uf��z��o��E�ژ{�gnMius���c��Z\\*\"\u001e�9� �i\"R �\\_u�J�>�޴����\u000e\u0000H������\u0013q�Gmc�{J.\u001e��f<[\u0000\u0000����R����\u001d��l\\*\\*N�\u0013U�L�V7��3\u001cI8ӹ]���ov��n1n20�\n��c�\f\u0004t����u�\\*\\_��Ɗc\u001c�\u0018�Ijr\n�o[�E�\u001bi��/]��HD>9I���Y�Z���+����C/.\u000e�=r`ߑ�U�+nz�d���&�|�cܡ�\nc����]��{������暔�5Ӭd����\"\u0000��XV����\u0003�p�����m�|�$�\u001f:������n�|�;�Y\u0004�03�]�5ˮ%\u0012\u0018\u000e;g�ڡϽSl�?��pC\u0003�6��\u001c\u0013N�<^Ξ�t��z\u00119�K�c���ȶ��J��W}{o��up�w� ��\u0013\u000e\u0000p��ʲ�Z��q/x�h\u00180~�\u001a3�\u0016�GS\"B�\u000f���9d�\n����+�\u0005�\u001f��\\_�,E\u001f9#��M)��\u000e?{9-�l�i}�ʯ���?��\u000bS��Bi� \u0007\u0000\u0018����w�ϼggm\\��b���fD��?��JH�P\u001f\b�\u00033wG�}�\u001c��x���\\���2Z��\u0016�==��\u0013\n��>��?�j/Ğp\u0000���Ϭ�9D£�D�s�TEœD�\\_H���F�\u001a�$\n�#\u0001��\u001e��}���{�\f�s�Cbv�w��Y�\\*i=\u0006>��6U\\*��\u0005�Ǵ4��r\u0015\u0018\u0017>G\u0005\u0000�l|��\u0016�\u001e��r��Y]�ݵDS\"B�Nn���q>��\u000e 8���3w�����ǿ�FU\\[k���\nZ�A���͝����@=g\u000b\\_K��UB�|�\n\u00008�g's�d�!�nU��� ��'�p~��\\*\\*�a\u0012�|�i}$�0��r�ҲԚ�ξS��:��\u0011\u0006���6po��w�:+�F�\u0016I8\u0000�qk�\\* MɈ\u0012�\u0004���[�ʮ��H�x�E\b|:A���a#��Y2��c���\u0007��xz�����G�\u000f��xƶ\u001a�т<o�\\_\b�oK�glV��h�\u0000����0/d�6�J$\u001206�ڸ�OP��$����\u0015��\u001a$ �]��7W]��\u0004\u001c\u0012s\u001f+���ٞ\u001a\u000fӜ⹣\u001c��Y��a�h5���z?��آ7w�De�g�L�\u0006\u0000��3���R�EsR�]�\u001c�\u0010\f$�����\u001a$��]���|o�����W-;'�\u0019����枚���TU\u0012������܁��J�݁�7����o�\u001c�}\u0007�g�=�}6\u000e�\u0013T��(\u0001ٵS��vcb(�\u001c\u0002%TH���\u00065��1��ܵ#�}�Z�TE�ELe�v�S�@�Jhi�@Z���@Lc/6�.N\u0010!r�B��\\_��ܻ[��\u0005g'�1������}>�\f�g�7�<;�w�y�ƥ3�H�����ӵj\u0000&�74�߬�K��\u0017\u001cV�I�׍�xX%�A\b��\u000e�l�Fuv��/�\u0004��h�\u0015���r�ذ����\u0014���7kI�7U\u0002��v�?��ks`�-\u0004\u0000�Q��������k��\u0018��\u0010.G��Ra\\*<�oxgw��guv��\u001a���\u001d\u001c���?����k��mt\\*\u0005pg~\u0017�}��||G��\u001a7\u0000��G\u001b�zj4{�Vը�+G\\_�秄p9����B4�;]\\*1���x�-׫\u0004�����\u001d\u001aikZ\n��]e�i\u0012��)�p\"������S�F\u0005� ��]뺪�R��œ�q�{��o��r� ��عݽ~�\u000b����o�pl��\u0001ՠ�\\���\u000fn��\u000b�r���m@1&|3��3\u001eN�}D��7'��v�T��\u000e@��]|y���Yƣ=\u001b+\u0019��Ag�� �r>��:vߧ\u0012�O�W�\u0011���\"� m���9�rw\\I�4u�\\_��O\u0016|�fb�0���u�xW\u001c�'��Ran�����\u001f�:\\*\u0000-\u001a���\u0001׹�v��wﷲ=\u001e=�5�;!\\�\u0007\f�����\f�X�����4C�=�qu\u001e?6p^������Dž�ZU}��J�b�ݳ�\u0010u�W%\u0000`��i'��R!�\u000f��z��K�NU˥�\\_��Յ\u001c�C��:\bᦈ�\u0001�\u0002���Wf➮gU������c\u0003�-�A�U-�\n���Ѳ��?���<������N\u001c\u00004�\u0003\u001bW\u0017��R��8?3\u0016�\\�i�S�\u0013?�\u0006\fq|$�PH�䣕Z��)7��������[��\u0002\u0019T-�\nI�,��:\\_����jy\\_���ͱ��#�Z����\u0010�\u0005\u0007\u000fķ߸T�X���ﶥ-��Ԯ|ZE\u0000`\u0012~\\_�^�5��5�\u0010z���'~���$\u001fk'<�sE!�\u0010�ka&��\u001c4��Tz�>\\*�xC�\\\\*�u��kwO�\\\\*D�߾%Ç�\u001f)�盧\\_�:^~d\\*��33�n�q�+l\"m��\u001bc�zE\u0015\u0000&Q��k\ng��\u0016\u001c��G�\u0016N\u001b\u000eQ��R�t\u0000�h�^{��1\u0013.�\u001a���8�O�\u0010\n!���x�`�~���R4O\u001b&���j\b-i\u001b��s�k�ݯ&U�$YT�u~/��;�P\u000b�=���q\u001c\u001fH��\u0018o���\u0010�]I�\\���+~�֩��nC��篊��\\_�?Nv;���\u0010��I�\\Z�u�����p�d�d�y�،��\u0019��h�\u0001��\u001c�Ji��g�Ox띙\n�r#'� G��?��0I��S�V����\u000bE\u001d��7��������¯j%�g��:%\u000f�x�y�\u0019�w��6ռ�����\u001d\\_��cq\u001c���:��b�az{g\b�&�Jx�0���8�\\_W����M��U���kq��\u0019w�\u0016g��Ir�KaՏ~P���6\u0012ͮ��]��������G\\�����\u001d ���Z\u0016�?ps\u0018�va��vG�\u000e���d\u001cǻ��q����6��Ph�� ÓZ\u0018\u001d���w�g}���/�\u000b��5M�Y\u0018>\u0010������3�\\����0��w�3�\u001e=\u0014\u0006�m����ʷf����'�֌`n���aqmW�ƞI��\\*��W��]\u001f󪳛P\u0002��\"N��q-{�\u000f�vg��\u0005�UN\\��floQ�@X1��ʼ\u000fp����'�����,�sT�,��}���\\�\u0017�\u000f\\�W9|ђfl�������8ޣ�\u0000L����a��kC��w��.Y������t��#a����\nC�\\_�J�ڪ\u0007B��(\u0016u\f@��S\u0019�Do+�a��pݴo��\u0006��X\u0012f��\u0007���������\u001c\u000b\u001f�}�'�N����gnt6VN�ܒ���.x�p8��\\*�H3ym^\u001af̻�2��������=\u001a.\u0018~�+\f���e&\u0000�)9^���x�%&J\bW\u001f�b\"���0����3�?�\u0001�y��\n'���8��Vm #�Ⲱ������\u001b��O�;\u001a�|�����\u0005\u0015\u0007\u0000 �\u0010�x������yM߮\u0010�>�E�\u0006��C\bsC\b���\u0003�\\��nv!\u0000\u0000\u0013��w��P����\n��X\u0000\u0000\u0000\u0000\u0019��w�h����yGß�y�\\�\u000eӔ\u0000\u0000\u0000\u0000�z�\u001b��VU��\u0010\u000e\u0000\u0000\u0000��\f\u000e<�\b�\u0011�\u0001\u0000\u0000\u0000P��V��\b�\u0000\u0000\u0000\u00002\\*ټ��Vl7��aկ�\u0010\u000e\u0000\u0000\u0000 ��|��\u0015!\u001b�p\u0000\u0000\u0000\u0000\u0019U9|�\u0012U�\u0006!\u001c\u0000\u0000\u0000\u0000�l���0\u000eB8\u0000\u0000\u0000\u0000���i߽O\u0015�'�\u0003\u0000\u0000\u0000Ƞ����nɆ����~�\nJ\u0000\u0000\u0000\u0000�=�|�����bӷ[-��I�`&\u001c\u0000\u0000\u0000@\u0006�\"�c��p\u0000\u0000\u0000\u0000\u0019�$��VlwQۀ⏓\u0010\u000e\u0000\u0000\u0000 k�-�ۊͮh�\\_V���\u000e/\u0000\u0000\u0000@Ƭ��7ڊ�Z\u000fn�̄\u0003\u0000\u0000\u0000Ȑ$I��B�\b�\u0000\u0000\u0000\u00002�ّ�^k�vg�\u0011ş\u0000!\u001c\u0000\u0000\u0000@�\u001c\u001aik�v?�Q�\u001e�\u0004x�\u0017\u0000\u0000\u0000 #�/=��r�\u0015�ض��&�L8\u0000\u0000\u0000��hU\u0000�� �\u0000\u0000\u0000\u00002 ���gZ��h�\u001bUg`b�p\u0000\u0000\u0000\u0000\u0019P9�\u001b׵l��\u0007�v\u0006&ƻ�\u0000\u0000\u0000\u0000)��w��P\u000b�RY\u000fn�̄\u0003\u0000\u0000\u0000H���\u0007״2��:�<�,L�\u0014\u0013\u0000\u0000\u0000 �V��\u001bm��͂�\u001cf�\u0001\u0000\u0000\u0000�Ԫ޽���\u000fB8\u0000\u0000\u0000�\u0014J�<�\f�8���}˝��a:!\u0000\u0000\u0000@�$��\\W\u0019��V�WQ'��p\u0000\u0000\u0000\u0000)�$ɥi\bࢎ��:\u001b�G� \u0000\u0000\u0000�\u0012I�|�R�ܟ�}1\u000bnr� \u0007\u0000\u0000\u0000�\u0002I���\u0012�E������h\u0002\u0000\u0000\u0000�X�y���ય�e̂�|f�\u0001\u0000\u0000\u0000�P���O�)��f�t��2���\u0000\u0000\u0000\u0000-�$I[��9���2\u000b�1̄\u0003\u0000\u0000\u0000h��G6�1���}K��Ɛl\u0002\u0000\u0000\u00004I�$K\\*���Ҹo��\u001c\u000b\u000fl\\-+j\u0010�\u0005\u0000\u0000\u0000h�$I�?uj����bj��k��UT\u0002\u0000\u0000\u0000��H�d����|�RK��`Q���W����p\u0002\u0000\u0000\u0000L�d�i�����\u001ajs(0\u0000\u0000\u0000�\u0004%I�\u0016����\f^�L���k���uT\u0000\u0000\u0000�:%I�\u0016B���������+�\u0010B-{�\u0011\u0015�\u0016x\n�9$�\u0000\u0000\u0000\u0000�#I�ea��wU\u0006����qE�;o�{6<�\f7�\u0010�N��X1�\n\u0000\u0000\u0000��e�j���+z�\u0019��D\u001f|��\u001b.R�晦\u0004\u0000\u0000\u0000\u0000S��B-\b��O\b\u0007\u0000\u0000\u00000�<w�roF��\u0010\u000e\u0000\u0000\u0000`��%��\u0011�\u0001\u0000\u0000\u0000L\u0001\u0002��\u0012�\u0001\u0000\u0000\u0000�\u0000���p\u0000\u0000\u0000\u00009&�K\u0007!\u001c\u0000\u0000\u0000@\u000e]��/�K\u0011!\u001c\u0000\u0000\u0000@�D\u001d{w<��\u0004p)RT\u0002\u0000\u0000\u0000����}��M�\u001e�H\u0017!\u001c\u0000\u0000\u0000@NT˥BU\u0019R��\u0000\u0000\u0000\u0000\u0019\u0017u�y��o�f&\u001c\u0000\u0000\u0000@�Ež\u0005���J��\u0010\u000e\u0000\u0000\u0000 ���{��]뺼~�\nB8\u0000\u0000\u0000����[�X\u0013\u000e\u0000\u0000\u0000 #�b�bk�e��p\u0000\u0000\u0000\u0000)\u0017�|�����\u0005�߲K\b\u0007\u0000\u0000\u0000�RQ�oe\u001cǻ�o�'�\u0003\u0000\u0000\u0000H��ط8��ׅo�!�\u0003\u0000\u0000\u0000H�h��\u0003��7.\n!\u0004�[�\b�\u0000\u0000\u0000\u0000ZȬ��A\b\u0007\u0000\u0000\u0000�D3�H�l�\u0017�q�f\bf�M\u0015B8\u0000\u0000\u0000�\u0006��/�\u0010�]�8��J2�\b�\u0000\u0000\u0000\u0000&QԱ��xӺ���3�\n!\u001c\u0000\u0000\u0000@���{4\\0x��g�\u0013��\u00027\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000��\u0001��.��x�/\u0000\u0000\u0000\u0000IEND�B`�\n\n# BitArray¶\n\n\n*class* BitArray(*\\_\\_auto: BitsType | int | None*, *length: int | None = None*, *offset: int | None = None*, *\\*\\*kwargs*)¶\nThe `Bits` class is the base class for `BitArray` and so (with the exception of `\\_\\_hash\\_\\_`) all of its methods are also available for `BitArray` objects. The initialiser is also the same as for `Bits` and so won’t be repeated here.\n\n\nA `BitArray` is a mutable `Bits`, and so the one thing all of the methods listed here have in common is that they can modify the contents of the bitstring.\n\n\n## Methods¶\n\n\nBitArray.append(*bs: BitsType*) → None¶\nJoin a `BitArray` to the end of the current `BitArray`.\n\n```\n>>> s = BitArray('0xbad')\n>>> s.append('0xf00d')\n>>> s\nBitArray('0xbadf00d')\n\nBitArray.byteswap(*fmt: str | int | Iterable[int] | None = None*, *start: int | None = None*, *end: int | None = None*,"
    },
    {
        "instruction": "Create a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using polynomial encoding and print the resulting data frame.",
        "api": "category-encoders",
        "output": "#!pip install category_encoders\nimport category_encoders as cat_encoder\nimport pandas as pd\n\n# creating the dictionary\ndictionary = {'OUTLOOK': ['Rainy', 'Rainy',\n                          'Overcast', 'Sunny',\n                          'Sunny', 'Sunny',\n                          'Overcast'],\n              'TEMPERATURE': ['Hot', 'Hot', 'Hot',\n                          'Mild', 'Cool',\n                          'Cool', 'Cool'],\n              'PLAY': ['No', 'No', 'Yes',\n                       'Yes', 'Yes', 'No',\n                       'Yes']}\n\n# converting the dictionary to DataFrame\ndf = pd.DataFrame(dictionary)\n\n# creating an object PolynomialEncoder\nencoder = cat_encoder.PolynomialEncoder(cols = ['OUTLOOK', 'TEMPERATURE'])\n\n# fitting the columns to a data frame\ndf_category_encoder = encoder.fit_transform(df[['OUTLOOK', 'TEMPERATURE']])\n\nprint(df_category_encoder)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n# Usage\n\n\ninstall as:\n\n```\npip install category\\_encoders\n\n\nor\n\n```\nconda install -c conda-forge category\\_encoders\n\n\nTo use:\n\n```\nimport category\\_encoders as ce\n\nencoder = ce.BackwardDifferenceEncoder(cols=[...])\nencoder = ce.BaseNEncoder(cols=[...])\nencoder = ce.BinaryEncoder(cols=[...])\nencoder = ce.CatBoostEncoder(cols=[...])\nencoder = ce.CountEncoder(cols=[...])\nencoder = ce.GLMMEncoder(cols=[...])\nencoder = ce.GrayEncoder(cols=[...])\nencoder = ce.HashingEncoder(cols=[...])\nencoder = ce.HelmertEncoder(cols=[...])\nencoder = ce.JamesSteinEncoder(cols=[...])\nencoder = ce.LeaveOneOutEncoder(cols=[...])\nencoder = ce.MEstimateEncoder(cols=[...])\nencoder = ce.OneHotEncoder(cols=[...])\nencoder = ce.OrdinalEncoder(cols=[...])\nencoder = ce.PolynomialEncoder(cols=[...])\nencoder = ce.QuantileEncoder(cols=[...])\nencoder = ce.RankHotEncoder(cols=[...])\nencoder = ce.SumEncoder(cols=[...])\nencoder = ce.TargetEncoder(cols=[...])\nencoder = ce.WOEEncoder(cols=[...])\n\nencoder.fit(X, y)\nX\\_cleaned = encoder.transform(X\\_dirty)\n\n\nAll of these are fully compatible sklearn transformers, so they can be used in pipelines or in your existing scripts. If\nthe cols parameter isn’t passed, every non-numeric column will be converted. See below for detailed documentation\n\n\n## Known issues:\n\n\nCategoryEncoders internally works with pandas DataFrames as apposed to sklearn which works with numpy arrays. This can cause problems in sklearn versions prior to 1.2.0. In order to ensure full compatibility with sklearn set sklearn to also output DataFrames. This can be done by\n\n\nfor a whole project or just for a single pipeline using\n\n\nIf you experience another bug, feel free to report it on [github](https://github.com/scikit-learn-contrib/category\\_encoders/issues)\n\n\n## Contents:\n\n* Backward Difference Coding\n\t+ `BackwardDifferenceEncoder`\n\t\t- `BackwardDifferenceEncoder.fit()`\n\t\t- `BackwardDifferenceEncoder.fit\\_transform()`\n\t\t- `BackwardDifferenceEncoder.get\\_feature\\_names\\_in()`\n\t\t- `BackwardDifferenceEncoder.get\\_feature\\_names\\_out()`\n\t\t- `BackwardDifferenceEncoder.get\\_metadata\\_routing()`\n\t\t- `BackwardDifferenceEncoder.get\\_params()`\n\t\t- `BackwardDifferenceEncoder.set\\_output()`\n\t\t- `BackwardDifferenceEncoder.set\\_params()`\n\t\t- `BackwardDifferenceEncoder.set\\_transform\\_request()`\n\t\t- `BackwardDifferenceEncoder.transform()`\n* BaseN\n\t+ `BaseNEncoder`\n\t\t- `BaseNEncoder.basen\\_encode()`\n\t\t- `BaseNEncoder.basen\\_to\\_integer()`\n\t\t- `BaseNEncoder.col\\_transform()`\n\t\t- `BaseNEncoder.fit()`\n\t\t- `BaseNEncoder.fit\\_transform()`\n\t\t- `BaseNEncoder.get\\_feature\\_names\\_in()`\n\t\t- `BaseNEncoder.get\\_feature\\_names\\_out()`\n\t\t- `BaseNEncoder.get\\_metadata\\_routing()`\n\t\t- `BaseNEncoder.get\\_params()`\n\t\t- `BaseNEncoder.inverse\\_transform()`\n\t\t- `BaseNEncoder.set\\_inverse\\_transform\\_request()`\n\t\t- `BaseNEncoder.set\\_output()`\n\t\t- `BaseNEncoder.set\\_params()`\n\t\t- `BaseNEncoder.set\\_transform\\_request()`\n\t\t- `BaseNEncoder.transform()`\n* Binary\n\t+ `BinaryEncoder`\n\t\t- `BinaryEncoder.basen\\_encode()`\n\t\t- `BinaryEncoder.basen\\_to\\_integer()`\n\t\t- `BinaryEncoder.col\\_transform()`\n\t\t- `BinaryEncoder.fit()`\n\t\t- `BinaryEncoder.fit\\_transform()`\n\t\t-\n\n==================\n Document 1 \n----------------\n Category Encoders\n\n\nA set of scikit-learn-style transformers for encoding categorical variables into numeric with different\ntechniques. While ordinal, one-hot, and hashing encoders have similar equivalents in the existing scikit-learn version, the\ntransformers in this library all share a few useful properties:\n\n> \n> * First-class support for pandas dataframes as an input (and optionally as output)\n> * Can explicitly configure which columns in the data are encoded by name or index, or infer non-numeric columns regardless of input type\n> * Can drop any columns with very low variance based on training set optionally\n> * Portability: train a transformer on data, pickle it, reuse it later and get the same thing out.\n> * Full compatibility with sklearn pipelines, input an array-like dataset like any other transformer (\\*)\n> \n> \n> \n\n\n(\\*) For full compatibility with Pipelines and ColumnTransformers, and consistent behaviour of get\\_feature\\_names\\_out, it’s recommended to upgrade sklearn to a version at least ‘1.2.0’ and to set output as pandas:\n\n```\nimport sklearn\nsklearn.set\\_config(transform\\_output=\"pandas\")\n\n```\n## Usage\n\n\ninstall as:\n\n```\npip install category\\_encoders\n\n\nor\n\n```\nconda install -c conda-forge category\\_encoders\n\n\nTo use:\n\n```\nimport category\\_encoders as ce\n\nencoder = ce.BackwardDifferenceEncoder(cols=[...])\nencoder = ce.BaseNEncoder(cols=[...])\nencoder = ce.BinaryEncoder(cols=[...])\nencoder = ce.CatBoostEncoder(cols=[...])\nencoder = ce.CountEncoder(cols=[...])\nencoder = ce.GLMMEncoder(cols=[...])\nencoder = ce.GrayEncoder(cols=[...])\nencoder = ce.HashingEncoder(cols=[...])\nencoder = ce.HelmertEncoder(cols=[...])\nencoder = ce.JamesSteinEncoder(cols=[...])\nencoder = ce.LeaveOneOutEncoder(cols=[...])\nencoder = ce.MEstimateEncoder(cols=[...])\nencoder = ce.OneHotEncoder(cols=[...])\nencoder =\n\n==================\n Document 2 \n----------------\n\n\n# Category Encoders\n\n\nA set of scikit-learn-style transformers for encoding categorical variables into numeric with different\ntechniques. While ordinal, one-hot, and hashing encoders have similar equivalents in the existing scikit-learn version, the\ntransformers in this library all share a few useful properties:\n\n> \n>"
    },
    {
        "instruction": "Create a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using James-Stein encoding and print the resulting data frame.",
        "api": "category-encoders",
        "output": "#!pip install category_encoders\nimport category_encoders as cat_encoder\nimport pandas as pd\n\n# creating the dictionary\ndictionary = {'OUTLOOK': ['Rainy', 'Rainy',\n                          'Overcast', 'Sunny',\n                          'Sunny', 'Sunny',\n                          'Overcast'],\n              'TEMPERATURE': ['Hot', 'Hot', 'Hot',\n                          'Mild', 'Cool',\n                          'Cool', 'Cool'],\n              'PLAY': ['No', 'No', 'Yes',\n                       'Yes', 'Yes', 'No',\n                       'Yes']}\n\n# converting the dictionary to DataFrame\ndf = pd.DataFrame(dictionary)\n\n# creating an object JamesSteinEncoder\nencoder = cat_encoder.JamesSteinEncoder(cols = ['OUTLOOK', 'TEMPERATURE'])\n\n# fitting the columns to a data frame\ndf_category_encoder = encoder.fit_transform(df[['OUTLOOK', 'TEMPERATURE']])\n\nprint(df_category_encoder)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n# Usage\n\n\ninstall as:\n\n```\npip install category\\_encoders\n\n\nor\n\n```\nconda install -c conda-forge category\\_encoders\n\n\nTo use:\n\n```\nimport category\\_encoders as ce\n\nencoder = ce.BackwardDifferenceEncoder(cols=[...])\nencoder = ce.BaseNEncoder(cols=[...])\nencoder = ce.BinaryEncoder(cols=[...])\nencoder = ce.CatBoostEncoder(cols=[...])\nencoder = ce.CountEncoder(cols=[...])\nencoder = ce.GLMMEncoder(cols=[...])\nencoder = ce.GrayEncoder(cols=[...])\nencoder = ce.HashingEncoder(cols=[...])\nencoder = ce.HelmertEncoder(cols=[...])\nencoder = ce.JamesSteinEncoder(cols=[...])\nencoder = ce.LeaveOneOutEncoder(cols=[...])\nencoder = ce.MEstimateEncoder(cols=[...])\nencoder = ce.OneHotEncoder(cols=[...])\nencoder = ce.OrdinalEncoder(cols=[...])\nencoder = ce.PolynomialEncoder(cols=[...])\nencoder = ce.QuantileEncoder(cols=[...])\nencoder = ce.RankHotEncoder(cols=[...])\nencoder = ce.SumEncoder(cols=[...])\nencoder = ce.TargetEncoder(cols=[...])\nencoder = ce.WOEEncoder(cols=[...])\n\nencoder.fit(X, y)\nX\\_cleaned = encoder.transform(X\\_dirty)\n\n\nAll of these are fully compatible sklearn transformers, so they can be used in pipelines or in your existing scripts. If\nthe cols parameter isn’t passed, every non-numeric column will be converted. See below for detailed documentation\n\n\n## Known issues:\n\n\nCategoryEncoders internally works with pandas DataFrames as apposed to sklearn which works with numpy arrays. This can cause problems in sklearn versions prior to 1.2.0. In order to ensure full compatibility with sklearn set sklearn to also output DataFrames. This can be done by\n\n\nfor a whole project or just for a single pipeline using\n\n\nIf you experience another bug, feel free to report it on [github](https://github.com/scikit-learn-contrib/category\\_encoders/issues)\n\n\n## Contents:\n\n* Backward Difference Coding\n\t+ `BackwardDifferenceEncoder`\n\t\t- `BackwardDifferenceEncoder.fit()`\n\t\t- `BackwardDifferenceEncoder.fit\\_transform()`\n\t\t- `BackwardDifferenceEncoder.get\\_feature\\_names\\_in()`\n\t\t- `BackwardDifferenceEncoder.get\\_feature\\_names\\_out()`\n\t\t- `BackwardDifferenceEncoder.get\\_metadata\\_routing()`\n\t\t- `BackwardDifferenceEncoder.get\\_params()`\n\t\t- `BackwardDifferenceEncoder.set\\_output()`\n\t\t- `BackwardDifferenceEncoder.set\\_params()`\n\t\t- `BackwardDifferenceEncoder.set\\_transform\\_request()`\n\t\t- `BackwardDifferenceEncoder.transform()`\n* BaseN\n\t+ `BaseNEncoder`\n\t\t- `BaseNEncoder.basen\\_encode()`\n\t\t- `BaseNEncoder.basen\\_to\\_integer()`\n\t\t- `BaseNEncoder.col\\_transform()`\n\t\t- `BaseNEncoder.fit()`\n\t\t- `BaseNEncoder.fit\\_transform()`\n\t\t- `BaseNEncoder.get\\_feature\\_names\\_in()`\n\t\t- `BaseNEncoder.get\\_feature\\_names\\_out()`\n\t\t- `BaseNEncoder.get\\_metadata\\_routing()`\n\t\t- `BaseNEncoder.get\\_params()`\n\t\t- `BaseNEncoder.inverse\\_transform()`\n\t\t- `BaseNEncoder.set\\_inverse\\_transform\\_request()`\n\t\t- `BaseNEncoder.set\\_output()`\n\t\t- `BaseNEncoder.set\\_params()`\n\t\t- `BaseNEncoder.set\\_transform\\_request()`\n\t\t- `BaseNEncoder.transform()`\n* Binary\n\t+ `BinaryEncoder`\n\t\t- `BinaryEncoder.basen\\_encode()`\n\t\t- `BinaryEncoder.basen\\_to\\_integer()`\n\t\t- `BinaryEncoder.col\\_transform()`\n\t\t- `BinaryEncoder.fit()`\n\t\t- `BinaryEncoder.fit\\_transform()`\n\t\t-\n\n==================\n Document 1 \n----------------\n Category Encoders\n\n\nA set of scikit-learn-style transformers for encoding categorical variables into numeric with different\ntechniques. While ordinal, one-hot, and hashing encoders have similar equivalents in the existing scikit-learn version, the\ntransformers in this library all share a few useful properties:\n\n> \n> * First-class support for pandas dataframes as an input (and optionally as output)\n> * Can explicitly configure which columns in the data are encoded by name or index, or infer non-numeric columns regardless of input type\n> * Can drop any columns with very low variance based on training set optionally\n> * Portability: train a transformer on data, pickle it, reuse it later and get the same thing out.\n> * Full compatibility with sklearn pipelines, input an array-like dataset like any other transformer (\\*)\n> \n> \n> \n\n\n(\\*) For full compatibility with Pipelines and ColumnTransformers, and consistent behaviour of get\\_feature\\_names\\_out, it’s recommended to upgrade sklearn to a version at least ‘1.2.0’ and to set output as pandas:\n\n```\nimport sklearn\nsklearn.set\\_config(transform\\_output=\"pandas\")\n\n```\n## Usage\n\n\ninstall as:\n\n```\npip install category\\_encoders\n\n\nor\n\n```\nconda install -c conda-forge category\\_encoders\n\n\nTo use:\n\n```\nimport category\\_encoders as ce\n\nencoder = ce.BackwardDifferenceEncoder(cols=[...])\nencoder = ce.BaseNEncoder(cols=[...])\nencoder = ce.BinaryEncoder(cols=[...])\nencoder = ce.CatBoostEncoder(cols=[...])\nencoder = ce.CountEncoder(cols=[...])\nencoder = ce.GLMMEncoder(cols=[...])\nencoder = ce.GrayEncoder(cols=[...])\nencoder = ce.HashingEncoder(cols=[...])\nencoder = ce.HelmertEncoder(cols=[...])\nencoder = ce.JamesSteinEncoder(cols=[...])\nencoder = ce.LeaveOneOutEncoder(cols=[...])\nencoder = ce.MEstimateEncoder(cols=[...])\nencoder = ce.OneHotEncoder(cols=[...])\nencoder =\n\n==================\n Document 2 \n----------------\n\n\n# Category Encoders\n\n\nA set of scikit-learn-style transformers for encoding categorical variables into numeric with different\ntechniques. While ordinal, one-hot, and hashing encoders have similar equivalents in the existing scikit-learn version, the\ntransformers in this library all share a few useful properties:\n\n> \n>"
    },
    {
        "instruction": "Develop a Python program that reads a text file containing mixed encodings and uses the 'chardet' API to detect the encoding of each line. The program should then print the encoding detection results for each line in the file.",
        "api": "chardet",
        "output": "#!pip install chardet\nimport chardet\n\nwith open('mixed_encodings.txt', 'r') as file:\n    for line in file:\n        result = chardet.detect(line.encode())\n        print(f\"Line: {line.strip()}\")\n        print(f\"Detected Encoding: {result['encoding']}\")",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# chardetÂ¶\n\n* chardet package\n\t+ Submodules\n\t+ chardet.big5freq module\n\t+ chardet.big5prober module\n\t+ chardet.chardetect module\n\t+ chardet.chardistribution module\n\t+ chardet.charsetgroupprober module\n\t+ chardet.charsetprober module\n\t+ chardet.codingstatemachine module\n\t+ chardet.compat module\n\t+ chardet.constants module\n\t+ chardet.cp949prober module\n\t+ chardet.escprober module\n\t+ chardet.escsm module\n\t+ chardet.eucjpprober module\n\t+ chardet.euckrfreq module\n\t+ chardet.euckrprober module\n\t+ chardet.euctwfreq module\n\t+ chardet.euctwprober module\n\t+ chardet.gb2312freq\n\n==================\n Document 1 \n----------------\n chardetÂ¶\n\n* chardet package\n\t+ Submodules\n\t+ chardet.big5freq module\n\t+ chardet.big5prober module\n\t+ chardet.chardetect module\n\t+ chardet.chardistribution module\n\t+ chardet.charsetgroupprober module\n\t+ chardet.charsetprober module\n\t+ chardet.codingstatemachine module\n\t+ chardet.compat module\n\t+ chardet.constants module\n\t+ chardet.cp949prober module\n\t+ chardet.escprober module\n\t+ chardet.escsm module\n\t+ chardet.eucjpprober module\n\t+ chardet.euckrfreq module\n\t+ chardet.euckrprober module\n\t+ chardet.euctwfreq module\n\t+ chardet.euctwprober module\n\t+ chardet.gb2312freq module\n\t+ chardet.gb2312prober module\n\t+ chardet.hebrewprober module\n\t+ chardet.jisfreq module\n\t+ chardet.jpcntx module\n\t+ chardet.langbulgarianmodel module\n\t+ chardet.langcyrillicmodel module\n\t+ chardet.langgreekmodel module\n\t+ chardet.langhebrewmodel module\n\t+ chardet.langhungarianmodel module\n\t+ chardet.langthaimodel module\n\t+ chardet.latin1prober module\n\t+ chardet.mbcharsetprober module\n\t+ chardet.mbcsgroupprober module\n\t+ chardet.mbcssm module\n\t+ chardet.sbcharsetprober module\n\t+ chardet.sbcsgroupprober module\n\t+ chardet.sjisprober module\n\t+ chardet.universaldetector module\n\t+ chardet.utf8prober module\n\t+ Module contents\n\n\n# chardet packageÂ¶\n\n\n## SubmodulesÂ¶\n\n\n\n## chardet.big5freq moduleÂ¶\n\n\n\n## chardet.big5prober moduleÂ¶\n\n\n*class* `chardet.big5prober.``Big5Prober`[source]Â¶\nBases: `chardet.mbcharsetprober.MultiByteCharSetProber`\n\n\n`charset_name`Â¶\n\n`language`Â¶\n\n\n\n## chardet.chardetect moduleÂ¶\n\n\n\n## chardet.chardistribution moduleÂ¶\n\n\n*class* `chardet.chardistribution.``Big5DistributionAnalysis`[source]Â¶\nBases: `chardet.chardistribution.CharDistributionAnalysis`\n\n\n`get_order`(*byte\\_str*)[source]Â¶\n\n\n*class* `chardet.chardistribution.``CharDistributionAnalysis`[source]Â¶\nBases: `object`\n\n\n`ENOUGH_DATA_THRESHOLD` *= 1024*Â¶\n\n`MINIMUM_DATA_THRESHOLD` *= 3*Â¶\n\n`SURE_NO` *= 0.01*Â¶\n\n`SURE_YES` *= 0.99*Â¶\n\n`feed`(*char*, *char\\_len*)[source]Â¶\nfeed a character with known length\n\n`get_confidence`()[source]Â¶\nreturn confidence based on existing data\n\n`get_order`(*\\_*)[source]Â¶\n\n`got_enough_data`()[source]Â¶\n\n`reset`()[source]Â¶\nreset analyser, clear any state\n\n\n*class* `chardet.chardistribution.``EUCJPDistributionAnalysis`[source]Â¶\nBases: `chardet.chardistribution.CharDistributionAnalysis`\n\n\n*class* `chardet.chardistribution.``EUCKRDistributionAnalysis`[source]Â¶\nBases: `chardet.chardistribution.CharDistributionAnalysis`\n\n\n*class* `chardet.chardistribution.``EUCTWDistributionAnalysis`[source]Â¶\nBases: `chardet.chardistribution.CharDistributionAnalysis`\n\n\n*class* `chardet.chardistribution.``GB2312DistributionAnalysis`[source]Â¶\nBases: `chardet.chardistribution.CharDistributionAnalysis`\n\n\n*class* `chardet.chardistribution.``JOHABDistributionAnalysis`[source]Â¶\nBases: `chardet.chardistribution.CharDistributionAnalysis`\n\n\n*class* `chardet.chardistribution.``SJISDistributionAnalysis`[source]Â¶\nBases: `chardet.chardistribution.CharDistributionAnalysis`\n\n\n\n## chardet.charsetgroupprober moduleÂ¶\n\n\n*class* `chardet.charsetgroupprober.``CharSetGroupProber`(*lang\\_filter=None*)[source]Â¶\nBases: `chardet.charsetprober.CharSetProber`\n\n`feed`(*byte\\_str*)[source]Â¶\n\n`get_confidence`()[source]Â¶\n\n`reset`()[source]Â¶\n\n\n## chardet.charsetprober moduleÂ¶\n\n\n*class* `chardet.charsetprober.``CharSetProber`(*lang\\_filter=None*)[source]Â¶\nBases: `object`\n\n\n`SHORTCUT_THRESHOLD` *= 0.95*Â¶\n\n`charset_name`Â¶\n\n*static* `filter_high_byte_only`(*buf*)[source]Â¶\n\n*static* `filter_international_words`(*buf*)[source]Â¶\nWe define three types of bytes:\nalphabet: english alphabets [a-zA-Z]\ninternational: international characters [Â-Ã¿]\nmarker: everything else [^a-zA-ZÂ-Ã¿]\nThe input buffer can be thought to contain a series of words delimited\nby markers. This function works to\n\n==================\n Document 2 \n----------------\n# chardet.universaldetector moduleÂ¶\n\n\nModule containing the UniversalDetector detector class, which is the primary\nclass a user of `chardet` should use.\n\n| author: | Mark Pilgrim (initial port to Python) |\n| author: | Shy Shalom (original C code) |\n| author: | Dan Blanchard (major refactoring for 3.0) |\n| author: | Ian Cordasco |\n\n\n*class* `chardet.universaldetector.``UniversalDetector`(*lang\\_filter=31*)[source]Â¶\nBases: `object`\n\n\nThe `UniversalDetector` class underlies the `chardet.detect` function\nand coordinates all of the different charset probers.\n\n\nTo get a `dict` containing an encoding and its confidence, you can simply\nrun:\n\n```\nu = UniversalDetector()\nu.feed(some\\_bytes)\nu.close()\ndetected = u.result\n\n```\n\n\n`ESC_DETECTOR` *= re.compile(b'(\\x1b|~{)')*Â¶\n\n`HIGH_BYTE_DETECTOR` *= re.compile(b'[\\x80-\\xff]')*Â¶\n\n`ISO_WIN_MAP` *= {'iso-8859-1': 'Windows-1252', 'iso-8859-13': 'Windows-1257', 'iso-8859-2': 'Windows-1250', 'iso-8859-5': 'Windows-1251', 'iso-8859-6': 'Windows-1256', 'iso-8859-7': 'Windows-1253', 'iso-8859-8': 'Windows-1255', 'iso-8859-9': 'Windows-1254'}*Â¶\n\n`MINIMUM_THRESHOLD` *= 0.2*Â¶\n\n`WIN_BYTE_DETECTOR` *= re.compile(b'[\\x80-\\x9f]')*Â¶\n\n`charset_probers`Â¶\n\n`close`()[source]Â¶\nStop analyzing the current document and come up with a final\nprediction.\n\n| Returns: | The `result` attribute, a `dict` with the keys\nencoding, confidence, and language. |\n\n`feed`(*byte\\_str*)[source]Â¶\nTakes a chunk of a document and feeds it through all of the relevant\ncharset probers.\n\n\nAfter calling `feed`, you can check the value of the `done`\nattribute to see if you need to continue feeding the\n`UniversalDetector` more data, or if it has made a prediction\n(in the `result` attribute).\n\nNote\n\n\nYou should always call `close` when youâre done feeding in your\ndocument if `done` is not already `True`.\n\n\n`has_win_bytes`Â¶\n\n`input_state`Â¶\n\n`reset`()[source]Â¶\nReset the UniversalDetector and all of its probers back to their\ninitial states. This is called by `\\_\\_init\\_\\_`, so you only need to\ncall this directly in between analyses of different documents.\n\n\n## chardet.utf8prober moduleÂ¶\n\n\n*class* `chardet.utf8prober.``UTF8Prober`[source]Â¶\nBases: `chardet.charsetprober.CharSetProber`\n\n\n`ONE_CHAR_PROB` *= 0.5*Â¶\n\n\n\n## Module contentsÂ¶\n\n\n*class* `chardet.``UniversalDetector`(*lang\\_filter=31*)[source]Â¶\nBases: `object`\n\n\n`chardet.``detect`(*byte\\_str*)[source]Â¶\nDetect the encoding of the given byte string.\n\n| Parameters: | **byte\\_str** (`bytes` or `bytearray`) â The byte sequence to examine. |\n\n`chardet.``detect_all`(*byte\\_str*, *ignore\\_threshold=False*)[source]Â¶\nDetect all the possible encodings of the given byte string.\n\n| Parameters: | * **byte\\_str** (`bytes` or `bytearray`) â The byte sequence to examine.\n* **ignore\\_threshold** (`bool`) â Include encodings that are below\n`UniversalDetector.MINIMUM\\_THRESHOLD`\nin results.\n |\n\n\n# chardet packageÂ¶\n\n\n# chardetÂ¶\n\n==================\n Document 3 \n----------------\n# chardet.codingstatemachine moduleÂ¶\n\n\n*class* `chardet.codingstatemachine.``CodingStateMachine`(*sm*)[source]Â¶\nBases: `object`\n\n\nA state machine to verify a byte sequence for a particular encoding. For\neach byte the detector receives, it will feed that byte to every active\nstate machine available, one byte at a time. The state machine changes its\nstate based on its previous state and the byte it receives. There are 3\nstates in a state machine that are of interest to an auto-detector:\n\nSTART state: This is the state to start with, or a legal byte sequence\n(i.e. a valid code point) for character has been identified.\nME state: This indicates that the state machine identified a byte sequence\nthat is specific to the charset it is designed for and that\nthere is no other possible encoding which can contain this byte\nsequence. This will to lead to an immediate positive answer for\nthe detector.\nERROR state: This indicates the state machine identified an illegal byte\nsequence for that encoding. This will lead to an immediate\nnegative answer for this encoding. Detector will exclude this\nencoding from consideration from here on.\n\n`get_coding_state_machine`()[source]Â¶\n\n`get_current_charlen`()[source]Â¶\n\n`next_state`(*c*)[source]Â¶\n\n\n## chardet.compat moduleÂ¶\n\n\n\n## chardet.constants moduleÂ¶\n\n\n\n## chardet.cp949prober moduleÂ¶\n\n\n*class* `chardet.cp949prober.``CP949Prober`[source]Â¶\nBases: `chardet.mbcharsetprober.MultiByteCharSetProber`\n\n\n\n## chardet.escprober moduleÂ¶\n\n\n*class* `chardet.escprober.``EscCharSetProber`(*lang\\_filter=None*)[source]Â¶\nBases: `chardet.charsetprober.CharSetProber`\n\n\nThis CharSetProber uses a âcode schemeâ approach for detecting encodings,\nwhereby easily recognizable escape or shift sequences are relied on to\nidentify these encodings.\n\n\n\n## chardet.escsm moduleÂ¶\n\n\n\n## chardet.eucjpprober moduleÂ¶\n\n\n*class* `chardet.eucjpprober.``EUCJPProber`[source]Â¶\nBases: `chardet.mbcharsetprober.MultiByteCharSetProber`\n\n\n\n## chardet.euckrfreq moduleÂ¶\n\n\n\n## chardet.euckrprober moduleÂ¶\n\n\n*class* `chardet.euckrprober.``EUCKRProber`[source]Â¶\nBases: `chardet.mbcharsetprober.MultiByteCharSetProber`\n\n\n\n## chardet.euctwfreq moduleÂ¶\n\n\n\n## chardet.euctwprober moduleÂ¶\n\n\n*class* `chardet.euctwprober.``EUCTWProber`[source]Â¶\nBases: `chardet.mbcharsetprober.MultiByteCharSetProber`\n\n\n\n## chardet.gb2312freq moduleÂ¶\n\n\n\n## chardet.gb2312prober moduleÂ¶\n\n\n*class* `chardet.gb2312prober.``GB2312Prober`[source]Â¶\nBases: `chardet.mbcharsetprober.MultiByteCharSetProber`\n\n\n\n## chardet.hebrewprober moduleÂ¶\n\n\n*class* `chardet.hebrewprober.``HebrewProber`[source]Â¶\nBases: `chardet.charsetprober.CharSetProber`\n\n\n`FINAL_KAF` *= 234*Â¶\n\n`FINAL_MEM` *= 237*Â¶\n\n`FINAL_NUN` *= 239*Â¶\n\n`FINAL_PE` *= 243*Â¶\n\n`FINAL_TSADI` *= 245*Â¶\n\n`LOGICAL_HEBREW_NAME` *= 'windows-1255'*Â¶\n\n`MIN_FINAL_CHAR_DISTANCE` *= 5*Â¶\n\n`MIN_MODEL_DISTANCE` *= 0.01*Â¶\n\n`NORMAL_KAF` *= 235*Â¶\n\n`NORMAL_MEM` *= 238*Â¶\n\n`NORMAL_NUN` *= 240*Â¶\n\n`NORMAL_PE` *= 244*Â¶\n\n`NORMAL_TSADI` *= 246*Â¶\n\n`VISUAL_HEBREW_NAME` *= 'ISO-8859-8'*Â¶\n\n`is_final`(*c*)[source]Â¶\n\n`is_non_final`(*c*)[source]Â¶\n\n`set_model_probers`(*logical\\_prober*, *visual\\_prober*)[source]Â¶\n\n\n\n## chardet.jisfreq moduleÂ¶\n\n\n\n## chardet.jpcntx moduleÂ¶\n\n\n*class* `chardet.jpcntx.``EUCJPContextAnalysis`[source]Â¶\nBases: `chardet.jpcntx.JapaneseContextAnalysis`\n\n\n*class* `chardet.jpcntx.``JapaneseContextAnalysis`[source]Â¶\nBases: `object`\n\n\n`DONT_KNOW` *= -1*Â¶\n\n`ENOUGH_REL_THRESHOLD` *= 100*Â¶\n\n`MAX_REL_THRESHOLD` *= 1000*Â¶\n\n`MINIMUM_DATA_THRESHOLD` *= 4*Â¶\n\n`NUM_OF_CATEGORY` *= 6*Â¶\n\n`feed`(*byte\\_str*, *num\\_bytes*)[source]Â¶\n\n\n*class* `chardet.jpcntx.``SJISContextAnalysis`[source]Â¶\nBases: `chardet.jpcntx.JapaneseContextAnalysis`\n\n`get_order`(*byte\\_str*)[source]Â¶\n\n\n\n## chardet.langbulgarianmodel moduleÂ¶\n\n\n\n## chardet.langcyrillicmodel moduleÂ¶\n\n\n\n## chardet.langgreekmodel moduleÂ¶\n\n\n\n## chardet.langhebrewmodel moduleÂ¶\n\n\n\n## chardet.langhungarianmodel moduleÂ¶\n\n\n\n## chardet.langthaimodel moduleÂ¶\n\n\n\n## chardet.latin1prober moduleÂ¶\n\n\n*class* `chardet.latin1prober.``Latin1Prober`[source]Â¶\nBases: `chardet.charsetprober.CharSetProber`\n\n\n\n## chardet.mbcharsetprober moduleÂ¶\n\n\n*class* `chardet.mbcharsetprober.``MultiByteCharSetProber`(*lang\\_filter=None*)[source]Â¶\nBases: `chardet.charsetprober.CharSetProber`\n\n\n\n## chardet.mbcsgroupprober moduleÂ¶\n\n\n*class* `chardet.mbcsgroupprober.``MBCSGroupProber`(*lang\\_filter=None*)[source]Â¶\nBases: `chardet.charsetgroupprober.CharSetGroupProber`\n\n\n## chardet.mbcssm moduleÂ¶\n\n\n\n## chardet.sbcharsetprober moduleÂ¶\n\n\n*class* `chardet.sbcharsetprober.``SingleByteCharSetModel`(*charset\\_name*, *language*, *char\\_to\\_order\\_map*, *language\\_model*, *typical\\_positive\\_ratio*, *keep\\_ascii\\_letters*, *alphabet*)Â¶\nBases: `tuple`\n\n\n`alphabet`Â¶\nAlias for field number 6\n\n`char_to_order_map`Â¶\nAlias for field number 2\n\n`charset_name`Â¶\nAlias for field number 0\n\n`keep_ascii_letters`Â¶\nAlias for field number 5\n\n`language`Â¶\nAlias for field number 1\n\n`language_model`Â¶\nAlias for field number 3\n\n`typical_positive_ratio`Â¶\nAlias for field number 4\n\n\n*class* `chardet.sbcharsetprober.``SingleByteCharSetProber`(*model*, *is\\_reversed=False*, *name\\_prober=None*)[source]Â¶\nBases: `chardet.charsetprober.CharSetProber`\n\n\n`NEGATIVE_SHORTCUT_THRESHOLD` *= 0.05*Â¶\n\n`POSITIVE_SHORTCUT_THRESHOLD` *= 0.95*Â¶\n\n`SAMPLE_SIZE` *= 64*Â¶\n\n`SB_ENOUGH_REL_THRESHOLD` *= 1024*Â¶\n\n\n\n## chardet.sbcsgroupprober moduleÂ¶\n\n\n*class* `chardet.sbcsgroupprober.``SBCSGroupProber`[source]Â¶\nBases: `chardet.charsetgroupprober.CharSetGroupProber`\n\n\n## chardet.sjisprober moduleÂ¶\n\n\n*class* `chardet.sjisprober.``SJISProber`[source]Â¶\nBases: `chardet.mbcharsetprober.MultiByteCharSetProber`\n\n\n## chardet.universaldetector moduleÂ¶\n\n\nModule containing the UniversalDetector detector class, which is the primary\nclass a user of `chardet` should use.\n\n| author: | Mark Pilgrim (initial port to Python) |\n| author: | Shy Shalom (original C code) |\n| author: | Dan Blanchard (major"
    },
    {
        "instruction": "Develop a Python program that reads a JSON file with text data in various encodings, uses the 'chardet' API to detect the encoding of each value, and then saves the JSON data with the detected encodings to a new file.",
        "api": "chardet",
        "output": "#!pip install chardet\nimport chardet\nimport json\n\nwith open('data.json', 'r') as jsonfile:\n    data = json.load(jsonfile)\n\ndetected_data = {}\nfor key, value in data.items():\n    result = chardet.detect(value.encode())\n    encoding = result['encoding']\n    detected_data[key] = value.encode(encoding).decode(encoding)\n\nwith open('detected_data.json', 'w') as output_file:\n    json.dump(detected_data, output_file, ensure_ascii=False)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n chardetÂ¶\n\n* chardet package\n\t+ Submodules\n\t+ chardet.big5freq module\n\t+ chardet.big5prober module\n\t+ chardet.chardetect module\n\t+ chardet.chardistribution module\n\t+ chardet.charsetgroupprober module\n\t+ chardet.charsetprober module\n\t+ chardet.codingstatemachine module\n\t+ chardet.compat module\n\t+ chardet.constants module\n\t+ chardet.cp949prober module\n\t+ chardet.escprober module\n\t+ chardet.escsm module\n\t+ chardet.eucjpprober module\n\t+ chardet.euckrfreq module\n\t+ chardet.euckrprober module\n\t+ chardet.euctwfreq module\n\t+ chardet.euctwprober module\n\t+ chardet.gb2312freq module\n\t+ chardet.gb2312prober module\n\t+ chardet.hebrewprober module\n\t+ chardet.jisfreq module\n\t+ chardet.jpcntx module\n\t+ chardet.langbulgarianmodel module\n\t+ chardet.langcyrillicmodel module\n\t+ chardet.langgreekmodel module\n\t+ chardet.langhebrewmodel module\n\t+ chardet.langhungarianmodel module\n\t+ chardet.langthaimodel module\n\t+ chardet.latin1prober module\n\t+ chardet.mbcharsetprober module\n\t+ chardet.mbcsgroupprober module\n\t+ chardet.mbcssm module\n\t+ chardet.sbcharsetprober module\n\t+ chardet.sbcsgroupprober module\n\t+ chardet.sjisprober module\n\t+ chardet.universaldetector module\n\t+ chardet.utf8prober module\n\t+ Module contents\n\n\n# chardet packageÂ¶\n\n\n## SubmodulesÂ¶\n\n\n\n## chardet.big5freq moduleÂ¶\n\n\n\n## chardet.big5prober moduleÂ¶\n\n\n*class* `chardet.big5prober.``Big5Prober`[source]Â¶\nBases: `chardet.mbcharsetprober.MultiByteCharSetProber`\n\n\n`charset_name`Â¶\n\n`language`Â¶\n\n\n\n## chardet.chardetect moduleÂ¶\n\n\n\n## chardet.chardistribution moduleÂ¶\n\n\n*class* `chardet.chardistribution.``Big5DistributionAnalysis`[source]Â¶\nBases: `chardet.chardistribution.CharDistributionAnalysis`\n\n\n`get_order`(*byte\\_str*)[source]Â¶\n\n\n*class* `chardet.chardistribution.``CharDistributionAnalysis`[source]Â¶\nBases: `object`\n\n\n`ENOUGH_DATA_THRESHOLD` *= 1024*Â¶\n\n`MINIMUM_DATA_THRESHOLD` *= 3*Â¶\n\n`SURE_NO` *= 0.01*Â¶\n\n`SURE_YES` *= 0.99*Â¶\n\n`feed`(*char*, *char\\_len*)[source]Â¶\nfeed a character with known length\n\n`get_confidence`()[source]Â¶\nreturn confidence based on existing data\n\n`get_order`(*\\_*)[source]Â¶\n\n`got_enough_data`()[source]Â¶\n\n`reset`()[source]Â¶\nreset analyser, clear any state\n\n\n*class* `chardet.chardistribution.``EUCJPDistributionAnalysis`[source]Â¶\nBases: `chardet.chardistribution.CharDistributionAnalysis`\n\n\n*class* `chardet.chardistribution.``EUCKRDistributionAnalysis`[source]Â¶\nBases: `chardet.chardistribution.CharDistributionAnalysis`\n\n\n*class* `chardet.chardistribution.``EUCTWDistributionAnalysis`[source]Â¶\nBases: `chardet.chardistribution.CharDistributionAnalysis`\n\n\n*class* `chardet.chardistribution.``GB2312DistributionAnalysis`[source]Â¶\nBases: `chardet.chardistribution.CharDistributionAnalysis`\n\n\n*class* `chardet.chardistribution.``JOHABDistributionAnalysis`[source]Â¶\nBases: `chardet.chardistribution.CharDistributionAnalysis`\n\n\n*class* `chardet.chardistribution.``SJISDistributionAnalysis`[source]Â¶\nBases: `chardet.chardistribution.CharDistributionAnalysis`\n\n\n\n## chardet.charsetgroupprober moduleÂ¶\n\n\n*class* `chardet.charsetgroupprober.``CharSetGroupProber`(*lang\\_filter=None*)[source]Â¶\nBases: `chardet.charsetprober.CharSetProber`\n\n`feed`(*byte\\_str*)[source]Â¶\n\n`get_confidence`()[source]Â¶\n\n`reset`()[source]Â¶\n\n\n## chardet.charsetprober moduleÂ¶\n\n\n*class* `chardet.charsetprober.``CharSetProber`(*lang\\_filter=None*)[source]Â¶\nBases: `object`\n\n\n`SHORTCUT_THRESHOLD` *= 0.95*Â¶\n\n`charset_name`Â¶\n\n*static* `filter_high_byte_only`(*buf*)[source]Â¶\n\n*static* `filter_international_words`(*buf*)[source]Â¶\nWe define three types of bytes:\nalphabet: english alphabets [a-zA-Z]\ninternational: international characters [Â-Ã¿]\nmarker: everything else [^a-zA-ZÂ-Ã¿]\nThe input buffer can be thought to contain a series of words delimited\nby markers. This function works to\n\n==================\n Document 1 \n----------------\n\n\n# chardetÂ¶\n\n* chardet package\n\t+ Submodules\n\t+ chardet.big5freq module\n\t+ chardet.big5prober module\n\t+ chardet.chardetect module\n\t+ chardet.chardistribution module\n\t+ chardet.charsetgroupprober module\n\t+ chardet.charsetprober module\n\t+ chardet.codingstatemachine module\n\t+ chardet.compat module\n\t+ chardet.constants module\n\t+ chardet.cp949prober module\n\t+ chardet.escprober module\n\t+ chardet.escsm module\n\t+ chardet.eucjpprober module\n\t+ chardet.euckrfreq module\n\t+ chardet.euckrprober module\n\t+ chardet.euctwfreq module\n\t+ chardet.euctwprober module\n\t+ chardet.gb2312freq\n\n==================\n Document 2 \n----------------\n# chardet.universaldetector moduleÂ¶\n\n\nModule containing the UniversalDetector detector class, which is the primary\nclass a user of `chardet` should use.\n\n| author: | Mark Pilgrim (initial port to Python) |\n| author: | Shy Shalom (original C code) |\n| author: | Dan Blanchard (major refactoring for 3.0) |\n| author: | Ian Cordasco |\n\n\n*class* `chardet.universaldetector.``UniversalDetector`(*lang\\_filter=31*)[source]Â¶\nBases: `object`\n\n\nThe `UniversalDetector` class underlies the `chardet.detect` function\nand coordinates all of the different charset probers.\n\n\nTo get a `dict` containing an encoding and its confidence, you can simply\nrun:\n\n```\nu = UniversalDetector()\nu.feed(some\\_bytes)\nu.close()\ndetected = u.result\n\n```\n\n\n`ESC_DETECTOR` *= re.compile(b'(\\x1b|~{)')*Â¶\n\n`HIGH_BYTE_DETECTOR` *= re.compile(b'[\\x80-\\xff]')*Â¶\n\n`ISO_WIN_MAP` *= {'iso-8859-1': 'Windows-1252', 'iso-8859-13': 'Windows-1257', 'iso-8859-2': 'Windows-1250', 'iso-8859-5': 'Windows-1251', 'iso-8859-6': 'Windows-1256', 'iso-8859-7': 'Windows-1253', 'iso-8859-8': 'Windows-1255', 'iso-8859-9': 'Windows-1254'}*Â¶\n\n`MINIMUM_THRESHOLD` *= 0.2*Â¶\n\n`WIN_BYTE_DETECTOR` *= re.compile(b'[\\x80-\\x9f]')*Â¶\n\n`charset_probers`Â¶\n\n`close`()[source]Â¶\nStop analyzing the current document and come up with a final\nprediction.\n\n| Returns: | The `result` attribute, a `dict` with the keys\nencoding, confidence, and language. |\n\n`feed`(*byte\\_str*)[source]Â¶\nTakes a chunk of a document and feeds it through all of the relevant\ncharset probers.\n\n\nAfter calling `feed`, you can check the value of the `done`\nattribute to see if you need to continue feeding the\n`UniversalDetector` more data, or if it has made a prediction\n(in the `result` attribute).\n\nNote\n\n\nYou should always call `close` when youâre done feeding in your\ndocument if `done` is not already `True`.\n\n\n`has_win_bytes`Â¶\n\n`input_state`Â¶\n\n`reset`()[source]Â¶\nReset the UniversalDetector and all of its probers back to their\ninitial states. This is called by `\\_\\_init\\_\\_`, so you only need to\ncall this directly in between analyses of different documents.\n\n\n## chardet.utf8prober moduleÂ¶\n\n\n*class* `chardet.utf8prober.``UTF8Prober`[source]Â¶\nBases: `chardet.charsetprober.CharSetProber`\n\n\n`ONE_CHAR_PROB` *= 0.5*Â¶\n\n\n\n## Module contentsÂ¶\n\n\n*class* `chardet.``UniversalDetector`(*lang\\_filter=31*)[source]Â¶\nBases: `object`\n\n\n`chardet.``detect`(*byte\\_str*)[source]Â¶\nDetect the encoding of the given byte string.\n\n| Parameters: | **byte\\_str** (`bytes` or `bytearray`) â The byte sequence to examine. |\n\n`chardet.``detect_all`(*byte\\_str*, *ignore\\_threshold=False*)[source]Â¶\nDetect all the possible encodings of the given byte string.\n\n| Parameters: | * **byte\\_str** (`bytes` or `bytearray`) â The byte sequence to examine.\n* **ignore\\_threshold** (`bool`) â Include encodings that are below\n`UniversalDetector.MINIMUM\\_THRESHOLD`\nin results.\n |\n\n\n# chardet packageÂ¶\n\n\n# chardetÂ¶\n\n==================\n Document 3 \n----------------\n# chardet.codingstatemachine moduleÂ¶\n\n\n*class* `chardet.codingstatemachine.``CodingStateMachine`(*sm*)[source]Â¶\nBases: `object`\n\n\nA state machine to verify a byte sequence for a particular encoding. For\neach byte the detector receives, it will feed that byte to every active\nstate machine available, one byte at a time. The state machine changes its\nstate based on its previous state and the byte it receives. There are 3\nstates in a state machine that are of interest to an auto-detector:\n\nSTART state: This is the state to start with, or a legal byte sequence\n(i.e. a valid code point) for character has been identified.\nME state: This indicates that the state machine identified a byte sequence\nthat is specific to the charset it is designed for and that\nthere is no other possible encoding which can contain this byte\nsequence. This will to lead to an immediate positive answer for\nthe detector.\nERROR state: This indicates the state machine identified an illegal byte\nsequence for that encoding. This will lead to an immediate\nnegative answer for this encoding. Detector will exclude this\nencoding from consideration from here on.\n\n`get_coding_state_machine`()[source]Â¶\n\n`get_current_charlen`()[source]Â¶\n\n`next_state`(*c*)[source]Â¶\n\n\n## chardet.compat moduleÂ¶\n\n\n\n## chardet.constants moduleÂ¶\n\n\n\n## chardet.cp949prober moduleÂ¶\n\n\n*class* `chardet.cp949prober.``CP949Prober`[source]Â¶\nBases: `chardet.mbcharsetprober.MultiByteCharSetProber`\n\n\n\n## chardet.escprober moduleÂ¶\n\n\n*class* `chardet.escprober.``EscCharSetProber`(*lang\\_filter=None*)[source]Â¶\nBases: `chardet.charsetprober.CharSetProber`\n\n\nThis CharSetProber uses a âcode schemeâ approach for detecting encodings,\nwhereby easily recognizable escape or shift sequences are relied on to\nidentify these encodings.\n\n\n\n## chardet.escsm moduleÂ¶\n\n\n\n## chardet.eucjpprober moduleÂ¶\n\n\n*class* `chardet.eucjpprober.``EUCJPProber`[source]Â¶\nBases: `chardet.mbcharsetprober.MultiByteCharSetProber`\n\n\n\n## chardet.euckrfreq moduleÂ¶\n\n\n\n## chardet.euckrprober moduleÂ¶\n\n\n*class* `chardet.euckrprober.``EUCKRProber`[source]Â¶\nBases: `chardet.mbcharsetprober.MultiByteCharSetProber`\n\n\n\n## chardet.euctwfreq moduleÂ¶\n\n\n\n## chardet.euctwprober moduleÂ¶\n\n\n*class* `chardet.euctwprober.``EUCTWProber`[source]Â¶\nBases: `chardet.mbcharsetprober.MultiByteCharSetProber`\n\n\n\n## chardet.gb2312freq moduleÂ¶\n\n\n\n## chardet.gb2312prober moduleÂ¶\n\n\n*class* `chardet.gb2312prober.``GB2312Prober`[source]Â¶\nBases: `chardet.mbcharsetprober.MultiByteCharSetProber`\n\n\n\n## chardet.hebrewprober moduleÂ¶\n\n\n*class* `chardet.hebrewprober.``HebrewProber`[source]Â¶\nBases: `chardet.charsetprober.CharSetProber`\n\n\n`FINAL_KAF` *= 234*Â¶\n\n`FINAL_MEM` *= 237*Â¶\n\n`FINAL_NUN` *= 239*Â¶\n\n`FINAL_PE` *= 243*Â¶\n\n`FINAL_TSADI` *= 245*Â¶\n\n`LOGICAL_HEBREW_NAME` *= 'windows-1255'*Â¶\n\n`MIN_FINAL_CHAR_DISTANCE` *= 5*Â¶\n\n`MIN_MODEL_DISTANCE` *= 0.01*Â¶\n\n`NORMAL_KAF` *= 235*Â¶\n\n`NORMAL_MEM` *= 238*Â¶\n\n`NORMAL_NUN` *= 240*Â¶\n\n`NORMAL_PE` *= 244*Â¶\n\n`NORMAL_TSADI` *= 246*Â¶\n\n`VISUAL_HEBREW_NAME` *= 'ISO-8859-8'*Â¶\n\n`is_final`(*c*)[source]Â¶\n\n`is_non_final`(*c*)[source]Â¶\n\n`set_model_probers`(*logical\\_prober*, *visual\\_prober*)[source]Â¶\n\n\n\n## chardet.jisfreq moduleÂ¶\n\n\n\n## chardet.jpcntx moduleÂ¶\n\n\n*class* `chardet.jpcntx.``EUCJPContextAnalysis`[source]Â¶\nBases: `chardet.jpcntx.JapaneseContextAnalysis`\n\n\n*class* `chardet.jpcntx.``JapaneseContextAnalysis`[source]Â¶\nBases: `object`\n\n\n`DONT_KNOW` *= -1*Â¶\n\n`ENOUGH_REL_THRESHOLD` *= 100*Â¶\n\n`MAX_REL_THRESHOLD` *= 1000*Â¶\n\n`MINIMUM_DATA_THRESHOLD` *= 4*Â¶\n\n`NUM_OF_CATEGORY` *= 6*Â¶\n\n`feed`(*byte\\_str*, *num\\_bytes*)[source]Â¶\n\n\n*class* `chardet.jpcntx.``SJISContextAnalysis`[source]Â¶\nBases: `chardet.jpcntx.JapaneseContextAnalysis`\n\n`get_order`(*byte\\_str*)[source]Â¶\n\n\n\n## chardet.langbulgarianmodel moduleÂ¶\n\n\n\n## chardet.langcyrillicmodel moduleÂ¶\n\n\n\n## chardet.langgreekmodel moduleÂ¶\n\n\n\n## chardet.langhebrewmodel moduleÂ¶\n\n\n\n## chardet.langhungarianmodel moduleÂ¶\n\n\n\n## chardet.langthaimodel moduleÂ¶\n\n\n\n## chardet.latin1prober moduleÂ¶\n\n\n*class* `chardet.latin1prober.``Latin1Prober`[source]Â¶\nBases: `chardet.charsetprober.CharSetProber`\n\n\n\n## chardet.mbcharsetprober moduleÂ¶\n\n\n*class* `chardet.mbcharsetprober.``MultiByteCharSetProber`(*lang\\_filter=None*)[source]Â¶\nBases: `chardet.charsetprober.CharSetProber`\n\n\n\n## chardet.mbcsgroupprober moduleÂ¶\n\n\n*class* `chardet.mbcsgroupprober.``MBCSGroupProber`(*lang\\_filter=None*)[source]Â¶\nBases: `chardet.charsetgroupprober.CharSetGroupProber`\n\n\n## chardet.mbcssm moduleÂ¶\n\n\n\n## chardet.sbcharsetprober moduleÂ¶\n\n\n*class* `chardet.sbcharsetprober.``SingleByteCharSetModel`(*charset\\_name*, *language*, *char\\_to\\_order\\_map*, *language\\_model*, *typical\\_positive\\_ratio*, *keep\\_ascii\\_letters*, *alphabet*)Â¶\nBases: `tuple`\n\n\n`alphabet`Â¶\nAlias for field number 6\n\n`char_to_order_map`Â¶\nAlias for field number 2\n\n`charset_name`Â¶\nAlias for field number 0\n\n`keep_ascii_letters`Â¶\nAlias for field number 5\n\n`language`Â¶\nAlias for field number 1\n\n`language_model`Â¶\nAlias for field number 3\n\n`typical_positive_ratio`Â¶\nAlias for field number 4\n\n\n*class* `chardet.sbcharsetprober.``SingleByteCharSetProber`(*model*, *is\\_reversed=False*, *name\\_prober=None*)[source]Â¶\nBases: `chardet.charsetprober.CharSetProber`\n\n\n`NEGATIVE_SHORTCUT_THRESHOLD` *= 0.05*Â¶\n\n`POSITIVE_SHORTCUT_THRESHOLD` *= 0.95*Â¶\n\n`SAMPLE_SIZE` *= 64*Â¶\n\n`SB_ENOUGH_REL_THRESHOLD` *= 1024*Â¶\n\n\n\n## chardet.sbcsgroupprober moduleÂ¶\n\n\n*class* `chardet.sbcsgroupprober.``SBCSGroupProber`[source]Â¶\nBases: `chardet.charsetgroupprober.CharSetGroupProber`\n\n\n## chardet.sjisprober moduleÂ¶\n\n\n*class* `chardet.sjisprober.``SJISProber`[source]Â¶\nBases: `chardet.mbcharsetprober.MultiByteCharSetProber`\n\n\n## chardet.universaldetector moduleÂ¶\n\n\nModule containing the UniversalDetector detector class, which is the primary\nclass a user of `chardet` should use.\n\n| author: | Mark Pilgrim (initial port to Python) |\n| author: | Shy Shalom (original C code) |\n| author: | Dan Blanchard (major"
    },
    {
        "instruction": "Develop a Python program that uses the 'chardet' API to detect the encoding of a given text. The program should encode the text in ASCII and use the detector to identify the encoding. Print the result of the encoding detection.",
        "api": "chardet",
        "output": "#!pip install chardet\nfrom chardet.universaldetector import UniversalDetector\n\ntext = \"Here is a text with numbers 22 and special charcaters ##%&\"\ntext = text.encode(encoding='ascii',errors='strict')\n\ndetector = UniversalDetector()\ndetector.feed(text)\ndetector.close()\n\nprint(detector.result)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n chardetÂ¶\n\n* chardet package\n\t+ Submodules\n\t+ chardet.big5freq module\n\t+ chardet.big5prober module\n\t+ chardet.chardetect module\n\t+ chardet.chardistribution module\n\t+ chardet.charsetgroupprober module\n\t+ chardet.charsetprober module\n\t+ chardet.codingstatemachine module\n\t+ chardet.compat module\n\t+ chardet.constants module\n\t+ chardet.cp949prober module\n\t+ chardet.escprober module\n\t+ chardet.escsm module\n\t+ chardet.eucjpprober module\n\t+ chardet.euckrfreq module\n\t+ chardet.euckrprober module\n\t+ chardet.euctwfreq module\n\t+ chardet.euctwprober module\n\t+ chardet.gb2312freq module\n\t+ chardet.gb2312prober module\n\t+ chardet.hebrewprober module\n\t+ chardet.jisfreq module\n\t+ chardet.jpcntx module\n\t+ chardet.langbulgarianmodel module\n\t+ chardet.langcyrillicmodel module\n\t+ chardet.langgreekmodel module\n\t+ chardet.langhebrewmodel module\n\t+ chardet.langhungarianmodel module\n\t+ chardet.langthaimodel module\n\t+ chardet.latin1prober module\n\t+ chardet.mbcharsetprober module\n\t+ chardet.mbcsgroupprober module\n\t+ chardet.mbcssm module\n\t+ chardet.sbcharsetprober module\n\t+ chardet.sbcsgroupprober module\n\t+ chardet.sjisprober module\n\t+ chardet.universaldetector module\n\t+ chardet.utf8prober module\n\t+ Module contents\n\n\n# chardet packageÂ¶\n\n\n## SubmodulesÂ¶\n\n\n\n## chardet.big5freq moduleÂ¶\n\n\n\n## chardet.big5prober moduleÂ¶\n\n\n*class* `chardet.big5prober.``Big5Prober`[source]Â¶\nBases: `chardet.mbcharsetprober.MultiByteCharSetProber`\n\n\n`charset_name`Â¶\n\n`language`Â¶\n\n\n\n## chardet.chardetect moduleÂ¶\n\n\n\n## chardet.chardistribution moduleÂ¶\n\n\n*class* `chardet.chardistribution.``Big5DistributionAnalysis`[source]Â¶\nBases: `chardet.chardistribution.CharDistributionAnalysis`\n\n\n`get_order`(*byte\\_str*)[source]Â¶\n\n\n*class* `chardet.chardistribution.``CharDistributionAnalysis`[source]Â¶\nBases: `object`\n\n\n`ENOUGH_DATA_THRESHOLD` *= 1024*Â¶\n\n`MINIMUM_DATA_THRESHOLD` *= 3*Â¶\n\n`SURE_NO` *= 0.01*Â¶\n\n`SURE_YES` *= 0.99*Â¶\n\n`feed`(*char*, *char\\_len*)[source]Â¶\nfeed a character with known length\n\n`get_confidence`()[source]Â¶\nreturn confidence based on existing data\n\n`get_order`(*\\_*)[source]Â¶\n\n`got_enough_data`()[source]Â¶\n\n`reset`()[source]Â¶\nreset analyser, clear any state\n\n\n*class* `chardet.chardistribution.``EUCJPDistributionAnalysis`[source]Â¶\nBases: `chardet.chardistribution.CharDistributionAnalysis`\n\n\n*class* `chardet.chardistribution.``EUCKRDistributionAnalysis`[source]Â¶\nBases: `chardet.chardistribution.CharDistributionAnalysis`\n\n\n*class* `chardet.chardistribution.``EUCTWDistributionAnalysis`[source]Â¶\nBases: `chardet.chardistribution.CharDistributionAnalysis`\n\n\n*class* `chardet.chardistribution.``GB2312DistributionAnalysis`[source]Â¶\nBases: `chardet.chardistribution.CharDistributionAnalysis`\n\n\n*class* `chardet.chardistribution.``JOHABDistributionAnalysis`[source]Â¶\nBases: `chardet.chardistribution.CharDistributionAnalysis`\n\n\n*class* `chardet.chardistribution.``SJISDistributionAnalysis`[source]Â¶\nBases: `chardet.chardistribution.CharDistributionAnalysis`\n\n\n\n## chardet.charsetgroupprober moduleÂ¶\n\n\n*class* `chardet.charsetgroupprober.``CharSetGroupProber`(*lang\\_filter=None*)[source]Â¶\nBases: `chardet.charsetprober.CharSetProber`\n\n`feed`(*byte\\_str*)[source]Â¶\n\n`get_confidence`()[source]Â¶\n\n`reset`()[source]Â¶\n\n\n## chardet.charsetprober moduleÂ¶\n\n\n*class* `chardet.charsetprober.``CharSetProber`(*lang\\_filter=None*)[source]Â¶\nBases: `object`\n\n\n`SHORTCUT_THRESHOLD` *= 0.95*Â¶\n\n`charset_name`Â¶\n\n*static* `filter_high_byte_only`(*buf*)[source]Â¶\n\n*static* `filter_international_words`(*buf*)[source]Â¶\nWe define three types of bytes:\nalphabet: english alphabets [a-zA-Z]\ninternational: international characters [Â-Ã¿]\nmarker: everything else [^a-zA-ZÂ-Ã¿]\nThe input buffer can be thought to contain a series of words delimited\nby markers. This function works to\n\n==================\n Document 1 \n----------------\n\n\n# chardetÂ¶\n\n* chardet package\n\t+ Submodules\n\t+ chardet.big5freq module\n\t+ chardet.big5prober module\n\t+ chardet.chardetect module\n\t+ chardet.chardistribution module\n\t+ chardet.charsetgroupprober module\n\t+ chardet.charsetprober module\n\t+ chardet.codingstatemachine module\n\t+ chardet.compat module\n\t+ chardet.constants module\n\t+ chardet.cp949prober module\n\t+ chardet.escprober module\n\t+ chardet.escsm module\n\t+ chardet.eucjpprober module\n\t+ chardet.euckrfreq module\n\t+ chardet.euckrprober module\n\t+ chardet.euctwfreq module\n\t+ chardet.euctwprober module\n\t+ chardet.gb2312freq\n\n==================\n Document 2 \n----------------\n# chardet.universaldetector moduleÂ¶\n\n\nModule containing the UniversalDetector detector class, which is the primary\nclass a user of `chardet` should use.\n\n| author: | Mark Pilgrim (initial port to Python) |\n| author: | Shy Shalom (original C code) |\n| author: | Dan Blanchard (major refactoring for 3.0) |\n| author: | Ian Cordasco |\n\n\n*class* `chardet.universaldetector.``UniversalDetector`(*lang\\_filter=31*)[source]Â¶\nBases: `object`\n\n\nThe `UniversalDetector` class underlies the `chardet.detect` function\nand coordinates all of the different charset probers.\n\n\nTo get a `dict` containing an encoding and its confidence, you can simply\nrun:\n\n```\nu = UniversalDetector()\nu.feed(some\\_bytes)\nu.close()\ndetected = u.result\n\n```\n\n\n`ESC_DETECTOR` *= re.compile(b'(\\x1b|~{)')*Â¶\n\n`HIGH_BYTE_DETECTOR` *= re.compile(b'[\\x80-\\xff]')*Â¶\n\n`ISO_WIN_MAP` *= {'iso-8859-1': 'Windows-1252', 'iso-8859-13': 'Windows-1257', 'iso-8859-2': 'Windows-1250', 'iso-8859-5': 'Windows-1251', 'iso-8859-6': 'Windows-1256', 'iso-8859-7': 'Windows-1253', 'iso-8859-8': 'Windows-1255', 'iso-8859-9': 'Windows-1254'}*Â¶\n\n`MINIMUM_THRESHOLD` *= 0.2*Â¶\n\n`WIN_BYTE_DETECTOR` *= re.compile(b'[\\x80-\\x9f]')*Â¶\n\n`charset_probers`Â¶\n\n`close`()[source]Â¶\nStop analyzing the current document and come up with a final\nprediction.\n\n| Returns: | The `result` attribute, a `dict` with the keys\nencoding, confidence, and language. |\n\n`feed`(*byte\\_str*)[source]Â¶\nTakes a chunk of a document and feeds it through all of the relevant\ncharset probers.\n\n\nAfter calling `feed`, you can check the value of the `done`\nattribute to see if you need to continue feeding the\n`UniversalDetector` more data, or if it has made a prediction\n(in the `result` attribute).\n\nNote\n\n\nYou should always call `close` when youâre done feeding in your\ndocument if `done` is not already `True`.\n\n\n`has_win_bytes`Â¶\n\n`input_state`Â¶\n\n`reset`()[source]Â¶\nReset the UniversalDetector and all of its probers back to their\ninitial states. This is called by `\\_\\_init\\_\\_`, so you only need to\ncall this directly in between analyses of different documents.\n\n\n## chardet.utf8prober moduleÂ¶\n\n\n*class* `chardet.utf8prober.``UTF8Prober`[source]Â¶\nBases: `chardet.charsetprober.CharSetProber`\n\n\n`ONE_CHAR_PROB` *= 0.5*Â¶\n\n\n\n## Module contentsÂ¶\n\n\n*class* `chardet.``UniversalDetector`(*lang\\_filter=31*)[source]Â¶\nBases: `object`\n\n\n`chardet.``detect`(*byte\\_str*)[source]Â¶\nDetect the encoding of the given byte string.\n\n| Parameters: | **byte\\_str** (`bytes` or `bytearray`) â The byte sequence to examine. |\n\n`chardet.``detect_all`(*byte\\_str*, *ignore\\_threshold=False*)[source]Â¶\nDetect all the possible encodings of the given byte string.\n\n| Parameters: | * **byte\\_str** (`bytes` or `bytearray`) â The byte sequence to examine.\n* **ignore\\_threshold** (`bool`) â Include encodings that are below\n`UniversalDetector.MINIMUM\\_THRESHOLD`\nin results.\n |\n\n\n# chardet packageÂ¶\n\n\n# chardetÂ¶"
    },
    {
        "instruction": "Create a Python program using the 'click' API to build a command-line tool. The program should accept two input options: 'number1' and 'number2'. It should then calculate the sum of 'number1' and 'number2' and print the result.",
        "api": "click",
        "output": "#!pip install click\nimport click\n\n@click.command()\n@click.option('--number1', prompt='Enter the first number: ',\n              help='First number.')\n@click.option('--number2', prompt='Enter the second number: ',\n              help='Second number.')\ndef main(number1, number2):\n  number1 = int(number1)\n  number2 = int(number2)\n  print(\"The sum is:\", number1 + number2)\n\nif __name__ == '__main__':\n    main()",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# API¶\n\n\nThis part of the documentation lists the full API reference of all public\nclasses and functions.\n\n## Decorators¶\n\n\nclick.command(*name: Callable[[...], Any]*) → Command¶\n\nclick.command(*name: str | None*, *cls: Type[CmdType]*, *\\*\\*attrs: Any*) → Callable[[Callable[[...], Any]], CmdType]\n\nclick.command(*name: None = None*, *\\**, *cls: Type[CmdType]*, *\\*\\*attrs: Any*) → Callable[[Callable[[...], Any]], CmdType]\n\nclick.command(*name: str | None = None*, *cls: None = None*, *\\*\\*attrs: Any*)\n\n==================\n Document 1 \n----------------\n# Decorators¶\n\n\nclick.command(*name: Callable[[...], Any]*) → Command¶\n\nclick.command(*name: str | None*, *cls: Type[CmdType]*, *\\*\\*attrs: Any*) → Callable[[Callable[[...], Any]], CmdType]\n\nclick.command(*name: None = None*, *\\**, *cls: Type[CmdType]*, *\\*\\*attrs: Any*) → Callable[[Callable[[...], Any]], CmdType]\n\nclick.command(*name: str | None = None*, *cls: None = None*, *\\*\\*attrs: Any*) → Callable[[Callable[[...], Any]], Command]\nCreates a new `Command` and uses the decorated function as\ncallback. This will also automatically attach all decorated\n`option()`s and `argument()`s as parameters to the command.\n\n\nThe name of the command defaults to the name of the function with\nunderscores replaced by dashes. If you want to change that, you can\npass the intended name as the first argument.\n\n\nAll keyword arguments are forwarded to the underlying command class.\nFor the `params` argument, any decorated params are appended to\nthe end of the list.\n\n\nOnce decorated the function turns into a `Command` instance\nthat can be invoked as a command line utility or be attached to a\ncommand `Group`.\n\nParameters:\n* **name** – the name of the command. This defaults to the function\nname with underscores replaced by dashes.\n* **cls** – the command class to instantiate. This defaults to\n`Command`.\n\nChanged in version 8.1: This decorator can be applied without parentheses.\n\n\nChanged in version 8.1: The `params` argument can be used. Decorated params are\nappended to the end of the list.\n\n\nclick.group(*name: Callable[[...], Any]*) → Group¶\n\nclick.group(*name: str | None*, *cls: Type[GrpType]*, *\\*\\*attrs: Any*) → Callable[[Callable[[...], Any]], GrpType]\n\nclick.group(*name: None = None*, *\\**, *cls: Type[GrpType]*, *\\*\\*attrs: Any*) → Callable[[Callable[[...], Any]], GrpType]\n\nclick.group(*name: str | None = None*, *cls: None = None*, *\\*\\*attrs: Any*) → Callable[[Callable[[...], Any]], Group]\nCreates a new `Group` with a function as callback. This\nworks otherwise the same as `command()` just that the cls\nparameter is set to `Group`.\n\n\nclick.argument(*\\*param\\_decls*, *cls=None*, *\\*\\*attrs*)¶\nAttaches an argument to the command. All positional arguments are\npassed as parameter declarations to `Argument`; all keyword\narguments are forwarded unchanged (except `cls`).\nThis is equivalent to creating an `Argument` instance manually\nand attaching it to the `Command.params` list.\n\n\nFor the default argument class, refer to `Argument` and\n`Parameter` for descriptions of parameters.\n\nParameters:\n* **cls** (*Type**[**Argument**]* *|* *None*) – the argument class to instantiate. This defaults to\n`Argument`.\n* **param\\_decls** (*str*) – Passed as positional arguments to the constructor of\n`cls`.\n* **attrs** (*Any*) – Passed as keyword arguments to the constructor of `cls`.\n\nReturn type:\n*Callable*[[*FC*], *FC*]\n\nclick.option(*\\*param\\_decls*, *cls=None*, *\\*\\*attrs*)¶\nAttaches an option to the command. All positional arguments are\npassed as parameter declarations to `Option`; all keyword\narguments are forwarded unchanged (except `cls`).\nThis is equivalent to creating an `Option` instance manually\nand attaching it to the `Command.params` list.\n\n\nFor the default option class, refer to `Option` and\n`Parameter` for descriptions of parameters.\n\nParameters:\n* **cls** (*Type**[**Option**]* *|* *None*) – the option class to instantiate. This defaults to\n`Option`.\n* **param\\_decls** (*str*) – Passed as positional arguments to the constructor of\n`cls`.\n* **attrs** (*Any*) – Passed as keyword arguments to the constructor of `cls`.\n\nclick.password\\_option(*\\*param\\_decls*, *\\*\\*kwargs*)¶\nAdd a `--password` option which prompts for a password, hiding\ninput and asking to enter the value again for confirmation.\n\nParameters:\n* **param\\_decls** (*str*) – One or more option names. Defaults to the single\nvalue `\"--password\"`.\n* **kwargs** (*Any*) – Extra arguments are passed to `option()`.\n\nclick.confirmation\\_option(*\\*param\\_decls*, *\\*\\*kwargs*)¶\nAdd a `--yes` option which shows a prompt before continuing if\nnot passed. If the prompt is declined, the program will exit.\n\nParameters:\n* **param\\_decls** (*str*) – One or more option names. Defaults to the single\nvalue `\"--yes\"`.\n* **kwargs** (*Any*) – Extra arguments are passed to `option()`.\n\nclick.version\\_option(*version=None*, *\\*param\\_decls*, *package\\_name=None*, *prog\\_name=None*, *message=None*, *\\*\\*kwargs*)¶\nAdd a `--version` option which immediately prints the version\nnumber and exits the program.\n\n\nIf `version` is not provided, Click will try to detect it using\n`importlib.metadata.version()` to get the version for the\n`package\\_name`. On Python < 3.8, the `importlib\\_metadata`\nbackport must be installed.\n\n\nIf `package\\_name` is not provided, Click will try to detect it by\ninspecting the stack frames. This will be used to detect the\nversion, so it must match the name of the installed package.\n\nParameters:\n* **version** (*str* *|* *None*) – The version number to show. If not provided, Click\nwill try to detect it.\n* **param\\_decls** (*str*) – One or more option names. Defaults to the single\nvalue `\"--version\"`.\n* **package\\_name** (*str* *|* *None*) – The package name to detect the version from. If\nnot provided, Click will try to detect it.\n* **prog\\_name** (*str* *|* *None*) – The name of the CLI to show in the message. If not\nprovided, it will be detected from the command.\n* **message** (*str* *|* *None*) – The message to show. The values `%(prog)s`,\n`%(package)s`, and `%(version)s` are available. Defaults to\n`\"%(prog)s, version %(version)s\"`.\n* **kwargs** (*Any*) – Extra arguments are passed to `option()`.\n\nRaises:\n**RuntimeError** – `version` could not be detected.\n\nChangelog\nChanged in version 8.0: Add the `package\\_name` parameter, and the `%(package)s`\nvalue for messages.\n\n\nChanged in version 8.0: Use `importlib.metadata` instead of `pkg\\_resources`. The\nversion is detected based on the package name, not the entry\npoint name. The Python package name must match the installed\npackage name, or be passed with `package\\_name=`.\n\n\nclick.help\\_option(*\\*param\\_decls*, *\\*\\*kwargs*)¶\nAdd a `--help` option which immediately prints the help page\nand exits the program.\n\n\nThis is usually unnecessary, as the `--help` option is added to\neach command automatically unless `add\\_help\\_option=False` is\npassed.\n\nParameters:\n* **param\\_decls** (*str*) – One or more option names. Defaults to the single\nvalue `\"--help\"`.\n* **kwargs** (*Any*) – Extra arguments are passed to `option()`.\n\nclick.pass\\_context(*f*)¶\nMarks a callback as wanting to receive the current context\nobject as first argument.\n\nParameters:\n**f** (*t.Callable**[**te.Concatenate**[**Context**,* *P**]**,* *R**]*) – \n\nReturn type:\nt.Callable[P, R]\n\nclick.pass\\_obj(*f*)¶\nSimilar to `pass\\_context()`, but only pass the object on the\ncontext onwards (`Context.obj`). This is useful if that object\nrepresents the state of a nested system.\n\nParameters:\n**f** (*t.Callable**[**te.Concatenate**[**t.Any**,* *P**]**,* *R**]*) – \n\nclick.make\\_pass\\_decorator(*object\\_type*, *ensure=False*)¶\nGiven an object type this creates a decorator that will work\nsimilar to `pass\\_obj()` but instead of passing the object of the\ncurrent context, it will find the innermost context of type\n`object\\_type()`.\n\n\nThis generates a decorator that works roughly like this:\n\n```\nfrom functools import update\\_wrapper\n\ndef decorator(f):\n    @pass\\_context\n    def new\\_func(ctx, \\*args, \\*\\*kwargs):\n        obj = ctx.find\\_object(object\\_type)\n        return ctx.invoke(f, obj, \\*args, \\*\\*kwargs)\n    return update\\_wrapper(new\\_func, f)\nreturn decorator\n\n```\n\nParameters:\n* **object\\_type** (*Type**[**T**]*) – the type of the object to pass.\n* **ensure** (*bool*) – if set to True, a new object will be created and\nremembered on the context if it’s not there yet.\n\nReturn type:\n*Callable*[[t.Callable[te.Concatenate[T, P], R]], t.Callable[P, R]]\n\nclick.decorators.pass\\_meta\\_key(*key*, *\\**, *doc\\_description=None*)¶\nCreate a decorator that passes a key from\n`click.Context.meta` as the first argument to the decorated\nfunction.\n\nParameters:\n* **key** (*str*) – Key in `Context.meta` to pass.\n* **doc\\_description** (*str* *|* *None*) – Description of the object being passed,\ninserted into the decorator’s docstring. Defaults to “the ‘key’\nkey from Context.meta”.\n\nReturn type:\nt.Callable[[t.Callable[te.Concatenate[t.Any, P], R]], t.Callable[P, R]]\n\nChangelog\nNew in version 8.0.\n\n## Utilities¶\n\n\nclick.echo(*message=None*, *file=None*, *nl=True*, *err=False*, *color=None*)¶\nPrint a message and newline to stdout or a file. This should be\nused instead of `print()` because it provides better support\nfor different data, files, and environments.\n\n\nCompared to `print()`, this does the following:\n\n\n* Ensures that the\n\n==================\n Document 2 \n----------------\n# Utilities¶\n\n\nclick.echo(*message=None*, *file=None*, *nl=True*, *err=False*, *color=None*)¶\nPrint a message and newline to stdout or a file. This should be\nused instead of `print()` because it provides better support\nfor different data, files, and environments.\n\n\nCompared to `print()`, this does the following:\n\n\n* Ensures that the output encoding is not misconfigured on Linux.\n* Supports Unicode in the Windows console.\n* Supports writing to binary outputs, and supports writing bytes\nto text outputs.\n* Supports colors and styles on Windows.\n* Removes ANSI color and style codes if the output does not look\nlike an interactive terminal.\n* Always flushes the output.\n\nParameters:\n* **message** (*Any* *|* *None*) – The string or bytes to output. Other objects are\nconverted to strings.\n* **file** (*IO**[**Any**]* *|* *None*) – The file to write to. Defaults to `stdout`.\n* **err** (*bool*) – Write to `stderr` instead of `stdout`.\n* **nl** (*bool*) – Print a newline after the message. Enabled by default.\n* **color** (*bool* *|* *None*) – Force showing or hiding colors and other styles. By\ndefault Click will remove color if the output does not look like\nan interactive terminal.\n\nReturn type:\nNone\n\nChangelog\nChanged in version 6.0: Support Unicode output on the Windows console. Click does not\nmodify `sys.stdout`, so `sys.stdout.write()` and `print()`\nwill still not support Unicode.\n\n\nChanged in version 4.0: Added the `color` parameter.\n\n\nNew in version 3.0: Added the `err` parameter.\n\n\nChanged in version 2.0: Support colors on Windows if colorama is installed.\n\n\nclick.echo\\_via\\_pager(*text\\_or\\_generator*, *color=None*)¶\nThis function takes a text and shows it via an environment specific\npager on stdout.\n\nChangelog\nChanged in version 3.0: Added the color flag.\n\n\nParameters:\n* **text\\_or\\_generator** (*Iterable**[**str**]* *|* *Callable**[**[**]**,* *Iterable**[**str**]**]* *|* *str*) – the text to page, or alternatively, a\ngenerator emitting the text to page.\n* **color** (*bool* *|* *None*) – controls if the pager supports ANSI colors or not. The\ndefault is autodetection.\n\nclick.prompt(*text*, *default=None*, *hide\\_input=False*, *confirmation\\_prompt=False*, *type=None*, *value\\_proc=None*, *prompt\\_suffix=': '*, *show\\_default=True*, *err=False*, *show\\_choices=True*)¶\nPrompts a user for input. This is a convenience function that can\nbe used to prompt a user for input later.\n\n\nIf the user aborts the input by sending an interrupt signal, this\nfunction will catch it and raise a `Abort` exception.\n\nParameters:\n* **text** (*str*) – the text to show for the prompt.\n* **default** (*Any* *|* *None*) – the default value to use if no input happens. If this\nis not given it will prompt until it’s aborted.\n* **hide\\_input** (*bool*) – if this is set to true then the input value will\nbe hidden.\n* **confirmation\\_prompt** (*bool* *|* *str*) – Prompt a second time to confirm the\nvalue. Can be set to a string instead of `True` to customize\nthe message.\n* **type** (*ParamType* *|* *Any* *|* *None*) – the type to use to check the value against.\n* **value\\_proc** (*Callable**[**[**str**]**,* *Any**]* *|* *None*) – if this parameter is provided it’s a function that\nis invoked instead of the type conversion to\nconvert a value.\n* **prompt\\_suffix** (*str*) – a suffix that should be added to the prompt.\n* **show\\_default** (*bool*) – shows or hides the default value in the prompt.\n* **err** (*bool*) – if set to true the file defaults to `stderr` instead of\n`stdout`, the same as with echo.\n* **show\\_choices** (*bool*) – Show or hide choices if the passed type is a Choice.\nFor example if type is a Choice of either day or week,\nshow\\_choices is true and text is “Group by” then the\nprompt will be “Group by (day, week): “.\n\nReturn type:\n*Any*\n\nChangelog\nNew in version 8.0: `confirmation\\_prompt` can be a custom string.\n\n\nNew in version 7.0: Added the `show\\_choices` parameter.\n\n\nNew in version 6.0: Added unicode support for cmd.exe on Windows.\n\n\nNew in version 4.0: Added the err parameter.\n\n\nclick.confirm(*text*, *default=False*, *abort=False*, *prompt\\_suffix=': '*, *show\\_default=True*, *err=False*)¶\nPrompts for confirmation (yes/no question).\n\n\nIf the user aborts the input by sending a interrupt signal this\nfunction will catch it and raise a `Abort` exception.\n\nParameters:\n* **text** (*str*) – the question to ask.\n* **default** (*bool* *|* *None*) – The default value to use when no input is given. If\n`None`, repeat until input is given.\n* **abort** (*bool*) – if this is set to True a negative answer aborts the\nexception by raising `Abort`.\n* **prompt\\_suffix** (*str*) – a suffix that should be added to the prompt.\n* **show\\_default** (*bool*) – shows or hides the default value in the prompt.\n* **err** (*bool*) – if set to true the file defaults to `stderr` instead of\n`stdout`, the same as with echo.\n\nReturn type:\nbool\n\nChangelog\nChanged in version 8.0: Repeat until input is given if `default` is `None`.\n\n\nNew in version 4.0: Added the `err` parameter.\n\n\nclick.progressbar(*iterable=None*, *length=None*, *label=None*, *show\\_eta=True*, *show\\_percent=None*, *show\\_pos=False*, *item\\_show\\_func=None*, *fill\\_char='#'*, *empty\\_char='-'*, *bar\\_template='%(label)s  [%(bar)s]  %(info)s'*, *info\\_sep='  '*, *width=36*, *file=None*, *color=None*, *update\\_min\\_steps=1*)¶\nThis function creates an iterable context manager that can be used\nto iterate over something while showing a progress bar. It will\neither iterate over the iterable or length items (that are counted\nup). While iteration happens, this function will print a rendered\nprogress bar to the given file (defaults to stdout) and will attempt\nto calculate remaining time and more. By default, this progress bar\nwill not be rendered if the file is not a terminal.\n\n\nThe context manager creates the progress bar. When the context\nmanager is entered the progress bar is already created. With every\niteration over the progress bar, the iterable passed to the bar is\nadvanced and the bar is updated. When the context manager exits,\na newline is printed and the progress bar is finalized on screen.\n\n\nNote: The progress bar is currently designed for use cases where the\ntotal progress can be expected to take at least several seconds.\nBecause of this, the ProgressBar class object won’t display\nprogress that is considered too fast, and progress where the time\nbetween steps is less than a second.\n\n\nNo printing must happen or the progress bar will be unintentionally\ndestroyed.\n\n\nExample usage:\n\n```\nwith progressbar(items) as bar:\n    for item in bar:\n        do\\_something\\_with(item)\n\n\nAlternatively, if no iterable is specified, one can manually update the\nprogress bar through the update() method instead of directly\niterating over the progress bar. The update method accepts the number\nof steps to increment the bar with:\n\n```\nwith progressbar(length=chunks.total\\_bytes) as bar:\n    for chunk in chunks:\n        process\\_chunk(chunk)\n        bar.update(chunks.bytes)\n\n\nThe `update()` method also takes an optional value specifying the\n`current\\_item` at the new position. This is useful when used\ntogether with `item\\_show\\_func` to customize the output for each\nmanual step:\n\n```\nwith click.progressbar(\n    length=total\\_size,\n    label='Unzipping archive',\n    item\\_show\\_func=lambda a: a.filename\n) as bar:\n    for archive in zip\\_file:\n        archive.extract()\n        bar.update(archive.size, archive)\n\nParameters:\n* **iterable** (*Iterable**[**V**]* *|* *None*) – an iterable to iterate over. If not provided the length\nis required.\n* **length** (*int* *|* *None*) – the number of items to iterate over. By default the\nprogressbar will attempt to ask the iterator about its\nlength, which might or might not work. If an iterable is\nalso provided this parameter can be used to override the\nlength. If an iterable is not provided the progress bar\nwill iterate over a range of that length.\n* **label** (*str* *|* *None*) – the label to show next to the progress bar.\n* **show\\_eta** (*bool*) – enables or disables the estimated time display. This is\nautomatically disabled if the length cannot be\ndetermined.\n* **show\\_percent** (*bool* *|* *None*) – enables or disables the percentage display. The\ndefault is True if the iterable has a length or\nFalse if not.\n* **show\\_pos** (*bool*) – enables or disables the absolute position display. The\ndefault is False.\n* **item\\_show\\_func** (*Callable**[**[**V* *|* *None**]**,* *str* *|* *None**]* *|* *None*) – A function called with the current item which\ncan return a string to show next to the progress bar. If the\nfunction returns `None` nothing is shown. The current item can\nbe `None`, such as when entering and exiting the bar.\n* **fill\\_char** (*str*) – the character to use to show the filled part of the\nprogress bar.\n* **empty\\_char** (*str*) – the character to use to show the non-filled part of\nthe progress bar.\n* **bar\\_template** (*str*) – the format string to use as template for the bar.\nThe parameters in it are `label` for the label,\n`bar` for the progress bar and `info` for the\ninfo section.\n* **info\\_sep** (*str*) – the separator between multiple info items (eta etc.)\n* **width** (*int*) – the width of the progress bar in characters, 0 means full\nterminal width\n* **file** (*TextIO* *|* *None*) – The file to write to. If this is not a terminal then\nonly the label is printed.\n* **color** (*bool* *|* *None*) – controls if the terminal supports ANSI colors or not. The\ndefault is autodetection. This is only needed if ANSI\ncodes are included anywhere in the progress bar output\nwhich is not the case by default.\n* **update\\_min\\_steps** (*int*) – Render only when this many updates have\ncompleted. This allows tuning for very fast iterators.\n\nReturn type:\nProgressBar[V]\n\nChangelog\nChanged in version 8.0: Output is shown even if execution time is less than 0.5 seconds.\n\n\nChanged in version 8.0: `item\\_show\\_func` shows the current item, not the previous one.\n\n\nChanged in version 8.0: Labels are echoed if the output is not a TTY. Reverts a change\nin 7.0 that removed all output.\n\n\nNew in version 8.0: Added the `update\\_min\\_steps` parameter.\n\n\nChanged in version 4.0: Added the `color` parameter. Added the `update` method to\nthe object.\n\n\nNew in version 2.0.\n\n\nclick.clear()¶\nClears the terminal screen. This will have the effect of clearing\nthe whole visible space of the terminal and moving the cursor to the\ntop left. This does not do anything if not connected to a terminal.\n\nChangelog\nNew in version 2.0.\n\n\nReturn type:\nNone\n\nclick.style(*text*, *fg=None*, *bg=None*, *bold=None*, *dim=None*, *underline=None*, *overline=None*, *italic=None*, *blink=None*, *reverse=None*, *strikethrough=None*, *reset=True*)¶\nStyles a text with ANSI styles and returns the new string. By\ndefault the styling is self contained which means that at the end\nof the string a reset code is issued. This can be prevented by\npassing `reset=False`.\n\n\nExamples:\n\n```\nclick.echo(click.style('Hello World!', fg='green'))\nclick.echo(click.style('ATTENTION!', blink=True))\nclick.echo(click.style('Some things', reverse=True, fg='cyan'))\nclick.echo(click.style('More colors', fg=(255, 12, 128), bg=117))\n\n\nSupported color names:\n\n\n* `black` (might be a gray)\n* `red`\n* `green`\n* `yellow` (might be an orange)\n* `blue`\n* `magenta`\n* `cyan`\n* `white` (might be light gray)\n* `bright\\_black`\n* `bright\\_red`\n* `bright\\_green`\n* `bright\\_yellow`\n* `bright\\_blue`\n* `bright\\_magenta`\n* `bright\\_cyan`\n* `bright\\_white`\n* `reset` (reset the color code only)\n\n\nIf the terminal supports it, color may also be specified as:\n\n\n* An integer in the interval [0, 255]. The terminal must support\n8-bit/256-color mode.\n* An RGB tuple of three integers in [0, 255]. The terminal must\nsupport 24-bit/true-color mode.\n\n\nSee https://en.wikipedia.org/wiki/ANSI\\_color and\nhttps://gist.github.com/XVilka/8346728 for more information.\n\nParameters:\n* **text** (*Any*) – the string to style with ansi codes.\n* **fg** (*int* *|* *Tuple**[**int**,* *int**,* *int**]* *|* *str* *|* *None*) – if provided this will become the foreground color.\n* **bg** (*int* *|* *Tuple**[**int**,* *int**,* *int**]* *|* *str* *|* *None*) – if provided this will become the background color.\n* **bold** (*bool* *|* *None*) – if provided this will enable or disable bold mode.\n* **dim** (*bool* *|* *None*) – if provided this will enable or disable dim mode. This is\nbadly supported.\n* **underline** (*bool* *|* *None*) – if provided this will enable or disable underline.\n* **overline** (*bool* *|* *None*) – if provided this will enable or disable overline.\n* **italic** (*bool* *|* *None*) – if provided this will enable or disable italic.\n* **blink** (*bool* *|* *None*) – if provided this will enable or disable blinking.\n* **reverse** (*bool* *|* *None*) – if provided this will enable or disable inverse\nrendering (foreground becomes background and the\nother way round).\n* **strikethrough** (*bool* *|* *None*) – if provided this will enable or disable\nstriking through text.\n* **reset** (*bool*) – by default a reset-all code is added at the end of the\nstring which means that styles do not carry over. This\ncan be disabled to compose styles.\n\nReturn type:\nstr\n\nChangelog\nChanged in version 8.0: A non-string `message` is converted to a string.\n\n\nChanged in version 8.0: Added support for 256 and RGB color codes.\n\n\nChanged in version 8.0: Added the `strikethrough`, `italic`, and `overline`\nparameters.\n\n\nChanged in version 7.0: Added support for bright colors.\n\n\nclick.unstyle(*text*)¶\nRemoves ANSI styling information from a string. Usually it’s not\nnecessary to use this function as Click’s echo function will\nautomatically remove styling if necessary.\n\n\nParameters:\n**text** (*str*) – the text to remove style information from.\n\nclick.secho(*message=None*, *file=None*, *nl=True*, *err=False*, *color=None*, *\\*\\*styles*)¶\nThis function combines `echo()` and `style()` into one\ncall. As such the following two calls are the same:\n\n```\nclick.secho('Hello World!', fg='green')\nclick.echo(click.style('Hello World!', fg='green'))\n\n\nAll keyword arguments are forwarded to the underlying functions\ndepending on which one they go with.\n\n\nNon-string types will be converted to `str`. However,\n`bytes` are passed directly to `echo()` without applying\nstyle. If you want to style bytes that represent text, call\n`bytes.decode()` first.\n\nChangelog\nChanged in version 8.0: A non-string `message` is converted to a string. Bytes are\npassed through without style applied.\n\n\nParameters:\n* **message** (*Any* *|* *None*) –\n* **file** (*IO* *|* *None*) –\n* **nl** (*bool*) –\n* **err** (*bool*) –\n* **color** (*bool* *|* *None*) –\n* **styles** (*Any*) –\n\nclick.edit(*text=None*, *editor=None*, *env=None*, *require\\_save=True*, *extension='.txt'*, *filename=None*)¶\nEdits the given text in the defined editor. If an editor is given\n(should be the full path to the executable but the regular operating\nsystem search path is used for finding the executable) it overrides\nthe detected editor. Optionally, some environment variables can be\nused. If the editor is closed without changes, None is returned. In\ncase a file is edited directly the return value is always None and\nrequire\\_save and extension are ignored.\n\n\nIf the editor cannot be opened a `UsageError` is raised.\n\n\nNote for Windows: to simplify cross-platform usage, the newlines are\nautomatically converted from POSIX to Windows and vice versa. As such,\nthe message here will have `\\n` as newline markers.\n\nParameters:\n* **text** (*AnyStr* *|* *None*) – the text to edit.\n* **editor** (*str* *|* *None*) – optionally the editor to use. Defaults to automatic\ndetection.\n* **env** (*Mapping**[**str**,* *str**]* *|* *None*) – environment variables to forward to the editor.\n* **require\\_save** (*bool*) – if this is true, then not saving in the editor\nwill make the return value become None.\n* **extension** (*str*) – the extension to tell the editor about. This defaults\nto .txt but changing this might change syntax\nhighlighting.\n* **filename** (*str* *|* *None*) – if provided it will edit this file instead of the\nprovided text contents. It will not use a temporary\nfile as an indirection in that case.\n\nReturn type:\nAnyStr | None\n\nclick.launch(*url*, *wait=False*, *locate=False*)¶\nThis function launches the given URL (or filename) in the default\nviewer application for this file type. If this is an executable, it\nmight launch the executable in a new session. The return value is\nthe exit code of the launched application. Usually, `0` indicates\nsuccess.\n\n```\nclick.launch('https://click.palletsprojects.com/')\nclick.launch('/my/downloaded/file', locate=True)\n\n\nParameters:\n* **url** (*str*) – URL or filename of the thing to launch.\n* **wait** (*bool*) – Wait for the program to exit before returning. This\nonly works if the launched program blocks. In particular,\n`xdg-open` on Linux does not block.\n* **locate** (*bool*) – if this is set to True then instead of launching the\napplication associated with the URL it will attempt to\nlaunch a file manager with the file located. This\nmight have weird effects if the URL does not point to\nthe filesystem.\n\nReturn type:\nint\n\nclick.getchar(*echo=False*)¶\nFetches a single character from the terminal and returns it. This\nwill always return a unicode character and under certain rare\ncircumstances this might return more than one character. The\nsituations which more than one character is returned is when for\nwhatever reason multiple characters end up in the terminal buffer or\nstandard input was not actually a terminal.\n\n\nNote that this will always read from the terminal, even if something\nis piped into the standard input.\n\n\nNote for Windows: in rare cases when typing non-ASCII characters, this\nfunction might wait for a second character and then return both at once.\nThis is because certain Unicode characters look like special-key markers.\n\n\nParameters:\n**echo** (*bool*) – if set to True, the character read will also show up on\nthe terminal. The default is to not show it.\n\nclick.pause(*info=None*, *err=False*)¶\nThis command stops execution and waits for the user to press any\nkey to continue. This is similar to the Windows batch “pause”\ncommand. If the program is not run through a terminal, this command\nwill instead do nothing.\n\nChangelog\nNew in version 4.0: Added the err parameter.\n\n\nParameters:\n* **info** (*str* *|* *None*) – The message to print before pausing. Defaults to\n`\"Press any key to continue...\"`.\n* **err** (*bool*) – if set to message goes to `stderr` instead of\n`stdout`, the same as with echo.\n\nclick.get\\_binary\\_stream(*name*)¶\nReturns a system stream for byte processing.\n\nParameters:\n**name** (*te.Literal**[**'stdin'**,* *'stdout'**,* *'stderr'**]*) – the name of the stream to open. Valid names are `'stdin'`,\n`'stdout'` and `'stderr'`\n\nReturn type:\n*BinaryIO*\n\nclick.get\\_text\\_stream(*name*, *encoding=None*, *errors='strict'*)¶\nReturns a system stream for text processing. This usually returns\na wrapped stream around a binary stream returned from\n`get\\_binary\\_stream()` but it also can take shortcuts for already\ncorrectly configured streams.\n\nParameters:\n* **name** (*te.Literal**[**'stdin'**,* *'stdout'**,* *'stderr'**]*) – the name of the stream to open. Valid names are `'stdin'`,\n`'stdout'` and `'stderr'`\n* **encoding** (*str* *|* *None*) – overrides the detected default encoding.\n* **errors** (*str* *|* *None*) – overrides the default error mode.\n\nReturn type:\n*TextIO*\n\nclick.open\\_file(*filename*, *mode='r'*, *encoding=None*, *errors='strict'*, *lazy=False*, *atomic=False*)¶\nOpen a file, with extra behavior to handle `'-'` to indicate\na standard stream, lazy open on write, and atomic write. Similar to\nthe behavior of the `File` param type.\n\n\nIf `'-'` is given to open `stdout` or `stdin`, the stream is\nwrapped so that using it in a context manager will not close it.\nThis makes it possible to use the function without accidentally\nclosing a standard stream:\n\n```\nwith open\\_file(filename) as f:\n    ...\n\nParameters:\n* **filename** (*str*) – The name of the file to open, or `'-'` for\n`stdin`/`stdout`.\n* **mode** (*str*) – The mode in which to open the file.\n* **encoding** (*str* *|* *None*) – The encoding to decode or encode a file opened in\ntext mode.\n* **errors** (*str* *|* *None*) – The error handling mode.\n* **lazy** (*bool*) – Wait to open the file until it is accessed. For read\nmode, the file is temporarily opened to raise access errors\nearly, then closed until it is read again.\n* **atomic** (*bool*) – Write to a temporary file and replace the given file\non close.\n\nReturn type:\n*IO*[*Any*]\n\nChangelog\nNew in version 3.0.\n\n\nclick.get\\_app\\_dir(*app\\_name*, *roaming=True*, *force\\_posix=False*)¶\nReturns the config folder for the application. The default behavior\nis to return whatever is most appropriate for the operating system.\n\n\nTo give you an idea, for an app called `\"Foo Bar\"`, something like\nthe following folders could be returned:\n\nMac OS X:`~/Library/Application Support/Foo Bar`\n\nMac OS X (POSIX):`~/.foo-bar`\n\nUnix:`~/.config/foo-bar`\n\nUnix (POSIX):`~/.foo-bar`\n\nWindows (roaming):`C:\\Users\\<user>\\AppData\\Roaming\\Foo Bar`\n\nWindows (not roaming):`C:\\Users\\<user>\\AppData\\Local\\Foo Bar`\n\n\nParameters:\n* **app\\_name** (*str*) – the application name. This should be properly capitalized\nand can contain whitespace.\n* **roaming** (*bool*) – controls if the folder should be roaming or not on Windows.\nHas no effect otherwise.\n* **force\\_posix** (*bool*) – if this is set to True then on any POSIX system the\nfolder will be stored in the home folder with a leading\ndot instead of the XDG config home or darwin’s\napplication support folder.\n\nclick.format\\_filename(*filename*, *shorten=False*)¶\nFormat a filename as a string for display. Ensures the filename can be\ndisplayed by replacing any invalid bytes or surrogate escapes in the name\nwith the replacement character `�`.\n\n\nInvalid bytes or surrogate escapes will raise an error when written to a\nstream with `errors=\"strict\". This will typically happen with ``stdout`\nwhen the locale is something like `en\\_GB.UTF-8`.\n\n\nMany scenarios *are* safe to write surrogates though, due to PEP 538 and\nPEP 540, including:\n\n\n* Writing to `stderr`, which uses `errors=\"backslashreplace\"`.\n* The system has `LANG=C.UTF-8`, `C`, or `POSIX`. Python opens\nstdout and stderr with `errors=\"surrogateescape\"`.\n* None of `LANG/LC\\_\\*` are set. Python assumes `LANG=C.UTF-8`.\n* Python is started in UTF-8 mode with `PYTHONUTF8=1` or `-X utf8`.\nPython opens stdout and stderr with `errors=\"surrogateescape\"`.\n\nParameters:\n* **filename** (*str* *|* *bytes* *|* *PathLike**[**str**]* *|* *PathLike**[**bytes**]*) – formats a filename for UI display. This will also convert\nthe filename into unicode without failing.\n* **shorten** (*bool*) – this optionally shortens the filename to strip of the\npath that leads up to it.\n## Commands¶\n\n\n*class* click.BaseCommand(*name*, *context\\_settings=None*)¶\nThe base command implements the minimal API contract of commands.\nMost code will never use this as it does not implement a lot of useful\nfunctionality but it can act as the direct subclass of alternative\nparsing methods that do\n\n==================\n Document 3 \n----------------\n# Formatting¶\n\n\n*class* click.HelpFormatter(*indent\\_increment=2*, *width=None*, *max\\_width=None*)¶\nThis class helps with formatting text-based help pages. It’s\nusually just needed for very special internal cases, but it’s also\nexposed so that developers can write their own fancy outputs.\n\n\nAt present, it always writes into memory.\n\nParameters:\n* **indent\\_increment** (*int*) – the additional increment for each level.\n* **width** (*int* *|* *None*) – the width for the text. This defaults to the terminal\nwidth clamped to a maximum of 78.\n* **max\\_width** (*int* *|* *None*) –\n\n\ndedent()¶\nDecreases the indentation.\n\ngetvalue()¶\nReturns the buffer contents.\n\nindent()¶\nIncreases the indentation.\n\nindentation()¶\nA context manager that increases the indentation.\n\nReturn type:\n*Iterator*[None]\n\nsection(*name*)¶\nHelpful context manager that writes a paragraph, a heading,\nand the indents.\n\nParameters:\n**name** (*str*) – the section name that is written as heading.\n\nwrite(*string*)¶\nWrites a unicode string into the internal buffer.\n\nParameters:\n**string** (*str*) – \n\nwrite\\_dl(*rows*, *col\\_max=30*, *col\\_spacing=2*)¶\nWrites a definition list into the buffer. This is how options\nand commands are usually formatted.\n\nParameters:\n* **rows** (*Sequence**[**Tuple**[**str**,* *str**]**]*) – a list of two item tuples for the terms and values.\n* **col\\_max** (*int*) – the maximum width of the first column.\n* **col\\_spacing** (*int*) – the number of spaces between the first and\nsecond column.\n\nwrite\\_heading(*heading*)¶\nWrites a heading into the buffer.\n\nParameters:\n**heading** (*str*) – \n\nwrite\\_paragraph()¶\nWrites a paragraph into the buffer.\n\nwrite\\_text(*text*)¶\nWrites re-indented text into the buffer. This rewraps and\npreserves paragraphs.\n\nParameters:\n**text** (*str*) – \n\nwrite\\_usage(*prog*, *args=''*, *prefix=None*)¶\nWrites a usage line into the buffer.\n\nParameters:\n* **prog** (*str*) – the program name.\n* **args** (*str*) – whitespace separated list of arguments.\n* **prefix** (*str* *|* *None*) – The prefix for the first line. Defaults to\n`\"Usage: \"`.\n\n\nclick.wrap\\_text(*text*, *width=78*, *initial\\_indent=''*, *subsequent\\_indent=''*, *preserve\\_paragraphs=False*)¶\nA helper function that intelligently wraps text. By default, it\nassumes that it operates on a single paragraph of text but if the\npreserve\\_paragraphs parameter is provided it will intelligently\nhandle paragraphs (defined by two empty lines).\n\n\nIf paragraphs are handled, a paragraph can be prefixed with an empty\nline containing the `\\b` character (`\\x08`) to indicate that\nno rewrapping should happen in that block.\n\nParameters:\n* **text** (*str*) – the text that should be rewrapped.\n* **width** (*int*) – the maximum width for the text.\n* **initial\\_indent** (*str*) – the initial indent that should be placed on the\nfirst line as a string.\n* **subsequent\\_indent** (*str*) – the indent string that should be placed on\neach consecutive line.\n* **preserve\\_paragraphs** (*bool*) – if this flag is set then the wrapping will\nintelligently handle paragraphs.\n## Parsing¶\n\n\n*class* click.OptionParser(*ctx=None*)¶\nThe option parser is an internal class that is ultimately used to\nparse options and arguments. It’s modelled after optparse and brings\na similar but vastly simplified API. It should generally not be used\ndirectly as the high level Click classes"
    },
    {
        "instruction": "Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on the 'employee_id' column, and produce a comparison report. Ensure that it reports matches and does not ignore extra columns.",
        "api": "datacompy",
        "output": "#!pip install datacompy\nfrom io import StringIO\nimport pandas as pd\nimport datacompy\n\ndata1 = \"\"\"employee_id, name\n1, islam mesabah\n\"\"\"\n\ndata2 = \"\"\"employee_id, name\n1, islam mesabah\n\"\"\"\n\ndf1 = pd.read_csv(StringIO(data1))\ndf2 = pd.read_csv(StringIO(data2))\n\ncompare = datacompy.Compare(\n  df1,\n  df2,\n  # You can also specify a list\n  # of columns\n  join_columns = 'employee_id',\n\n  # Optional, defaults to 'df1'\n  df1_name = 'Original',\n\n  # Optional, defaults to 'df2'\n  df2_name = 'New'\n  )\ncompare.matches(ignore_extra_columns = False)\nprint(compare.report())",
        "documentation": "\n\n==================\n Document 0 \n----------------\n datacompy#\n\n* datacompy package\n\t+ Submodules\n\t+ datacompy.core module\n\t\t- `Compare`\n\t\t\t* `Compare.all\\_columns\\_match()`\n\t\t\t* `Compare.all\\_mismatch()`\n\t\t\t* `Compare.all\\_rows\\_overlap()`\n\t\t\t* `Compare.count\\_matching\\_rows()`\n\t\t\t* `Compare.df1`\n\t\t\t* `Compare.df1\\_unq\\_columns()`\n\t\t\t* `Compare.df2`\n\t\t\t* `Compare.df2\\_unq\\_columns()`\n\t\t\t* `Compare.intersect\\_columns()`\n\t\t\t* `Compare.intersect\\_rows\\_match()`\n\t\t\t* `Compare.matches()`\n\t\t\t* `Compare.report()`\n\t\t\t* `Compare.sample\\_mismatch()`\n\t\t\t* `Compare.subset()`\n\t\t- `calculate\\_max\\_diff()`\n\t\t- `columns\\_equal()`\n\t\t- `compare\\_string\\_and\\_date\\_columns()`\n\t\t- `generate\\_id\\_within\\_group()`\n\t\t- `get\\_merged\\_columns()`\n\t\t- `render()`\n\t\t- `temp\\_column\\_name()`\n\t+ datacompy.fugue module\n\t\t- `all\\_columns\\_match()`\n\t\t- `intersect\\_columns()`\n\t\t- `is\\_match()`\n\t\t- `report()`\n\t\t- `unq\\_columns()`\n\t+ datacompy.spark module\n\t\t- `MatchType`\n\t\t\t* `MatchType.KNOWN\\_DIFFERENCE`\n\t\t\t* `MatchType.MATCH`\n\t\t\t* `MatchType.MISMATCH`\n\t\t- `SparkCompare`\n\t\t\t* `SparkCompare.base\\_row\\_count`\n\t\t\t* `SparkCompare.columns\\_compared`\n\t\t\t* `SparkCompare.columns\\_in\\_both`\n\t\t\t* `SparkCompare.columns\\_only\\_base`\n\t\t\t* `SparkCompare.columns\\_only\\_compare`\n\t\t\t* `SparkCompare.common\\_row\\_count`\n\t\t\t* `SparkCompare.compare\\_row\\_count`\n\t\t\t* `SparkCompare.report()`\n\t\t\t* `SparkCompare.rows\\_both\\_all`\n\t\t\t* `SparkCompare.rows\\_both\\_mismatch`\n\t\t\t* `SparkCompare.rows\\_only\\_base`\n\t\t\t* `SparkCompare.rows\\_only\\_compare`\n\t\t- `decimal\\_comparator()`\n\t+ Module contents\n\n\n# datacompy package#\n\n\n## Submodules#\n\n\n## datacompy.core module#\n\n\nCompare two Pandas DataFrames\n\n\nOriginally this package was meant to provide similar functionality to\nPROC COMPARE in SAS - i.e. human-readable reporting on the difference between\ntwo dataframes.\n\n\n*class* datacompy.core.Compare(*df1*, *df2*, *join\\_columns=None*, *on\\_index=False*, *abs\\_tol=0*, *rel\\_tol=0*, *df1\\_name='df1'*, *df2\\_name='df2'*, *ignore\\_spaces=False*, *ignore\\_case=False*, *cast\\_column\\_names\\_lower=True*)#\nBases: `object`\n\n\nComparison class\n\n==================\n Document 1 \n----------------\n\n# datacompy#\n\n* datacompy package\n\t+ Submodules\n\t+ datacompy.core module\n\t\t- `Compare`\n\t\t\t* `Compare.all\\_columns\\_match()`\n\t\t\t* `Compare.all\\_mismatch()`\n\t\t\t* `Compare.all\\_rows\\_overlap()`\n\t\t\t* `Compare.count\\_matching\\_rows()`\n\t\t\t* `Compare.df1`\n\t\t\t* `Compare.df1\\_unq\\_columns()`\n\t\t\t* `Compare.df2`\n\t\t\t* `Compare.df2\\_unq\\_columns()`\n\t\t\t* `Compare.intersect\\_columns()`\n\t\t\t* `Compare.intersect\\_rows\\_match()`\n\t\t\t* `Compare.matches()`\n\t\t\t* `Compare.report()`\n\t\t\t* `Compare.sample\\_mismatch()`\n\t\t\t* `Compare.subset()`\n\t\t- `calculate\\_max\\_diff()`\n\t\t- `columns\\_equal()`\n\t\t- `compare\\_string\\_and\\_date\\_columns()`\n\t\t- `generate\\_id\\_within\\_group()`\n\t\t- `get\\_merged\\_columns()`\n\t\t- `render()`\n\t\t- `temp\\_column\\_name()`\n\t+ datacompy.fugue module\n\t\t- `all\\_columns\\_match()`\n\t\t- `intersect\\_columns()`\n\t\t- `is\\_match()`\n\t\t- `report()`\n\t\t- `unq\\_columns()`\n\t+ datacompy.spark module\n\t\t- `MatchType`\n\t\t\t* `MatchType.KNOWN\\_DIFFERENCE`\n\t\t\t*\n\n==================\n Document 2 \n----------------\n# datacompy.fugue module#\n\n\nCompare two DataFrames that are supported by Fugue\n\n\ndatacompy.fugue.all\\_columns\\_match(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nWhether the columns all match in the dataframes\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n\nReturns:\nBoolean indicating whether the columns all match in the dataframes\n\ndatacompy.fugue.intersect\\_columns(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nGet columns that are shared between the two dataframes\n\nReturns:\nSet of that are shared between the two dataframes\n\nReturn type:\nOrderedSet\n\ndatacompy.fugue.is\\_match(*df1: AnyDataFrame*, *df2: AnyDataFrame*, *join\\_columns: str | List[str]*, *abs\\_tol: float = 0*, *rel\\_tol: float = 0*, *df1\\_name: str = 'df1'*, *df2\\_name: str = 'df2'*, *ignore\\_spaces: bool = False*, *ignore\\_case: bool = False*, *cast\\_column\\_names\\_lower: bool = True*, *parallelism: int | None = None*, *strict\\_schema: bool = False*) → bool#\nCheck whether two dataframes match.\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n* **join\\_columns** (*list* *or* *str**,* *optional*) – Column(s) to join dataframes on. If a string is passed in, that one\ncolumn will be used.\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance between two values.\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance between two values.\n* **df1\\_name** (*str**,* *optional*) – A string name for the first dataframe. This allows the reporting to\nprint out an actual name instead of “df1”, and allows human users to\nmore easily track the dataframes.\n* **df2\\_name** (*str**,* *optional*) – A string name for the second dataframe\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns (including any join\ncolumns)\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n* **cast\\_column\\_names\\_lower** (*bool**,* *optional*) – Boolean indicator that controls of column names will be cast into lower case\n* **parallelism** (*int**,* *optional*) – An integer representing the amount of parallelism. Entering a value for this\nwill force to use of Fugue over just vanilla Pandas\n* **strict\\_schema** (*bool**,* *optional*) – The schema must match exactly if set to `True`. This includes the names and types. Allows for a fast fail.\n\nReturns:\nReturns boolean as to if the DataFrames match.\n\ndatacompy.fugue.report(*df1: AnyDataFrame*, *df2: AnyDataFrame*, *join\\_columns: str | List[str]*, *abs\\_tol: float = 0*, *rel\\_tol: float = 0*, *df1\\_name: str = 'df1'*, *df2\\_name: str = 'df2'*, *ignore\\_spaces: bool = False*, *ignore\\_case: bool = False*, *cast\\_column\\_names\\_lower: bool = True*, *sample\\_count=10*, *column\\_count=10*, *html\\_file=None*, *parallelism: int | None = None*) → None#\nReturns a string representation of a report. The representation can\nthen be printed or saved to a file.\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n* **join\\_columns** (*list* *or* *str**,* *optional*) – Column(s) to join dataframes on. If a string is passed in, that one\ncolumn will be used.\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance between two values.\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance between two values.\n* **df1\\_name** (*str**,* *optional*) – A string name for the first dataframe. This allows the reporting to\nprint out an actual name instead of “df1”, and allows human users to\nmore easily track the dataframes.\n* **df2\\_name** (*str**,* *optional*) – A string name for the second dataframe\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns (including any join\ncolumns)\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n* **cast\\_column\\_names\\_lower** (*bool**,* *optional*) – Boolean indicator that controls of column names will be cast into lower case\n* **parallelism** (*int**,* *optional*) – An integer representing the amount of parallelism. Entering a value for this\nwill force to use of Fugue over just vanilla Pandas\n* **strict\\_schema** (*bool**,* *optional*) – The schema must match exactly if set to `True`. This includes the names and types. Allows for a fast fail.\n* **sample\\_count** (*int**,* *optional*) – The number of sample records to return. Defaults to 10.\n* **column\\_count** (*int**,* *optional*) – The number of columns to display in the sample records output. Defaults to 10.\n* **html\\_file** (*str**,* *optional*) – HTML file name to save report output to. If `None` the file creation will be skipped.\n\ndatacompy.fugue.unq\\_columns(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nGet columns that are unique to df1\n\nReturns:\nSet of columns that are unique to df1\n## datacompy.spark module#\n\n\n*class* datacompy.spark.MatchType(*value*)#\nBases: `Enum`\n\n\nAn enumeration.\n\n\nKNOWN\\_DIFFERENCE *= 2*#\n\nMATCH *= 1*#\n\nMISMATCH *= 0*#\n\n\n*class* datacompy.spark.SparkCompare(*spark\\_session*, *base\\_df*, *compare\\_df*, *join\\_columns*, *column\\_mapping=None*, *cache\\_intermediates=False*, *known\\_differences=None*, *rel\\_tol=0*, *abs\\_tol=0*, *show\\_all\\_columns=False*, *match\\_rates=False*)#\nBases: `object`\n\n\nComparison class used to compare two Spark Dataframes.\n\n\nExtends the `Compare` functionality to the wide world of Spark\n\n==================\n Document 3 \n----------------\n# datacompy.core module#\n\n\nCompare two Pandas DataFrames\n\n\nOriginally this package was meant to provide similar functionality to\nPROC COMPARE in SAS - i.e. human-readable reporting on the difference between\ntwo dataframes.\n\n\n*class* datacompy.core.Compare(*df1*, *df2*, *join\\_columns=None*, *on\\_index=False*, *abs\\_tol=0*, *rel\\_tol=0*, *df1\\_name='df1'*, *df2\\_name='df2'*, *ignore\\_spaces=False*, *ignore\\_case=False*, *cast\\_column\\_names\\_lower=True*)#\nBases: `object`\n\n\nComparison class to be used to compare whether two dataframes as equal.\n\n\nBoth df1 and df2 should be dataframes containing all of the join\\_columns,\nwith unique column names. Differences between values are compared to\nabs\\_tol + rel\\_tol \\* abs(df2[‘value’]).\n\nParameters:\n* **df1** (pandas `DataFrame`) – First dataframe to check\n* **df2** (pandas `DataFrame`) – Second dataframe to check\n* **join\\_columns** (*list* *or* *str**,* *optional*) – Column(s) to join dataframes on. If a string is passed in, that one\ncolumn will be used.\n* **on\\_index** (*bool**,* *optional*) – If True, the index will be used to join the two dataframes. If both\n`join\\_columns` and `on\\_index` are provided, an exception will be\nraised.\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance between two values.\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance between two values.\n* **df1\\_name** (*str**,* *optional*) – A string name for the first dataframe. This allows the reporting to\nprint out an actual name instead of “df1”, and allows human users to\nmore easily track the dataframes.\n* **df2\\_name** (*str**,* *optional*) – A string name for the second dataframe\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns (including any join\ncolumns)\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n* **cast\\_column\\_names\\_lower** (*bool**,* *optional*) – Boolean indicator that controls of column names will be cast into lower case\n\nVariables:\n* **df1\\_unq\\_rows** (pandas `DataFrame`) – All records that are only in df1 (based on a join on join\\_columns)\n* **df2\\_unq\\_rows** (pandas `DataFrame`) – All records that are only in df2 (based on a join on join\\_columns)\n\n\nall\\_columns\\_match()#\nWhether the columns all match in the dataframes\n\nall\\_mismatch(*ignore\\_matching\\_cols=False*)#\nAll rows with any columns that have a mismatch. Returns all df1 and df2 versions of the columns and join\ncolumns.\n\nParameters:\n**ignore\\_matching\\_cols** (*bool**,* *optional*) – Whether showing the matching columns in the output or not. The default is False.\n\nReturns:\nAll rows of the intersection dataframe, containing any columns, that don’t match.\n\nReturn type:\nPandas.DataFrame\n\nall\\_rows\\_overlap()#\nWhether the rows are all present in both dataframes\n\nReturns:\nTrue if all rows in df1 are in df2 and vice versa (based on\nexistence for join option)\n\nReturn type:\nbool\n\ncount\\_matching\\_rows()#\nCount the number of rows match (on overlapping fields)\n\nReturns:\nNumber of matching rows\n\nReturn type:\nint\n\n*property* df1#\n\ndf1\\_unq\\_columns()#\nGet columns that are unique to df1\n\n*property* df2#\n\ndf2\\_unq\\_columns()#\nGet columns that are unique to df2\n\nintersect\\_columns()#\nGet columns that are shared between the two dataframes\n\nintersect\\_rows\\_match()#\nCheck whether the intersect rows all match\n\nmatches(*ignore\\_extra\\_columns=False*)#\nReturn True or False if the dataframes match.\n\nParameters:\n**ignore\\_extra\\_columns** (*bool*) – Ignores any columns in one dataframe and not in the other.\n\nreport(*sample\\_count=10*, *column\\_count=10*, *html\\_file=None*)#\nReturns a string representation of a report. The representation can\nthen be printed or saved to a file.\n\nParameters:\n* **sample\\_count** (*int**,* *optional*) – The number of sample records to return. Defaults to 10.\n* **column\\_count** (*int**,* *optional*) – The number of columns to display in the sample records output. Defaults to 10.\n* **html\\_file** (*str**,* *optional*) – HTML file name to save report output to. If `None` the file creation will be skipped.\n\nReturns:\nThe report, formatted kinda nicely.\n\nReturn type:\nstr\n\nsample\\_mismatch(*column*, *sample\\_count=10*, *for\\_display=False*)#\nReturns a sample sub-dataframe which contains the identifying\ncolumns, and df1 and df2 versions of the column.\n\nParameters:\n* **column** (*str*) – The raw column name (i.e. without `\\_df1` appended)\n* **sample\\_count** (*int**,* *optional*) – The number of sample records to return. Defaults to 10.\n* **for\\_display** (*bool**,* *optional*) – Whether this is just going to be used for display (overwrite the\ncolumn names)\n\nReturns:\nA sample of the intersection dataframe, containing only the\n“pertinent” columns, for rows that don’t match on the provided\ncolumn.\n\nsubset()#\nReturn True if dataframe 2 is a subset of dataframe 1.\n\n\nDataframe 2 is considered a subset if all of its columns are in\ndataframe 1, and all of its rows match rows in dataframe 1 for the\nshared columns.\n\n\ndatacompy.core.calculate\\_max\\_diff(*col\\_1*, *col\\_2*)#\nGet a maximum difference between two columns\n\nParameters:\n* **col\\_1** (*Pandas.Series*) – The first column\n* **col\\_2** (*Pandas.Series*) – The second column\n\nReturns:\nNumeric field, or zero.\n\nReturn type:\nNumeric\n\ndatacompy.core.columns\\_equal(*col\\_1*, *col\\_2*, *rel\\_tol=0*, *abs\\_tol=0*, *ignore\\_spaces=False*, *ignore\\_case=False*)#\nCompares two columns from a dataframe, returning a True/False series,\nwith the same index as column 1.\n\n\n* Two nulls (np.nan) will evaluate to True.\n* A null and a non-null value will evaluate to False.\n* Numeric values will use the relative and absolute tolerances.\n* Decimal values (decimal.Decimal) will attempt to be converted to floats\nbefore comparing\n* Non-numeric values (i.e. where np.isclose can’t be used) will just\ntrigger True on two nulls or exact matches.\n\nParameters:\n* **col\\_1** (*Pandas.Series*) – The first column to look at\n* **col\\_2** (*Pandas.Series*) – The second column\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n\nReturns:\nA series of Boolean values. True == the values match, False == the\nvalues don’t match.\n\nReturn type:\npandas.Series\n\ndatacompy.core.compare\\_string\\_and\\_date\\_columns(*col\\_1*, *col\\_2*)#\nCompare a string column and date column, value-wise. This tries to\nconvert a string column to a date column and compare that way.\n\nParameters:\n* **col\\_1** (*Pandas.Series*) – The first column to look at\n* **col\\_2** (*Pandas.Series*) – The second column\n\ndatacompy.core.generate\\_id\\_within\\_group(*dataframe*, *join\\_columns*)#\nGenerate an ID column that can be used to deduplicate identical rows. The series generated\nis the order within a unique group, and it handles nulls.\n\nParameters:\n* **dataframe** (*Pandas.DataFrame*) – The dataframe to operate on\n* **join\\_columns** (*list*) – List of strings which are the join columns\n\nReturns:\nThe ID column that’s unique in each group.\n\nReturn type:\nPandas.Series\n\ndatacompy.core.get\\_merged\\_columns(*original\\_df*, *merged\\_df*, *suffix*)#\nGets the columns from an original dataframe, in the new merged dataframe\n\nParameters:\n* **original\\_df** (*Pandas.DataFrame*) – The original, pre-merge dataframe\n* **merged\\_df** (*Pandas.DataFrame*) – Post-merge with another dataframe, with suffixes added in.\n* **suffix** (*str*) – What suffix was used to distinguish when the original dataframe was\noverlapping with the other merged dataframe.\n\ndatacompy.core.render(*filename*, *\\*fields*)#\nRenders out an individual template. This basically just reads in a\ntemplate file, and applies `.format()` on the fields.\n\nParameters:\n* **filename** (*str*) – The file that contains the template. Will automagically prepend the\ntemplates directory before opening\n* **fields** (*list*) – Fields to be rendered out in the template\n\nReturns:\nThe fully rendered out file.\n\ndatacompy.core.temp\\_column\\_name(*\\*dataframes*)#\nGets a temp column name that isn’t included in columns of any dataframes\n\nParameters:\n**dataframes** (*list* *of* *Pandas.DataFrame*) – The DataFrames to create a temporary column name for\n\nReturns:\nString column name that looks like ‘\\_temp\\_x’ for some integer x\n## datacompy.fugue module#\n\n\nCompare two DataFrames that are supported by Fugue\n\n\ndatacompy.fugue.all\\_columns\\_match(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nWhether the columns all match in the dataframes\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n\nReturns:\nBoolean indicating whether the columns all match"
    },
    {
        "instruction": "Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on multiple columns ('employee_id' and 'department'), and produce a comparison report. Ensure that it reports matches and ignores extra columns.",
        "api": "datacompy",
        "output": "#!pip install datacompy\nfrom io import StringIO\nimport pandas as pd\nimport datacompy\n\ndata1 = \"\"\"employee_id, name, department\n1, islam mesabah, IT\n2, john doe, HR\n\"\"\"\n\ndata2 = \"\"\"employee_id, name, department\n1, islam mesabah, IT\n2, john doe, HR\n\"\"\"\n\ndf1 = pd.read_csv(StringIO(data1))\ndf2 = pd.read_csv(StringIO(data2))\n\ncompare = datacompy.Compare(\n  df1,\n  df2,\n  # You can also specify a list\n  # of columns\n  join_columns = ['employee_id', 'department'],\n\n  # Optional, defaults to 'df1'\n  df1_name = 'Original',\n\n  # Optional, defaults to 'df2'\n  df2_name = 'New'\n  )\ncompare.matches(ignore_extra_columns = True)\nprint(compare.report())",
        "documentation": "\n\n==================\n Document 0 \n----------------\n datacompy#\n\n* datacompy package\n\t+ Submodules\n\t+ datacompy.core module\n\t\t- `Compare`\n\t\t\t* `Compare.all\\_columns\\_match()`\n\t\t\t* `Compare.all\\_mismatch()`\n\t\t\t* `Compare.all\\_rows\\_overlap()`\n\t\t\t* `Compare.count\\_matching\\_rows()`\n\t\t\t* `Compare.df1`\n\t\t\t* `Compare.df1\\_unq\\_columns()`\n\t\t\t* `Compare.df2`\n\t\t\t* `Compare.df2\\_unq\\_columns()`\n\t\t\t* `Compare.intersect\\_columns()`\n\t\t\t* `Compare.intersect\\_rows\\_match()`\n\t\t\t* `Compare.matches()`\n\t\t\t* `Compare.report()`\n\t\t\t* `Compare.sample\\_mismatch()`\n\t\t\t* `Compare.subset()`\n\t\t- `calculate\\_max\\_diff()`\n\t\t- `columns\\_equal()`\n\t\t- `compare\\_string\\_and\\_date\\_columns()`\n\t\t- `generate\\_id\\_within\\_group()`\n\t\t- `get\\_merged\\_columns()`\n\t\t- `render()`\n\t\t- `temp\\_column\\_name()`\n\t+ datacompy.fugue module\n\t\t- `all\\_columns\\_match()`\n\t\t- `intersect\\_columns()`\n\t\t- `is\\_match()`\n\t\t- `report()`\n\t\t- `unq\\_columns()`\n\t+ datacompy.spark module\n\t\t- `MatchType`\n\t\t\t* `MatchType.KNOWN\\_DIFFERENCE`\n\t\t\t* `MatchType.MATCH`\n\t\t\t* `MatchType.MISMATCH`\n\t\t- `SparkCompare`\n\t\t\t* `SparkCompare.base\\_row\\_count`\n\t\t\t* `SparkCompare.columns\\_compared`\n\t\t\t* `SparkCompare.columns\\_in\\_both`\n\t\t\t* `SparkCompare.columns\\_only\\_base`\n\t\t\t* `SparkCompare.columns\\_only\\_compare`\n\t\t\t* `SparkCompare.common\\_row\\_count`\n\t\t\t* `SparkCompare.compare\\_row\\_count`\n\t\t\t* `SparkCompare.report()`\n\t\t\t* `SparkCompare.rows\\_both\\_all`\n\t\t\t* `SparkCompare.rows\\_both\\_mismatch`\n\t\t\t* `SparkCompare.rows\\_only\\_base`\n\t\t\t* `SparkCompare.rows\\_only\\_compare`\n\t\t- `decimal\\_comparator()`\n\t+ Module contents\n\n\n# datacompy package#\n\n\n## Submodules#\n\n\n## datacompy.core module#\n\n\nCompare two Pandas DataFrames\n\n\nOriginally this package was meant to provide similar functionality to\nPROC COMPARE in SAS - i.e. human-readable reporting on the difference between\ntwo dataframes.\n\n\n*class* datacompy.core.Compare(*df1*, *df2*, *join\\_columns=None*, *on\\_index=False*, *abs\\_tol=0*, *rel\\_tol=0*, *df1\\_name='df1'*, *df2\\_name='df2'*, *ignore\\_spaces=False*, *ignore\\_case=False*, *cast\\_column\\_names\\_lower=True*)#\nBases: `object`\n\n\nComparison class\n\n==================\n Document 1 \n----------------\n\n# datacompy#\n\n* datacompy package\n\t+ Submodules\n\t+ datacompy.core module\n\t\t- `Compare`\n\t\t\t* `Compare.all\\_columns\\_match()`\n\t\t\t* `Compare.all\\_mismatch()`\n\t\t\t* `Compare.all\\_rows\\_overlap()`\n\t\t\t* `Compare.count\\_matching\\_rows()`\n\t\t\t* `Compare.df1`\n\t\t\t* `Compare.df1\\_unq\\_columns()`\n\t\t\t* `Compare.df2`\n\t\t\t* `Compare.df2\\_unq\\_columns()`\n\t\t\t* `Compare.intersect\\_columns()`\n\t\t\t* `Compare.intersect\\_rows\\_match()`\n\t\t\t* `Compare.matches()`\n\t\t\t* `Compare.report()`\n\t\t\t* `Compare.sample\\_mismatch()`\n\t\t\t* `Compare.subset()`\n\t\t- `calculate\\_max\\_diff()`\n\t\t- `columns\\_equal()`\n\t\t- `compare\\_string\\_and\\_date\\_columns()`\n\t\t- `generate\\_id\\_within\\_group()`\n\t\t- `get\\_merged\\_columns()`\n\t\t- `render()`\n\t\t- `temp\\_column\\_name()`\n\t+ datacompy.fugue module\n\t\t- `all\\_columns\\_match()`\n\t\t- `intersect\\_columns()`\n\t\t- `is\\_match()`\n\t\t- `report()`\n\t\t- `unq\\_columns()`\n\t+ datacompy.spark module\n\t\t- `MatchType`\n\t\t\t* `MatchType.KNOWN\\_DIFFERENCE`\n\t\t\t*\n\n==================\n Document 2 \n----------------\n# datacompy.fugue module#\n\n\nCompare two DataFrames that are supported by Fugue\n\n\ndatacompy.fugue.all\\_columns\\_match(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nWhether the columns all match in the dataframes\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n\nReturns:\nBoolean indicating whether the columns all match in the dataframes\n\ndatacompy.fugue.intersect\\_columns(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nGet columns that are shared between the two dataframes\n\nReturns:\nSet of that are shared between the two dataframes\n\nReturn type:\nOrderedSet\n\ndatacompy.fugue.is\\_match(*df1: AnyDataFrame*, *df2: AnyDataFrame*, *join\\_columns: str | List[str]*, *abs\\_tol: float = 0*, *rel\\_tol: float = 0*, *df1\\_name: str = 'df1'*, *df2\\_name: str = 'df2'*, *ignore\\_spaces: bool = False*, *ignore\\_case: bool = False*, *cast\\_column\\_names\\_lower: bool = True*, *parallelism: int | None = None*, *strict\\_schema: bool = False*) → bool#\nCheck whether two dataframes match.\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n* **join\\_columns** (*list* *or* *str**,* *optional*) – Column(s) to join dataframes on. If a string is passed in, that one\ncolumn will be used.\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance between two values.\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance between two values.\n* **df1\\_name** (*str**,* *optional*) – A string name for the first dataframe. This allows the reporting to\nprint out an actual name instead of “df1”, and allows human users to\nmore easily track the dataframes.\n* **df2\\_name** (*str**,* *optional*) – A string name for the second dataframe\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns (including any join\ncolumns)\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n* **cast\\_column\\_names\\_lower** (*bool**,* *optional*) – Boolean indicator that controls of column names will be cast into lower case\n* **parallelism** (*int**,* *optional*) – An integer representing the amount of parallelism. Entering a value for this\nwill force to use of Fugue over just vanilla Pandas\n* **strict\\_schema** (*bool**,* *optional*) – The schema must match exactly if set to `True`. This includes the names and types. Allows for a fast fail.\n\nReturns:\nReturns boolean as to if the DataFrames match.\n\ndatacompy.fugue.report(*df1: AnyDataFrame*, *df2: AnyDataFrame*, *join\\_columns: str | List[str]*, *abs\\_tol: float = 0*, *rel\\_tol: float = 0*, *df1\\_name: str = 'df1'*, *df2\\_name: str = 'df2'*, *ignore\\_spaces: bool = False*, *ignore\\_case: bool = False*, *cast\\_column\\_names\\_lower: bool = True*, *sample\\_count=10*, *column\\_count=10*, *html\\_file=None*, *parallelism: int | None = None*) → None#\nReturns a string representation of a report. The representation can\nthen be printed or saved to a file.\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n* **join\\_columns** (*list* *or* *str**,* *optional*) – Column(s) to join dataframes on. If a string is passed in, that one\ncolumn will be used.\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance between two values.\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance between two values.\n* **df1\\_name** (*str**,* *optional*) – A string name for the first dataframe. This allows the reporting to\nprint out an actual name instead of “df1”, and allows human users to\nmore easily track the dataframes.\n* **df2\\_name** (*str**,* *optional*) – A string name for the second dataframe\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns (including any join\ncolumns)\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n* **cast\\_column\\_names\\_lower** (*bool**,* *optional*) – Boolean indicator that controls of column names will be cast into lower case\n* **parallelism** (*int**,* *optional*) – An integer representing the amount of parallelism. Entering a value for this\nwill force to use of Fugue over just vanilla Pandas\n* **strict\\_schema** (*bool**,* *optional*) – The schema must match exactly if set to `True`. This includes the names and types. Allows for a fast fail.\n* **sample\\_count** (*int**,* *optional*) – The number of sample records to return. Defaults to 10.\n* **column\\_count** (*int**,* *optional*) – The number of columns to display in the sample records output. Defaults to 10.\n* **html\\_file** (*str**,* *optional*) – HTML file name to save report output to. If `None` the file creation will be skipped.\n\ndatacompy.fugue.unq\\_columns(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nGet columns that are unique to df1\n\nReturns:\nSet of columns that are unique to df1\n## datacompy.spark module#\n\n\n*class* datacompy.spark.MatchType(*value*)#\nBases: `Enum`\n\n\nAn enumeration.\n\n\nKNOWN\\_DIFFERENCE *= 2*#\n\nMATCH *= 1*#\n\nMISMATCH *= 0*#\n\n\n*class* datacompy.spark.SparkCompare(*spark\\_session*, *base\\_df*, *compare\\_df*, *join\\_columns*, *column\\_mapping=None*, *cache\\_intermediates=False*, *known\\_differences=None*, *rel\\_tol=0*, *abs\\_tol=0*, *show\\_all\\_columns=False*, *match\\_rates=False*)#\nBases: `object`\n\n\nComparison class used to compare two Spark Dataframes.\n\n\nExtends the `Compare` functionality to the wide world of Spark\n\n==================\n Document 3 \n----------------\n# datacompy.core module#\n\n\nCompare two Pandas DataFrames\n\n\nOriginally this package was meant to provide similar functionality to\nPROC COMPARE in SAS - i.e. human-readable reporting on the difference between\ntwo dataframes.\n\n\n*class* datacompy.core.Compare(*df1*, *df2*, *join\\_columns=None*, *on\\_index=False*, *abs\\_tol=0*, *rel\\_tol=0*, *df1\\_name='df1'*, *df2\\_name='df2'*, *ignore\\_spaces=False*, *ignore\\_case=False*, *cast\\_column\\_names\\_lower=True*)#\nBases: `object`\n\n\nComparison class to be used to compare whether two dataframes as equal.\n\n\nBoth df1 and df2 should be dataframes containing all of the join\\_columns,\nwith unique column names. Differences between values are compared to\nabs\\_tol + rel\\_tol \\* abs(df2[‘value’]).\n\nParameters:\n* **df1** (pandas `DataFrame`) – First dataframe to check\n* **df2** (pandas `DataFrame`) – Second dataframe to check\n* **join\\_columns** (*list* *or* *str**,* *optional*) – Column(s) to join dataframes on. If a string is passed in, that one\ncolumn will be used.\n* **on\\_index** (*bool**,* *optional*) – If True, the index will be used to join the two dataframes. If both\n`join\\_columns` and `on\\_index` are provided, an exception will be\nraised.\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance between two values.\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance between two values.\n* **df1\\_name** (*str**,* *optional*) – A string name for the first dataframe. This allows the reporting to\nprint out an actual name instead of “df1”, and allows human users to\nmore easily track the dataframes.\n* **df2\\_name** (*str**,* *optional*) – A string name for the second dataframe\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns (including any join\ncolumns)\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n* **cast\\_column\\_names\\_lower** (*bool**,* *optional*) – Boolean indicator that controls of column names will be cast into lower case\n\nVariables:\n* **df1\\_unq\\_rows** (pandas `DataFrame`) – All records that are only in df1 (based on a join on join\\_columns)\n* **df2\\_unq\\_rows** (pandas `DataFrame`) – All records that are only in df2 (based on a join on join\\_columns)\n\n\nall\\_columns\\_match()#\nWhether the columns all match in the dataframes\n\nall\\_mismatch(*ignore\\_matching\\_cols=False*)#\nAll rows with any columns that have a mismatch. Returns all df1 and df2 versions of the columns and join\ncolumns.\n\nParameters:\n**ignore\\_matching\\_cols** (*bool**,* *optional*) – Whether showing the matching columns in the output or not. The default is False.\n\nReturns:\nAll rows of the intersection dataframe, containing any columns, that don’t match.\n\nReturn type:\nPandas.DataFrame\n\nall\\_rows\\_overlap()#\nWhether the rows are all present in both dataframes\n\nReturns:\nTrue if all rows in df1 are in df2 and vice versa (based on\nexistence for join option)\n\nReturn type:\nbool\n\ncount\\_matching\\_rows()#\nCount the number of rows match (on overlapping fields)\n\nReturns:\nNumber of matching rows\n\nReturn type:\nint\n\n*property* df1#\n\ndf1\\_unq\\_columns()#\nGet columns that are unique to df1\n\n*property* df2#\n\ndf2\\_unq\\_columns()#\nGet columns that are unique to df2\n\nintersect\\_columns()#\nGet columns that are shared between the two dataframes\n\nintersect\\_rows\\_match()#\nCheck whether the intersect rows all match\n\nmatches(*ignore\\_extra\\_columns=False*)#\nReturn True or False if the dataframes match.\n\nParameters:\n**ignore\\_extra\\_columns** (*bool*) – Ignores any columns in one dataframe and not in the other.\n\nreport(*sample\\_count=10*, *column\\_count=10*, *html\\_file=None*)#\nReturns a string representation of a report. The representation can\nthen be printed or saved to a file.\n\nParameters:\n* **sample\\_count** (*int**,* *optional*) – The number of sample records to return. Defaults to 10.\n* **column\\_count** (*int**,* *optional*) – The number of columns to display in the sample records output. Defaults to 10.\n* **html\\_file** (*str**,* *optional*) – HTML file name to save report output to. If `None` the file creation will be skipped.\n\nReturns:\nThe report, formatted kinda nicely.\n\nReturn type:\nstr\n\nsample\\_mismatch(*column*, *sample\\_count=10*, *for\\_display=False*)#\nReturns a sample sub-dataframe which contains the identifying\ncolumns, and df1 and df2 versions of the column.\n\nParameters:\n* **column** (*str*) – The raw column name (i.e. without `\\_df1` appended)\n* **sample\\_count** (*int**,* *optional*) – The number of sample records to return. Defaults to 10.\n* **for\\_display** (*bool**,* *optional*) – Whether this is just going to be used for display (overwrite the\ncolumn names)\n\nReturns:\nA sample of the intersection dataframe, containing only the\n“pertinent” columns, for rows that don’t match on the provided\ncolumn.\n\nsubset()#\nReturn True if dataframe 2 is a subset of dataframe 1.\n\n\nDataframe 2 is considered a subset if all of its columns are in\ndataframe 1, and all of its rows match rows in dataframe 1 for the\nshared columns.\n\n\ndatacompy.core.calculate\\_max\\_diff(*col\\_1*, *col\\_2*)#\nGet a maximum difference between two columns\n\nParameters:\n* **col\\_1** (*Pandas.Series*) – The first column\n* **col\\_2** (*Pandas.Series*) – The second column\n\nReturns:\nNumeric field, or zero.\n\nReturn type:\nNumeric\n\ndatacompy.core.columns\\_equal(*col\\_1*, *col\\_2*, *rel\\_tol=0*, *abs\\_tol=0*, *ignore\\_spaces=False*, *ignore\\_case=False*)#\nCompares two columns from a dataframe, returning a True/False series,\nwith the same index as column 1.\n\n\n* Two nulls (np.nan) will evaluate to True.\n* A null and a non-null value will evaluate to False.\n* Numeric values will use the relative and absolute tolerances.\n* Decimal values (decimal.Decimal) will attempt to be converted to floats\nbefore comparing\n* Non-numeric values (i.e. where np.isclose can’t be used) will just\ntrigger True on two nulls or exact matches.\n\nParameters:\n* **col\\_1** (*Pandas.Series*) – The first column to look at\n* **col\\_2** (*Pandas.Series*) – The second column\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n\nReturns:\nA series of Boolean values. True == the values match, False == the\nvalues don’t match.\n\nReturn type:\npandas.Series\n\ndatacompy.core.compare\\_string\\_and\\_date\\_columns(*col\\_1*, *col\\_2*)#\nCompare a string column and date column, value-wise. This tries to\nconvert a string column to a date column and compare that way.\n\nParameters:\n* **col\\_1** (*Pandas.Series*) – The first column to look at\n* **col\\_2** (*Pandas.Series*) – The second column\n\ndatacompy.core.generate\\_id\\_within\\_group(*dataframe*, *join\\_columns*)#\nGenerate an ID column that can be used to deduplicate identical rows. The series generated\nis the order within a unique group, and it handles nulls.\n\nParameters:\n* **dataframe** (*Pandas.DataFrame*) – The dataframe to operate on\n* **join\\_columns** (*list*) – List of strings which are the join columns\n\nReturns:\nThe ID column that’s unique in each group.\n\nReturn type:\nPandas.Series\n\ndatacompy.core.get\\_merged\\_columns(*original\\_df*, *merged\\_df*, *suffix*)#\nGets the columns from an original dataframe, in the new merged dataframe\n\nParameters:\n* **original\\_df** (*Pandas.DataFrame*) – The original, pre-merge dataframe\n* **merged\\_df** (*Pandas.DataFrame*) – Post-merge with another dataframe, with suffixes added in.\n* **suffix** (*str*) – What suffix was used to distinguish when the original dataframe was\noverlapping with the other merged dataframe.\n\ndatacompy.core.render(*filename*, *\\*fields*)#\nRenders out an individual template. This basically just reads in a\ntemplate file, and applies `.format()` on the fields.\n\nParameters:\n* **filename** (*str*) – The file that contains the template. Will automagically prepend the\ntemplates directory before opening\n* **fields** (*list*) – Fields to be rendered out in the template\n\nReturns:\nThe fully rendered out file.\n\ndatacompy.core.temp\\_column\\_name(*\\*dataframes*)#\nGets a temp column name that isn’t included in columns of any dataframes\n\nParameters:\n**dataframes** (*list* *of* *Pandas.DataFrame*) – The DataFrames to create a temporary column name for\n\nReturns:\nString column name that looks like ‘\\_temp\\_x’ for some integer x\n## datacompy.fugue module#\n\n\nCompare two DataFrames that are supported by Fugue\n\n\ndatacompy.fugue.all\\_columns\\_match(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nWhether the columns all match in the dataframes\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n\nReturns:\nBoolean indicating whether the columns all match"
    },
    {
        "instruction": "Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on a specific column ('employee_id'), and produce a comparison report. Ensure that it reports matches and ignores extra columns.",
        "api": "datacompy",
        "output": "#!pip install datacompy\nfrom io import StringIO\nimport pandas as pd\nimport datacompy\n\ndata1 = \"\"\"employee_id, name, age\n1, islam mesabah, 25\n2, john doe, 30\n\"\"\"\n\ndata2 = \"\"\"employee_id, name, age\n1, islam mesabah, 25\n2, john doe, 30\n\"\"\"\n\ndf1 = pd.read_csv(StringIO(data1))\ndf2 = pd.read_csv(StringIO(data2))\n\ncompare = datacompy.Compare(\n  df1,\n  df2,\n  # You can also specify a list\n  # of columns\n  join_columns = 'employee_id',\n\n  # Optional, defaults to 'df1'\n  df1_name = 'Original',\n\n  # Optional, defaults to 'df2'\n  df2_name = 'New'\n  )\ncompare.matches(ignore_extra_columns = True)\nprint(compare.report())",
        "documentation": "\n\n==================\n Document 0 \n----------------\n datacompy#\n\n* datacompy package\n\t+ Submodules\n\t+ datacompy.core module\n\t\t- `Compare`\n\t\t\t* `Compare.all\\_columns\\_match()`\n\t\t\t* `Compare.all\\_mismatch()`\n\t\t\t* `Compare.all\\_rows\\_overlap()`\n\t\t\t* `Compare.count\\_matching\\_rows()`\n\t\t\t* `Compare.df1`\n\t\t\t* `Compare.df1\\_unq\\_columns()`\n\t\t\t* `Compare.df2`\n\t\t\t* `Compare.df2\\_unq\\_columns()`\n\t\t\t* `Compare.intersect\\_columns()`\n\t\t\t* `Compare.intersect\\_rows\\_match()`\n\t\t\t* `Compare.matches()`\n\t\t\t* `Compare.report()`\n\t\t\t* `Compare.sample\\_mismatch()`\n\t\t\t* `Compare.subset()`\n\t\t- `calculate\\_max\\_diff()`\n\t\t- `columns\\_equal()`\n\t\t- `compare\\_string\\_and\\_date\\_columns()`\n\t\t- `generate\\_id\\_within\\_group()`\n\t\t- `get\\_merged\\_columns()`\n\t\t- `render()`\n\t\t- `temp\\_column\\_name()`\n\t+ datacompy.fugue module\n\t\t- `all\\_columns\\_match()`\n\t\t- `intersect\\_columns()`\n\t\t- `is\\_match()`\n\t\t- `report()`\n\t\t- `unq\\_columns()`\n\t+ datacompy.spark module\n\t\t- `MatchType`\n\t\t\t* `MatchType.KNOWN\\_DIFFERENCE`\n\t\t\t* `MatchType.MATCH`\n\t\t\t* `MatchType.MISMATCH`\n\t\t- `SparkCompare`\n\t\t\t* `SparkCompare.base\\_row\\_count`\n\t\t\t* `SparkCompare.columns\\_compared`\n\t\t\t* `SparkCompare.columns\\_in\\_both`\n\t\t\t* `SparkCompare.columns\\_only\\_base`\n\t\t\t* `SparkCompare.columns\\_only\\_compare`\n\t\t\t* `SparkCompare.common\\_row\\_count`\n\t\t\t* `SparkCompare.compare\\_row\\_count`\n\t\t\t* `SparkCompare.report()`\n\t\t\t* `SparkCompare.rows\\_both\\_all`\n\t\t\t* `SparkCompare.rows\\_both\\_mismatch`\n\t\t\t* `SparkCompare.rows\\_only\\_base`\n\t\t\t* `SparkCompare.rows\\_only\\_compare`\n\t\t- `decimal\\_comparator()`\n\t+ Module contents\n\n\n# datacompy package#\n\n\n## Submodules#\n\n\n## datacompy.core module#\n\n\nCompare two Pandas DataFrames\n\n\nOriginally this package was meant to provide similar functionality to\nPROC COMPARE in SAS - i.e. human-readable reporting on the difference between\ntwo dataframes.\n\n\n*class* datacompy.core.Compare(*df1*, *df2*, *join\\_columns=None*, *on\\_index=False*, *abs\\_tol=0*, *rel\\_tol=0*, *df1\\_name='df1'*, *df2\\_name='df2'*, *ignore\\_spaces=False*, *ignore\\_case=False*, *cast\\_column\\_names\\_lower=True*)#\nBases: `object`\n\n\nComparison class\n\n==================\n Document 1 \n----------------\n\n# datacompy#\n\n* datacompy package\n\t+ Submodules\n\t+ datacompy.core module\n\t\t- `Compare`\n\t\t\t* `Compare.all\\_columns\\_match()`\n\t\t\t* `Compare.all\\_mismatch()`\n\t\t\t* `Compare.all\\_rows\\_overlap()`\n\t\t\t* `Compare.count\\_matching\\_rows()`\n\t\t\t* `Compare.df1`\n\t\t\t* `Compare.df1\\_unq\\_columns()`\n\t\t\t* `Compare.df2`\n\t\t\t* `Compare.df2\\_unq\\_columns()`\n\t\t\t* `Compare.intersect\\_columns()`\n\t\t\t* `Compare.intersect\\_rows\\_match()`\n\t\t\t* `Compare.matches()`\n\t\t\t* `Compare.report()`\n\t\t\t* `Compare.sample\\_mismatch()`\n\t\t\t* `Compare.subset()`\n\t\t- `calculate\\_max\\_diff()`\n\t\t- `columns\\_equal()`\n\t\t- `compare\\_string\\_and\\_date\\_columns()`\n\t\t- `generate\\_id\\_within\\_group()`\n\t\t- `get\\_merged\\_columns()`\n\t\t- `render()`\n\t\t- `temp\\_column\\_name()`\n\t+ datacompy.fugue module\n\t\t- `all\\_columns\\_match()`\n\t\t- `intersect\\_columns()`\n\t\t- `is\\_match()`\n\t\t- `report()`\n\t\t- `unq\\_columns()`\n\t+ datacompy.spark module\n\t\t- `MatchType`\n\t\t\t* `MatchType.KNOWN\\_DIFFERENCE`\n\t\t\t*\n\n==================\n Document 2 \n----------------\n# datacompy.fugue module#\n\n\nCompare two DataFrames that are supported by Fugue\n\n\ndatacompy.fugue.all\\_columns\\_match(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nWhether the columns all match in the dataframes\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n\nReturns:\nBoolean indicating whether the columns all match in the dataframes\n\ndatacompy.fugue.intersect\\_columns(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nGet columns that are shared between the two dataframes\n\nReturns:\nSet of that are shared between the two dataframes\n\nReturn type:\nOrderedSet\n\ndatacompy.fugue.is\\_match(*df1: AnyDataFrame*, *df2: AnyDataFrame*, *join\\_columns: str | List[str]*, *abs\\_tol: float = 0*, *rel\\_tol: float = 0*, *df1\\_name: str = 'df1'*, *df2\\_name: str = 'df2'*, *ignore\\_spaces: bool = False*, *ignore\\_case: bool = False*, *cast\\_column\\_names\\_lower: bool = True*, *parallelism: int | None = None*, *strict\\_schema: bool = False*) → bool#\nCheck whether two dataframes match.\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n* **join\\_columns** (*list* *or* *str**,* *optional*) – Column(s) to join dataframes on. If a string is passed in, that one\ncolumn will be used.\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance between two values.\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance between two values.\n* **df1\\_name** (*str**,* *optional*) – A string name for the first dataframe. This allows the reporting to\nprint out an actual name instead of “df1”, and allows human users to\nmore easily track the dataframes.\n* **df2\\_name** (*str**,* *optional*) – A string name for the second dataframe\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns (including any join\ncolumns)\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n* **cast\\_column\\_names\\_lower** (*bool**,* *optional*) – Boolean indicator that controls of column names will be cast into lower case\n* **parallelism** (*int**,* *optional*) – An integer representing the amount of parallelism. Entering a value for this\nwill force to use of Fugue over just vanilla Pandas\n* **strict\\_schema** (*bool**,* *optional*) – The schema must match exactly if set to `True`. This includes the names and types. Allows for a fast fail.\n\nReturns:\nReturns boolean as to if the DataFrames match.\n\ndatacompy.fugue.report(*df1: AnyDataFrame*, *df2: AnyDataFrame*, *join\\_columns: str | List[str]*, *abs\\_tol: float = 0*, *rel\\_tol: float = 0*, *df1\\_name: str = 'df1'*, *df2\\_name: str = 'df2'*, *ignore\\_spaces: bool = False*, *ignore\\_case: bool = False*, *cast\\_column\\_names\\_lower: bool = True*, *sample\\_count=10*, *column\\_count=10*, *html\\_file=None*, *parallelism: int | None = None*) → None#\nReturns a string representation of a report. The representation can\nthen be printed or saved to a file.\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n* **join\\_columns** (*list* *or* *str**,* *optional*) – Column(s) to join dataframes on. If a string is passed in, that one\ncolumn will be used.\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance between two values.\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance between two values.\n* **df1\\_name** (*str**,* *optional*) – A string name for the first dataframe. This allows the reporting to\nprint out an actual name instead of “df1”, and allows human users to\nmore easily track the dataframes.\n* **df2\\_name** (*str**,* *optional*) – A string name for the second dataframe\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns (including any join\ncolumns)\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n* **cast\\_column\\_names\\_lower** (*bool**,* *optional*) – Boolean indicator that controls of column names will be cast into lower case\n* **parallelism** (*int**,* *optional*) – An integer representing the amount of parallelism. Entering a value for this\nwill force to use of Fugue over just vanilla Pandas\n* **strict\\_schema** (*bool**,* *optional*) – The schema must match exactly if set to `True`. This includes the names and types. Allows for a fast fail.\n* **sample\\_count** (*int**,* *optional*) – The number of sample records to return. Defaults to 10.\n* **column\\_count** (*int**,* *optional*) – The number of columns to display in the sample records output. Defaults to 10.\n* **html\\_file** (*str**,* *optional*) – HTML file name to save report output to. If `None` the file creation will be skipped.\n\ndatacompy.fugue.unq\\_columns(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nGet columns that are unique to df1\n\nReturns:\nSet of columns that are unique to df1\n## datacompy.spark module#\n\n\n*class* datacompy.spark.MatchType(*value*)#\nBases: `Enum`\n\n\nAn enumeration.\n\n\nKNOWN\\_DIFFERENCE *= 2*#\n\nMATCH *= 1*#\n\nMISMATCH *= 0*#\n\n\n*class* datacompy.spark.SparkCompare(*spark\\_session*, *base\\_df*, *compare\\_df*, *join\\_columns*, *column\\_mapping=None*, *cache\\_intermediates=False*, *known\\_differences=None*, *rel\\_tol=0*, *abs\\_tol=0*, *show\\_all\\_columns=False*, *match\\_rates=False*)#\nBases: `object`\n\n\nComparison class used to compare two Spark Dataframes.\n\n\nExtends the `Compare` functionality to the wide world of Spark"
    },
    {
        "instruction": "Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on a specific column ('employee_id'), and produce a comparison report. Ensure that it reports matches and includes extra columns in the comparison.",
        "api": "datacompy",
        "output": "#!pip install datacompy\nfrom io import StringIO\nimport pandas as pd\nimport datacompy\n\ndata1 = \"\"\"employee_id, name, age\n1, islam mesabah, 25\n2, john doe, 30\n\"\"\"\n\ndata2 = \"\"\"employee_id, name, age\n1, islam mesabah, 25\n2, john doe, 30\n\"\"\"\n\ndf1 = pd.read_csv(StringIO(data1))\ndf2 = pd.read_csv(StringIO(data2))\n\ncompare = datacompy.Compare(\n  df1,\n  df2,\n  # You can also specify a list\n  # of columns\n  join_columns = 'employee_id',\n\n  # Optional, defaults to 'df1'\n  df1_name = 'Original',\n\n  # Optional, defaults to 'df2'\n  df2_name = 'New'\n  )\ncompare.matches(ignore_extra_columns = False)\nprint(compare.report())",
        "documentation": "\n\n==================\n Document 0 \n----------------\n datacompy#\n\n* datacompy package\n\t+ Submodules\n\t+ datacompy.core module\n\t\t- `Compare`\n\t\t\t* `Compare.all\\_columns\\_match()`\n\t\t\t* `Compare.all\\_mismatch()`\n\t\t\t* `Compare.all\\_rows\\_overlap()`\n\t\t\t* `Compare.count\\_matching\\_rows()`\n\t\t\t* `Compare.df1`\n\t\t\t* `Compare.df1\\_unq\\_columns()`\n\t\t\t* `Compare.df2`\n\t\t\t* `Compare.df2\\_unq\\_columns()`\n\t\t\t* `Compare.intersect\\_columns()`\n\t\t\t* `Compare.intersect\\_rows\\_match()`\n\t\t\t* `Compare.matches()`\n\t\t\t* `Compare.report()`\n\t\t\t* `Compare.sample\\_mismatch()`\n\t\t\t* `Compare.subset()`\n\t\t- `calculate\\_max\\_diff()`\n\t\t- `columns\\_equal()`\n\t\t- `compare\\_string\\_and\\_date\\_columns()`\n\t\t- `generate\\_id\\_within\\_group()`\n\t\t- `get\\_merged\\_columns()`\n\t\t- `render()`\n\t\t- `temp\\_column\\_name()`\n\t+ datacompy.fugue module\n\t\t- `all\\_columns\\_match()`\n\t\t- `intersect\\_columns()`\n\t\t- `is\\_match()`\n\t\t- `report()`\n\t\t- `unq\\_columns()`\n\t+ datacompy.spark module\n\t\t- `MatchType`\n\t\t\t* `MatchType.KNOWN\\_DIFFERENCE`\n\t\t\t* `MatchType.MATCH`\n\t\t\t* `MatchType.MISMATCH`\n\t\t- `SparkCompare`\n\t\t\t* `SparkCompare.base\\_row\\_count`\n\t\t\t* `SparkCompare.columns\\_compared`\n\t\t\t* `SparkCompare.columns\\_in\\_both`\n\t\t\t* `SparkCompare.columns\\_only\\_base`\n\t\t\t* `SparkCompare.columns\\_only\\_compare`\n\t\t\t* `SparkCompare.common\\_row\\_count`\n\t\t\t* `SparkCompare.compare\\_row\\_count`\n\t\t\t* `SparkCompare.report()`\n\t\t\t* `SparkCompare.rows\\_both\\_all`\n\t\t\t* `SparkCompare.rows\\_both\\_mismatch`\n\t\t\t* `SparkCompare.rows\\_only\\_base`\n\t\t\t* `SparkCompare.rows\\_only\\_compare`\n\t\t- `decimal\\_comparator()`\n\t+ Module contents\n\n\n# datacompy package#\n\n\n## Submodules#\n\n\n## datacompy.core module#\n\n\nCompare two Pandas DataFrames\n\n\nOriginally this package was meant to provide similar functionality to\nPROC COMPARE in SAS - i.e. human-readable reporting on the difference between\ntwo dataframes.\n\n\n*class* datacompy.core.Compare(*df1*, *df2*, *join\\_columns=None*, *on\\_index=False*, *abs\\_tol=0*, *rel\\_tol=0*, *df1\\_name='df1'*, *df2\\_name='df2'*, *ignore\\_spaces=False*, *ignore\\_case=False*, *cast\\_column\\_names\\_lower=True*)#\nBases: `object`\n\n\nComparison class\n\n==================\n Document 1 \n----------------\n\n# datacompy#\n\n* datacompy package\n\t+ Submodules\n\t+ datacompy.core module\n\t\t- `Compare`\n\t\t\t* `Compare.all\\_columns\\_match()`\n\t\t\t* `Compare.all\\_mismatch()`\n\t\t\t* `Compare.all\\_rows\\_overlap()`\n\t\t\t* `Compare.count\\_matching\\_rows()`\n\t\t\t* `Compare.df1`\n\t\t\t* `Compare.df1\\_unq\\_columns()`\n\t\t\t* `Compare.df2`\n\t\t\t* `Compare.df2\\_unq\\_columns()`\n\t\t\t* `Compare.intersect\\_columns()`\n\t\t\t* `Compare.intersect\\_rows\\_match()`\n\t\t\t* `Compare.matches()`\n\t\t\t* `Compare.report()`\n\t\t\t* `Compare.sample\\_mismatch()`\n\t\t\t* `Compare.subset()`\n\t\t- `calculate\\_max\\_diff()`\n\t\t- `columns\\_equal()`\n\t\t- `compare\\_string\\_and\\_date\\_columns()`\n\t\t- `generate\\_id\\_within\\_group()`\n\t\t- `get\\_merged\\_columns()`\n\t\t- `render()`\n\t\t- `temp\\_column\\_name()`\n\t+ datacompy.fugue module\n\t\t- `all\\_columns\\_match()`\n\t\t- `intersect\\_columns()`\n\t\t- `is\\_match()`\n\t\t- `report()`\n\t\t- `unq\\_columns()`\n\t+ datacompy.spark module\n\t\t- `MatchType`\n\t\t\t* `MatchType.KNOWN\\_DIFFERENCE`\n\t\t\t*\n\n==================\n Document 2 \n----------------\n# datacompy.fugue module#\n\n\nCompare two DataFrames that are supported by Fugue\n\n\ndatacompy.fugue.all\\_columns\\_match(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nWhether the columns all match in the dataframes\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n\nReturns:\nBoolean indicating whether the columns all match in the dataframes\n\ndatacompy.fugue.intersect\\_columns(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nGet columns that are shared between the two dataframes\n\nReturns:\nSet of that are shared between the two dataframes\n\nReturn type:\nOrderedSet\n\ndatacompy.fugue.is\\_match(*df1: AnyDataFrame*, *df2: AnyDataFrame*, *join\\_columns: str | List[str]*, *abs\\_tol: float = 0*, *rel\\_tol: float = 0*, *df1\\_name: str = 'df1'*, *df2\\_name: str = 'df2'*, *ignore\\_spaces: bool = False*, *ignore\\_case: bool = False*, *cast\\_column\\_names\\_lower: bool = True*, *parallelism: int | None = None*, *strict\\_schema: bool = False*) → bool#\nCheck whether two dataframes match.\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n* **join\\_columns** (*list* *or* *str**,* *optional*) – Column(s) to join dataframes on. If a string is passed in, that one\ncolumn will be used.\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance between two values.\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance between two values.\n* **df1\\_name** (*str**,* *optional*) – A string name for the first dataframe. This allows the reporting to\nprint out an actual name instead of “df1”, and allows human users to\nmore easily track the dataframes.\n* **df2\\_name** (*str**,* *optional*) – A string name for the second dataframe\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns (including any join\ncolumns)\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n* **cast\\_column\\_names\\_lower** (*bool**,* *optional*) – Boolean indicator that controls of column names will be cast into lower case\n* **parallelism** (*int**,* *optional*) – An integer representing the amount of parallelism. Entering a value for this\nwill force to use of Fugue over just vanilla Pandas\n* **strict\\_schema** (*bool**,* *optional*) – The schema must match exactly if set to `True`. This includes the names and types. Allows for a fast fail.\n\nReturns:\nReturns boolean as to if the DataFrames match.\n\ndatacompy.fugue.report(*df1: AnyDataFrame*, *df2: AnyDataFrame*, *join\\_columns: str | List[str]*, *abs\\_tol: float = 0*, *rel\\_tol: float = 0*, *df1\\_name: str = 'df1'*, *df2\\_name: str = 'df2'*, *ignore\\_spaces: bool = False*, *ignore\\_case: bool = False*, *cast\\_column\\_names\\_lower: bool = True*, *sample\\_count=10*, *column\\_count=10*, *html\\_file=None*, *parallelism: int | None = None*) → None#\nReturns a string representation of a report. The representation can\nthen be printed or saved to a file.\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n* **join\\_columns** (*list* *or* *str**,* *optional*) – Column(s) to join dataframes on. If a string is passed in, that one\ncolumn will be used.\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance between two values.\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance between two values.\n* **df1\\_name** (*str**,* *optional*) – A string name for the first dataframe. This allows the reporting to\nprint out an actual name instead of “df1”, and allows human users to\nmore easily track the dataframes.\n* **df2\\_name** (*str**,* *optional*) – A string name for the second dataframe\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns (including any join\ncolumns)\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n* **cast\\_column\\_names\\_lower** (*bool**,* *optional*) – Boolean indicator that controls of column names will be cast into lower case\n* **parallelism** (*int**,* *optional*) – An integer representing the amount of parallelism. Entering a value for this\nwill force to use of Fugue over just vanilla Pandas\n* **strict\\_schema** (*bool**,* *optional*) – The schema must match exactly if set to `True`. This includes the names and types. Allows for a fast fail.\n* **sample\\_count** (*int**,* *optional*) – The number of sample records to return. Defaults to 10.\n* **column\\_count** (*int**,* *optional*) – The number of columns to display in the sample records output. Defaults to 10.\n* **html\\_file** (*str**,* *optional*) – HTML file name to save report output to. If `None` the file creation will be skipped.\n\ndatacompy.fugue.unq\\_columns(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nGet columns that are unique to df1\n\nReturns:\nSet of columns that are unique to df1\n## datacompy.spark module#\n\n\n*class* datacompy.spark.MatchType(*value*)#\nBases: `Enum`\n\n\nAn enumeration.\n\n\nKNOWN\\_DIFFERENCE *= 2*#\n\nMATCH *= 1*#\n\nMISMATCH *= 0*#\n\n\n*class* datacompy.spark.SparkCompare(*spark\\_session*, *base\\_df*, *compare\\_df*, *join\\_columns*, *column\\_mapping=None*, *cache\\_intermediates=False*, *known\\_differences=None*, *rel\\_tol=0*, *abs\\_tol=0*, *show\\_all\\_columns=False*, *match\\_rates=False*)#\nBases: `object`\n\n\nComparison class used to compare two Spark Dataframes.\n\n\nExtends the `Compare` functionality to the wide world of Spark\n\n==================\n Document 3 \n----------------\n# datacompy.core module#\n\n\nCompare two Pandas DataFrames\n\n\nOriginally this package was meant to provide similar functionality to\nPROC COMPARE in SAS - i.e. human-readable reporting on the difference between\ntwo dataframes.\n\n\n*class* datacompy.core.Compare(*df1*, *df2*, *join\\_columns=None*, *on\\_index=False*, *abs\\_tol=0*, *rel\\_tol=0*, *df1\\_name='df1'*, *df2\\_name='df2'*, *ignore\\_spaces=False*, *ignore\\_case=False*, *cast\\_column\\_names\\_lower=True*)#\nBases: `object`\n\n\nComparison class to be used to compare whether two dataframes as equal.\n\n\nBoth df1 and df2 should be dataframes containing all of the join\\_columns,\nwith unique column names. Differences between values are compared to\nabs\\_tol + rel\\_tol \\* abs(df2[‘value’]).\n\nParameters:\n* **df1** (pandas `DataFrame`) – First dataframe to check\n* **df2** (pandas `DataFrame`) – Second dataframe to check\n* **join\\_columns** (*list* *or* *str**,* *optional*) – Column(s) to join dataframes on. If a string is passed in, that one\ncolumn will be used.\n* **on\\_index** (*bool**,* *optional*) – If True, the index will be used to join the two dataframes. If both\n`join\\_columns` and `on\\_index` are provided, an exception will be\nraised.\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance between two values.\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance between two values.\n* **df1\\_name** (*str**,* *optional*) – A string name for the first dataframe. This allows the reporting to\nprint out an actual name instead of “df1”, and allows human users to\nmore easily track the dataframes.\n* **df2\\_name** (*str**,* *optional*) – A string name for the second dataframe\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns (including any join\ncolumns)\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n* **cast\\_column\\_names\\_lower** (*bool**,* *optional*) – Boolean indicator that controls of column names will be cast into lower case\n\nVariables:\n* **df1\\_unq\\_rows** (pandas `DataFrame`) – All records that are only in df1 (based on a join on join\\_columns)\n* **df2\\_unq\\_rows** (pandas `DataFrame`) – All records that are only in df2 (based on a join on join\\_columns)\n\n\nall\\_columns\\_match()#\nWhether the columns all match in the dataframes\n\nall\\_mismatch(*ignore\\_matching\\_cols=False*)#\nAll rows with any columns that have a mismatch. Returns all df1 and df2 versions of the columns and join\ncolumns.\n\nParameters:\n**ignore\\_matching\\_cols** (*bool**,* *optional*) – Whether showing the matching columns in the output or not. The default is False.\n\nReturns:\nAll rows of the intersection dataframe, containing any columns, that don’t match.\n\nReturn type:\nPandas.DataFrame\n\nall\\_rows\\_overlap()#\nWhether the rows are all present in both dataframes\n\nReturns:\nTrue if all rows in df1 are in df2 and vice versa (based on\nexistence for join option)\n\nReturn type:\nbool\n\ncount\\_matching\\_rows()#\nCount the number of rows match (on overlapping fields)\n\nReturns:\nNumber of matching rows\n\nReturn type:\nint\n\n*property* df1#\n\ndf1\\_unq\\_columns()#\nGet columns that are unique to df1\n\n*property* df2#\n\ndf2\\_unq\\_columns()#\nGet columns that are unique to df2\n\nintersect\\_columns()#\nGet columns that are shared between the two dataframes\n\nintersect\\_rows\\_match()#\nCheck whether the intersect rows all match\n\nmatches(*ignore\\_extra\\_columns=False*)#\nReturn True or False if the dataframes match.\n\nParameters:\n**ignore\\_extra\\_columns** (*bool*) – Ignores any columns in one dataframe and not in the other.\n\nreport(*sample\\_count=10*, *column\\_count=10*, *html\\_file=None*)#\nReturns a string representation of a report. The representation can\nthen be printed or saved to a file.\n\nParameters:\n* **sample\\_count** (*int**,* *optional*) – The number of sample records to return. Defaults to 10.\n* **column\\_count** (*int**,* *optional*) – The number of columns to display in the sample records output. Defaults to 10.\n* **html\\_file** (*str**,* *optional*) – HTML file name to save report output to. If `None` the file creation will be skipped.\n\nReturns:\nThe report, formatted kinda nicely.\n\nReturn type:\nstr\n\nsample\\_mismatch(*column*, *sample\\_count=10*, *for\\_display=False*)#\nReturns a sample sub-dataframe which contains the identifying\ncolumns, and df1 and df2 versions of the column.\n\nParameters:\n* **column** (*str*) – The raw column name (i.e. without `\\_df1` appended)\n* **sample\\_count** (*int**,* *optional*) – The number of sample records to return. Defaults to 10.\n* **for\\_display** (*bool**,* *optional*) – Whether this is just going to be used for display (overwrite the\ncolumn names)\n\nReturns:\nA sample of the intersection dataframe, containing only the\n“pertinent” columns, for rows that don’t match on the provided\ncolumn.\n\nsubset()#\nReturn True if dataframe 2 is a subset of dataframe 1.\n\n\nDataframe 2 is considered a subset if all of its columns are in\ndataframe 1, and all of its rows match rows in dataframe 1 for the\nshared columns.\n\n\ndatacompy.core.calculate\\_max\\_diff(*col\\_1*, *col\\_2*)#\nGet a maximum difference between two columns\n\nParameters:\n* **col\\_1** (*Pandas.Series*) – The first column\n* **col\\_2** (*Pandas.Series*) – The second column\n\nReturns:\nNumeric field, or zero.\n\nReturn type:\nNumeric\n\ndatacompy.core.columns\\_equal(*col\\_1*, *col\\_2*, *rel\\_tol=0*, *abs\\_tol=0*, *ignore\\_spaces=False*, *ignore\\_case=False*)#\nCompares two columns from a dataframe, returning a True/False series,\nwith the same index as column 1.\n\n\n* Two nulls (np.nan) will evaluate to True.\n* A null and a non-null value will evaluate to False.\n* Numeric values will use the relative and absolute tolerances.\n* Decimal values (decimal.Decimal) will attempt to be converted to floats\nbefore comparing\n* Non-numeric values (i.e. where np.isclose can’t be used) will just\ntrigger True on two nulls or exact matches.\n\nParameters:\n* **col\\_1** (*Pandas.Series*) – The first column to look at\n* **col\\_2** (*Pandas.Series*) – The second column\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n\nReturns:\nA series of Boolean values. True == the values match, False == the\nvalues don’t match.\n\nReturn type:\npandas.Series\n\ndatacompy.core.compare\\_string\\_and\\_date\\_columns(*col\\_1*, *col\\_2*)#\nCompare a string column and date column, value-wise. This tries to\nconvert a string column to a date column and compare that way.\n\nParameters:\n* **col\\_1** (*Pandas.Series*) – The first column to look at\n* **col\\_2** (*Pandas.Series*) – The second column\n\ndatacompy.core.generate\\_id\\_within\\_group(*dataframe*, *join\\_columns*)#\nGenerate an ID column that can be used to deduplicate identical rows. The series generated\nis the order within a unique group, and it handles nulls.\n\nParameters:\n* **dataframe** (*Pandas.DataFrame*) – The dataframe to operate on\n* **join\\_columns** (*list*) – List of strings which are the join columns\n\nReturns:\nThe ID column that’s unique in each group.\n\nReturn type:\nPandas.Series\n\ndatacompy.core.get\\_merged\\_columns(*original\\_df*, *merged\\_df*, *suffix*)#\nGets the columns from an original dataframe, in the new merged dataframe\n\nParameters:\n* **original\\_df** (*Pandas.DataFrame*) – The original, pre-merge dataframe\n* **merged\\_df** (*Pandas.DataFrame*) – Post-merge with another dataframe, with suffixes added in.\n* **suffix** (*str*) – What suffix was used to distinguish when the original dataframe was\noverlapping with the other merged dataframe.\n\ndatacompy.core.render(*filename*, *\\*fields*)#\nRenders out an individual template. This basically just reads in a\ntemplate file, and applies `.format()` on the fields.\n\nParameters:\n* **filename** (*str*) – The file that contains the template. Will automagically prepend the\ntemplates directory before opening\n* **fields** (*list*) – Fields to be rendered out in the template\n\nReturns:\nThe fully rendered out file.\n\ndatacompy.core.temp\\_column\\_name(*\\*dataframes*)#\nGets a temp column name that isn’t included in columns of any dataframes\n\nParameters:\n**dataframes** (*list* *of* *Pandas.DataFrame*) – The DataFrames to create a temporary column name for\n\nReturns:\nString column name that looks like ‘\\_temp\\_x’ for some integer x\n## datacompy.fugue module#\n\n\nCompare two DataFrames that are supported by Fugue\n\n\ndatacompy.fugue.all\\_columns\\_match(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nWhether the columns all match in the dataframes\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n\nReturns:\nBoolean indicating whether the columns all match"
    },
    {
        "instruction": "Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on multiple columns ('employee_id' and 'department'), and produce a comparison report. Ensure that it reports matches and does not ignore extra columns.",
        "api": "datacompy",
        "output": "#!pip install datacompy\nfrom io import StringIO\nimport pandas as pd\nimport datacompy\n\ndata1 = \"\"\"employee_id, name, department\n1, islam mesabah, IT\n2, john doe, HR\n\"\"\"\n\ndata2 = \"\"\"employee_id, name, department\n1, islam mesabah, IT\n2, john doe, HR\n\"\"\"\n\ndf1 = pd.read_csv(StringIO(data1))\ndf2 = pd.read_csv(StringIO(data2))\n\ncompare = datacompy.Compare(\n  df1,\n  df2,\n  # You can also specify a list\n  # of columns\n  join_columns = ['employee_id', 'department'],\n\n  # Optional, defaults to 'df1'\n  df1_name = 'Original',\n\n  # Optional, defaults to 'df2'\n  df2_name = 'New'\n  )\ncompare.matches(ignore_extra_columns = False)\nprint(compare.report())",
        "documentation": "\n\n==================\n Document 0 \n----------------\n datacompy#\n\n* datacompy package\n\t+ Submodules\n\t+ datacompy.core module\n\t\t- `Compare`\n\t\t\t* `Compare.all\\_columns\\_match()`\n\t\t\t* `Compare.all\\_mismatch()`\n\t\t\t* `Compare.all\\_rows\\_overlap()`\n\t\t\t* `Compare.count\\_matching\\_rows()`\n\t\t\t* `Compare.df1`\n\t\t\t* `Compare.df1\\_unq\\_columns()`\n\t\t\t* `Compare.df2`\n\t\t\t* `Compare.df2\\_unq\\_columns()`\n\t\t\t* `Compare.intersect\\_columns()`\n\t\t\t* `Compare.intersect\\_rows\\_match()`\n\t\t\t* `Compare.matches()`\n\t\t\t* `Compare.report()`\n\t\t\t* `Compare.sample\\_mismatch()`\n\t\t\t* `Compare.subset()`\n\t\t- `calculate\\_max\\_diff()`\n\t\t- `columns\\_equal()`\n\t\t- `compare\\_string\\_and\\_date\\_columns()`\n\t\t- `generate\\_id\\_within\\_group()`\n\t\t- `get\\_merged\\_columns()`\n\t\t- `render()`\n\t\t- `temp\\_column\\_name()`\n\t+ datacompy.fugue module\n\t\t- `all\\_columns\\_match()`\n\t\t- `intersect\\_columns()`\n\t\t- `is\\_match()`\n\t\t- `report()`\n\t\t- `unq\\_columns()`\n\t+ datacompy.spark module\n\t\t- `MatchType`\n\t\t\t* `MatchType.KNOWN\\_DIFFERENCE`\n\t\t\t* `MatchType.MATCH`\n\t\t\t* `MatchType.MISMATCH`\n\t\t- `SparkCompare`\n\t\t\t* `SparkCompare.base\\_row\\_count`\n\t\t\t* `SparkCompare.columns\\_compared`\n\t\t\t* `SparkCompare.columns\\_in\\_both`\n\t\t\t* `SparkCompare.columns\\_only\\_base`\n\t\t\t* `SparkCompare.columns\\_only\\_compare`\n\t\t\t* `SparkCompare.common\\_row\\_count`\n\t\t\t* `SparkCompare.compare\\_row\\_count`\n\t\t\t* `SparkCompare.report()`\n\t\t\t* `SparkCompare.rows\\_both\\_all`\n\t\t\t* `SparkCompare.rows\\_both\\_mismatch`\n\t\t\t* `SparkCompare.rows\\_only\\_base`\n\t\t\t* `SparkCompare.rows\\_only\\_compare`\n\t\t- `decimal\\_comparator()`\n\t+ Module contents\n\n\n# datacompy package#\n\n\n## Submodules#\n\n\n## datacompy.core module#\n\n\nCompare two Pandas DataFrames\n\n\nOriginally this package was meant to provide similar functionality to\nPROC COMPARE in SAS - i.e. human-readable reporting on the difference between\ntwo dataframes.\n\n\n*class* datacompy.core.Compare(*df1*, *df2*, *join\\_columns=None*, *on\\_index=False*, *abs\\_tol=0*, *rel\\_tol=0*, *df1\\_name='df1'*, *df2\\_name='df2'*, *ignore\\_spaces=False*, *ignore\\_case=False*, *cast\\_column\\_names\\_lower=True*)#\nBases: `object`\n\n\nComparison class\n\n==================\n Document 1 \n----------------\n\n# datacompy#\n\n* datacompy package\n\t+ Submodules\n\t+ datacompy.core module\n\t\t- `Compare`\n\t\t\t* `Compare.all\\_columns\\_match()`\n\t\t\t* `Compare.all\\_mismatch()`\n\t\t\t* `Compare.all\\_rows\\_overlap()`\n\t\t\t* `Compare.count\\_matching\\_rows()`\n\t\t\t* `Compare.df1`\n\t\t\t* `Compare.df1\\_unq\\_columns()`\n\t\t\t* `Compare.df2`\n\t\t\t* `Compare.df2\\_unq\\_columns()`\n\t\t\t* `Compare.intersect\\_columns()`\n\t\t\t* `Compare.intersect\\_rows\\_match()`\n\t\t\t* `Compare.matches()`\n\t\t\t* `Compare.report()`\n\t\t\t* `Compare.sample\\_mismatch()`\n\t\t\t* `Compare.subset()`\n\t\t- `calculate\\_max\\_diff()`\n\t\t- `columns\\_equal()`\n\t\t- `compare\\_string\\_and\\_date\\_columns()`\n\t\t- `generate\\_id\\_within\\_group()`\n\t\t- `get\\_merged\\_columns()`\n\t\t- `render()`\n\t\t- `temp\\_column\\_name()`\n\t+ datacompy.fugue module\n\t\t- `all\\_columns\\_match()`\n\t\t- `intersect\\_columns()`\n\t\t- `is\\_match()`\n\t\t- `report()`\n\t\t- `unq\\_columns()`\n\t+ datacompy.spark module\n\t\t- `MatchType`\n\t\t\t* `MatchType.KNOWN\\_DIFFERENCE`\n\t\t\t*\n\n==================\n Document 2 \n----------------\n# datacompy.fugue module#\n\n\nCompare two DataFrames that are supported by Fugue\n\n\ndatacompy.fugue.all\\_columns\\_match(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nWhether the columns all match in the dataframes\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n\nReturns:\nBoolean indicating whether the columns all match in the dataframes\n\ndatacompy.fugue.intersect\\_columns(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nGet columns that are shared between the two dataframes\n\nReturns:\nSet of that are shared between the two dataframes\n\nReturn type:\nOrderedSet\n\ndatacompy.fugue.is\\_match(*df1: AnyDataFrame*, *df2: AnyDataFrame*, *join\\_columns: str | List[str]*, *abs\\_tol: float = 0*, *rel\\_tol: float = 0*, *df1\\_name: str = 'df1'*, *df2\\_name: str = 'df2'*, *ignore\\_spaces: bool = False*, *ignore\\_case: bool = False*, *cast\\_column\\_names\\_lower: bool = True*, *parallelism: int | None = None*, *strict\\_schema: bool = False*) → bool#\nCheck whether two dataframes match.\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n* **join\\_columns** (*list* *or* *str**,* *optional*) – Column(s) to join dataframes on. If a string is passed in, that one\ncolumn will be used.\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance between two values.\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance between two values.\n* **df1\\_name** (*str**,* *optional*) – A string name for the first dataframe. This allows the reporting to\nprint out an actual name instead of “df1”, and allows human users to\nmore easily track the dataframes.\n* **df2\\_name** (*str**,* *optional*) – A string name for the second dataframe\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns (including any join\ncolumns)\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n* **cast\\_column\\_names\\_lower** (*bool**,* *optional*) – Boolean indicator that controls of column names will be cast into lower case\n* **parallelism** (*int**,* *optional*) – An integer representing the amount of parallelism. Entering a value for this\nwill force to use of Fugue over just vanilla Pandas\n* **strict\\_schema** (*bool**,* *optional*) – The schema must match exactly if set to `True`. This includes the names and types. Allows for a fast fail.\n\nReturns:\nReturns boolean as to if the DataFrames match.\n\ndatacompy.fugue.report(*df1: AnyDataFrame*, *df2: AnyDataFrame*, *join\\_columns: str | List[str]*, *abs\\_tol: float = 0*, *rel\\_tol: float = 0*, *df1\\_name: str = 'df1'*, *df2\\_name: str = 'df2'*, *ignore\\_spaces: bool = False*, *ignore\\_case: bool = False*, *cast\\_column\\_names\\_lower: bool = True*, *sample\\_count=10*, *column\\_count=10*, *html\\_file=None*, *parallelism: int | None = None*) → None#\nReturns a string representation of a report. The representation can\nthen be printed or saved to a file.\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n* **join\\_columns** (*list* *or* *str**,* *optional*) – Column(s) to join dataframes on. If a string is passed in, that one\ncolumn will be used.\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance between two values.\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance between two values.\n* **df1\\_name** (*str**,* *optional*) – A string name for the first dataframe. This allows the reporting to\nprint out an actual name instead of “df1”, and allows human users to\nmore easily track the dataframes.\n* **df2\\_name** (*str**,* *optional*) – A string name for the second dataframe\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns (including any join\ncolumns)\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n* **cast\\_column\\_names\\_lower** (*bool**,* *optional*) – Boolean indicator that controls of column names will be cast into lower case\n* **parallelism** (*int**,* *optional*) – An integer representing the amount of parallelism. Entering a value for this\nwill force to use of Fugue over just vanilla Pandas\n* **strict\\_schema** (*bool**,* *optional*) – The schema must match exactly if set to `True`. This includes the names and types. Allows for a fast fail.\n* **sample\\_count** (*int**,* *optional*) – The number of sample records to return. Defaults to 10.\n* **column\\_count** (*int**,* *optional*) – The number of columns to display in the sample records output. Defaults to 10.\n* **html\\_file** (*str**,* *optional*) – HTML file name to save report output to. If `None` the file creation will be skipped.\n\ndatacompy.fugue.unq\\_columns(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nGet columns that are unique to df1\n\nReturns:\nSet of columns that are unique to df1\n## datacompy.spark module#\n\n\n*class* datacompy.spark.MatchType(*value*)#\nBases: `Enum`\n\n\nAn enumeration.\n\n\nKNOWN\\_DIFFERENCE *= 2*#\n\nMATCH *= 1*#\n\nMISMATCH *= 0*#\n\n\n*class* datacompy.spark.SparkCompare(*spark\\_session*, *base\\_df*, *compare\\_df*, *join\\_columns*, *column\\_mapping=None*, *cache\\_intermediates=False*, *known\\_differences=None*, *rel\\_tol=0*, *abs\\_tol=0*, *show\\_all\\_columns=False*, *match\\_rates=False*)#\nBases: `object`\n\n\nComparison class used to compare two Spark Dataframes.\n\n\nExtends the `Compare` functionality to the wide world of Spark\n\n==================\n Document 3 \n----------------\n# datacompy.core module#\n\n\nCompare two Pandas DataFrames\n\n\nOriginally this package was meant to provide similar functionality to\nPROC COMPARE in SAS - i.e. human-readable reporting on the difference between\ntwo dataframes.\n\n\n*class* datacompy.core.Compare(*df1*, *df2*, *join\\_columns=None*, *on\\_index=False*, *abs\\_tol=0*, *rel\\_tol=0*, *df1\\_name='df1'*, *df2\\_name='df2'*, *ignore\\_spaces=False*, *ignore\\_case=False*, *cast\\_column\\_names\\_lower=True*)#\nBases: `object`\n\n\nComparison class to be used to compare whether two dataframes as equal.\n\n\nBoth df1 and df2 should be dataframes containing all of the join\\_columns,\nwith unique column names. Differences between values are compared to\nabs\\_tol + rel\\_tol \\* abs(df2[‘value’]).\n\nParameters:\n* **df1** (pandas `DataFrame`) – First dataframe to check\n* **df2** (pandas `DataFrame`) – Second dataframe to check\n* **join\\_columns** (*list* *or* *str**,* *optional*) – Column(s) to join dataframes on. If a string is passed in, that one\ncolumn will be used.\n* **on\\_index** (*bool**,* *optional*) – If True, the index will be used to join the two dataframes. If both\n`join\\_columns` and `on\\_index` are provided, an exception will be\nraised.\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance between two values.\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance between two values.\n* **df1\\_name** (*str**,* *optional*) – A string name for the first dataframe. This allows the reporting to\nprint out an actual name instead of “df1”, and allows human users to\nmore easily track the dataframes.\n* **df2\\_name** (*str**,* *optional*) – A string name for the second dataframe\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns (including any join\ncolumns)\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n* **cast\\_column\\_names\\_lower** (*bool**,* *optional*) – Boolean indicator that controls of column names will be cast into lower case\n\nVariables:\n* **df1\\_unq\\_rows** (pandas `DataFrame`) – All records that are only in df1 (based on a join on join\\_columns)\n* **df2\\_unq\\_rows** (pandas `DataFrame`) – All records that are only in df2 (based on a join on join\\_columns)\n\n\nall\\_columns\\_match()#\nWhether the columns all match in the dataframes\n\nall\\_mismatch(*ignore\\_matching\\_cols=False*)#\nAll rows with any columns that have a mismatch. Returns all df1 and df2 versions of the columns and join\ncolumns.\n\nParameters:\n**ignore\\_matching\\_cols** (*bool**,* *optional*) – Whether showing the matching columns in the output or not. The default is False.\n\nReturns:\nAll rows of the intersection dataframe, containing any columns, that don’t match.\n\nReturn type:\nPandas.DataFrame\n\nall\\_rows\\_overlap()#\nWhether the rows are all present in both dataframes\n\nReturns:\nTrue if all rows in df1 are in df2 and vice versa (based on\nexistence for join option)\n\nReturn type:\nbool\n\ncount\\_matching\\_rows()#\nCount the number of rows match (on overlapping fields)\n\nReturns:\nNumber of matching rows\n\nReturn type:\nint\n\n*property* df1#\n\ndf1\\_unq\\_columns()#\nGet columns that are unique to df1\n\n*property* df2#\n\ndf2\\_unq\\_columns()#\nGet columns that are unique to df2\n\nintersect\\_columns()#\nGet columns that are shared between the two dataframes\n\nintersect\\_rows\\_match()#\nCheck whether the intersect rows all match\n\nmatches(*ignore\\_extra\\_columns=False*)#\nReturn True or False if the dataframes match.\n\nParameters:\n**ignore\\_extra\\_columns** (*bool*) – Ignores any columns in one dataframe and not in the other.\n\nreport(*sample\\_count=10*, *column\\_count=10*, *html\\_file=None*)#\nReturns a string representation of a report. The representation can\nthen be printed or saved to a file.\n\nParameters:\n* **sample\\_count** (*int**,* *optional*) – The number of sample records to return. Defaults to 10.\n* **column\\_count** (*int**,* *optional*) – The number of columns to display in the sample records output. Defaults to 10.\n* **html\\_file** (*str**,* *optional*) – HTML file name to save report output to. If `None` the file creation will be skipped.\n\nReturns:\nThe report, formatted kinda nicely.\n\nReturn type:\nstr\n\nsample\\_mismatch(*column*, *sample\\_count=10*, *for\\_display=False*)#\nReturns a sample sub-dataframe which contains the identifying\ncolumns, and df1 and df2 versions of the column.\n\nParameters:\n* **column** (*str*) – The raw column name (i.e. without `\\_df1` appended)\n* **sample\\_count** (*int**,* *optional*) – The number of sample records to return. Defaults to 10.\n* **for\\_display** (*bool**,* *optional*) – Whether this is just going to be used for display (overwrite the\ncolumn names)\n\nReturns:\nA sample of the intersection dataframe, containing only the\n“pertinent” columns, for rows that don’t match on the provided\ncolumn.\n\nsubset()#\nReturn True if dataframe 2 is a subset of dataframe 1.\n\n\nDataframe 2 is considered a subset if all of its columns are in\ndataframe 1, and all of its rows match rows in dataframe 1 for the\nshared columns.\n\n\ndatacompy.core.calculate\\_max\\_diff(*col\\_1*, *col\\_2*)#\nGet a maximum difference between two columns\n\nParameters:\n* **col\\_1** (*Pandas.Series*) – The first column\n* **col\\_2** (*Pandas.Series*) – The second column\n\nReturns:\nNumeric field, or zero.\n\nReturn type:\nNumeric\n\ndatacompy.core.columns\\_equal(*col\\_1*, *col\\_2*, *rel\\_tol=0*, *abs\\_tol=0*, *ignore\\_spaces=False*, *ignore\\_case=False*)#\nCompares two columns from a dataframe, returning a True/False series,\nwith the same index as column 1.\n\n\n* Two nulls (np.nan) will evaluate to True.\n* A null and a non-null value will evaluate to False.\n* Numeric values will use the relative and absolute tolerances.\n* Decimal values (decimal.Decimal) will attempt to be converted to floats\nbefore comparing\n* Non-numeric values (i.e. where np.isclose can’t be used) will just\ntrigger True on two nulls or exact matches.\n\nParameters:\n* **col\\_1** (*Pandas.Series*) – The first column to look at\n* **col\\_2** (*Pandas.Series*) – The second column\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n\nReturns:\nA series of Boolean values. True == the values match, False == the\nvalues don’t match.\n\nReturn type:\npandas.Series\n\ndatacompy.core.compare\\_string\\_and\\_date\\_columns(*col\\_1*, *col\\_2*)#\nCompare a string column and date column, value-wise. This tries to\nconvert a string column to a date column and compare that way.\n\nParameters:\n* **col\\_1** (*Pandas.Series*) – The first column to look at\n* **col\\_2** (*Pandas.Series*) – The second column\n\ndatacompy.core.generate\\_id\\_within\\_group(*dataframe*, *join\\_columns*)#\nGenerate an ID column that can be used to deduplicate identical rows. The series generated\nis the order within a unique group, and it handles nulls.\n\nParameters:\n* **dataframe** (*Pandas.DataFrame*) – The dataframe to operate on\n* **join\\_columns** (*list*) – List of strings which are the join columns\n\nReturns:\nThe ID column that’s unique in each group.\n\nReturn type:\nPandas.Series\n\ndatacompy.core.get\\_merged\\_columns(*original\\_df*, *merged\\_df*, *suffix*)#\nGets the columns from an original dataframe, in the new merged dataframe\n\nParameters:\n* **original\\_df** (*Pandas.DataFrame*) – The original, pre-merge dataframe\n* **merged\\_df** (*Pandas.DataFrame*) – Post-merge with another dataframe, with suffixes added in.\n* **suffix** (*str*) – What suffix was used to distinguish when the original dataframe was\noverlapping with the other merged dataframe.\n\ndatacompy.core.render(*filename*, *\\*fields*)#\nRenders out an individual template. This basically just reads in a\ntemplate file, and applies `.format()` on the fields.\n\nParameters:\n* **filename** (*str*) – The file that contains the template. Will automagically prepend the\ntemplates directory before opening\n* **fields** (*list*) – Fields to be rendered out in the template\n\nReturns:\nThe fully rendered out file.\n\ndatacompy.core.temp\\_column\\_name(*\\*dataframes*)#\nGets a temp column name that isn’t included in columns of any dataframes\n\nParameters:\n**dataframes** (*list* *of* *Pandas.DataFrame*) – The DataFrames to create a temporary column name for\n\nReturns:\nString column name that looks like ‘\\_temp\\_x’ for some integer x\n## datacompy.fugue module#\n\n\nCompare two DataFrames that are supported by Fugue\n\n\ndatacompy.fugue.all\\_columns\\_match(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nWhether the columns all match in the dataframes\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n\nReturns:\nBoolean indicating whether the columns all match"
    },
    {
        "instruction": "Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on a specific column ('employee_id'), and produce a comparison report. Ensure that it reports matches and does not ignore extra columns.",
        "api": "datacompy",
        "output": "#!pip install datacompy\nfrom io import StringIO\nimport pandas as pd\nimport datacompy\n\ndata1 = \"\"\"employee_id, name, age\n1, islam mesabah, 25\n2, john doe, 30\n\"\"\"\n\ndata2 = \"\"\"employee_id, name, age\n1, islam mesabah, 25\n2, john doe, 30\n\"\"\"\n\ndf1 = pd.read_csv(StringIO(data1))\ndf2 = pd.read_csv(StringIO(data2))\n\ncompare = datacompy.Compare(\n  df1,\n  df2,\n  # You can also specify a list\n  # of columns\n  join_columns = 'employee_id',\n\n  # Optional, defaults to 'df1'\n  df1_name = 'Original',\n\n  # Optional, defaults to 'df2'\n  df2_name = 'New'\n  )\ncompare.matches(ignore_extra_columns = False)\nprint(compare.report())",
        "documentation": "\n\n==================\n Document 0 \n----------------\n datacompy#\n\n* datacompy package\n\t+ Submodules\n\t+ datacompy.core module\n\t\t- `Compare`\n\t\t\t* `Compare.all\\_columns\\_match()`\n\t\t\t* `Compare.all\\_mismatch()`\n\t\t\t* `Compare.all\\_rows\\_overlap()`\n\t\t\t* `Compare.count\\_matching\\_rows()`\n\t\t\t* `Compare.df1`\n\t\t\t* `Compare.df1\\_unq\\_columns()`\n\t\t\t* `Compare.df2`\n\t\t\t* `Compare.df2\\_unq\\_columns()`\n\t\t\t* `Compare.intersect\\_columns()`\n\t\t\t* `Compare.intersect\\_rows\\_match()`\n\t\t\t* `Compare.matches()`\n\t\t\t* `Compare.report()`\n\t\t\t* `Compare.sample\\_mismatch()`\n\t\t\t* `Compare.subset()`\n\t\t- `calculate\\_max\\_diff()`\n\t\t- `columns\\_equal()`\n\t\t- `compare\\_string\\_and\\_date\\_columns()`\n\t\t- `generate\\_id\\_within\\_group()`\n\t\t- `get\\_merged\\_columns()`\n\t\t- `render()`\n\t\t- `temp\\_column\\_name()`\n\t+ datacompy.fugue module\n\t\t- `all\\_columns\\_match()`\n\t\t- `intersect\\_columns()`\n\t\t- `is\\_match()`\n\t\t- `report()`\n\t\t- `unq\\_columns()`\n\t+ datacompy.spark module\n\t\t- `MatchType`\n\t\t\t* `MatchType.KNOWN\\_DIFFERENCE`\n\t\t\t* `MatchType.MATCH`\n\t\t\t* `MatchType.MISMATCH`\n\t\t- `SparkCompare`\n\t\t\t* `SparkCompare.base\\_row\\_count`\n\t\t\t* `SparkCompare.columns\\_compared`\n\t\t\t* `SparkCompare.columns\\_in\\_both`\n\t\t\t* `SparkCompare.columns\\_only\\_base`\n\t\t\t* `SparkCompare.columns\\_only\\_compare`\n\t\t\t* `SparkCompare.common\\_row\\_count`\n\t\t\t* `SparkCompare.compare\\_row\\_count`\n\t\t\t* `SparkCompare.report()`\n\t\t\t* `SparkCompare.rows\\_both\\_all`\n\t\t\t* `SparkCompare.rows\\_both\\_mismatch`\n\t\t\t* `SparkCompare.rows\\_only\\_base`\n\t\t\t* `SparkCompare.rows\\_only\\_compare`\n\t\t- `decimal\\_comparator()`\n\t+ Module contents\n\n\n# datacompy package#\n\n\n## Submodules#\n\n\n## datacompy.core module#\n\n\nCompare two Pandas DataFrames\n\n\nOriginally this package was meant to provide similar functionality to\nPROC COMPARE in SAS - i.e. human-readable reporting on the difference between\ntwo dataframes.\n\n\n*class* datacompy.core.Compare(*df1*, *df2*, *join\\_columns=None*, *on\\_index=False*, *abs\\_tol=0*, *rel\\_tol=0*, *df1\\_name='df1'*, *df2\\_name='df2'*, *ignore\\_spaces=False*, *ignore\\_case=False*, *cast\\_column\\_names\\_lower=True*)#\nBases: `object`\n\n\nComparison class\n\n==================\n Document 1 \n----------------\n\n# datacompy#\n\n* datacompy package\n\t+ Submodules\n\t+ datacompy.core module\n\t\t- `Compare`\n\t\t\t* `Compare.all\\_columns\\_match()`\n\t\t\t* `Compare.all\\_mismatch()`\n\t\t\t* `Compare.all\\_rows\\_overlap()`\n\t\t\t* `Compare.count\\_matching\\_rows()`\n\t\t\t* `Compare.df1`\n\t\t\t* `Compare.df1\\_unq\\_columns()`\n\t\t\t* `Compare.df2`\n\t\t\t* `Compare.df2\\_unq\\_columns()`\n\t\t\t* `Compare.intersect\\_columns()`\n\t\t\t* `Compare.intersect\\_rows\\_match()`\n\t\t\t* `Compare.matches()`\n\t\t\t* `Compare.report()`\n\t\t\t* `Compare.sample\\_mismatch()`\n\t\t\t* `Compare.subset()`\n\t\t- `calculate\\_max\\_diff()`\n\t\t- `columns\\_equal()`\n\t\t- `compare\\_string\\_and\\_date\\_columns()`\n\t\t- `generate\\_id\\_within\\_group()`\n\t\t- `get\\_merged\\_columns()`\n\t\t- `render()`\n\t\t- `temp\\_column\\_name()`\n\t+ datacompy.fugue module\n\t\t- `all\\_columns\\_match()`\n\t\t- `intersect\\_columns()`\n\t\t- `is\\_match()`\n\t\t- `report()`\n\t\t- `unq\\_columns()`\n\t+ datacompy.spark module\n\t\t- `MatchType`\n\t\t\t* `MatchType.KNOWN\\_DIFFERENCE`\n\t\t\t*\n\n==================\n Document 2 \n----------------\n# datacompy.fugue module#\n\n\nCompare two DataFrames that are supported by Fugue\n\n\ndatacompy.fugue.all\\_columns\\_match(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nWhether the columns all match in the dataframes\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n\nReturns:\nBoolean indicating whether the columns all match in the dataframes\n\ndatacompy.fugue.intersect\\_columns(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nGet columns that are shared between the two dataframes\n\nReturns:\nSet of that are shared between the two dataframes\n\nReturn type:\nOrderedSet\n\ndatacompy.fugue.is\\_match(*df1: AnyDataFrame*, *df2: AnyDataFrame*, *join\\_columns: str | List[str]*, *abs\\_tol: float = 0*, *rel\\_tol: float = 0*, *df1\\_name: str = 'df1'*, *df2\\_name: str = 'df2'*, *ignore\\_spaces: bool = False*, *ignore\\_case: bool = False*, *cast\\_column\\_names\\_lower: bool = True*, *parallelism: int | None = None*, *strict\\_schema: bool = False*) → bool#\nCheck whether two dataframes match.\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n* **join\\_columns** (*list* *or* *str**,* *optional*) – Column(s) to join dataframes on. If a string is passed in, that one\ncolumn will be used.\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance between two values.\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance between two values.\n* **df1\\_name** (*str**,* *optional*) – A string name for the first dataframe. This allows the reporting to\nprint out an actual name instead of “df1”, and allows human users to\nmore easily track the dataframes.\n* **df2\\_name** (*str**,* *optional*) – A string name for the second dataframe\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns (including any join\ncolumns)\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n* **cast\\_column\\_names\\_lower** (*bool**,* *optional*) – Boolean indicator that controls of column names will be cast into lower case\n* **parallelism** (*int**,* *optional*) – An integer representing the amount of parallelism. Entering a value for this\nwill force to use of Fugue over just vanilla Pandas\n* **strict\\_schema** (*bool**,* *optional*) – The schema must match exactly if set to `True`. This includes the names and types. Allows for a fast fail.\n\nReturns:\nReturns boolean as to if the DataFrames match.\n\ndatacompy.fugue.report(*df1: AnyDataFrame*, *df2: AnyDataFrame*, *join\\_columns: str | List[str]*, *abs\\_tol: float = 0*, *rel\\_tol: float = 0*, *df1\\_name: str = 'df1'*, *df2\\_name: str = 'df2'*, *ignore\\_spaces: bool = False*, *ignore\\_case: bool = False*, *cast\\_column\\_names\\_lower: bool = True*, *sample\\_count=10*, *column\\_count=10*, *html\\_file=None*, *parallelism: int | None = None*) → None#\nReturns a string representation of a report. The representation can\nthen be printed or saved to a file.\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n* **join\\_columns** (*list* *or* *str**,* *optional*) – Column(s) to join dataframes on. If a string is passed in, that one\ncolumn will be used.\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance between two values.\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance between two values.\n* **df1\\_name** (*str**,* *optional*) – A string name for the first dataframe. This allows the reporting to\nprint out an actual name instead of “df1”, and allows human users to\nmore easily track the dataframes.\n* **df2\\_name** (*str**,* *optional*) – A string name for the second dataframe\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns (including any join\ncolumns)\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n* **cast\\_column\\_names\\_lower** (*bool**,* *optional*) – Boolean indicator that controls of column names will be cast into lower case\n* **parallelism** (*int**,* *optional*) – An integer representing the amount of parallelism. Entering a value for this\nwill force to use of Fugue over just vanilla Pandas\n* **strict\\_schema** (*bool**,* *optional*) – The schema must match exactly if set to `True`. This includes the names and types. Allows for a fast fail.\n* **sample\\_count** (*int**,* *optional*) – The number of sample records to return. Defaults to 10.\n* **column\\_count** (*int**,* *optional*) – The number of columns to display in the sample records output. Defaults to 10.\n* **html\\_file** (*str**,* *optional*) – HTML file name to save report output to. If `None` the file creation will be skipped.\n\ndatacompy.fugue.unq\\_columns(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nGet columns that are unique to df1\n\nReturns:\nSet of columns that are unique to df1\n## datacompy.spark module#\n\n\n*class* datacompy.spark.MatchType(*value*)#\nBases: `Enum`\n\n\nAn enumeration.\n\n\nKNOWN\\_DIFFERENCE *= 2*#\n\nMATCH *= 1*#\n\nMISMATCH *= 0*#\n\n\n*class* datacompy.spark.SparkCompare(*spark\\_session*, *base\\_df*, *compare\\_df*, *join\\_columns*, *column\\_mapping=None*, *cache\\_intermediates=False*, *known\\_differences=None*, *rel\\_tol=0*, *abs\\_tol=0*, *show\\_all\\_columns=False*, *match\\_rates=False*)#\nBases: `object`\n\n\nComparison class used to compare two Spark Dataframes.\n\n\nExtends the `Compare` functionality to the wide world of Spark\n\n==================\n Document 3 \n----------------\n# datacompy.core module#\n\n\nCompare two Pandas DataFrames\n\n\nOriginally this package was meant to provide similar functionality to\nPROC COMPARE in SAS - i.e. human-readable reporting on the difference between\ntwo dataframes.\n\n\n*class* datacompy.core.Compare(*df1*, *df2*, *join\\_columns=None*, *on\\_index=False*, *abs\\_tol=0*, *rel\\_tol=0*, *df1\\_name='df1'*, *df2\\_name='df2'*, *ignore\\_spaces=False*, *ignore\\_case=False*, *cast\\_column\\_names\\_lower=True*)#\nBases: `object`\n\n\nComparison class to be used to compare whether two dataframes as equal.\n\n\nBoth df1 and df2 should be dataframes containing all of the join\\_columns,\nwith unique column names. Differences between values are compared to\nabs\\_tol + rel\\_tol \\* abs(df2[‘value’]).\n\nParameters:\n* **df1** (pandas `DataFrame`) – First dataframe to check\n* **df2** (pandas `DataFrame`) – Second dataframe to check\n* **join\\_columns** (*list* *or* *str**,* *optional*) – Column(s) to join dataframes on. If a string is passed in, that one\ncolumn will be used.\n* **on\\_index** (*bool**,* *optional*) – If True, the index will be used to join the two dataframes. If both\n`join\\_columns` and `on\\_index` are provided, an exception will be\nraised.\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance between two values.\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance between two values.\n* **df1\\_name** (*str**,* *optional*) – A string name for the first dataframe. This allows the reporting to\nprint out an actual name instead of “df1”, and allows human users to\nmore easily track the dataframes.\n* **df2\\_name** (*str**,* *optional*) – A string name for the second dataframe\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns (including any join\ncolumns)\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n* **cast\\_column\\_names\\_lower** (*bool**,* *optional*) – Boolean indicator that controls of column names will be cast into lower case\n\nVariables:\n* **df1\\_unq\\_rows** (pandas `DataFrame`) – All records that are only in df1 (based on a join on join\\_columns)\n* **df2\\_unq\\_rows** (pandas `DataFrame`) – All records that are only in df2 (based on a join on join\\_columns)\n\n\nall\\_columns\\_match()#\nWhether the columns all match in the dataframes\n\nall\\_mismatch(*ignore\\_matching\\_cols=False*)#\nAll rows with any columns that have a mismatch. Returns all df1 and df2 versions of the columns and join\ncolumns.\n\nParameters:\n**ignore\\_matching\\_cols** (*bool**,* *optional*) – Whether showing the matching columns in the output or not. The default is False.\n\nReturns:\nAll rows of the intersection dataframe, containing any columns, that don’t match.\n\nReturn type:\nPandas.DataFrame\n\nall\\_rows\\_overlap()#\nWhether the rows are all present in both dataframes\n\nReturns:\nTrue if all rows in df1 are in df2 and vice versa (based on\nexistence for join option)\n\nReturn type:\nbool\n\ncount\\_matching\\_rows()#\nCount the number of rows match (on overlapping fields)\n\nReturns:\nNumber of matching rows\n\nReturn type:\nint\n\n*property* df1#\n\ndf1\\_unq\\_columns()#\nGet columns that are unique to df1\n\n*property* df2#\n\ndf2\\_unq\\_columns()#\nGet columns that are unique to df2\n\nintersect\\_columns()#\nGet columns that are shared between the two dataframes\n\nintersect\\_rows\\_match()#\nCheck whether the intersect rows all match\n\nmatches(*ignore\\_extra\\_columns=False*)#\nReturn True or False if the dataframes match.\n\nParameters:\n**ignore\\_extra\\_columns** (*bool*) – Ignores any columns in one dataframe and not in the other.\n\nreport(*sample\\_count=10*, *column\\_count=10*, *html\\_file=None*)#\nReturns a string representation of a report. The representation can\nthen be printed or saved to a file.\n\nParameters:\n* **sample\\_count** (*int**,* *optional*) – The number of sample records to return. Defaults to 10.\n* **column\\_count** (*int**,* *optional*) – The number of columns to display in the sample records output. Defaults to 10.\n* **html\\_file** (*str**,* *optional*) – HTML file name to save report output to. If `None` the file creation will be skipped.\n\nReturns:\nThe report, formatted kinda nicely.\n\nReturn type:\nstr\n\nsample\\_mismatch(*column*, *sample\\_count=10*, *for\\_display=False*)#\nReturns a sample sub-dataframe which contains the identifying\ncolumns, and df1 and df2 versions of the column.\n\nParameters:\n* **column** (*str*) – The raw column name (i.e. without `\\_df1` appended)\n* **sample\\_count** (*int**,* *optional*) – The number of sample records to return. Defaults to 10.\n* **for\\_display** (*bool**,* *optional*) – Whether this is just going to be used for display (overwrite the\ncolumn names)\n\nReturns:\nA sample of the intersection dataframe, containing only the\n“pertinent” columns, for rows that don’t match on the provided\ncolumn.\n\nsubset()#\nReturn True if dataframe 2 is a subset of dataframe 1.\n\n\nDataframe 2 is considered a subset if all of its columns are in\ndataframe 1, and all of its rows match rows in dataframe 1 for the\nshared columns.\n\n\ndatacompy.core.calculate\\_max\\_diff(*col\\_1*, *col\\_2*)#\nGet a maximum difference between two columns\n\nParameters:\n* **col\\_1** (*Pandas.Series*) – The first column\n* **col\\_2** (*Pandas.Series*) – The second column\n\nReturns:\nNumeric field, or zero.\n\nReturn type:\nNumeric\n\ndatacompy.core.columns\\_equal(*col\\_1*, *col\\_2*, *rel\\_tol=0*, *abs\\_tol=0*, *ignore\\_spaces=False*, *ignore\\_case=False*)#\nCompares two columns from a dataframe, returning a True/False series,\nwith the same index as column 1.\n\n\n* Two nulls (np.nan) will evaluate to True.\n* A null and a non-null value will evaluate to False.\n* Numeric values will use the relative and absolute tolerances.\n* Decimal values (decimal.Decimal) will attempt to be converted to floats\nbefore comparing\n* Non-numeric values (i.e. where np.isclose can’t be used) will just\ntrigger True on two nulls or exact matches.\n\nParameters:\n* **col\\_1** (*Pandas.Series*) – The first column to look at\n* **col\\_2** (*Pandas.Series*) – The second column\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n\nReturns:\nA series of Boolean values. True == the values match, False == the\nvalues don’t match.\n\nReturn type:\npandas.Series\n\ndatacompy.core.compare\\_string\\_and\\_date\\_columns(*col\\_1*, *col\\_2*)#\nCompare a string column and date column, value-wise. This tries to\nconvert a string column to a date column and compare that way.\n\nParameters:\n* **col\\_1** (*Pandas.Series*) – The first column to look at\n* **col\\_2** (*Pandas.Series*) – The second column\n\ndatacompy.core.generate\\_id\\_within\\_group(*dataframe*, *join\\_columns*)#\nGenerate an ID column that can be used to deduplicate identical rows. The series generated\nis the order within a unique group, and it handles nulls.\n\nParameters:\n* **dataframe** (*Pandas.DataFrame*) – The dataframe to operate on\n* **join\\_columns** (*list*) – List of strings which are the join columns\n\nReturns:\nThe ID column that’s unique in each group.\n\nReturn type:\nPandas.Series\n\ndatacompy.core.get\\_merged\\_columns(*original\\_df*, *merged\\_df*, *suffix*)#\nGets the columns from an original dataframe, in the new merged dataframe\n\nParameters:\n* **original\\_df** (*Pandas.DataFrame*) – The original, pre-merge dataframe\n* **merged\\_df** (*Pandas.DataFrame*) – Post-merge with another dataframe, with suffixes added in.\n* **suffix** (*str*) – What suffix was used to distinguish when the original dataframe was\noverlapping with the other merged dataframe.\n\ndatacompy.core.render(*filename*, *\\*fields*)#\nRenders out an individual template. This basically just reads in a\ntemplate file, and applies `.format()` on the fields.\n\nParameters:\n* **filename** (*str*) – The file that contains the template. Will automagically prepend the\ntemplates directory before opening\n* **fields** (*list*) – Fields to be rendered out in the template\n\nReturns:\nThe fully rendered out file.\n\ndatacompy.core.temp\\_column\\_name(*\\*dataframes*)#\nGets a temp column name that isn’t included in columns of any dataframes\n\nParameters:\n**dataframes** (*list* *of* *Pandas.DataFrame*) – The DataFrames to create a temporary column name for\n\nReturns:\nString column name that looks like ‘\\_temp\\_x’ for some integer x\n## datacompy.fugue module#\n\n\nCompare two DataFrames that are supported by Fugue\n\n\ndatacompy.fugue.all\\_columns\\_match(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nWhether the columns all match in the dataframes\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n\nReturns:\nBoolean indicating whether the columns all match"
    },
    {
        "instruction": "Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on multiple columns ('employee_id' and 'department'), and produce a comparison report. Ensure that it reports matches and includes extra columns in the comparison.",
        "api": "datacompy",
        "output": "#!pip install datacompy\nfrom io import StringIO\nimport pandas as pd\nimport datacompy\n\ndata1 = \"\"\"employee_id, name, department\n1, islam mesabah, IT\n2, john doe, HR\n\"\"\"\n\ndata2 = \"\"\"employee_id, name, department\n1, islam mesabah, IT\n2, john doe, HR\n\"\"\"\n\ndf1 = pd.read_csv(StringIO(data1))\ndf2 = pd.read_csv(StringIO(data2))\n\ncompare = datacompy.Compare(\n  df1,\n  df2,\n  # You can also specify a list\n  # of columns\n  join_columns = ['employee_id', 'department'],\n\n  # Optional, defaults to 'df1'\n  df1_name = 'Original',\n\n  # Optional, defaults to 'df2'\n  df2_name = 'New'\n  )\ncompare.matches(ignore_extra_columns = False)\nprint(compare.report())",
        "documentation": "\n\n==================\n Document 0 \n----------------\n datacompy#\n\n* datacompy package\n\t+ Submodules\n\t+ datacompy.core module\n\t\t- `Compare`\n\t\t\t* `Compare.all\\_columns\\_match()`\n\t\t\t* `Compare.all\\_mismatch()`\n\t\t\t* `Compare.all\\_rows\\_overlap()`\n\t\t\t* `Compare.count\\_matching\\_rows()`\n\t\t\t* `Compare.df1`\n\t\t\t* `Compare.df1\\_unq\\_columns()`\n\t\t\t* `Compare.df2`\n\t\t\t* `Compare.df2\\_unq\\_columns()`\n\t\t\t* `Compare.intersect\\_columns()`\n\t\t\t* `Compare.intersect\\_rows\\_match()`\n\t\t\t* `Compare.matches()`\n\t\t\t* `Compare.report()`\n\t\t\t* `Compare.sample\\_mismatch()`\n\t\t\t* `Compare.subset()`\n\t\t- `calculate\\_max\\_diff()`\n\t\t- `columns\\_equal()`\n\t\t- `compare\\_string\\_and\\_date\\_columns()`\n\t\t- `generate\\_id\\_within\\_group()`\n\t\t- `get\\_merged\\_columns()`\n\t\t- `render()`\n\t\t- `temp\\_column\\_name()`\n\t+ datacompy.fugue module\n\t\t- `all\\_columns\\_match()`\n\t\t- `intersect\\_columns()`\n\t\t- `is\\_match()`\n\t\t- `report()`\n\t\t- `unq\\_columns()`\n\t+ datacompy.spark module\n\t\t- `MatchType`\n\t\t\t* `MatchType.KNOWN\\_DIFFERENCE`\n\t\t\t* `MatchType.MATCH`\n\t\t\t* `MatchType.MISMATCH`\n\t\t- `SparkCompare`\n\t\t\t* `SparkCompare.base\\_row\\_count`\n\t\t\t* `SparkCompare.columns\\_compared`\n\t\t\t* `SparkCompare.columns\\_in\\_both`\n\t\t\t* `SparkCompare.columns\\_only\\_base`\n\t\t\t* `SparkCompare.columns\\_only\\_compare`\n\t\t\t* `SparkCompare.common\\_row\\_count`\n\t\t\t* `SparkCompare.compare\\_row\\_count`\n\t\t\t* `SparkCompare.report()`\n\t\t\t* `SparkCompare.rows\\_both\\_all`\n\t\t\t* `SparkCompare.rows\\_both\\_mismatch`\n\t\t\t* `SparkCompare.rows\\_only\\_base`\n\t\t\t* `SparkCompare.rows\\_only\\_compare`\n\t\t- `decimal\\_comparator()`\n\t+ Module contents\n\n\n# datacompy package#\n\n\n## Submodules#\n\n\n## datacompy.core module#\n\n\nCompare two Pandas DataFrames\n\n\nOriginally this package was meant to provide similar functionality to\nPROC COMPARE in SAS - i.e. human-readable reporting on the difference between\ntwo dataframes.\n\n\n*class* datacompy.core.Compare(*df1*, *df2*, *join\\_columns=None*, *on\\_index=False*, *abs\\_tol=0*, *rel\\_tol=0*, *df1\\_name='df1'*, *df2\\_name='df2'*, *ignore\\_spaces=False*, *ignore\\_case=False*, *cast\\_column\\_names\\_lower=True*)#\nBases: `object`\n\n\nComparison class\n\n==================\n Document 1 \n----------------\n\n# datacompy#\n\n* datacompy package\n\t+ Submodules\n\t+ datacompy.core module\n\t\t- `Compare`\n\t\t\t* `Compare.all\\_columns\\_match()`\n\t\t\t* `Compare.all\\_mismatch()`\n\t\t\t* `Compare.all\\_rows\\_overlap()`\n\t\t\t* `Compare.count\\_matching\\_rows()`\n\t\t\t* `Compare.df1`\n\t\t\t* `Compare.df1\\_unq\\_columns()`\n\t\t\t* `Compare.df2`\n\t\t\t* `Compare.df2\\_unq\\_columns()`\n\t\t\t* `Compare.intersect\\_columns()`\n\t\t\t* `Compare.intersect\\_rows\\_match()`\n\t\t\t* `Compare.matches()`\n\t\t\t* `Compare.report()`\n\t\t\t* `Compare.sample\\_mismatch()`\n\t\t\t* `Compare.subset()`\n\t\t- `calculate\\_max\\_diff()`\n\t\t- `columns\\_equal()`\n\t\t- `compare\\_string\\_and\\_date\\_columns()`\n\t\t- `generate\\_id\\_within\\_group()`\n\t\t- `get\\_merged\\_columns()`\n\t\t- `render()`\n\t\t- `temp\\_column\\_name()`\n\t+ datacompy.fugue module\n\t\t- `all\\_columns\\_match()`\n\t\t- `intersect\\_columns()`\n\t\t- `is\\_match()`\n\t\t- `report()`\n\t\t- `unq\\_columns()`\n\t+ datacompy.spark module\n\t\t- `MatchType`\n\t\t\t* `MatchType.KNOWN\\_DIFFERENCE`\n\t\t\t*\n\n==================\n Document 2 \n----------------\n# datacompy.fugue module#\n\n\nCompare two DataFrames that are supported by Fugue\n\n\ndatacompy.fugue.all\\_columns\\_match(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nWhether the columns all match in the dataframes\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n\nReturns:\nBoolean indicating whether the columns all match in the dataframes\n\ndatacompy.fugue.intersect\\_columns(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nGet columns that are shared between the two dataframes\n\nReturns:\nSet of that are shared between the two dataframes\n\nReturn type:\nOrderedSet\n\ndatacompy.fugue.is\\_match(*df1: AnyDataFrame*, *df2: AnyDataFrame*, *join\\_columns: str | List[str]*, *abs\\_tol: float = 0*, *rel\\_tol: float = 0*, *df1\\_name: str = 'df1'*, *df2\\_name: str = 'df2'*, *ignore\\_spaces: bool = False*, *ignore\\_case: bool = False*, *cast\\_column\\_names\\_lower: bool = True*, *parallelism: int | None = None*, *strict\\_schema: bool = False*) → bool#\nCheck whether two dataframes match.\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n* **join\\_columns** (*list* *or* *str**,* *optional*) – Column(s) to join dataframes on. If a string is passed in, that one\ncolumn will be used.\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance between two values.\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance between two values.\n* **df1\\_name** (*str**,* *optional*) – A string name for the first dataframe. This allows the reporting to\nprint out an actual name instead of “df1”, and allows human users to\nmore easily track the dataframes.\n* **df2\\_name** (*str**,* *optional*) – A string name for the second dataframe\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns (including any join\ncolumns)\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n* **cast\\_column\\_names\\_lower** (*bool**,* *optional*) – Boolean indicator that controls of column names will be cast into lower case\n* **parallelism** (*int**,* *optional*) – An integer representing the amount of parallelism. Entering a value for this\nwill force to use of Fugue over just vanilla Pandas\n* **strict\\_schema** (*bool**,* *optional*) – The schema must match exactly if set to `True`. This includes the names and types. Allows for a fast fail.\n\nReturns:\nReturns boolean as to if the DataFrames match.\n\ndatacompy.fugue.report(*df1: AnyDataFrame*, *df2: AnyDataFrame*, *join\\_columns: str | List[str]*, *abs\\_tol: float = 0*, *rel\\_tol: float = 0*, *df1\\_name: str = 'df1'*, *df2\\_name: str = 'df2'*, *ignore\\_spaces: bool = False*, *ignore\\_case: bool = False*, *cast\\_column\\_names\\_lower: bool = True*, *sample\\_count=10*, *column\\_count=10*, *html\\_file=None*, *parallelism: int | None = None*) → None#\nReturns a string representation of a report. The representation can\nthen be printed or saved to a file.\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n* **join\\_columns** (*list* *or* *str**,* *optional*) – Column(s) to join dataframes on. If a string is passed in, that one\ncolumn will be used.\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance between two values.\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance between two values.\n* **df1\\_name** (*str**,* *optional*) – A string name for the first dataframe. This allows the reporting to\nprint out an actual name instead of “df1”, and allows human users to\nmore easily track the dataframes.\n* **df2\\_name** (*str**,* *optional*) – A string name for the second dataframe\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns (including any join\ncolumns)\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n* **cast\\_column\\_names\\_lower** (*bool**,* *optional*) – Boolean indicator that controls of column names will be cast into lower case\n* **parallelism** (*int**,* *optional*) – An integer representing the amount of parallelism. Entering a value for this\nwill force to use of Fugue over just vanilla Pandas\n* **strict\\_schema** (*bool**,* *optional*) – The schema must match exactly if set to `True`. This includes the names and types. Allows for a fast fail.\n* **sample\\_count** (*int**,* *optional*) – The number of sample records to return. Defaults to 10.\n* **column\\_count** (*int**,* *optional*) – The number of columns to display in the sample records output. Defaults to 10.\n* **html\\_file** (*str**,* *optional*) – HTML file name to save report output to. If `None` the file creation will be skipped.\n\ndatacompy.fugue.unq\\_columns(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nGet columns that are unique to df1\n\nReturns:\nSet of columns that are unique to df1\n## datacompy.spark module#\n\n\n*class* datacompy.spark.MatchType(*value*)#\nBases: `Enum`\n\n\nAn enumeration.\n\n\nKNOWN\\_DIFFERENCE *= 2*#\n\nMATCH *= 1*#\n\nMISMATCH *= 0*#\n\n\n*class* datacompy.spark.SparkCompare(*spark\\_session*, *base\\_df*, *compare\\_df*, *join\\_columns*, *column\\_mapping=None*, *cache\\_intermediates=False*, *known\\_differences=None*, *rel\\_tol=0*, *abs\\_tol=0*, *show\\_all\\_columns=False*, *match\\_rates=False*)#\nBases: `object`\n\n\nComparison class used to compare two Spark Dataframes.\n\n\nExtends the `Compare` functionality to the wide world of Spark\n\n==================\n Document 3 \n----------------\n# datacompy.core module#\n\n\nCompare two Pandas DataFrames\n\n\nOriginally this package was meant to provide similar functionality to\nPROC COMPARE in SAS - i.e. human-readable reporting on the difference between\ntwo dataframes.\n\n\n*class* datacompy.core.Compare(*df1*, *df2*, *join\\_columns=None*, *on\\_index=False*, *abs\\_tol=0*, *rel\\_tol=0*, *df1\\_name='df1'*, *df2\\_name='df2'*, *ignore\\_spaces=False*, *ignore\\_case=False*, *cast\\_column\\_names\\_lower=True*)#\nBases: `object`\n\n\nComparison class to be used to compare whether two dataframes as equal.\n\n\nBoth df1 and df2 should be dataframes containing all of the join\\_columns,\nwith unique column names. Differences between values are compared to\nabs\\_tol + rel\\_tol \\* abs(df2[‘value’]).\n\nParameters:\n* **df1** (pandas `DataFrame`) – First dataframe to check\n* **df2** (pandas `DataFrame`) – Second dataframe to check\n* **join\\_columns** (*list* *or* *str**,* *optional*) – Column(s) to join dataframes on. If a string is passed in, that one\ncolumn will be used.\n* **on\\_index** (*bool**,* *optional*) – If True, the index will be used to join the two dataframes. If both\n`join\\_columns` and `on\\_index` are provided, an exception will be\nraised.\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance between two values.\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance between two values.\n* **df1\\_name** (*str**,* *optional*) – A string name for the first dataframe. This allows the reporting to\nprint out an actual name instead of “df1”, and allows human users to\nmore easily track the dataframes.\n* **df2\\_name** (*str**,* *optional*) – A string name for the second dataframe\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns (including any join\ncolumns)\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n* **cast\\_column\\_names\\_lower** (*bool**,* *optional*) – Boolean indicator that controls of column names will be cast into lower case\n\nVariables:\n* **df1\\_unq\\_rows** (pandas `DataFrame`) – All records that are only in df1 (based on a join on join\\_columns)\n* **df2\\_unq\\_rows** (pandas `DataFrame`) – All records that are only in df2 (based on a join on join\\_columns)\n\n\nall\\_columns\\_match()#\nWhether the columns all match in the dataframes\n\nall\\_mismatch(*ignore\\_matching\\_cols=False*)#\nAll rows with any columns that have a mismatch. Returns all df1 and df2 versions of the columns and join\ncolumns.\n\nParameters:\n**ignore\\_matching\\_cols** (*bool**,* *optional*) – Whether showing the matching columns in the output or not. The default is False.\n\nReturns:\nAll rows of the intersection dataframe, containing any columns, that don’t match.\n\nReturn type:\nPandas.DataFrame\n\nall\\_rows\\_overlap()#\nWhether the rows are all present in both dataframes\n\nReturns:\nTrue if all rows in df1 are in df2 and vice versa (based on\nexistence for join option)\n\nReturn type:\nbool\n\ncount\\_matching\\_rows()#\nCount the number of rows match (on overlapping fields)\n\nReturns:\nNumber of matching rows\n\nReturn type:\nint\n\n*property* df1#\n\ndf1\\_unq\\_columns()#\nGet columns that are unique to df1\n\n*property* df2#\n\ndf2\\_unq\\_columns()#\nGet columns that are unique to df2\n\nintersect\\_columns()#\nGet columns that are shared between the two dataframes\n\nintersect\\_rows\\_match()#\nCheck whether the intersect rows all match\n\nmatches(*ignore\\_extra\\_columns=False*)#\nReturn True or False if the dataframes match.\n\nParameters:\n**ignore\\_extra\\_columns** (*bool*) – Ignores any columns in one dataframe and not in the other.\n\nreport(*sample\\_count=10*, *column\\_count=10*, *html\\_file=None*)#\nReturns a string representation of a report. The representation can\nthen be printed or saved to a file.\n\nParameters:\n* **sample\\_count** (*int**,* *optional*) – The number of sample records to return. Defaults to 10.\n* **column\\_count** (*int**,* *optional*) – The number of columns to display in the sample records output. Defaults to 10.\n* **html\\_file** (*str**,* *optional*) – HTML file name to save report output to. If `None` the file creation will be skipped.\n\nReturns:\nThe report, formatted kinda nicely.\n\nReturn type:\nstr\n\nsample\\_mismatch(*column*, *sample\\_count=10*, *for\\_display=False*)#\nReturns a sample sub-dataframe which contains the identifying\ncolumns, and df1 and df2 versions of the column.\n\nParameters:\n* **column** (*str*) – The raw column name (i.e. without `\\_df1` appended)\n* **sample\\_count** (*int**,* *optional*) – The number of sample records to return. Defaults to 10.\n* **for\\_display** (*bool**,* *optional*) – Whether this is just going to be used for display (overwrite the\ncolumn names)\n\nReturns:\nA sample of the intersection dataframe, containing only the\n“pertinent” columns, for rows that don’t match on the provided\ncolumn.\n\nsubset()#\nReturn True if dataframe 2 is a subset of dataframe 1.\n\n\nDataframe 2 is considered a subset if all of its columns are in\ndataframe 1, and all of its rows match rows in dataframe 1 for the\nshared columns.\n\n\ndatacompy.core.calculate\\_max\\_diff(*col\\_1*, *col\\_2*)#\nGet a maximum difference between two columns\n\nParameters:\n* **col\\_1** (*Pandas.Series*) – The first column\n* **col\\_2** (*Pandas.Series*) – The second column\n\nReturns:\nNumeric field, or zero.\n\nReturn type:\nNumeric\n\ndatacompy.core.columns\\_equal(*col\\_1*, *col\\_2*, *rel\\_tol=0*, *abs\\_tol=0*, *ignore\\_spaces=False*, *ignore\\_case=False*)#\nCompares two columns from a dataframe, returning a True/False series,\nwith the same index as column 1.\n\n\n* Two nulls (np.nan) will evaluate to True.\n* A null and a non-null value will evaluate to False.\n* Numeric values will use the relative and absolute tolerances.\n* Decimal values (decimal.Decimal) will attempt to be converted to floats\nbefore comparing\n* Non-numeric values (i.e. where np.isclose can’t be used) will just\ntrigger True on two nulls or exact matches.\n\nParameters:\n* **col\\_1** (*Pandas.Series*) – The first column to look at\n* **col\\_2** (*Pandas.Series*) – The second column\n* **rel\\_tol** (*float**,* *optional*) – Relative tolerance\n* **abs\\_tol** (*float**,* *optional*) – Absolute tolerance\n* **ignore\\_spaces** (*bool**,* *optional*) – Flag to strip whitespace (including newlines) from string columns\n* **ignore\\_case** (*bool**,* *optional*) – Flag to ignore the case of string columns\n\nReturns:\nA series of Boolean values. True == the values match, False == the\nvalues don’t match.\n\nReturn type:\npandas.Series\n\ndatacompy.core.compare\\_string\\_and\\_date\\_columns(*col\\_1*, *col\\_2*)#\nCompare a string column and date column, value-wise. This tries to\nconvert a string column to a date column and compare that way.\n\nParameters:\n* **col\\_1** (*Pandas.Series*) – The first column to look at\n* **col\\_2** (*Pandas.Series*) – The second column\n\ndatacompy.core.generate\\_id\\_within\\_group(*dataframe*, *join\\_columns*)#\nGenerate an ID column that can be used to deduplicate identical rows. The series generated\nis the order within a unique group, and it handles nulls.\n\nParameters:\n* **dataframe** (*Pandas.DataFrame*) – The dataframe to operate on\n* **join\\_columns** (*list*) – List of strings which are the join columns\n\nReturns:\nThe ID column that’s unique in each group.\n\nReturn type:\nPandas.Series\n\ndatacompy.core.get\\_merged\\_columns(*original\\_df*, *merged\\_df*, *suffix*)#\nGets the columns from an original dataframe, in the new merged dataframe\n\nParameters:\n* **original\\_df** (*Pandas.DataFrame*) – The original, pre-merge dataframe\n* **merged\\_df** (*Pandas.DataFrame*) – Post-merge with another dataframe, with suffixes added in.\n* **suffix** (*str*) – What suffix was used to distinguish when the original dataframe was\noverlapping with the other merged dataframe.\n\ndatacompy.core.render(*filename*, *\\*fields*)#\nRenders out an individual template. This basically just reads in a\ntemplate file, and applies `.format()` on the fields.\n\nParameters:\n* **filename** (*str*) – The file that contains the template. Will automagically prepend the\ntemplates directory before opening\n* **fields** (*list*) – Fields to be rendered out in the template\n\nReturns:\nThe fully rendered out file.\n\ndatacompy.core.temp\\_column\\_name(*\\*dataframes*)#\nGets a temp column name that isn’t included in columns of any dataframes\n\nParameters:\n**dataframes** (*list* *of* *Pandas.DataFrame*) – The DataFrames to create a temporary column name for\n\nReturns:\nString column name that looks like ‘\\_temp\\_x’ for some integer x\n## datacompy.fugue module#\n\n\nCompare two DataFrames that are supported by Fugue\n\n\ndatacompy.fugue.all\\_columns\\_match(*df1: AnyDataFrame*, *df2: AnyDataFrame*)#\nWhether the columns all match in the dataframes\n\nParameters:\n* **df1** (`AnyDataFrame`) – First dataframe to check\n* **df2** (`AnyDataFrame`) – Second dataframe to check\n\nReturns:\nBoolean indicating whether the columns all match"
    },
    {
        "instruction": "Develop a Python program using the 'datasets' API to load the SNLI dataset and count the number of unique genres in the training set. The program should load the SNLI dataset, access the training genres, and count the unique genre labels.",
        "api": "datasets",
        "output": "#!pip install datasets\nfrom datasets import load_dataset\n\n# Load SNLI dataset\ndataset = load_dataset(\"snli\")\n\n# Get the training genres\ntrain_genres = dataset[\"train\"][\"genre\"]\n\n# Count unique genre labels\nunique_genres = set(train_genres)\n\nprint(\"Number of unique genres in the SNLI training set:\", len(unique_genres))",
        "documentation": "\n\n==================\n Document 0 \n----------------\n### get\\_all\\_exported\\_dataset\\_infos\n\nEmpty dict if doesn’t exist\n\n```\n>>> from datasets import load_dataset_builder\n>>> ds_builder = load_dataset_builder('rotten\\_tomatoes')\n>>> ds_builder.get_all_exported_dataset_infos()\n{'default': DatasetInfo(description=\"Movie Review Dataset.\na dataset of containing 5,331 positive and 5,331 negative processed\ns from Rotten Tomatoes movie reviews. This data was first used in Bo\n Lillian Lee, ``Seeing stars: Exploiting class relationships for\nt categorization with respect to rating scales.'', Proceedings of the\n5.\nion='@InProceedings{Pang+Lee:05a,\n = {Bo Pang and Lillian Lee},\n= {Seeing stars: Exploiting class relationships for sentiment\n categorization with respect to rating scales},\ntle = {Proceedings of the ACL},\n 2005\n\nage='http://www.cs.cornell.edu/people/pabo/movie-review-data/', license='', features={'text': Value(dtype='string', id=None), 'label': ClassLabel(num\\_classes=2, names=['neg', 'pos'], id=None)}, post\\_processed=None, supervised\\_keys=SupervisedKeysData(input='', output=''), task\\_templates=[TextClassification(task='text-classification', text\\_column='text', label\\_column='label')], builder\\_name='rotten\\_tomatoes\\_movie\\_review', config\\_name='default', version=1.0.0, splits={'train': SplitInfo(name='train', num\\_bytes=1074810, num\\_examples=8530, dataset\\_name='rotten\\_tomatoes\\_movie\\_review'), 'validation': SplitInfo(name='validation', num\\_bytes=134679, num\\_examples=1066, dataset\\_name='rotten\\_tomatoes\\_movie\\_review'), 'test': SplitInfo(name='test', num\\_bytes=135972, num\\_examples=1066, dataset\\_name='rotten\\_tomatoes\\_movie\\_review')}, download\\_checksums={'https://storage.googleapis.com/seldon-datasets/sentence\\_polarity\\_v1/rt-polaritydata.tar.gz': {'num\\_bytes': 487770, 'checksum': 'a05befe52aafda71d458d188a1c54506a998b1308613ba76bbda2e5029409ce9'}}, download\\_size=487770, post\\_processing\\_size=None, dataset\\_size=1345461, size\\_in\\_bytes=1833231)}\n```\n\n#### get\\_exported\\_dataset\\_info\n\nEmpty `DatasetInfo` if doesn’t exist\n\n```\n>>> from datasets import load_dataset_builder\n>>> ds_builder = load_dataset_builder('rotten\\_tomatoes')\n>>> ds_builder.get_exported_dataset_info()\nDatasetInfo(description=\"Movie Review Dataset.\na dataset of containing 5,331 positive and 5,331 negative processed\ns from Rotten Tomatoes movie reviews. This data was first used in Bo\n Lillian Lee, ``Seeing\n\n==================\n Document 1 \n----------------\n### take\n\n\n* **n** (`int`) —\nNumber of elements to take.\n\nCreate a new IterableDataset with only the first `n` elements.\n\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten\\_tomatoes\", split=\"train\", streaming=True)\n>>> small_ds = ds.take(2)\n>>> list(small_ds)\n[{'label': 1,\n 'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'},\n {'label': 1,\n 'text': 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .'}]\n```\n\n\n#### info\n\n\n## \nIterableDatasetDict\n\n\nDictionary with split names as keys (‘train’, ‘test’ for example), and `IterableDataset` objects as values.\n\n### class datasets.IterableDatasetDict\n\n<\nsource\n>\n(\nfunction: typing.Optional[typing.Callable] = None\nwith\\_indices: bool = False\ninput\\_columns: typing.Union[str, typing.List[str], NoneType] = None\nbatched: bool = False\nbatch\\_size: int = 1000\ndrop\\_last\\_batch: bool = False\nremove\\_columns: typing.Union[str, typing.List[str], NoneType] = None\nfn\\_kwargs: typing.Optional[dict] = None\n\n\t+ `function(example: Dict[str, Any]) -> Dict[str, Any]` if `batched=False` and\n\n==================\n Document 2 \n----------------\n## class datasets.NamedSplit\n\nDescriptor corresponding to a named split (train, test, …).\n\nEach descriptor can be composed with other using addition or slice:\n\n```\nsplit = datasets.Split.TRAIN.subsplit(datasets.percent[0:25]) + datasets.Split.TEST\n```\n\nThe resulting split will correspond to 25% of the train split merged with\n100% of the test split.\n\nA split cannot be added twice, so the following will fail:\n\n```\nsplit = (\n        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +\n        datasets.Split.TRAIN.subsplit(datasets.percent[75:])\n)  # Error\nsplit = datasets.Split.TEST + datasets.Split.ALL  # Error\n```\n\n\nThe slices can be applied only one time. So the following are valid:\n\n```\nsplit = (\n        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +\n        datasets.Split.TEST.subsplit(datasets.percent[:50])\n)\nsplit = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:50])\n```\n\n\nBut this is not valid:\n\n```\ntrain = datasets.Split.TRAIN\ntest = datasets.Split.TEST\nsplit = train.subsplit(datasets.percent[:25]).subsplit(datasets.percent[:25])\nsplit = (train.subsplit(datasets.percent[:25]) + test).subsplit(datasets.percent[:50])\n```\n\n\n### class datasets.NamedSplitAll\n\nSplit corresponding to the union of all defined dataset splits.\n\n\n### class datasets.ReadInstruction\n\n<\nsource\n>\n(\nsplit\\_name\nrounding = None\nfrom\\_ = None\nto = None\nunit = None\n\nReading instruction for a dataset.\n\n```\n\n# The following lines are equivalent:\nds = datasets.load_dataset('mnist', split='test[:33%]')\nds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec('test[:33%]'))\nds = datasets.load_dataset('mnist', split=datasets.ReadInstruction('test', to=33, unit='%'))\nds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(\n'test', from_=0, to=33, unit='%'))\n\n\n# The following lines are equivalent:\nds = datasets.load_dataset('mnist', split='test[:33%]+train[1:-1]')\nds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(\n'test[:33%]+train[1:-1]'))\nds = datasets.load_dataset('mnist', split=(\ndatasets.ReadInstruction('test', to=33, unit='%') +\ndatasets.ReadInstruction('train', from_=1, to=-1, unit='abs')))\n\n\n# The following lines are equivalent:\nds = datasets.load_dataset('mnist', split='test[:33%](pct1\\_dropremainder)')\nds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(\n'test[:33%](pct1\\_dropremainder)'))\nds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(\n'test', from_=0, to=33, unit='%', rounding=\"pct1\\_dropremainder\"))\n\n\n# 10-fold validation:\ntests = datasets.load_dataset(\n'mnist',\n[datasets.ReadInstruction('train', from_=k, to=k+10, unit='%')\nfor k in range(0, 100, 10)])\ntrains = datasets.load_dataset(\n'mnist',\n[datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')\nfor k in range(0, 100, 10)])\n```\n\n\n\n#### from\\_spec\n\n<\nsource\n>\n(\nspec\n\n\n* **spec** (`str`) —\nSplit(s) + optional slice(s) to read + optional rounding\nif percents are used as the slicing unit. A slice can be specified,\nusing absolute numbers (`int`) or percentages (`int`).\n\nCreates a `ReadInstruction` instance out of a string spec.\n\n```\ntest: test split.\ntest + validation: test split + validation split.\ntest[10:]: test split, minus its first 10 records.\ntest[:10%]: first 10% records of test split.\ntest[:20%](pct1_dropremainder): first 10% records, rounded with the pct1_dropremainder rounding.\ntest[:-5%]+train[40%:60%]: first 95% of test + middle 20% of train.\n```\n\n\n\n#### to\\_absolute\n\n<\nsource\n>\n(\nname2len\n\n\n* **name2len** (`dict`) —\nAssociating split names to number of examples.\n\nTranslate instruction into a list of absolute instructions.\n\n\nThose absolute instructions are then to be added together.\n\n\n\n## \nVersion\n\n\n### class datasets.Version\n\n<\nsource\n>\n(\nversion\\_str: str\ndescription: typing.Optional[str] = None\nmajor: typing.Union[str, int, NoneType] = None\nminor: typing.Union[str, int, NoneType] = None\npatch: typing.Union[str, int, NoneType] = None\n\n\n* **version\\_str** (`str`) —\nThe dataset version.\n* **description** (`str`) —\nA description of what is new in this version.\n* **major** (`str`) —\n* **minor** (`str`) —\n* **patch** (`str`) —\n\nDataset version `MAJOR.MINOR.PATCH`.\n\n```\n>>> VERSION = datasets.Version(\"1.0.0\")\n```\n\n\n# \nLoading methods\n\n\nMethods for listing and loading datasets and metrics:\n\n\n\n## \nDatasets\n\n\n#### datasets.list\\_datasets\n\n<\nsource\n>\n(\nwith\\_community\\_datasets = True\nwith\\_details = False\n\n\n* **with\\_community\\_datasets** (`bool`, *optional*, defaults to `True`) —\nInclude the community provided datasets.\n* **with\\_details** (`bool`, *optional*, defaults to `False`) —\nReturn the full details on the datasets instead of only the short name.\n\nList all the datasets scripts available on the Hugging Face Hub.\n\n```\n>>> from datasets import list_datasets\n>>> list_datasets()\n['acronym\\_identification',\n 'ade\\_corpus\\_v2',\n 'adversarial\\_qa',\n 'aeslc',\n 'afrikaans\\_ner\\_corpus',\n 'ag\\_news',\n ...\n]\n```\n\n\n#### datasets.load\\_dataset\n\n<\nsource\n>\n(\npath: str\nname: typing.Optional[str] = None\ndata\\_dir: typing.Optional[str] = None\ndata\\_files: typing.Union[str, typing.Sequence[str], typing.Mapping[str, typing.Union[str, typing.Sequence[str]]], NoneType] = None\nsplit: typing.Union[str, datasets.splits.Split, NoneType] = None\ncache\\_dir: typing.Optional[str] = None\nfeatures: typing.Optional[datasets.features.features.Features] = None\ndownload\\_config: typing.Optional[datasets.download.download\\_config.DownloadConfig] = None\ndownload\\_mode: typing.Union[datasets.download.download\\_manager.DownloadMode, str, NoneType] = None\nverification\\_mode: typing.Union[datasets.utils.info\\_utils.VerificationMode, str, NoneType] =\n\n==================\n Document 3 \n----------------\nLogging methods\n\n\n🤗 Datasets strives to be transparent and explicit about how it works, but this can be quite verbose at times. We have included a series of logging methods which allow you to easily adjust the level of verbosity of the entire library. Currently the default verbosity of the library is set to `WARNING`.\n\n\nTo change the level of verbosity, use one of the direct setters. For instance, here is how to change the verbosity to the `INFO` level:\n\n```\nimport datasets\ndatasets.logging.set_verbosity_info()\n```\n\nYou can also use the environment variable `DATASETS_VERBOSITY` to override the default verbosity, and set it to one of the following: `debug`, `info`, `warning`, `error`, `critical`:\n\n```\nDATASETS_VERBOSITY=error ./myprogram.py\n```\n\nAll the methods of this logging module are documented below. The main ones are:\n\n\n* logging.get\\_verbosity() to get the current level of verbosity in the logger\n* logging.set\\_verbosity() to set the verbosity to the level of your choice\n\n\nIn order from the least to the most verbose (with their corresponding `int` values):\n\n\n1. `logging.CRITICAL` or `logging.FATAL` (int value, 50): only report the most critical errors.\n2. `logging.ERROR` (int value, 40): only report errors.\n3. `logging.WARNING` or `logging.WARN` (int value, 30): only reports error and warnings. This the default level used by the library.\n4. `logging.INFO` (int value, 20): reports error, warnings and basic information.\n5. `logging.DEBUG` (int value, 10): report all information.\n\n\nBy default, `tqdm` progress bars will be displayed during dataset download and preprocessing. logging.disable\\_progress\\_bar() and logging.enable\\_progress\\_bar() can be used to suppress or unsuppress this behavior. \n\n\n## \nFunctions\n\n\n#### datasets.utils.logging.get\\_verbosity\n\nReturn the current level for the HuggingFace datasets library’s root logger.\n\n\nHuggingFace datasets library has following logging levels:\n\n\n* `datasets.logging.CRITICAL`, `datasets.logging.FATAL`\n* `datasets.logging.ERROR`\n* `datasets.logging.WARNING`, `datasets.logging.WARN`\n* `datasets.logging.INFO`\n* `datasets.logging.DEBUG`\n\n\n\n#### datasets.utils.logging.set\\_verbosity\n\n<\nsource\n>\n(\nverbosity: int\n\nSet the level for the Hugging Face Datasets library’s root logger.\n\n\n#### datasets.utils.logging.set\\_verbosity\\_info\n\nSet the level for the Hugging Face datasets library’s root logger to `INFO`.\n\n\nThis will display most of the logging information and tqdm bars.\n\n\nShortcut to `datasets.logging.set_verbosity(datasets.logging.INFO)`.\n\n\n#### datasets.utils.logging.set\\_verbosity\\_warning\n\nSet the level for the Hugging Face datasets library’s root logger to `WARNING`.\n\n\nThis will display only the warning and errors logging information and tqdm bars.\n\n\nShortcut to `datasets.logging.set_verbosity(datasets.logging.WARNING)`.\n\n\n#### datasets.utils.logging.set\\_verbosity\\_debug\n\nSet the level for the Hugging Face datasets library’s root logger to `DEBUG`.\n\n\nThis will display all the logging information and tqdm bars.\n\n\nShortcut to `datasets.logging.set_verbosity(datasets.logging.DEBUG)`.\n\n\n#### datasets.utils.logging.set\\_verbosity\\_error\n\nSet the level for the Hugging Face datasets library’s root logger to `ERROR`.\n\n\nThis will display only the errors logging information and tqdm bars.\n\n\nShortcut to `datasets.logging.set_verbosity(datasets.logging.ERROR)`.\n\n\n#### datasets.utils.logging.disable\\_propagation\n\nDisable propagation of the library log outputs.\nNote that log propagation is disabled by default.\n\n\n#### datasets.utils.logging.enable\\_propagation\n\nEnable propagation of the library log outputs.\nPlease disable the Hugging Face datasets library’s default handler to prevent double logging if the root logger has\nbeen configured.\n\n\n#### datasets.utils.logging.get\\_logger\n\n<\nsource\n>\n(\nname: typing.Optional[str] = None\n\nReturn a logger with the specified name.\nThis function can be used in dataset scripts.\n\n\n#### datasets.enable\\_progress\\_bar\n\nEnable tqdm progress bar.\n\n\n#### datasets.disable\\_progress\\_bar\n\nDisable tqdm progress bar.\n\n\n#### datasets.is\\_progress\\_bar\\_enabled\n\nReturn a boolean indicating whether tqdm progress bars are enabled.\n\n\n\n## \nLevels\n\n\n\n### \ndatasets.logging.CRITICAL\n\n\ndatasets.logging.CRITICAL = 50\n\n\n\n### \ndatasets.logging.DEBUG\n\n\ndatasets.logging.DEBUG = 10\n\n\n\n### \ndatasets.logging.ERROR\n\n\ndatasets.logging.ERROR = 40\n\n\n\n### \ndatasets.logging.FATAL\n\n\ndatasets.logging.FATAL = 50\n\n\n\n### \ndatasets.logging.INFO\n\n\ndatasets.logging.INFO = 20\n\n\n\n### \ndatasets.logging.NOTSET\n\n\ndatasets.logging.NOTSET = 0\n\n\n\n### \ndatasets.logging.WARN\n\n\ndatasets.logging.WARN = 30\n\n\n\n### \ndatasets.logging.WARNING\n\n\ndatasets.logging.WARNING = 30\n\n\n\n# \nTask templates\n\n\nThe Task API is deprecated in favor of `train-eval-index` and will be removed in the next major release.\n\n\nThe tasks supported by Dataset.prepare\\_for\\_task() and DatasetDict.prepare\\_for\\_task().\n\n\n### class datasets.AutomaticSpeechRecognition\n\n<\nsource\n>\n(\ntask: str = 'automatic-speech-recognition'\naudio\\_column: str = 'audio'\ntranscription\\_column: str = 'transcription'\n\n\n\n### class datasets.AudioClassification\n\n<\nsource\n>\n(\ntask: str = 'audio-classification'\naudio\\_column: str = 'audio'\nlabel\\_column: str = 'labels'\n\n\n\n### class datasets.ImageClassification\n\n<\nsource\n>\n(\ntask: str = 'image-classification'\nimage\\_column: str = 'image'\nlabel\\_column: str = 'labels'\n\n\n\n#### align\\_with\\_features\n\n<\nsource\n>\n(\nfeatures\n\n\n\n### class datasets.LanguageModeling\n\n<\nsource\n>\n(\ntask: str = 'language-modeling'\ntext\\_column: str = 'text'\n\n\n\n### class datasets.QuestionAnsweringExtractive\n\n<\nsource\n>\n(\ntask: str = 'question-answering-extractive'\nquestion\\_column: str = 'question'\ncontext\\_column: str = 'context'\nanswers\\_column: str = 'answers'\n\n\n\n### class datasets.Summarization\n\n<\nsource\n>\n(\ntask: str = 'summarization'\ntext\\_column: str = 'text'\nsummary\\_column: str = 'summary'\n\n\n\n### class datasets.TextClassification\n\n<\nsource\n>\n(\ntask: str = 'text-classification'\ntext\\_column: str = 'text'\nlabel\\_column: str = 'labels'\n\n==================\n Document 4 \n----------------\n\n\n \n\n# \nMain classes\n\n\n\n## \nDatasetInfo\n\n### class datasets.DatasetInfo\n\n<\nsource\n>\n(\ndescription: str = <factory>\ncitation: str = <factory>\nhomepage: str = <factory>\nlicense: str = <factory>\nfeatures: typing.Optional[datasets.features.features.Features] = None\npost\\_processed: typing.Optional[datasets.info.PostProcessedInfo] = None\nsupervised\\_keys: typing.Optional[datasets.info.SupervisedKeysData] = None\ntask\\_templates: typing.Optional[typing.List[datasets.tasks.base.TaskTemplate]] = None\nbuilder\\_name: typing.Optional[str] = None\ndataset\\_name: typing.Optional[str] = None\nconfig\\_name: typing.Optional[str] = None\nversion: typing.Union[str, datasets.utils.version.Version, NoneType] ="
    },
    {
        "instruction": "Develop a Python program using the 'datasets' API to load the SAMSum dataset and extract a random conversation. The program should load the SAMSum dataset, select a random conversation, and print its content.",
        "api": "datasets",
        "output": "#!pip install datasets\nfrom datasets import load_dataset\nimport random\n\n# Load SAMSum dataset\ndataset = load_dataset(\"samsum\")\n\n# Select a random conversation\nrandom_conversation = random.choice(dataset[\"train\"][\"dialog\"])\n\nprint(\"Random Conversation:\")\nfor message in random_conversation:\n    print(message)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n### datasets.enable\\_caching\n\nWhen applying transforms on a dataset, the data are stored in cache files.\nThe caching mechanism allows to reload an existing cache file if it’s already been computed.\n\n\nReloading a dataset is possible since the cache files are named using the dataset fingerprint, which is updated\nafter each transform.\n\n\nIf disabled, the library will no longer reload cached datasets files when applying transforms to the datasets.\nMore precisely, if the caching is disabled:\n\n\n* cache files are always recreated\n* cache files are written to a temporary directory that is deleted when session closes\n* cache files are named using a random hash instead of the dataset fingerprint\n* use save\\_to\\_disk() to save a transformed dataset or it will be deleted when session closes\n* caching doesn’t affect load\\_dataset(). If you want to regenerate a dataset from scratch you should use\nthe `download_mode` parameter in load\\_dataset().\n\n\n#### datasets.disable\\_caching\n\n\n\n#### datasets.is\\_caching\\_enabled\n\n\n* cache files are always recreated\n* cache files are written to a temporary directory that is deleted when session closes\n* cache files are named using a random hash instead of the dataset fingerprint\n* use save\\_to\\_disk()] to save a transformed dataset or it will be deleted when session closes\n* caching doesn’t affect load\\_dataset(). If you want to regenerate a dataset from scratch you should use\nthe `download_mode` parameter in load\\_dataset().\n\n\n## \nDatasetDict\n\n\nDictionary with split names as keys (‘train’, ‘test’ for example), and `Dataset` objects as values.\nIt also has dataset transform methods like map or filter, to process all the splits at once.\n\n\n### class datasets.DatasetDict\n\nA dictionary (dict of str: datasets.Dataset) with dataset transforms methods (map, filter, etc.)\n\n#### data\n\nThe Apache Arrow tables backing each split.\n\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten\\_tomatoes\")\n>>> ds.data\n```\n\nThe cache files containing the Apache Arrow table backing each split.\n\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten\\_tomatoes\")\n>>> ds.cache_files\n{'test': [{'filename': '/root/.cache/huggingface/datasets/rotten\\_tomatoes\\_movie\\_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten\\_tomatoes\\_movie\\_review-test.arrow'}],\n 'train': [{'filename': '/root/.cache/huggingface/datasets/rotten\\_tomatoes\\_movie\\_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten\\_tomatoes\\_movie\\_review-train.arrow'}],\n 'validation':\n\n==================\n Document 1 \n----------------\n### datasets.load\\_dataset\n\n<\nsource\n>\n(\npath: str\nname: typing.Optional[str] = None\ndata\\_dir: typing.Optional[str] = None\ndata\\_files: typing.Union[str, typing.Sequence[str], typing.Mapping[str, typing.Union[str, typing.Sequence[str]]], NoneType] = None\nsplit: typing.Union[str, datasets.splits.Split, NoneType] = None\ncache\\_dir: typing.Optional[str] = None\nfeatures: typing.Optional[datasets.features.features.Features] = None\ndownload\\_config: typing.Optional[datasets.download.download\\_config.DownloadConfig] = None\ndownload\\_mode: typing.Union[datasets.download.download\\_manager.DownloadMode, str, NoneType] = None\nverification\\_mode: typing.Union[datasets.utils.info\\_utils.VerificationMode, str, NoneType] = None\nignore\\_verifications = 'deprecated'\nkeep\\_in\\_memory: typing.Optional[bool] = None\nsave\\_infos: bool = False\nrevision: typing.Union[str, datasets.utils.version.Version, NoneType] = None\ntoken: typing.Union[bool, str, NoneType] = None\nuse\\_auth\\_token = 'deprecated'\ntask = 'deprecated'\nstreaming: bool = False\nnum\\_proc: typing.Optional[int] = None\nstorage\\_options: typing.Optional[typing.Dict] = None\n\\*\\*config\\_kwargs\n\n\n* **path** (`str`) —\nPath or name of the dataset.\nDepending on `path`, the dataset builder that is used comes from a generic dataset script (JSON, CSV, Parquet, text etc.) or from the dataset script (a python file) inside the dataset directory.\nFor local datasets:\n\n\t+ if `path` is a local directory (containing data files only)\n\t-> load a generic dataset builder (csv, json, text etc.) based on the content of the directory\n\te.g. `'./path/to/directory/with/my/csv/data'`.\n\t+ if `path` is a local dataset script or a directory containing a local dataset script (if the script has the same name as the directory)\n\t-> load the dataset builder from the dataset script\n\te.g. `'./dataset/squad'` or `'./dataset/squad/squad.py'`.\nFor datasets on the Hugging Face Hub (list all available datasets with `huggingface_hub.list_datasets`)\n\n\t+ if `path` is a dataset repository on the HF hub (containing data files only)\n\t-> load a generic dataset builder (csv, text etc.) based on the content of the repository\n\te.g. `'username/dataset_name'`, a dataset repository on the HF hub containing your data files.\n\t+ if `path` is a dataset repository on the HF hub with a dataset script (if the script has the same name as the directory)\n\t-> load the dataset builder from the dataset script in the dataset repository\n\te.g. `glue`, `squad`, `'username/dataset_name'`, a dataset repository on the HF hub containing a dataset script `'dataset_name.py'`.\n* **name** (`str`, *optional*) —\nDefining the name of the dataset configuration.\n* **data\\_dir** (`str`, *optional*) —\nDefining the `data_dir` of the dataset configuration. If specified for the generic builders (csv, text etc.) or the Hub datasets and `data_files` is `None`,\nthe behavior is equal to passing `os.path.join(data_dir, **)` as `data_files` to reference all the files in a directory.\n* **data\\_files** (`str` or `Sequence` or `Mapping`, *optional*) —\nPath(s) to source data file(s).\n* **split** (`Split` or `str`) —\nWhich split of the data to load.\nIf `None`, will return a `dict` with all splits (typically `datasets.Split.TRAIN` and `datasets.Split.TEST`).\nIf given, will return a single Dataset.\nSplits can be combined and specified like in tensorflow-datasets.\n* **cache\\_dir** (`str`, *optional*) —\nDirectory to read/write data. Defaults to `\"~/.cache/huggingface/datasets\"`.\n* **features** (`Features`, *optional*) —\nSet the features type to use for this dataset.\n* **download\\_config** (DownloadConfig, *optional*) —\nSpecific download configuration parameters.\n* **download\\_mode** (DownloadMode or `str`, defaults to `REUSE_DATASET_IF_EXISTS`) —\nDownload/generate mode.\n* **verification\\_mode** (VerificationMode or `str`, defaults to `BASIC_CHECKS`) —\nVerification mode determining the checks to run on the downloaded/processed dataset information (checksums/size/splits/…).\n\n\n`ignore_verifications` was deprecated in version 2.9.1 and will be removed in 3.0.0.\nPlease use `verification_mode` instead.\n* **keep\\_in\\_memory** (`bool`, defaults to `None`) —\nWhether to copy the dataset in-memory. If `None`, the dataset\nwill not be copied in-memory unless explicitly enabled by setting `datasets.config.IN_MEMORY_MAX_SIZE` to\nnonzero. See more details in the improve performance section.\n* **save\\_infos** (`bool`, defaults to `False`) —\nSave the dataset information (checksums/size/splits/…).\n* **revision** (Version or `str`, *optional*) —\nVersion of the dataset script to load.\nAs datasets have their own git repository on the Datasets Hub, the default version “main” corresponds to their “main” branch.\nYou can specify a different version than the default “main” by using a commit SHA or a git tag of the dataset repository.\n* **token** (`str` or `bool`, *optional*) —\nOptional string or boolean to use as Bearer token for remote files on the Datasets Hub.\nIf `True`, or not specified, will get token from `\"~/.huggingface\"`.\n* **use\\_auth\\_token** (`str` or `bool`, *optional*) —\nOptional string or boolean to use as Bearer token for remote files on the Datasets Hub.\nIf `True`, or not specified, will get token from `\"~/.huggingface\"`.\n\n\n`use_auth_token` was deprecated in favor of `token` in version 2.14.0 and will be removed in 3.0.0.\n* **task** (`str`) —\nThe task to prepare the dataset for during training and evaluation. Casts the dataset’s Features to standardized column names and types as detailed in `datasets.tasks`.\n\nDeprecated in 2.13.0\n\n\n`task` was deprecated in version 2.13.0 and will be removed in 3.0.0.\n* **streaming** (`bool`, defaults to `False`) —\nIf set to `True`, don’t download the data files. Instead, it streams the data progressively while\niterating on the dataset. An IterableDataset or IterableDatasetDict is returned instead in this case.\nNote that streaming works for datasets that use data formats that support being iterated over like txt, csv, jsonl for example.\nJson files may be downloaded completely. Also streaming from remote zip or gzip files is supported but other compressed formats\nlike rar and xz are not yet supported. The tgz format doesn’t allow streaming.\n* **num\\_proc** (`int`, *optional*, defaults to `None`) —\nNumber of processes when downloading and generating the dataset locally.\nMultiprocessing is disabled by default.\n\nAdded in 2.7.0\n* **storage\\_options** (`dict`, *optional*, defaults to `None`) —\n**Experimental**. Key/value pairs to be passed on to the dataset file-system backend, if any.\n\nAdded in 2.11.0\n* \\***\\*config\\_kwargs** (additional keyword arguments) —\nKeyword arguments to be passed to the `BuilderConfig`\nand used in the DatasetBuilder.\n\n* if `split` is not `None`: the dataset requested,\n* if `split` is `None`, a DatasetDict with each split.\n\n\nor IterableDataset or IterableDatasetDict: if `streaming=True`\n\n\n* if `split` is not `None`, the dataset is requested\n* if `split` is `None`, a `~datasets.streaming.IterableDatasetDict` with each split.\n\n\nLoad a dataset from the Hugging Face Hub, or a local dataset.\n\n\nYou can find the list of datasets on the Hub or with `huggingface_hub.list_datasets`.\n\n\nA dataset is a directory that contains:\n\n\n* some data files in generic formats (JSON, CSV, Parquet, text, etc.).\n* and optionally a dataset script, if it requires some code to read the data files. This is used to load any kind of formats or structures.\n\n\nNote that dataset scripts can also download and read data files from anywhere - in case your data files already exist online.\n\n\nThis function does the following under the hood:\n\n\n1. Download and import in the library the dataset script from `path` if it’s not already cached inside the library.\n\n\nIf the dataset has no dataset script, then a generic dataset script is imported instead (JSON, CSV, Parquet, text, etc.)\n\n\nDataset scripts are small python scripts that define dataset builders. They define the citation, info and format of the dataset,\ncontain the path or URL to the original data files and the code to load examples from the original data files.\n\n\nYou can find the complete list of datasets in the Datasets Hub.\n2. Run the dataset script which will:\n\n\n\t* Download the dataset file from the original URL (see the script) if it’s not already available locally or cached.\n\t* Process and cache the dataset in typed Arrow tables for caching.\n\t\n\t\n\tArrow table are arbitrarily long, typed tables which can store nested objects and be mapped to numpy/pandas/python generic types.\n\tThey can be directly accessed from disk, loaded in RAM or even streamed over the web.\n3. Return a dataset built from the requested splits in `split` (default: all).\n\n\nIt also allows to load a dataset from a local directory or a dataset repository on the Hugging Face Hub without dataset script.\nIn this case, it automatically loads all the data files from the directory or the dataset repository.\n\nLoad a dataset from the Hugging Face Hub:\n\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset('rotten\\_tomatoes', split='train')\n\n# Map data files to splits\n>>> data_files = {'train': 'train.csv', 'test': 'test.csv'}\n>>> ds = load_dataset('namespace/your\\_dataset\\_name', data_files=data_files)\n```\n\n\nLoad a local dataset:\n\n```\n\n# Load a CSV file\n>>> from datasets import load_dataset\n>>> ds = load_dataset('csv', data_files='path/to/local/my\\_dataset.csv')\n\n\n# Load a JSON file\n>>> from datasets import load_dataset\n>>> ds = load_dataset('json', data_files='path/to/local/my\\_dataset.json')\n\n\n# Load from a local loading script\n>>> from datasets import load_dataset\n>>> ds = load_dataset('path/to/local/loading\\_script/loading\\_script.py', split='train')\n```\n\n\nLoad an IterableDataset:\n\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset('rotten\\_tomatoes', split='train', streaming=True)\n```\n\n\nLoad an image dataset with the `ImageFolder` dataset builder:\n\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset('imagefolder', data_dir='/path/to/images', split='train')\n```\n\n\n#### datasets.load\\_from\\_disk\n\n\n* **dataset\\_path** (`str`) —\nPath (e.g. `\"dataset/train\"`) or remote URI (e.g.\n`\"s3://my-bucket/dataset/train\"`) of the Dataset or DatasetDict directory where the dataset will be\nloaded from.\n* **fs** (`~filesystems.S3FileSystem` or `fsspec.spec.AbstractFileSystem`, *optional*) —\nInstance of the remote filesystem used to download the files from.\n\n\n`fs` was\n\n==================\n Document 2 \n----------------\n### to\\_sql\n\n<\nsource\n>\n(\nname: str\ncon: typing.Union[str, ForwardRef('sqlalchemy.engine.Connection'), ForwardRef('sqlalchemy.engine.Engine'), ForwardRef('sqlite3.Connection')]\nbatch\\_size: typing.Optional[int] = None\n\\*\\*sql\\_writer\\_kwargs\n\n\n* **name** (`str`) —\nName of SQL table.\n* **con** (`str` or `sqlite3.Connection` or `sqlalchemy.engine.Connection` or `sqlalchemy.engine.Connection`) —\nA URI string or a SQLite3/SQLAlchemy connection object used to write to a database.\n* **batch\\_size** (`int`, *optional*) —\nSize of the batch to load in memory and write at once.\nDefaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n* \\***\\*sql\\_writer\\_kwargs** (additional keyword arguments) —\nParameters to pass to pandas’s `pandas.DataFrame.to_sql`.\n\nThe number of records written.\n\n\nExports the dataset to a SQL database.\n\n```\n>>> # con provided as a connection URI string\n>>> ds.to_sql(\"data\", \"sqlite:///my\\_own\\_db.sql\")\n>>> # con provided as a sqlite3 connection object\n>>> import sqlite3\n>>> con = sqlite3.connect(\"my\\_own\\_db.sql\")\n>>> with con:\n...     ds.to_sql(\"data\", con)\n```\n\n#### to\\_iterable\\_dataset\n\n<\nsource\n>\n(\nnum\\_shards: typing.Optional[int] = 1\n\n\n* **num\\_shards** (`int`, default to `1`) —\nNumber of shards to define when instantiating the iterable dataset. This is especially useful for big datasets to be able to shuffle properly,\nand also to enable fast parallel loading using\n\n==================\n Document 3 \n----------------\nLogging methods\n\n\n🤗 Datasets strives to be transparent and explicit about how it works, but this can be quite verbose at times. We have included a series of logging methods which allow you to easily adjust the level of verbosity of the entire library. Currently the default verbosity of the library is set to `WARNING`.\n\n\nTo change the level of verbosity, use one of the direct setters. For instance, here is how to change the verbosity to the `INFO` level:\n\n```\nimport datasets\ndatasets.logging.set_verbosity_info()\n```\n\nYou can also use the environment variable `DATASETS_VERBOSITY` to override the default verbosity, and set it to one of the following: `debug`, `info`, `warning`, `error`, `critical`:\n\n```\nDATASETS_VERBOSITY=error ./myprogram.py\n```\n\nAll the methods of this logging module are documented below. The main ones are:\n\n\n* logging.get\\_verbosity() to get the current level of verbosity in the logger\n* logging.set\\_verbosity() to set the verbosity to the level of your choice\n\n\nIn order from the least to the most verbose (with their corresponding `int` values):\n\n\n1. `logging.CRITICAL` or `logging.FATAL` (int value, 50): only report the most critical errors.\n2. `logging.ERROR` (int value, 40): only report errors.\n3. `logging.WARNING` or `logging.WARN` (int value, 30): only reports error and warnings. This the default level used by the library.\n4. `logging.INFO` (int value, 20): reports error, warnings and basic information.\n5. `logging.DEBUG` (int value, 10): report all information.\n\n\nBy default, `tqdm` progress bars will be displayed during dataset download and preprocessing. logging.disable\\_progress\\_bar() and logging.enable\\_progress\\_bar() can be used to suppress or unsuppress this behavior. \n\n\n## \nFunctions\n\n\n#### datasets.utils.logging.get\\_verbosity\n\nReturn the current level for the HuggingFace datasets library’s root logger.\n\n\nHuggingFace datasets library has following logging levels:\n\n\n* `datasets.logging.CRITICAL`, `datasets.logging.FATAL`\n* `datasets.logging.ERROR`\n* `datasets.logging.WARNING`, `datasets.logging.WARN`\n* `datasets.logging.INFO`\n* `datasets.logging.DEBUG`\n\n\n\n#### datasets.utils.logging.set\\_verbosity\n\n<\nsource\n>\n(\nverbosity: int\n\nSet the level for the Hugging Face Datasets library’s root logger.\n\n\n#### datasets.utils.logging.set\\_verbosity\\_info\n\nSet the level for the Hugging Face datasets library’s root logger to `INFO`.\n\n\nThis will display most of the logging information and tqdm bars.\n\n\nShortcut to `datasets.logging.set_verbosity(datasets.logging.INFO)`.\n\n\n#### datasets.utils.logging.set\\_verbosity\\_warning\n\nSet the level for the Hugging Face datasets library’s root logger to `WARNING`.\n\n\nThis will display only the warning and errors logging information and tqdm bars.\n\n\nShortcut to `datasets.logging.set_verbosity(datasets.logging.WARNING)`.\n\n\n#### datasets.utils.logging.set\\_verbosity\\_debug\n\nSet the level for the Hugging Face datasets library’s root logger to `DEBUG`.\n\n\nThis will display all the logging information and tqdm bars.\n\n\nShortcut to `datasets.logging.set_verbosity(datasets.logging.DEBUG)`.\n\n\n#### datasets.utils.logging.set\\_verbosity\\_error\n\nSet the level for the Hugging Face datasets library’s root logger to `ERROR`.\n\n\nThis will display only the errors logging information and tqdm bars.\n\n\nShortcut to `datasets.logging.set_verbosity(datasets.logging.ERROR)`.\n\n\n#### datasets.utils.logging.disable\\_propagation\n\nDisable propagation of the library log outputs.\nNote that log propagation is disabled by default.\n\n\n#### datasets.utils.logging.enable\\_propagation\n\nEnable propagation of the library log outputs.\nPlease disable the Hugging Face datasets library’s default handler to prevent double logging if the root logger has\nbeen configured.\n\n\n#### datasets.utils.logging.get\\_logger\n\n<\nsource\n>\n(\nname: typing.Optional[str] = None\n\nReturn a logger with the specified name.\nThis function can be used in dataset scripts.\n\n\n#### datasets.enable\\_progress\\_bar\n\nEnable tqdm progress bar.\n\n\n#### datasets.disable\\_progress\\_bar\n\nDisable tqdm progress bar.\n\n\n#### datasets.is\\_progress\\_bar\\_enabled\n\nReturn a boolean indicating whether tqdm progress bars are enabled.\n\n\n\n## \nLevels\n\n\n\n### \ndatasets.logging.CRITICAL\n\n\ndatasets.logging.CRITICAL = 50\n\n\n\n### \ndatasets.logging.DEBUG\n\n\ndatasets.logging.DEBUG = 10\n\n\n\n### \ndatasets.logging.ERROR\n\n\ndatasets.logging.ERROR = 40\n\n\n\n### \ndatasets.logging.FATAL\n\n\ndatasets.logging.FATAL = 50\n\n\n\n### \ndatasets.logging.INFO\n\n\ndatasets.logging.INFO = 20\n\n\n\n### \ndatasets.logging.NOTSET\n\n\ndatasets.logging.NOTSET = 0\n\n\n\n### \ndatasets.logging.WARN\n\n\ndatasets.logging.WARN = 30\n\n\n\n### \ndatasets.logging.WARNING\n\n\ndatasets.logging.WARNING = 30\n\n\n\n# \nTask templates\n\n\nThe Task API is deprecated in favor of `train-eval-index` and will be removed in the next major release.\n\n\nThe tasks supported by Dataset.prepare\\_for\\_task() and DatasetDict.prepare\\_for\\_task().\n\n\n### class datasets.AutomaticSpeechRecognition\n\n<\nsource\n>\n(\ntask: str = 'automatic-speech-recognition'\naudio\\_column: str = 'audio'\ntranscription\\_column: str = 'transcription'\n\n\n\n### class datasets.AudioClassification\n\n<\nsource\n>\n(\ntask: str = 'audio-classification'\naudio\\_column: str = 'audio'\nlabel\\_column: str = 'labels'\n\n\n\n### class datasets.ImageClassification\n\n<\nsource\n>\n(\ntask: str = 'image-classification'\nimage\\_column: str = 'image'\nlabel\\_column: str = 'labels'\n\n\n\n#### align\\_with\\_features\n\n<\nsource\n>\n(\nfeatures\n\n\n\n### class datasets.LanguageModeling\n\n<\nsource\n>\n(\ntask: str = 'language-modeling'\ntext\\_column: str = 'text'\n\n\n\n### class datasets.QuestionAnsweringExtractive\n\n<\nsource\n>\n(\ntask: str = 'question-answering-extractive'\nquestion\\_column: str = 'question'\ncontext\\_column: str = 'context'\nanswers\\_column: str = 'answers'\n\n\n\n### class datasets.Summarization\n\n<\nsource\n>\n(\ntask: str = 'summarization'\ntext\\_column: str = 'text'\nsummary\\_column: str = 'summary'\n\n\n\n### class datasets.TextClassification\n\n<\nsource\n>\n(\ntask: str = 'text-classification'\ntext\\_column: str = 'text'\nlabel\\_column: str = 'labels'\n\n==================\n Document 4 \n----------------\n## class datasets.NamedSplit\n\nDescriptor corresponding to a named split (train, test, …).\n\nEach descriptor can be composed with other using addition or slice:\n\n```\nsplit = datasets.Split.TRAIN.subsplit(datasets.percent[0:25]) + datasets.Split.TEST\n```\n\nThe resulting split will correspond to 25% of the train split merged with\n100% of the test split.\n\nA split cannot be added twice, so the following will fail:\n\n```\nsplit = (\n        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +\n        datasets.Split.TRAIN.subsplit(datasets.percent[75:])\n)  # Error\nsplit = datasets.Split.TEST + datasets.Split.ALL  # Error\n```\n\n\nThe slices can be applied only one time. So the following are valid:\n\n```\nsplit = (\n        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +\n        datasets.Split.TEST.subsplit(datasets.percent[:50])\n)\nsplit = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:50])\n```\n\n\nBut this is not valid:\n\n```\ntrain = datasets.Split.TRAIN\ntest = datasets.Split.TEST\nsplit = train.subsplit(datasets.percent[:25]).subsplit(datasets.percent[:25])\nsplit = (train.subsplit(datasets.percent[:25]) + test).subsplit(datasets.percent[:50])\n```\n\n\n### class datasets.NamedSplitAll\n\nSplit corresponding to the union of all defined dataset splits.\n\n\n### class datasets.ReadInstruction\n\n<\nsource\n>\n(\nsplit\\_name\nrounding = None\nfrom\\_ = None\nto = None\nunit = None\n\nReading instruction for a dataset.\n\n```\n\n# The following lines are equivalent:\nds = datasets.load_dataset('mnist', split='test[:33%]')\nds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec('test[:33%]'))\nds = datasets.load_dataset('mnist', split=datasets.ReadInstruction('test', to=33, unit='%'))\nds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(\n'test', from_=0, to=33, unit='%'))\n\n\n# The following lines are equivalent:\nds = datasets.load_dataset('mnist', split='test[:33%]+train[1:-1]')\nds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(\n'test[:33%]+train[1:-1]'))\nds = datasets.load_dataset('mnist', split=(\ndatasets.ReadInstruction('test', to=33, unit='%') +\ndatasets.ReadInstruction('train', from_=1, to=-1, unit='abs')))\n\n\n# The following lines are equivalent:\nds = datasets.load_dataset('mnist', split='test[:33%](pct1\\_dropremainder)')\nds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(\n'test[:33%](pct1\\_dropremainder)'))\nds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(\n'test', from_=0, to=33, unit='%', rounding=\"pct1\\_dropremainder\"))\n\n\n# 10-fold validation:\ntests = datasets.load_dataset(\n'mnist',\n[datasets.ReadInstruction('train', from_=k, to=k+10, unit='%')\nfor k in range(0, 100, 10)])\ntrains = datasets.load_dataset(\n'mnist',\n[datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')\nfor k in range(0, 100, 10)])\n```\n\n\n\n#### from\\_spec\n\n<\nsource\n>\n(\nspec\n\n\n* **spec** (`str`) —\nSplit(s) + optional slice(s) to read + optional rounding\nif percents are used as the slicing unit. A slice can be specified,\nusing absolute numbers (`int`) or percentages (`int`).\n\nCreates a `ReadInstruction` instance out of a string spec.\n\n```\ntest: test split.\ntest + validation: test split + validation split.\ntest[10:]: test split, minus its first 10 records.\ntest[:10%]: first 10% records of test split.\ntest[:20%](pct1_dropremainder): first 10% records, rounded with the pct1_dropremainder rounding.\ntest[:-5%]+train[40%:60%]: first 95% of test + middle 20% of train.\n```\n\n\n\n#### to\\_absolute\n\n<\nsource\n>\n(\nname2len\n\n\n* **name2len** (`dict`) —\nAssociating split names to number of examples.\n\nTranslate instruction into a list of absolute instructions.\n\n\nThose absolute instructions are then to be added together.\n\n\n\n## \nVersion\n\n\n### class datasets.Version\n\n<\nsource\n>\n(\nversion\\_str: str\ndescription: typing.Optional[str] = None\nmajor: typing.Union[str, int, NoneType] = None\nminor: typing.Union[str, int, NoneType] = None\npatch: typing.Union[str, int, NoneType] = None\n\n\n* **version\\_str** (`str`) —\nThe dataset version.\n* **description** (`str`) —\nA description of what is new in this version.\n* **major** (`str`) —\n* **minor** (`str`) —\n* **patch** (`str`) —\n\nDataset version `MAJOR.MINOR.PATCH`.\n\n```\n>>> VERSION = datasets.Version(\"1.0.0\")\n```\n\n\n# \nLoading methods\n\n\nMethods for listing and loading datasets and metrics:\n\n\n\n## \nDatasets\n\n\n#### datasets.list\\_datasets\n\n<\nsource\n>\n(\nwith\\_community\\_datasets = True\nwith\\_details = False\n\n\n* **with\\_community\\_datasets** (`bool`, *optional*, defaults to `True`) —\nInclude the community provided datasets.\n* **with\\_details** (`bool`, *optional*, defaults to `False`) —\nReturn the full details on the datasets instead of only the short name.\n\nList all the datasets scripts available on the Hugging Face Hub.\n\n```\n>>> from datasets import list_datasets\n>>> list_datasets()\n['acronym\\_identification',\n 'ade\\_corpus\\_v2',\n 'adversarial\\_qa',\n 'aeslc',\n 'afrikaans\\_ner\\_corpus',\n 'ag\\_news',\n ...\n]\n```\n\n\n#### datasets.load\\_dataset\n\n<\nsource\n>\n(\npath: str\nname: typing.Optional[str] = None\ndata\\_dir: typing.Optional[str] = None\ndata\\_files: typing.Union[str, typing.Sequence[str], typing.Mapping[str, typing.Union[str, typing.Sequence[str]]], NoneType] = None\nsplit: typing.Union[str, datasets.splits.Split, NoneType] = None\ncache\\_dir: typing.Optional[str] = None\nfeatures: typing.Optional[datasets.features.features.Features] = None\ndownload\\_config: typing.Optional[datasets.download.download\\_config.DownloadConfig] = None\ndownload\\_mode: typing.Union[datasets.download.download\\_manager.DownloadMode, str, NoneType] = None\nverification\\_mode: typing.Union[datasets.utils.info\\_utils.VerificationMode, str, NoneType] ="
    },
    {
        "instruction": "Develop a Python program using the 'datasets' API to load the WikiText-103 dataset and extract a random article. The program should load the WikiText-103 dataset, select a random article, and print its content.",
        "api": "datasets",
        "output": "#!pip install datasets\nfrom datasets import load_dataset\nimport random\n\n# Load WikiText-103 dataset\ndataset = load_dataset(\"wikitext\", \"103\")\n\n# Select a random article\nrandom_article = random.choice(dataset[\"train\"][\"text\"])\n\nprint(\"Random Article:\")\nprint(random_article)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n### datasets.enable\\_caching\n\nWhen applying transforms on a dataset, the data are stored in cache files.\nThe caching mechanism allows to reload an existing cache file if it’s already been computed.\n\n\nReloading a dataset is possible since the cache files are named using the dataset fingerprint, which is updated\nafter each transform.\n\n\nIf disabled, the library will no longer reload cached datasets files when applying transforms to the datasets.\nMore precisely, if the caching is disabled:\n\n\n* cache files are always recreated\n* cache files are written to a temporary directory that is deleted when session closes\n* cache files are named using a random hash instead of the dataset fingerprint\n* use save\\_to\\_disk() to save a transformed dataset or it will be deleted when session closes\n* caching doesn’t affect load\\_dataset(). If you want to regenerate a dataset from scratch you should use\nthe `download_mode` parameter in load\\_dataset().\n\n\n#### datasets.disable\\_caching\n\n\n\n#### datasets.is\\_caching\\_enabled\n\n\n* cache files are always recreated\n* cache files are written to a temporary directory that is deleted when session closes\n* cache files are named using a random hash instead of the dataset fingerprint\n* use save\\_to\\_disk()] to save a transformed dataset or it will be deleted when session closes\n* caching doesn’t affect load\\_dataset(). If you want to regenerate a dataset from scratch you should use\nthe `download_mode` parameter in load\\_dataset().\n\n\n## \nDatasetDict\n\n\nDictionary with split names as keys (‘train’, ‘test’ for example), and `Dataset` objects as values.\nIt also has dataset transform methods like map or filter, to process all the splits at once.\n\n\n### class datasets.DatasetDict\n\nA dictionary (dict of str: datasets.Dataset) with dataset transforms methods (map, filter, etc.)\n\n#### data\n\nThe Apache Arrow tables backing each split.\n\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten\\_tomatoes\")\n>>> ds.data\n```\n\nThe cache files containing the Apache Arrow table backing each split.\n\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten\\_tomatoes\")\n>>> ds.cache_files\n{'test': [{'filename': '/root/.cache/huggingface/datasets/rotten\\_tomatoes\\_movie\\_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten\\_tomatoes\\_movie\\_review-test.arrow'}],\n 'train': [{'filename': '/root/.cache/huggingface/datasets/rotten\\_tomatoes\\_movie\\_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten\\_tomatoes\\_movie\\_review-train.arrow'}],\n 'validation':\n\n==================\n Document 1 \n----------------\n### datasets.load\\_dataset\n\n<\nsource\n>\n(\npath: str\nname: typing.Optional[str] = None\ndata\\_dir: typing.Optional[str] = None\ndata\\_files: typing.Union[str, typing.Sequence[str], typing.Mapping[str, typing.Union[str, typing.Sequence[str]]], NoneType] = None\nsplit: typing.Union[str, datasets.splits.Split, NoneType] = None\ncache\\_dir: typing.Optional[str] = None\nfeatures: typing.Optional[datasets.features.features.Features] = None\ndownload\\_config: typing.Optional[datasets.download.download\\_config.DownloadConfig] = None\ndownload\\_mode: typing.Union[datasets.download.download\\_manager.DownloadMode, str, NoneType] = None\nverification\\_mode: typing.Union[datasets.utils.info\\_utils.VerificationMode, str, NoneType] = None\nignore\\_verifications = 'deprecated'\nkeep\\_in\\_memory: typing.Optional[bool] = None\nsave\\_infos: bool = False\nrevision: typing.Union[str, datasets.utils.version.Version, NoneType] = None\ntoken: typing.Union[bool, str, NoneType] = None\nuse\\_auth\\_token = 'deprecated'\ntask = 'deprecated'\nstreaming: bool = False\nnum\\_proc: typing.Optional[int] = None\nstorage\\_options: typing.Optional[typing.Dict] = None\n\\*\\*config\\_kwargs\n\n\n* **path** (`str`) —\nPath or name of the dataset.\nDepending on `path`, the dataset builder that is used comes from a generic dataset script (JSON, CSV, Parquet, text etc.) or from the dataset script (a python file) inside the dataset directory.\nFor local datasets:\n\n\t+ if `path` is a local directory (containing data files only)\n\t-> load a generic dataset builder (csv, json, text etc.) based on the content of the directory\n\te.g. `'./path/to/directory/with/my/csv/data'`.\n\t+ if `path` is a local dataset script or a directory containing a local dataset script (if the script has the same name as the directory)\n\t-> load the dataset builder from the dataset script\n\te.g. `'./dataset/squad'` or `'./dataset/squad/squad.py'`.\nFor datasets on the Hugging Face Hub (list all available datasets with `huggingface_hub.list_datasets`)\n\n\t+ if `path` is a dataset repository on the HF hub (containing data files only)\n\t-> load a generic dataset builder (csv, text etc.) based on the content of the repository\n\te.g. `'username/dataset_name'`, a dataset repository on the HF hub containing your data files.\n\t+ if `path` is a dataset repository on the HF hub with a dataset script (if the script has the same name as the directory)\n\t-> load the dataset builder from the dataset script in the dataset repository\n\te.g. `glue`, `squad`, `'username/dataset_name'`, a dataset repository on the HF hub containing a dataset script `'dataset_name.py'`.\n* **name** (`str`, *optional*) —\nDefining the name of the dataset configuration.\n* **data\\_dir** (`str`, *optional*) —\nDefining the `data_dir` of the dataset configuration. If specified for the generic builders (csv, text etc.) or the Hub datasets and `data_files` is `None`,\nthe behavior is equal to passing `os.path.join(data_dir, **)` as `data_files` to reference all the files in a directory.\n* **data\\_files** (`str` or `Sequence` or `Mapping`, *optional*) —\nPath(s) to source data file(s).\n* **split** (`Split` or `str`) —\nWhich split of the data to load.\nIf `None`, will return a `dict` with all splits (typically `datasets.Split.TRAIN` and `datasets.Split.TEST`).\nIf given, will return a single Dataset.\nSplits can be combined and specified like in tensorflow-datasets.\n* **cache\\_dir** (`str`, *optional*) —\nDirectory to read/write data. Defaults to `\"~/.cache/huggingface/datasets\"`.\n* **features** (`Features`, *optional*) —\nSet the features type to use for this dataset.\n* **download\\_config** (DownloadConfig, *optional*) —\nSpecific download configuration parameters.\n* **download\\_mode** (DownloadMode or `str`, defaults to `REUSE_DATASET_IF_EXISTS`) —\nDownload/generate mode.\n* **verification\\_mode** (VerificationMode or `str`, defaults to `BASIC_CHECKS`) —\nVerification mode determining the checks to run on the downloaded/processed dataset information (checksums/size/splits/…).\n\n\n`ignore_verifications` was deprecated in version 2.9.1 and will be removed in 3.0.0.\nPlease use `verification_mode` instead.\n* **keep\\_in\\_memory** (`bool`, defaults to `None`) —\nWhether to copy the dataset in-memory. If `None`, the dataset\nwill not be copied in-memory unless explicitly enabled by setting `datasets.config.IN_MEMORY_MAX_SIZE` to\nnonzero. See more details in the improve performance section.\n* **save\\_infos** (`bool`, defaults to `False`) —\nSave the dataset information (checksums/size/splits/…).\n* **revision** (Version or `str`, *optional*) —\nVersion of the dataset script to load.\nAs datasets have their own git repository on the Datasets Hub, the default version “main” corresponds to their “main” branch.\nYou can specify a different version than the default “main” by using a commit SHA or a git tag of the dataset repository.\n* **token** (`str` or `bool`, *optional*) —\nOptional string or boolean to use as Bearer token for remote files on the Datasets Hub.\nIf `True`, or not specified, will get token from `\"~/.huggingface\"`.\n* **use\\_auth\\_token** (`str` or `bool`, *optional*) —\nOptional string or boolean to use as Bearer token for remote files on the Datasets Hub.\nIf `True`, or not specified, will get token from `\"~/.huggingface\"`.\n\n\n`use_auth_token` was deprecated in favor of `token` in version 2.14.0 and will be removed in 3.0.0.\n* **task** (`str`) —\nThe task to prepare the dataset for during training and evaluation. Casts the dataset’s Features to standardized column names and types as detailed in `datasets.tasks`.\n\nDeprecated in 2.13.0\n\n\n`task` was deprecated in version 2.13.0 and will be removed in 3.0.0.\n* **streaming** (`bool`, defaults to `False`) —\nIf set to `True`, don’t download the data files. Instead, it streams the data progressively while\niterating on the dataset. An IterableDataset or IterableDatasetDict is returned instead in this case.\nNote that streaming works for datasets that use data formats that support being iterated over like txt, csv, jsonl for example.\nJson files may be downloaded completely. Also streaming from remote zip or gzip files is supported but other compressed formats\nlike rar and xz are not yet supported. The tgz format doesn’t allow streaming.\n* **num\\_proc** (`int`, *optional*, defaults to `None`) —\nNumber of processes when downloading and generating the dataset locally.\nMultiprocessing is disabled by default.\n\nAdded in 2.7.0\n* **storage\\_options** (`dict`, *optional*, defaults to `None`) —\n**Experimental**. Key/value pairs to be passed on to the dataset file-system backend, if any.\n\nAdded in 2.11.0\n* \\***\\*config\\_kwargs** (additional keyword arguments) —\nKeyword arguments to be passed to the `BuilderConfig`\nand used in the DatasetBuilder.\n\n* if `split` is not `None`: the dataset requested,\n* if `split` is `None`, a DatasetDict with each split.\n\n\nor IterableDataset or IterableDatasetDict: if `streaming=True`\n\n\n* if `split` is not `None`, the dataset is requested\n* if `split` is `None`, a `~datasets.streaming.IterableDatasetDict` with each split.\n\n\nLoad a dataset from the Hugging Face Hub, or a local dataset.\n\n\nYou can find the list of datasets on the Hub or with `huggingface_hub.list_datasets`.\n\n\nA dataset is a directory that contains:\n\n\n* some data files in generic formats (JSON, CSV, Parquet, text, etc.).\n* and optionally a dataset script, if it requires some code to read the data files. This is used to load any kind of formats or structures.\n\n\nNote that dataset scripts can also download and read data files from anywhere - in case your data files already exist online.\n\n\nThis function does the following under the hood:\n\n\n1. Download and import in the library the dataset script from `path` if it’s not already cached inside the library.\n\n\nIf the dataset has no dataset script, then a generic dataset script is imported instead (JSON, CSV, Parquet, text, etc.)\n\n\nDataset scripts are small python scripts that define dataset builders. They define the citation, info and format of the dataset,\ncontain the path or URL to the original data files and the code to load examples from the original data files.\n\n\nYou can find the complete list of datasets in the Datasets Hub.\n2. Run the dataset script which will:\n\n\n\t* Download the dataset file from the original URL (see the script) if it’s not already available locally or cached.\n\t* Process and cache the dataset in typed Arrow tables for caching.\n\t\n\t\n\tArrow table are arbitrarily long, typed tables which can store nested objects and be mapped to numpy/pandas/python generic types.\n\tThey can be directly accessed from disk, loaded in RAM or even streamed over the web.\n3. Return a dataset built from the requested splits in `split` (default: all).\n\n\nIt also allows to load a dataset from a local directory or a dataset repository on the Hugging Face Hub without dataset script.\nIn this case, it automatically loads all the data files from the directory or the dataset repository.\n\nLoad a dataset from the Hugging Face Hub:\n\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset('rotten\\_tomatoes', split='train')\n\n# Map data files to splits\n>>> data_files = {'train': 'train.csv', 'test': 'test.csv'}\n>>> ds = load_dataset('namespace/your\\_dataset\\_name', data_files=data_files)\n```\n\n\nLoad a local dataset:\n\n```\n\n# Load a CSV file\n>>> from datasets import load_dataset\n>>> ds = load_dataset('csv', data_files='path/to/local/my\\_dataset.csv')\n\n\n# Load a JSON file\n>>> from datasets import load_dataset\n>>> ds = load_dataset('json', data_files='path/to/local/my\\_dataset.json')\n\n\n# Load from a local loading script\n>>> from datasets import load_dataset\n>>> ds = load_dataset('path/to/local/loading\\_script/loading\\_script.py', split='train')\n```\n\n\nLoad an IterableDataset:\n\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset('rotten\\_tomatoes', split='train', streaming=True)\n```\n\n\nLoad an image dataset with the `ImageFolder` dataset builder:\n\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset('imagefolder', data_dir='/path/to/images', split='train')\n```\n\n\n#### datasets.load\\_from\\_disk\n\n\n* **dataset\\_path** (`str`) —\nPath (e.g. `\"dataset/train\"`) or remote URI (e.g.\n`\"s3://my-bucket/dataset/train\"`) of the Dataset or DatasetDict directory where the dataset will be\nloaded from.\n* **fs** (`~filesystems.S3FileSystem` or `fsspec.spec.AbstractFileSystem`, *optional*) —\nInstance of the remote filesystem used to download the files from.\n\n\n`fs` was\n\n==================\n Document 2 \n----------------\n\n\n \n\n# \nMain classes\n\n\n\n## \nDatasetInfo\n\n### class datasets.DatasetInfo\n\n<\nsource\n>\n(\ndescription: str = <factory>\ncitation: str = <factory>\nhomepage: str = <factory>\nlicense: str = <factory>\nfeatures: typing.Optional[datasets.features.features.Features] = None\npost\\_processed: typing.Optional[datasets.info.PostProcessedInfo] = None\nsupervised\\_keys: typing.Optional[datasets.info.SupervisedKeysData] = None\ntask\\_templates: typing.Optional[typing.List[datasets.tasks.base.TaskTemplate]] = None\nbuilder\\_name: typing.Optional[str] = None\ndataset\\_name: typing.Optional[str] = None\nconfig\\_name: typing.Optional[str] = None\nversion: typing.Union[str, datasets.utils.version.Version, NoneType] =\n\n==================\n Document 3 \n----------------\n### select\n\n<\nsource\n>\n(\nindices: typing.Iterable\nkeep\\_in\\_memory: bool = False\nindices\\_cache\\_file\\_name: typing.Optional[str] = None\nwriter\\_batch\\_size: typing.Optional[int] = 1000\nnew\\_fingerprint: typing.Optional[str] = None\n\n\n* **indices** (`range`, `list`, `iterable`, `ndarray` or `Series`) —\nRange, list or 1D-array of integer indices for indexing.\nIf the indices correspond to a contiguous range, the Arrow table is simply sliced.\nHowever passing a list of indices that are not contiguous creates indices mapping, which is much less efficient,\nbut still faster than recreating an Arrow table made of the requested rows.\n* **keep\\_in\\_memory** (`bool`, defaults to `False`) —\nKeep the indices mapping in memory instead of writing it to a cache file.\n* **indices\\_cache\\_file\\_name** (`str`, *optional*, defaults to `None`) —\nProvide the name of a path for the cache file. It is used to store the\nindices mapping instead of the automatically generated cache file name.\n* **writer\\_batch\\_size** (`int`, defaults to `1000`) —\nNumber of rows per write operation for the cache file writer.\nThis value is a good trade-off between memory usage during the processing, and processing speed.\nHigher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n* **new\\_fingerprint** (`str`, *optional*, defaults to `None`) —\nThe new fingerprint of the dataset after transform.\nIf `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n\nCreate a new dataset with rows selected following the list/array of indices.\n\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten\\_tomatoes\", split=\"validation\")\n>>> ds.select(range(4))\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n```\n\n#### sort\n\n<\nsource\n>\n(\ncolumn\\_names: typing.Union[str, typing.Sequence[str]]\nreverse: typing.Union[bool, typing.Sequence[bool]] = False\nkind = 'deprecated'\nnull\\_placement: str = 'at\\_end'\nkeep\\_in\\_memory: bool = False\nload\\_from\\_cache\\_file: typing.Optional[bool] = None\nindices\\_cache\\_file\\_name: typing.Optional[str] = None\nwriter\\_batch\\_size: typing.Optional[int] = 1000\nnew\\_fingerprint: typing.Optional[str] = None\n\n\n* **column\\_names** (`Union[str, Sequence[str]]`) —\nColumn name(s) to sort by.\n* **reverse** (`Union[bool, Sequence[bool]]`, defaults"
    },
    {
        "instruction": "Develop a Python program using the 'datasets' API to load the TED MultiTranslation dataset and retrieve a random translated speech. The program should load the TED MultiTranslation dataset, select a random translated speech, and print the source and target language transcripts.",
        "api": "datasets",
        "output": "#!pip install datasets\nfrom datasets import load_dataset\nimport random\n\n# Load TED MultiTranslation dataset\ndataset = load_dataset(\"ted_multi\")\n\n# Select a random translated speech\nrandom_speech = random.choice(dataset[\"train\"][\"translation\"])\n\nsource_language = random_speech[\"en\"]\ntarget_language = random_speech[\"fr\"]\n\nprint(f\"Source Transcript (English): {source_language}\")\nprint(f\"Target Transcript (French): {target_language}\")",
        "documentation": "\n\n==================\n Document 0 \n----------------\n## class datasets.TranslationVariableLanguages\n\n<\nsource\n>\n(\nlanguages: typing.Optional[typing.List] = None\nnum\\_languages: typing.Optional[int] = None\nid: typing.Optional[str] = None\n\n)\n→\n\n* `language` or `translation` (variable-length 1D `tf.Tensor` of `tf.string`)\n\n\n* **languages** (`dict`) —\nA dictionary for each example mapping string language codes to one or more string translations.\nThe languages present may vary from example to example.\n\nLanguage codes sorted in ascending order or plain text translations, sorted to align with language codes.\n\n\n`FeatureConnector` for translations with variable languages per example.\nHere for compatiblity with tfds.\n\n```\n>>> # At construction time:\n>>> datasets.features.TranslationVariableLanguages(languages=['en', 'fr', 'de'])\n>>> # During data generation:\n>>> yield {\n...         'en': 'the cat',\n...         'fr': ['le chat', 'la chatte,']\n...         'de': 'die katze'\n... }\n>>> # Tensor returned :\n>>> {\n...         'language': ['en', 'de', 'fr', 'fr'],\n...         'translation': ['the cat', 'die katze', 'la chatte', 'le chat'],\n... }\n```\n\nFlatten the TranslationVariableLanguages feature into a dictionary.\n\n### class datasets.Array2D\n\n<\nsource\n>\n(\nshape: tuple\ndtype: str\nid: typing.Optional[str] = None\n\n\n* **shape** (`tuple`) —\nThe size of each dimension.\n* **dtype** (`str`) —\nThe value of the data type.\n\nCreate a two-dimensional array.\n\n```\n>>> from datasets import Features\n>>> features = Features({'x': Array2D(shape=(1, 3), dtype='int32')})\n```\n\n\n\n### class datasets.Array3D\n\nCreate a three-dimensional array.\n\n```\n>>> from datasets import Features\n>>> features = Features({'x': Array3D(shape=(1, 2, 3), dtype='int32')})\n```\n\n\n\n### class datasets.Array4D\n\nCreate a four-dimensional array.\n\n```\n>>> from datasets import Features\n>>> features = Features({'x': Array4D(shape=(1, 2, 2, 3), dtype='int32')})\n```\n\n\n\n### class datasets.Array5D\n\nCreate a five-dimensional array.\n\n```\n>>> from datasets import Features\n>>> features = Features({'x': Array5D(shape=(1, 2, 2, 3, 3), dtype='int32')})\n```\n\n\n### class datasets.Audio\n\n<\nsource\n>\n(\nsampling\\_rate: typing.Optional[int] = None\nmono: bool = True\ndecode: bool = True\nid: typing.Optional[str] = None\n\n\n* **sampling\\_rate** (`int`, *optional*) —\nTarget sampling rate. If `None`, the native sampling rate is used.\n* **mono** (`bool`, defaults to `True`) —\nWhether to convert the audio signal\n\n==================\n Document 1 \n----------------\n### datasets.enable\\_caching\n\nWhen applying transforms on a dataset, the data are stored in cache files.\nThe caching mechanism allows to reload an existing cache file if it’s already been computed.\n\n\nReloading a dataset is possible since the cache files are named using the dataset fingerprint, which is updated\nafter each transform.\n\n\nIf disabled, the library will no longer reload cached datasets files when applying transforms to the datasets.\nMore precisely, if the caching is disabled:\n\n\n* cache files are always recreated\n* cache files are written to a temporary directory that is deleted when session closes\n* cache files are named using a random hash instead of the dataset fingerprint\n* use save\\_to\\_disk() to save a transformed dataset or it will be deleted when session closes\n* caching doesn’t affect load\\_dataset(). If you want to regenerate a dataset from scratch you should use\nthe `download_mode` parameter in load\\_dataset().\n\n\n#### datasets.disable\\_caching\n\n\n\n#### datasets.is\\_caching\\_enabled\n\n\n* cache files are always recreated\n* cache files are written to a temporary directory that is deleted when session closes\n* cache files are named using a random hash instead of the dataset fingerprint\n* use save\\_to\\_disk()] to save a transformed dataset or it will be deleted when session closes\n* caching doesn’t affect load\\_dataset(). If you want to regenerate a dataset from scratch you should use\nthe `download_mode` parameter in load\\_dataset().\n\n\n## \nDatasetDict\n\n\nDictionary with split names as keys (‘train’, ‘test’ for example), and `Dataset` objects as values.\nIt also has dataset transform methods like map or filter, to process all the splits at once.\n\n\n### class datasets.DatasetDict\n\nA dictionary (dict of str: datasets.Dataset) with dataset transforms methods (map, filter, etc.)\n\n#### data\n\nThe Apache Arrow tables backing each split.\n\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten\\_tomatoes\")\n>>> ds.data\n```\n\nThe cache files containing the Apache Arrow table backing each split.\n\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten\\_tomatoes\")\n>>> ds.cache_files\n{'test': [{'filename': '/root/.cache/huggingface/datasets/rotten\\_tomatoes\\_movie\\_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten\\_tomatoes\\_movie\\_review-test.arrow'}],\n 'train': [{'filename': '/root/.cache/huggingface/datasets/rotten\\_tomatoes\\_movie\\_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten\\_tomatoes\\_movie\\_review-train.arrow'}],\n 'validation':\n\n==================\n Document 2 \n----------------\n## class datasets.Audio\n\n<\nsource\n>\n(\nsampling\\_rate: typing.Optional[int] = None\nmono: bool = True\ndecode: bool = True\nid: typing.Optional[str] = None\n\n\n* **sampling\\_rate** (`int`, *optional*) —\nTarget sampling rate. If `None`, the native sampling rate is used.\n* **mono** (`bool`, defaults to `True`) —\nWhether to convert the audio signal to mono by averaging samples across\nchannels.\n* **decode** (`bool`, defaults to `True`) —\nWhether to decode the audio data. If `False`,\nreturns the underlying dictionary in the format `{\"path\": audio_path, \"bytes\": audio_bytes}`.\n\nAudio `Feature` to extract audio data from an audio file.\n\n\nInput: The Audio feature accepts as input:\n\n\n* A `str`: Absolute path to the audio file (i.e. random access is allowed).\n* A `dict` with the keys:\n\n\n\t+ `path`: String with relative path of the audio file to the archive file.\n\t+ `bytes`: Bytes content of the audio file.This is useful for archived files with sequential access.\n* A `dict` with the keys:\n\n\n\t+ `path`: String with relative path of the audio file to the archive file.\n\t+ `array`: Array containing the audio sample\n\t+ `sampling_rate`: Integer corresponding to the sampling rate of the audio sample.This is useful for archived files with sequential access.\n\n```\n>>> from datasets import load_dataset, Audio\n>>> ds = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16000))\n>>> ds[0][\"audio\"]\n{'array': array([ 2.3443763e-05,  2.1729663e-04,  2.2145823e-04, ...,\n     3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32),\n 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT\\_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n 'sampling\\_rate': 16000}\n```\n\n<\nsource\n>\n(\nstorage: typing.Union[pyarrow.lib.StringArray, pyarrow.lib.StructArray]\n\n)\n→\n`pa.StructArray`\n\n\n* **storage** (`Union[pa.StringArray, pa.StructArray]`) —\nPyArrow array to cast.\n\n`pa.StructArray`\n\nArray in the Audio arrow storage type, that is\n`pa.struct({\"bytes\": pa.binary(), \"path\": pa.string()})`\n\n\nCast an Arrow array to the Audio arrow storage type.\nThe Arrow types that can be converted to the Audio pyarrow storage type are:\n\n\n* `pa.string()` - it must contain the “path” data\n* `pa.binary()` - it must contain the audio bytes\n* `pa.struct({\"bytes\": pa.binary()})`\n* `pa.struct({\"path\": pa.string()})`\n* `pa.struct({\"bytes\": pa.binary(), \"path\": pa.string()})` - order doesn’t matter\n\n\n#### decode\\_example\n\n<\nsource\n>\n(\nvalue: dict\ntoken\\_per\\_repo\\_id: typing.Union[typing.Dict[str, typing.Union[str, bool, NoneType]], NoneType] = None\n\n)\n→\n`dict`\n\n\n* **value** (`dict`) —\nA dictionary with keys:\n\n\t+ `path`: String with relative audio file path.\n\t+ `bytes`: Bytes of the audio file.\n* **token\\_per\\_repo\\_id** (`dict`, *optional*) —\nTo access and decode\naudio files from private repositories on the Hub, you can pass\na dictionary repo\\_id (`str`) -> token (`bool` or `str`)\n\n`dict`\n\n\nDecode example audio file into audio data.\n\n\n#### embed\\_storage\n\n<\nsource\n>\n(\nstorage: StructArray\n\n\n* **storage** (`pa.StructArray`) —\nPyArrow array to embed.\n\nArray in the Audio arrow storage type, that is\n`pa.struct({\"bytes\": pa.binary(), \"path\": pa.string()})`.\n\n\nEmbed audio files into the Arrow array.\n\n<\nsource\n>\n(\nvalue: typing.Union[str, bytes, dict]\n\n\n* **value** (`str` or `dict`) —\nData passed as input to Audio feature.\n\n\nEncode example into a format for Arrow.\n\nIf in the decodable state, raise an error, otherwise flatten the feature into a dictionary.\n\n### class datasets.Image\n\n<\nsource\n>\n(\ndecode: bool = True\nid: typing.Optional[str] = None\n\n\n* **decode** (`bool`, defaults to `True`) —\nWhether to decode the image data. If `False`,\nreturns the underlying dictionary in the format `{\"path\": image_path, \"bytes\": image_bytes}`.\n\nImage `Feature` to read image data from an image\n\n==================\n Document 3 \n----------------\n### datasets.load\\_dataset\n\n<\nsource\n>\n(\npath: str\nname: typing.Optional[str] = None\ndata\\_dir: typing.Optional[str] = None\ndata\\_files: typing.Union[str, typing.Sequence[str], typing.Mapping[str, typing.Union[str, typing.Sequence[str]]], NoneType] = None\nsplit: typing.Union[str, datasets.splits.Split, NoneType] = None\ncache\\_dir: typing.Optional[str] = None\nfeatures: typing.Optional[datasets.features.features.Features] = None\ndownload\\_config: typing.Optional[datasets.download.download\\_config.DownloadConfig] = None\ndownload\\_mode: typing.Union[datasets.download.download\\_manager.DownloadMode, str, NoneType] = None\nverification\\_mode: typing.Union[datasets.utils.info\\_utils.VerificationMode, str, NoneType] = None\nignore\\_verifications = 'deprecated'\nkeep\\_in\\_memory: typing.Optional[bool] = None\nsave\\_infos: bool = False\nrevision: typing.Union[str, datasets.utils.version.Version, NoneType] = None\ntoken: typing.Union[bool, str, NoneType] = None\nuse\\_auth\\_token = 'deprecated'\ntask = 'deprecated'\nstreaming: bool = False\nnum\\_proc: typing.Optional[int] = None\nstorage\\_options: typing.Optional[typing.Dict] = None\n\\*\\*config\\_kwargs\n\n\n* **path** (`str`) —\nPath or name of the dataset.\nDepending on `path`, the dataset builder that is used comes from a generic dataset script (JSON, CSV, Parquet, text etc.) or from the dataset script (a python file) inside the dataset directory.\nFor local datasets:\n\n\t+ if `path` is a local directory (containing data files only)\n\t-> load a generic dataset builder (csv, json, text etc.) based on the content of the directory\n\te.g. `'./path/to/directory/with/my/csv/data'`.\n\t+ if `path` is a local dataset script or a directory containing a local dataset script (if the script has the same name as the directory)\n\t-> load the dataset builder from the dataset script\n\te.g. `'./dataset/squad'` or `'./dataset/squad/squad.py'`.\nFor datasets on the Hugging Face Hub (list all available datasets with `huggingface_hub.list_datasets`)\n\n\t+ if `path` is a dataset repository on the HF hub (containing data files only)\n\t-> load a generic dataset builder (csv, text etc.) based on the content of the repository\n\te.g. `'username/dataset_name'`, a dataset repository on the HF hub containing your data files.\n\t+ if `path` is a dataset repository on the HF hub with a dataset script (if the script has the same name as the directory)\n\t-> load the dataset builder from the dataset script in the dataset repository\n\te.g. `glue`, `squad`, `'username/dataset_name'`, a dataset repository on the HF hub containing a dataset script `'dataset_name.py'`.\n* **name** (`str`, *optional*) —\nDefining the name of the dataset configuration.\n* **data\\_dir** (`str`, *optional*) —\nDefining the `data_dir` of the dataset configuration. If specified for the generic builders (csv, text etc.) or the Hub datasets and `data_files` is `None`,\nthe behavior is equal to passing `os.path.join(data_dir, **)` as `data_files` to reference all the files in a directory.\n* **data\\_files** (`str` or `Sequence` or `Mapping`, *optional*) —\nPath(s) to source data file(s).\n* **split** (`Split` or `str`) —\nWhich split of the data to load.\nIf `None`, will return a `dict` with all splits (typically `datasets.Split.TRAIN` and `datasets.Split.TEST`).\nIf given, will return a single Dataset.\nSplits can be combined and specified like in tensorflow-datasets.\n* **cache\\_dir** (`str`, *optional*) —\nDirectory to read/write data. Defaults to `\"~/.cache/huggingface/datasets\"`.\n* **features** (`Features`, *optional*) —\nSet the features type to use for this dataset.\n* **download\\_config** (DownloadConfig, *optional*) —\nSpecific download configuration parameters.\n* **download\\_mode** (DownloadMode or `str`, defaults to `REUSE_DATASET_IF_EXISTS`) —\nDownload/generate mode.\n* **verification\\_mode** (VerificationMode or `str`, defaults to `BASIC_CHECKS`) —\nVerification mode determining the checks to run on the downloaded/processed dataset information (checksums/size/splits/…).\n\n\n`ignore_verifications` was deprecated in version 2.9.1 and will be removed in 3.0.0.\nPlease use `verification_mode` instead.\n* **keep\\_in\\_memory** (`bool`, defaults to `None`) —\nWhether to copy the dataset in-memory. If `None`, the dataset\nwill not be copied in-memory unless explicitly enabled by setting `datasets.config.IN_MEMORY_MAX_SIZE` to\nnonzero. See more details in the improve performance section.\n* **save\\_infos** (`bool`, defaults to `False`) —\nSave the dataset information (checksums/size/splits/…).\n* **revision** (Version or `str`, *optional*) —\nVersion of the dataset script to load.\nAs datasets have their own git repository on the Datasets Hub, the default version “main” corresponds to their “main” branch.\nYou can specify a different version than the default “main” by using a commit SHA or a git tag of the dataset repository.\n* **token** (`str` or `bool`, *optional*) —\nOptional string or boolean to use as Bearer token for remote files on the Datasets Hub.\nIf `True`, or not specified, will get token from `\"~/.huggingface\"`.\n* **use\\_auth\\_token** (`str` or `bool`, *optional*) —\nOptional string or boolean to use as Bearer token for remote files on the Datasets Hub.\nIf `True`, or not specified, will get token from `\"~/.huggingface\"`.\n\n\n`use_auth_token` was deprecated in favor of `token` in version 2.14.0 and will be removed in 3.0.0.\n* **task** (`str`) —\nThe task to prepare the dataset for during training and evaluation. Casts the dataset’s Features to standardized column names and types as detailed in `datasets.tasks`.\n\nDeprecated in 2.13.0\n\n\n`task` was deprecated in version 2.13.0 and will be removed in 3.0.0.\n* **streaming** (`bool`, defaults to `False`) —\nIf set to `True`, don’t download the data files. Instead, it streams the data progressively while\niterating on the dataset. An IterableDataset or IterableDatasetDict is returned instead in this case.\nNote that streaming works for datasets that use data formats that support being iterated over like txt, csv, jsonl for example.\nJson files may be downloaded completely. Also streaming from remote zip or gzip files is supported but other compressed formats\nlike rar and xz are not yet supported. The tgz format doesn’t allow streaming.\n* **num\\_proc** (`int`, *optional*, defaults to `None`) —\nNumber of processes when downloading and generating the dataset locally.\nMultiprocessing is disabled by default.\n\nAdded in 2.7.0\n* **storage\\_options** (`dict`, *optional*, defaults to `None`) —\n**Experimental**. Key/value pairs to be passed on to the dataset file-system backend, if any.\n\nAdded in 2.11.0\n* \\***\\*config\\_kwargs** (additional keyword arguments) —\nKeyword arguments to be passed to the `BuilderConfig`\nand used in the DatasetBuilder.\n\n* if `split` is not `None`: the dataset requested,\n* if `split` is `None`, a DatasetDict with each split.\n\n\nor IterableDataset or IterableDatasetDict: if `streaming=True`\n\n\n* if `split` is not `None`, the dataset is requested\n* if `split` is `None`, a `~datasets.streaming.IterableDatasetDict` with each split.\n\n\nLoad a dataset from the Hugging Face Hub, or a local dataset.\n\n\nYou can find the list of datasets on the Hub or with `huggingface_hub.list_datasets`.\n\n\nA dataset is a directory that contains:\n\n\n* some data files in generic formats (JSON, CSV, Parquet, text, etc.).\n* and optionally a dataset script, if it requires some code to read the data files. This is used to load any kind of formats or structures.\n\n\nNote that dataset scripts can also download and read data files from anywhere - in case your data files already exist online.\n\n\nThis function does the following under the hood:\n\n\n1. Download and import in the library the dataset script from `path` if it’s not already cached inside the library.\n\n\nIf the dataset has no dataset script, then a generic dataset script is imported instead (JSON, CSV, Parquet, text, etc.)\n\n\nDataset scripts are small python scripts that define dataset builders. They define the citation, info and format of the dataset,\ncontain the path or URL to the original data files and the code to load examples from the original data files.\n\n\nYou can find the complete list of datasets in the Datasets Hub.\n2. Run the dataset script which will:\n\n\n\t* Download the dataset file from the original URL (see the script) if it’s not already available locally or cached.\n\t* Process and cache the dataset in typed Arrow tables for caching.\n\t\n\t\n\tArrow table are arbitrarily long, typed tables which can store nested objects and be mapped to numpy/pandas/python generic types.\n\tThey can be directly accessed from disk, loaded in RAM or even streamed over the web.\n3. Return a dataset built from the requested splits in `split` (default: all).\n\n\nIt also allows to load a dataset from a local directory or a dataset repository on the Hugging Face Hub without dataset script.\nIn this case, it automatically loads all the data files from the directory or the dataset repository.\n\nLoad a dataset from the Hugging Face Hub:\n\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset('rotten\\_tomatoes', split='train')\n\n# Map data files to splits\n>>> data_files = {'train': 'train.csv', 'test': 'test.csv'}\n>>> ds = load_dataset('namespace/your\\_dataset\\_name', data_files=data_files)\n```\n\n\nLoad a local dataset:\n\n```\n\n# Load a CSV file\n>>> from datasets import load_dataset\n>>> ds = load_dataset('csv', data_files='path/to/local/my\\_dataset.csv')\n\n\n# Load a JSON file\n>>> from datasets import load_dataset\n>>> ds = load_dataset('json', data_files='path/to/local/my\\_dataset.json')\n\n\n# Load from a local loading script\n>>> from datasets import load_dataset\n>>> ds = load_dataset('path/to/local/loading\\_script/loading\\_script.py', split='train')\n```\n\n\nLoad an IterableDataset:\n\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset('rotten\\_tomatoes', split='train', streaming=True)\n```\n\n\nLoad an image dataset with the `ImageFolder` dataset builder:\n\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset('imagefolder', data_dir='/path/to/images', split='train')\n```\n\n\n#### datasets.load\\_from\\_disk\n\n\n* **dataset\\_path** (`str`) —\nPath (e.g. `\"dataset/train\"`) or remote URI (e.g.\n`\"s3://my-bucket/dataset/train\"`) of the Dataset or DatasetDict directory where the dataset will be\nloaded from.\n* **fs** (`~filesystems.S3FileSystem` or `fsspec.spec.AbstractFileSystem`, *optional*) —\nInstance of the remote filesystem used to download the files from.\n\n\n`fs` was\n\n==================\n Document 4 \n----------------\n\n\n \n\n# \nMain classes\n\n\n\n## \nDatasetInfo\n\n### class datasets.DatasetInfo\n\n<\nsource\n>\n(\ndescription: str = <factory>\ncitation: str = <factory>\nhomepage: str = <factory>\nlicense: str = <factory>\nfeatures: typing.Optional[datasets.features.features.Features] = None\npost\\_processed: typing.Optional[datasets.info.PostProcessedInfo] = None\nsupervised\\_keys: typing.Optional[datasets.info.SupervisedKeysData] = None\ntask\\_templates: typing.Optional[typing.List[datasets.tasks.base.TaskTemplate]] = None\nbuilder\\_name: typing.Optional[str] = None\ndataset\\_name: typing.Optional[str] = None\nconfig\\_name: typing.Optional[str] = None\nversion: typing.Union[str, datasets.utils.version.Version, NoneType] =\n\n==================\n Document 5 \n----------------\nLogging methods\n\n\n🤗 Datasets strives to be transparent and explicit about how it works, but this can be quite verbose at times. We have included a series of logging methods which allow you to easily adjust the level of verbosity of the entire library. Currently the default verbosity of the library is set to `WARNING`.\n\n\nTo change the level of verbosity, use one of the direct setters. For instance, here is how to change the verbosity to the `INFO` level:\n\n```\nimport datasets\ndatasets.logging.set_verbosity_info()\n```\n\nYou can also use the environment variable `DATASETS_VERBOSITY` to override the default verbosity, and set it to one of the following: `debug`, `info`, `warning`, `error`, `critical`:\n\n```\nDATASETS_VERBOSITY=error ./myprogram.py\n```\n\nAll the methods of this logging module are documented below. The main ones are:\n\n\n* logging.get\\_verbosity() to get the current level of verbosity in the logger\n* logging.set\\_verbosity() to set the verbosity to the level of your choice\n\n\nIn order from the least to the most verbose (with their corresponding `int` values):\n\n\n1. `logging.CRITICAL` or `logging.FATAL` (int value, 50): only report the most critical errors.\n2. `logging.ERROR` (int value, 40): only report errors.\n3. `logging.WARNING` or `logging.WARN` (int value, 30): only reports error and warnings. This the default level used by the library.\n4. `logging.INFO` (int value, 20): reports error, warnings and basic information.\n5. `logging.DEBUG` (int value, 10): report all information.\n\n\nBy default, `tqdm` progress bars will be displayed during dataset download and preprocessing. logging.disable\\_progress\\_bar() and logging.enable\\_progress\\_bar() can be used to suppress or unsuppress this behavior. \n\n\n## \nFunctions\n\n\n#### datasets.utils.logging.get\\_verbosity\n\nReturn the current level for the HuggingFace datasets library’s root logger.\n\n\nHuggingFace datasets library has following logging levels:\n\n\n* `datasets.logging.CRITICAL`, `datasets.logging.FATAL`\n* `datasets.logging.ERROR`\n* `datasets.logging.WARNING`, `datasets.logging.WARN`\n* `datasets.logging.INFO`\n* `datasets.logging.DEBUG`\n\n\n\n#### datasets.utils.logging.set\\_verbosity\n\n<\nsource\n>\n(\nverbosity: int\n\nSet the level for the Hugging Face Datasets library’s root logger.\n\n\n#### datasets.utils.logging.set\\_verbosity\\_info\n\nSet the level for the Hugging Face datasets library’s root logger to `INFO`.\n\n\nThis will display most of the logging information and tqdm bars.\n\n\nShortcut to `datasets.logging.set_verbosity(datasets.logging.INFO)`.\n\n\n#### datasets.utils.logging.set\\_verbosity\\_warning\n\nSet the level for the Hugging Face datasets library’s root logger to `WARNING`.\n\n\nThis will display only the warning and errors logging information and tqdm bars.\n\n\nShortcut to `datasets.logging.set_verbosity(datasets.logging.WARNING)`.\n\n\n#### datasets.utils.logging.set\\_verbosity\\_debug\n\nSet the level for the Hugging Face datasets library’s root logger to `DEBUG`.\n\n\nThis will display all the logging information and tqdm bars.\n\n\nShortcut to `datasets.logging.set_verbosity(datasets.logging.DEBUG)`.\n\n\n#### datasets.utils.logging.set\\_verbosity\\_error\n\nSet the level for the Hugging Face datasets library’s root logger to `ERROR`.\n\n\nThis will display only the errors logging information and tqdm bars.\n\n\nShortcut to `datasets.logging.set_verbosity(datasets.logging.ERROR)`.\n\n\n#### datasets.utils.logging.disable\\_propagation\n\nDisable propagation of the library log outputs.\nNote that log propagation is disabled by default.\n\n\n#### datasets.utils.logging.enable\\_propagation\n\nEnable propagation of the library log outputs.\nPlease disable the Hugging Face datasets library’s default handler to prevent double logging if the root logger has\nbeen configured.\n\n\n#### datasets.utils.logging.get\\_logger\n\n<\nsource\n>\n(\nname: typing.Optional[str] = None\n\nReturn a logger with the specified name.\nThis function can be used in dataset scripts.\n\n\n#### datasets.enable\\_progress\\_bar\n\nEnable tqdm progress bar.\n\n\n#### datasets.disable\\_progress\\_bar\n\nDisable tqdm progress bar.\n\n\n#### datasets.is\\_progress\\_bar\\_enabled\n\nReturn a boolean indicating whether tqdm progress bars are enabled.\n\n\n\n## \nLevels\n\n\n\n### \ndatasets.logging.CRITICAL\n\n\ndatasets.logging.CRITICAL = 50\n\n\n\n### \ndatasets.logging.DEBUG\n\n\ndatasets.logging.DEBUG = 10\n\n\n\n### \ndatasets.logging.ERROR\n\n\ndatasets.logging.ERROR = 40\n\n\n\n### \ndatasets.logging.FATAL\n\n\ndatasets.logging.FATAL = 50\n\n\n\n### \ndatasets.logging.INFO\n\n\ndatasets.logging.INFO = 20\n\n\n\n### \ndatasets.logging.NOTSET\n\n\ndatasets.logging.NOTSET = 0\n\n\n\n### \ndatasets.logging.WARN\n\n\ndatasets.logging.WARN = 30\n\n\n\n### \ndatasets.logging.WARNING\n\n\ndatasets.logging.WARNING = 30\n\n\n\n# \nTask templates\n\n\nThe Task API is deprecated in favor of `train-eval-index` and will be removed in the next major release.\n\n\nThe tasks supported by Dataset.prepare\\_for\\_task() and DatasetDict.prepare\\_for\\_task().\n\n\n### class datasets.AutomaticSpeechRecognition\n\n<\nsource\n>\n(\ntask: str = 'automatic-speech-recognition'\naudio\\_column: str = 'audio'\ntranscription\\_column: str = 'transcription'\n\n\n\n### class datasets.AudioClassification\n\n<\nsource\n>\n(\ntask: str = 'audio-classification'\naudio\\_column: str = 'audio'\nlabel\\_column: str = 'labels'\n\n\n\n### class datasets.ImageClassification\n\n<\nsource\n>\n(\ntask: str = 'image-classification'\nimage\\_column: str = 'image'\nlabel\\_column: str = 'labels'\n\n\n\n#### align\\_with\\_features\n\n<\nsource\n>\n(\nfeatures\n\n\n\n### class datasets.LanguageModeling\n\n<\nsource\n>\n(\ntask: str = 'language-modeling'\ntext\\_column: str = 'text'\n\n\n\n### class datasets.QuestionAnsweringExtractive\n\n<\nsource\n>\n(\ntask: str = 'question-answering-extractive'\nquestion\\_column: str = 'question'\ncontext\\_column: str = 'context'\nanswers\\_column: str = 'answers'\n\n\n\n### class datasets.Summarization\n\n<\nsource\n>\n(\ntask: str = 'summarization'\ntext\\_column: str = 'text'\nsummary\\_column: str = 'summary'\n\n\n\n### class datasets.TextClassification\n\n<\nsource\n>\n(\ntask: str = 'text-classification'\ntext\\_column: str = 'text'\nlabel\\_column: str = 'labels'"
    },
    {
        "instruction": "Write a Python program that utilizes the 'diffusers' API to generate a list of possible completions for the provided prompt, 'What are the benefits of reading books?'",
        "api": "diffusers",
        "output": "#!pip install diffusers\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-3-diffusers\",\n    use_auth_token=True,\n    revision=\"fp16\",\n    torch_dtype=torch.float16\n)\npipe = pipe.to(\"cpu\")\n\nprompt = \"What are the benefits of reading books?\"\ncompletions = pipe(prompt)[\"sample\"][0]\n\nprint(completions)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n## \nOptimizing for memory\n\n\nWhen optimizing for GPU memory, we can use the standard diffusers cpu offloading APIs.\n\n\nEither the model based CPU offloading,\n\n```\npipe = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\n```\n\nor the more aggressive layer based CPU offloading.\n\n```\npipe = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16)\npipe.enable_sequential_cpu_offload()\n```\n\nAdditionally, T5 can be loaded in 8bit precision\n\n```\nfrom transformers import T5EncoderModel\n\ntext_encoder = T5EncoderModel.from_pretrained(\n    \"DeepFloyd/IF-I-XL-v1.0\", subfolder=\"text\\_encoder\", device_map=\"auto\", load_in_8bit=True, variant=\"8bit\"\n)\n\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\n    \"DeepFloyd/IF-I-XL-v1.0\",\n    text_encoder=text_encoder,  # pass the previously instantiated 8bit text encoder\n    unet=None,\n    device_map=\"auto\",\n)\n\nprompt_embeds, negative_embeds = pipe.encode_prompt(\"<prompt>\")\n```\n\nFor CPU RAM constrained machines like google colab free tier where we can’t load all\nmodel components to the CPU at once, we can manually only load the pipeline with\nthe text encoder or unet when the respective model components are needed.\n\n```\nfrom diffusers import IFPipeline, IFSuperResolutionPipeline\nimport torch\nimport gc\nfrom transformers import T5EncoderModel\nfrom diffusers.utils import pt_to_pil\n\n# text to image\n\nprompt = 'a photo of a kangaroo wearing an orange hoodie and blue sunglasses standing in front of the eiffel tower holding a sign that says \"very deep learning\"'\nprompt_embeds, negative_embeds = pipe.encode_prompt(prompt)\n\n\n# Remove the pipeline so we can re-load the pipeline with the unet\ndel text_encoder\ndel pipe\ngc.collect()\ntorch.cuda.empty_cache()\n\npipe = IFPipeline.from_pretrained(\n    \"DeepFloyd/IF-I-XL-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16, device_map=\"auto\"\n)\n\ngenerator = torch.Generator().manual_seed(0)\nimage = pipe(\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    output_type=\"pt\",\n    generator=generator,\n).images\n\npt_to_pil(image)[0].save(\"./if\\_stage\\_I.png\")\n\n\n# Remove the pipeline so we can load the super-resolution pipeline\ndel pipe\ngc.collect()\ntorch.cuda.empty_cache()\n\n\n# First super resolution\n\npipe = IFSuperResolutionPipeline.from_pretrained(\n    \"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16, device_map=\"auto\"\n)\n\ngenerator = torch.Generator().manual_seed(0)\nimage = pipe(\n    image=image,\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    output_type=\"pt\",\n    generator=generator,\n).images\n\npt_to_pil(image)[0].save(\"./if\\_stage\\_II.png\")\n```\n\n\n## \nAvailable Pipelines:\n\n\n| Pipeline | Tasks | Colab |\n| --- | --- | --- |\n| pipeline\\_if.py | *Text-to-Image Generation* | - |\n| pipeline\\_if\\_superresolution.py | *Text-to-Image Generation* | - |\n| pipeline\\_if\\_img2img.py | *Image-to-Image Generation* | - |\n| pipeline\\_if\\_img2img\\_superresolution.py | *Image-to-Image Generation* | - |\n| pipeline\\_if\\_inpainting.py | *Image-to-Image Generation* | - |\n| pipeline\\_if\\_inpainting\\_superresolution.py | *Image-to-Image Generation* | - |\n\n\n\n## \nIFPipeline\n\n\n### class diffusers.IFPipeline\n\n<\nsource\n>\n(\ntokenizer: T5Tokenizer\ntext\\_encoder: T5EncoderModel\nunet: UNet2DConditionModel\nscheduler: DDPMScheduler\nsafety\\_checker: typing.Optional[diffusers.pipelines.deepfloyd\\_if.safety\\_checker.IFSafetyChecker]\nfeature\\_extractor: typing.Optional[transformers.models.clip.image\\_processing\\_clip.CLIPImageProcessor]\nwatermarker: typing.Optional[diffusers.pipelines.deepfloyd\\_if.watermark.IFWatermarker]\nrequires\\_safety\\_checker: bool = True\n\n\n#### \\_\\_call\\_\\_\n\n<\nsource\n>\n(\nprompt: typing.Union[str, typing.List[str]] = None\nnum\\_inference\\_steps: int = 100\ntimesteps: typing.List[int] = None\nguidance\\_scale: float = 7.0\nnegative\\_prompt: typing.Union[str, typing.List[str], NoneType] = None\nnum\\_images\\_per\\_prompt: typing.Optional[int] = 1\nheight: typing.Optional[int] = None\nwidth: typing.Optional[int] = None\neta: float = 0.0\ngenerator: typing.Union[torch.\\_C.Generator, typing.List[torch.\\_C.Generator], NoneType] = None\nprompt\\_embeds: typing.Optional[torch.FloatTensor] = None\nnegative\\_prompt\\_embeds:\n\n==================\n Document 1 \n----------------\n \nText-to-image\n\n\nThe Stable Diffusion model was created by researchers and engineers from CompVis, Stability AI, Runway, and LAION. The StableDiffusionPipeline is capable of generating photorealistic images given any text input. It’s trained on 512x512 images from a subset of the LAION-5B dataset. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on consumer GPUs. Latent diffusion is the research on top of which Stable Diffusion was built. It was proposed in High-Resolution Image Synthesis with Latent Diffusion Models by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer.\n\n\n*By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion.*\n\n\nMake sure to check out the Stable Diffusion Tips section to learn how to explore the tradeoff between scheduler speed and quality, and how to reuse pipeline components efficiently! \n\n\nIf you’re interested in using one of the official checkpoints for a task, explore the CompVis, Runway, and Stability AI Hub organizations!\n\n\n## \nStableDiffusionPipeline\n\n### class diffusers.StableDiffusionPipeline\n\n\n* **prompt** (`str` or `List[str]`, *optional*) —\nThe prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`.\n* **height** (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`) —\nThe height in pixels of the generated image.\n*\n\n==================\n Document 2 \n----------------\n \nText-to-video\n\n\nVideoFusion: Decomposed Diffusion Models for High-Quality Video Generation is by Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, Tieniu Tan.\n\n\n*A diffusion probabilistic model (DPM), which constructs a forward diffusion process by gradually adding noise to data points and learns the reverse denoising process to generate new samples, has been shown to handle complex data distribution. Despite its recent success in image synthesis, applying DPMs to video generation is still challenging due to high-dimensional data spaces. Previous methods usually adopt a standard diffusion process, where frames in the same video clip are destroyed with independent noises, ignoring the content redundancy and temporal correlation. This work presents a decomposed diffusion process via resolving the per-frame noise into a base noise that is shared among all frames and a residual noise that varies along the time axis. The denoising pipeline employs two jointly-learned networks to match the noise decomposition accordingly. Experiments on various datasets confirm that our approach, termed as VideoFusion, surpasses both GAN-based and diffusion-based alternatives in high-quality video generation. We further show that our decomposed formulation can benefit from pre-trained image diffusion models and well-support text-conditioned video creation.*\n\n\nYou can find additional information about Text-to-Video on the project page, original codebase, and try it out in a demo. Official checkpoints can be found at damo-vilab and cerspense.\n\n### \n`text-to-video-ms-1.7b`\n\n\nLet’s start by generating a short video with the default length of 16 frames (2s at 8 fps):\n\n```\nimport torch\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\")\npipe = pipe.to(\"cuda\")\n\nprompt = \"Spiderman is surfing\"\nvideo_frames = pipe(prompt).frames\nvideo_path = export_to_video(video_frames)\nvideo_path\n```\n\nDiffusers\n\n==================\n Document 3 \n----------------\n \nVersatile Diffusion\n\n\nVersatile Diffusion was proposed in Versatile Diffusion: Text, Images and Variations All in One Diffusion Model by Xingqian Xu, Zhangyang Wang, Eric Zhang, Kai Wang, Humphrey Shi .\n\n\n*The recent advances in diffusion models have set an impressive milestone in many generation tasks. Trending works such as DALL-E2, Imagen, and Stable Diffusion have attracted great interest in academia and industry. Despite the rapid landscape changes, recent new approaches focus on extensions and performance rather than capacity, thus requiring separate models for separate tasks. In this work, we expand the existing single-flow diffusion pipeline into a multi-flow network, dubbed Versatile Diffusion (VD), that handles text-to-image, image-to-text, image-variation, and text-variation in one unified model. Moreover, we generalize VD to a unified multi-flow multimodal diffusion framework with grouped layers, swappable streams, and other propositions that can process modalities beyond images and text. Through our experiments, we demonstrate that VD and its underlying framework have the following merits: a) VD handles all subtasks with competitive quality; b) VD initiates novel extensions and applications such as disentanglement of style and semantic, image-text dual-guided generation, etc.; c) Through these experiments and applications, VD provides more semantic insights of the generated outputs.*\n\n\nYou can load the more memory intensive “all-in-one” VersatileDiffusionPipeline that supports all the tasks or use the individual pipelines which are more memory efficient.\n\n\n| **Pipeline** | **Supported tasks** |\n| --- | --- |\n| VersatileDiffusionPipeline | all of the below |\n| VersatileDiffusionTextToImagePipeline | text-to-image |\n| VersatileDiffusionImageVariationPipeline | image variation |\n| VersatileDiffusionDualGuidedPipeline | image-text dual guided generation |\n\n\n## \nVersatileDiffusionPipeline\n\n\n### class diffusers.VersatileDiffusionPipeline\n\n<\nsource\n>\n(\ntokenizer: CLIPTokenizer\nimage\\_feature\\_extractor: CLIPImageProcessor\ntext\\_encoder: CLIPTextModel\nimage\\_encoder: CLIPVisionModel\nimage\\_unet: UNet2DConditionModel\ntext\\_unet: UNet2DConditionModel\nvae: AutoencoderKL\nscheduler: KarrasDiffusionSchedulers\n\n#### dual\\_guided\n\n<\nsource\n>\n(\nprompt: typing.Union[PIL.Image.Image, typing.List[PIL.Image.Image]]\nimage: typing.Union[str, typing.List[str]]\ntext\\_to\\_image\\_strength: float = 0.5\nheight: typing.Optional[int] = None\nwidth: typing.Optional[int] = None\nnum\\_inference\\_steps: int = 50\nguidance\\_scale: float = 7.5\nnum\\_images\\_per\\_prompt: typing.Optional[int] = 1\neta: float = 0.0\ngenerator: typing.Union[torch.\\_C.Generator, typing.List[torch.\\_C.Generator], NoneType] = None\nlatents: typing.Optional[torch.FloatTensor] = None\noutput\\_type: typing.Optional[str] = 'pil'\nreturn\\_dict: bool =\n\n==================\n Document 4 \n----------------\n \nWürstchen\n\n\n\nWürstchen: Efficient Pretraining of Text-to-Image Models is by Pablo Pernias, Dominic Rampas, and Marc Aubreville.\n\n\n*We introduce Würstchen, a novel technique for text-to-image synthesis that unites competitive performance with unprecedented cost-effectiveness and ease of training on constrained hardware. Building on recent advancements in machine learning, our approach, which utilizes latent diffusion strategies at strong latent image compression rates, significantly reduces the computational burden, typically associated with state-of-the-art models, while preserving, if not enhancing, the quality of generated images. Wuerstchen achieves notable speed improvements at inference time, thereby rendering real-time applications more viable. One of the key advantages of our method lies in its modest training requirements of only 9,200 GPU hours, slashing the usual costs significantly without compromising the end performance. In a comparison against the state-of-the-art, we found the approach to yield strong competitiveness. This paper opens the door to a new line of research that prioritizes both performance and computational accessibility, hence democratizing the use of sophisticated AI technologies. Through Wuerstchen, we demonstrate a compelling stride forward in the realm of text-to-image synthesis, offering an innovative path to explore in future research.*\n\n## \nWürstchen v2 comes to Diffusers\n\n\nAfter the initial paper release, we have improved numerous things in the architecture, training and sampling, making Würstchen competetive to current state-of-the-art models in many ways. We are excited to release this new version together"
    },
    {
        "instruction": "Write a Python program that utilizes the 'diffusers' API to generate a list of possible completions for the provided prompt, 'What are the benefits of regular exercise?'",
        "api": "diffusers",
        "output": "#!pip install diffusers\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-3-diffusers\",\n    use_auth_token=True,\n    revision=\"fp16\",\n    torch_dtype=torch.float16\n)\npipe = pipe.to(\"cpu\")\n\nprompt = \"What are the benefits of regular exercise?\"\ncompletions = pipe(prompt)[\"sample\"][0]\n\nprint(completions)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n \nText-to-image\n\n\nThe Stable Diffusion model was created by researchers and engineers from CompVis, Stability AI, Runway, and LAION. The StableDiffusionPipeline is capable of generating photorealistic images given any text input. It’s trained on 512x512 images from a subset of the LAION-5B dataset. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on consumer GPUs. Latent diffusion is the research on top of which Stable Diffusion was built. It was proposed in High-Resolution Image Synthesis with Latent Diffusion Models by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer.\n\n\n*By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion.*\n\n\nMake sure to check out the Stable Diffusion Tips section to learn how to explore the tradeoff between scheduler speed and quality, and how to reuse pipeline components efficiently! \n\n\nIf you’re interested in using one of the official checkpoints for a task, explore the CompVis, Runway, and Stability AI Hub organizations!\n\n\n## \nStableDiffusionPipeline\n\n### class diffusers.StableDiffusionPipeline\n\n\n* **prompt** (`str` or `List[str]`, *optional*) —\nThe prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`.\n* **height** (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`) —\nThe height in pixels of the generated image.\n*\n\n==================\n Document 1 \n----------------\n# \nUsage\n\n\nBefore you can use IF, you need to accept its usage conditions. To do so:\n\n\n1. Make sure to have a Hugging Face account and be logged in\n2. Accept the license on the model card of DeepFloyd/IF-I-XL-v1.0. Accepting the license on the stage I model card will auto accept for the other IF models.\n3. Make sure to login locally. Install `huggingface_hub`\n\n```\npip install huggingface_hub --upgrade\n```\n\nrun the login function in a Python shell\n\n```\nfrom huggingface_hub import login\n\nlogin()\n```\n\nand enter your Hugging Face Hub access token.\n\n\nNext we install `diffusers` and dependencies:\n\n```\npip install diffusers accelerate transformers safetensors\n```\n\nThe following sections give more in-detail examples of how to use IF. Specifically:\n\n\n* Text-to-Image Generation\n* Image-to-Image Generation\n* Inpainting\n* Reusing model weights\n* Speed optimization\n* Memory optimization\n\n\n**Available checkpoints**\n\n\n* *Stage-1*\n\n\n\t+ DeepFloyd/IF-I-XL-v1.0\n\t+ DeepFloyd/IF-I-L-v1.0\n\t+ DeepFloyd/IF-I-M-v1.0\n* *Stage-2*\n\n\n\t+ DeepFloyd/IF-II-L-v1.0\n\t+ DeepFloyd/IF-II-M-v1.0\n* *Stage-3*\n\n\n\t+ stabilityai/stable-diffusion-x4-upscaler\n\n\n**Demo**\n\n\n\n**Google Colab**\n\n\n\n### \nText-to-Image Generation\n\n\nBy default diffusers makes use of model cpu offloading\nto run the whole IF pipeline with as little as 14 GB of VRAM.\n\n```\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import pt_to_pil\nimport torch\n\n\n# stage 1\nstage_1 = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16)\nstage_1.enable_model_cpu_offload()\n\n\n# stage 2\nstage_2 = DiffusionPipeline.from_pretrained(\n    \"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16\n)\nstage_2.enable_model_cpu_offload()\n\n\n# stage 3\nsafety_modules = {\n    \"feature\\_extractor\": stage_1.feature_extractor,\n    \"safety\\_checker\": stage_1.safety_checker,\n    \"watermarker\": stage_1.watermarker,\n}\nstage_3 = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-x4-upscaler\", **safety_modules, torch_dtype=torch.float16\n)\nstage_3.enable_model_cpu_offload()\n\nprompt = 'a photo of a kangaroo wearing an orange hoodie and blue sunglasses standing in front of the eiffel tower holding a sign that says \"very deep learning\"'\ngenerator = torch.manual_seed(1)\n\n\n# text embeds\nprompt_embeds, negative_embeds = stage_1.encode_prompt(prompt)\n\n\n# stage 1\nimage = stage_1(\n    prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds, generator=generator, output_type=\"pt\"\n).images\npt_to_pil(image)[0].save(\"./if\\_stage\\_I.png\")\n\n\n# stage 2\nimage = stage_2(\n    image=image,\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    generator=generator,\n    output_type=\"pt\",\n).images\npt_to_pil(image)[0].save(\"./if\\_stage\\_II.png\")\n\n\n# stage 3\nimage = stage_3(prompt=prompt, image=image, noise_level=100, generator=generator).images\nimage[0].save(\"./if\\_stage\\_III.png\")\n```\n\n\n### \nText Guided Image-to-Image Generation\n\n\nThe same IF model weights can be used for text-guided image-to-image translation or image variation.\nIn this case just make sure to load the weights using the IFInpaintingPipeline and IFInpaintingSuperResolutionPipeline pipelines.\n\n\n**Note**: You can also directly move the weights of the text-to-image pipelines to the image-to-image pipelines\nwithout loading them twice by making use of the `~DiffusionPipeline.components()` function as explained here.\n\n```\nfrom diffusers import IFImg2ImgPipeline, IFImg2ImgSuperResolutionPipeline, DiffusionPipeline\nfrom diffusers.utils import pt_to_pil\n\nimport torch\n\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\n\n# download image\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\nresponse = requests.get(url)\noriginal_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\noriginal_image = original_image.resize((768, 512))\n\n\n# stage 1\nstage_1 = IFImg2ImgPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16)\nstage_1.enable_model_cpu_offload()\n\n\n# stage 2\nstage_2 = IFImg2ImgSuperResolutionPipeline.from_pretrained(\n    \"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16\n)\nstage_2.enable_model_cpu_offload()\n\nprompt = \"A fantasy landscape in style minecraft\"\ngenerator = torch.manual_seed(1)\n\n\n# stage 1\nimage = stage_1(\n    image=original_image,\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    generator=generator,\n    output_type=\"pt\",\n).images\npt_to_pil(image)[0].save(\"./if\\_stage\\_I.png\")\n\n\n# stage 2\nimage = stage_2(\n    image=image,\n    original_image=original_image,\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    generator=generator,\n    output_type=\"pt\",\n).images\npt_to_pil(image)[0].save(\"./if\\_stage\\_II.png\")\n\n\n# stage 3\nimage = stage_3(prompt=prompt, image=image, generator=generator, noise_level=100).images\nimage[0].save(\"./if\\_stage\\_III.png\")\n```\n\n\n### \nText Guided Inpainting Generation\n\n```\nfrom diffusers import IFInpaintingPipeline, IFInpaintingSuperResolutionPipeline, DiffusionPipeline\nfrom diffusers.utils import pt_to_pil\nimport torch\n\n\n# download image\nurl = \"https://huggingface.co/datasets/diffusers/docs-images/resolve/main/if/person.png\"\nresponse = requests.get(url)\noriginal_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\noriginal_image = original_image\n\n\n# download mask\nurl = \"https://huggingface.co/datasets/diffusers/docs-images/resolve/main/if/glasses\\_mask.png\"\nresponse = requests.get(url)\nmask_image = Image.open(BytesIO(response.content))\nmask_image = mask_image\n\n\n# stage 1\nstage_1 = IFInpaintingPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16)\nstage_1.enable_model_cpu_offload()\n\n\n# stage 2\nstage_2 = IFInpaintingSuperResolutionPipeline.from_pretrained(\n    \"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16\n)\nstage_2.enable_model_cpu_offload()\n\nprompt = \"blue sunglasses\"\ngenerator = torch.manual_seed(1)\n\n\n# stage 1\nimage = stage_1(\n    image=original_image,\n    mask_image=mask_image,\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    generator=generator,\n    output_type=\"pt\",\n).images\npt_to_pil(image)[0].save(\"./if\\_stage\\_I.png\")\n\n\n# stage 2\nimage = stage_2(\n    image=image,\n    original_image=original_image,\n    mask_image=mask_image,\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    generator=generator,\n    output_type=\"pt\",\n).images\npt_to_pil(image)[0].save(\"./if\\_stage\\_II.png\")\n\n\n### \nConverting between different pipelines\n\n\nIn addition to being loaded with `from_pretrained`, Pipelines can also be loaded directly from each other.\n\n```\nfrom diffusers import IFPipeline, IFSuperResolutionPipeline\n\npipe_1 = IFPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\")\npipe_2 = IFSuperResolutionPipeline.from_pretrained(\"DeepFloyd/IF-II-L-v1.0\")\n\n\nfrom diffusers import IFImg2ImgPipeline, IFImg2ImgSuperResolutionPipeline\n\npipe_1 = IFImg2ImgPipeline(**pipe_1.components)\npipe_2 = IFImg2ImgSuperResolutionPipeline(**pipe_2.components)\n\n\nfrom diffusers import IFInpaintingPipeline, IFInpaintingSuperResolutionPipeline\n\npipe_1 = IFInpaintingPipeline(**pipe_1.components)\npipe_2 = IFInpaintingSuperResolutionPipeline(**pipe_2.components)\n```\n\n### \nOptimizing for speed\n\n\nThe simplest optimization to run IF faster is to move all model components to the GPU.\n\n```\npipe = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16)\npipe.to(\"cuda\")\n```\n\nYou can also run the diffusion process for a shorter number of timesteps.\n\n\nThis can either be done with\n\n==================\n Document 2 \n----------------\n## \nOptimizing for memory\n\n\nWhen optimizing for GPU memory, we can use the standard diffusers cpu offloading APIs.\n\n\nEither the model based CPU offloading,\n\n```\npipe = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\n```\n\nor the more aggressive layer based CPU offloading.\n\n```\npipe = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16)\npipe.enable_sequential_cpu_offload()\n```\n\nAdditionally, T5 can be loaded in 8bit precision\n\n```\nfrom transformers import T5EncoderModel\n\ntext_encoder = T5EncoderModel.from_pretrained(\n    \"DeepFloyd/IF-I-XL-v1.0\", subfolder=\"text\\_encoder\", device_map=\"auto\", load_in_8bit=True, variant=\"8bit\"\n)\n\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\n    \"DeepFloyd/IF-I-XL-v1.0\",\n    text_encoder=text_encoder,  # pass the previously instantiated 8bit text encoder\n    unet=None,\n    device_map=\"auto\",\n)\n\nprompt_embeds, negative_embeds = pipe.encode_prompt(\"<prompt>\")\n```\n\nFor CPU RAM constrained machines like google colab free tier where we can’t load all\nmodel components to the CPU at once, we can manually only load the pipeline with\nthe text encoder or unet when the respective model components are needed.\n\n```\nfrom diffusers import IFPipeline, IFSuperResolutionPipeline\nimport torch\nimport gc\nfrom transformers import T5EncoderModel\nfrom diffusers.utils import pt_to_pil\n\n# text to image\n\nprompt = 'a photo of a kangaroo wearing an orange hoodie and blue sunglasses standing in front of the eiffel tower holding a sign that says \"very deep learning\"'\nprompt_embeds, negative_embeds = pipe.encode_prompt(prompt)\n\n\n# Remove the pipeline so we can re-load the pipeline with the unet\ndel text_encoder\ndel pipe\ngc.collect()\ntorch.cuda.empty_cache()\n\npipe = IFPipeline.from_pretrained(\n    \"DeepFloyd/IF-I-XL-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16, device_map=\"auto\"\n)\n\ngenerator = torch.Generator().manual_seed(0)\nimage = pipe(\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    output_type=\"pt\",\n    generator=generator,\n).images\n\npt_to_pil(image)[0].save(\"./if\\_stage\\_I.png\")\n\n\n# Remove the pipeline so we can load the super-resolution pipeline\ndel pipe\ngc.collect()\ntorch.cuda.empty_cache()\n\n\n# First super resolution\n\npipe = IFSuperResolutionPipeline.from_pretrained(\n    \"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16, device_map=\"auto\"\n)\n\ngenerator = torch.Generator().manual_seed(0)\nimage = pipe(\n    image=image,\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    output_type=\"pt\",\n    generator=generator,\n).images\n\npt_to_pil(image)[0].save(\"./if\\_stage\\_II.png\")\n```\n\n\n## \nAvailable Pipelines:\n\n\n| Pipeline | Tasks | Colab |\n| --- | --- | --- |\n| pipeline\\_if.py | *Text-to-Image Generation* | - |\n| pipeline\\_if\\_superresolution.py | *Text-to-Image Generation* | - |\n| pipeline\\_if\\_img2img.py | *Image-to-Image Generation* | - |\n| pipeline\\_if\\_img2img\\_superresolution.py | *Image-to-Image Generation* | - |\n| pipeline\\_if\\_inpainting.py | *Image-to-Image Generation* | - |\n| pipeline\\_if\\_inpainting\\_superresolution.py | *Image-to-Image Generation* | - |\n\n\n\n## \nIFPipeline\n\n\n### class diffusers.IFPipeline\n\n<\nsource\n>\n(\ntokenizer: T5Tokenizer\ntext\\_encoder: T5EncoderModel\nunet: UNet2DConditionModel\nscheduler: DDPMScheduler\nsafety\\_checker: typing.Optional[diffusers.pipelines.deepfloyd\\_if.safety\\_checker.IFSafetyChecker]\nfeature\\_extractor: typing.Optional[transformers.models.clip.image\\_processing\\_clip.CLIPImageProcessor]\nwatermarker: typing.Optional[diffusers.pipelines.deepfloyd\\_if.watermark.IFWatermarker]\nrequires\\_safety\\_checker: bool = True\n\n\n#### \\_\\_call\\_\\_\n\n<\nsource\n>\n(\nprompt: typing.Union[str, typing.List[str]] = None\nnum\\_inference\\_steps: int = 100\ntimesteps: typing.List[int] = None\nguidance\\_scale: float = 7.0\nnegative\\_prompt: typing.Union[str, typing.List[str], NoneType] = None\nnum\\_images\\_per\\_prompt: typing.Optional[int] = 1\nheight: typing.Optional[int] = None\nwidth: typing.Optional[int] = None\neta: float = 0.0\ngenerator: typing.Union[torch.\\_C.Generator, typing.List[torch.\\_C.Generator], NoneType] = None\nprompt\\_embeds: typing.Optional[torch.FloatTensor] = None\nnegative\\_prompt\\_embeds:\n\n==================\n Document 3 \n----------------\n## \nChoosing a checkpoint\n\n\nAudioLDM2 comes in three variants. Two of these checkpoints are applicable to the general task of text-to-audio\ngeneration. The third checkpoint is trained exclusively on text-to-music generation.\n\n\nAll checkpoints share the same model size for the text encoders and VAE. They differ in the size and depth of the UNet.\nSee table below for details on the three checkpoints:\n\n\n| Checkpoint | Task | UNet Model Size | Total Model Size | Training Data / h |\n| --- | --- | --- | --- | --- |\n| audioldm2 | Text-to-audio | 350M | 1.1B | 1150k |\n| audioldm2-large | Text-to-audio | 750M | 1.5B | 1150k |\n| audioldm2-music | Text-to-music | 350M | 1.1B | 665k |\n\n\n### \nConstructing a prompt\n\n\n* Descriptive prompt inputs work best: use adjectives to describe the sound (e.g. “high quality” or “clear”) and make the prompt context specific (e.g. “water stream in a forest” instead of “stream”).\n* It’s best to use general terms like “cat” or “dog” instead of specific names or abstract objects the model may not be familiar with.\n* Using a **negative prompt** can significantly improve the quality of the generated waveform, by guiding the generation away from terms that correspond to poor quality audio. Try using a negative prompt of “Low quality.”\n\n\n\n### \nControlling inference\n\n\n### \nEvaluating generated waveforms:\n\n\n* The quality of the generated waveforms can vary significantly based on the seed. Try generating with different seeds until you find a satisfactory generation\n* Multiple waveforms can be generated in one go: set `num_waveforms_per_prompt` to a\n\n==================\n Document 4 \n----------------\n \nText-to-video\n\n\nVideoFusion: Decomposed Diffusion Models for High-Quality Video Generation is by Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, Tieniu Tan.\n\n\n*A diffusion probabilistic model (DPM), which constructs a forward diffusion process by gradually adding noise to data points and learns the reverse denoising process to generate new samples, has been shown to handle complex data distribution. Despite its recent success in image synthesis, applying DPMs to video generation is still challenging due to high-dimensional data spaces. Previous methods usually adopt a standard diffusion process, where frames in the same video clip are destroyed with independent noises, ignoring the content redundancy and temporal correlation. This work presents a decomposed diffusion process via resolving the per-frame noise into a base noise that is shared among all frames and a residual noise that varies along the time axis. The denoising pipeline employs two jointly-learned networks to match the noise decomposition accordingly. Experiments on various datasets confirm that our approach, termed as VideoFusion, surpasses both GAN-based and diffusion-based alternatives in high-quality video generation. We further show that our decomposed formulation can benefit from pre-trained image diffusion models and well-support text-conditioned video creation.*\n\n\nYou can find additional information about Text-to-Video on the project page, original codebase, and try it out in a demo. Official checkpoints can be found at damo-vilab and cerspense.\n\n### \n`text-to-video-ms-1.7b`\n\n\nLet’s start by generating a short video with the default length of 16 frames (2s at 8 fps):\n\n```\nimport torch\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\")\npipe = pipe.to(\"cuda\")\n\nprompt = \"Spiderman is surfing\"\nvideo_frames = pipe(prompt).frames\nvideo_path = export_to_video(video_frames)\nvideo_path\n```\n\nDiffusers\n\n==================\n Document 5 \n----------------\n \nWürstchen\n\n\n\nWürstchen: Efficient Pretraining of Text-to-Image Models is by Pablo Pernias, Dominic Rampas, and Marc Aubreville.\n\n\n*We introduce Würstchen, a novel technique for text-to-image synthesis that unites competitive performance with unprecedented cost-effectiveness and ease of training on constrained hardware. Building on recent advancements in machine learning, our approach, which utilizes latent diffusion strategies at strong latent image compression rates, significantly reduces the computational burden, typically associated with state-of-the-art models, while preserving, if not enhancing, the quality of generated images. Wuerstchen achieves notable speed improvements at inference time, thereby rendering real-time applications more viable. One of the key advantages of our method lies in its modest training requirements of only 9,200 GPU hours, slashing the usual costs significantly without compromising the end performance. In a comparison against the state-of-the-art, we found the approach to yield strong competitiveness. This paper opens the door to a new line of research that prioritizes both performance and computational accessibility, hence democratizing the use of sophisticated AI technologies. Through Wuerstchen, we demonstrate a compelling stride forward in the realm of text-to-image synthesis, offering an innovative path to explore in future research.*\n\n## \nWürstchen v2 comes to Diffusers\n\n\nAfter the initial paper release, we have improved numerous things in the architecture, training and sampling, making Würstchen competetive to current state-of-the-art models in many ways. We are excited to release this new version together"
    },
    {
        "instruction": "Create a Python program that uses the 'emoji' API to check if a given text contains any specific emoji codes. The program should take a text and a list of emoji codes as input and return True if the text contains any emojis that match the specified codes, and False otherwise. Run the program on a list of sample texts containing emojis and non-emojis, and specify a list of emoji codes to check for each text.",
        "api": "emoji",
        "output": "#!pip install emoji\nimport emoji\n\ndef has_emoji_code(txt, codes):\n  for char in txt:\n    for code in codes:\n      if emoji.demojize(char) == code:\n        return True\n  return False\n\ntxts = [\"I am so happy 😀\", \n          \"Python is :thumbs_up:\", \n          \"Funny 😂\", \n          \"I liked it :red_heart:\",\n          \"I am so happy\",\n          \"This is a normal text\"]\n\ncodes_to_check = [\":thumbs_up:\", \":red_heart:\", \":smile:\"]\n\nfor txt in txts:\n  print(has_emoji_code(txt, codes_to_check))\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n API Reference¶\n\n\n| Table of Contents |  |\n| --- | --- |\n| **Functions:** |  |\n| `emojize()` | Replace emoji names with Unicode codes |\n| `demojize()` | Replace Unicode emoji with emoji shortcodes |\n| `analyze()` | Find Unicode emoji in a string |\n| `replace\\_emoji()` | Replace Unicode emoji with a customizable string |\n| `emoji\\_list()` | Location of all emoji in a string |\n| `distinct\\_emoji\\_list()` | Distinct list of emojis in the string |\n| `emoji\\_count()` | Number of emojis in a string |\n| `is\\_emoji()` | Check if a string/character is a single emoji |\n| `purely\\_emoji()` | Check if a string contains only emojis |\n| `version()` | Find Unicode/Emoji version of an emoji |\n| **Module variables:** |  |\n| `EMOJI\\_DATA` | Dict of all emoji |\n| `STATUS` | Dict of Unicode/Emoji status |\n| `config` | Module wide configuration |\n| **Classes:** |  |\n| `EmojiMatch` |  |\n| `EmojiMatchZWJ` |  |\n| `EmojiMatchZWJNonRGI` |  |\n| `Token` |  |\n## emoji for Python¶\n\n\nemoji terminal output for Python.\n\n```\n>>> import emoji\n>>> print(emoji.emojize('Python is :thumbsup:', language='alias'))\nPython is 👍\n>>> print(emoji.emojize('Python is :thumbs\\_up:'))\nPython is 👍\n\n```\n\n*class* emoji.EmojiMatch(*emoji: str*, *start: int*, *end: int*, *data: dict | None*)[source]¶\nRepresents a match of a “recommended for general interchange” (RGI)\nemoji\n\n==================\n Document 1 \n----------------\n\n# API Reference¶\n\n\n| Table of Contents |  |\n| --- | --- |\n| **Functions:** |  |\n| `emojize()` | Replace emoji names with Unicode codes |\n| `demojize()` | Replace Unicode emoji with emoji shortcodes |\n| `analyze()` | Find Unicode emoji\n\n==================\n Document 2 \n----------------\n# Emoji version¶\n\n\nEvery emoji in `emoji.EMOJI\\_DATA` has a version number. The number refers to the release of\nthat emoji in the Unicode Standard.\nIt is stored in the key `'E'`. For example the emoji 🥇 `:1st\\_place\\_medal:` is version\n`E3.0` that is Emoji 3.0 or Unicode 9.0:\n\n```\n>>> emoji.EMOJI\\_DATA['🥇']['E']\n3\n\n\nFor more information see http://www.unicode.org/reports/tr51/#Versioning\n\n\nThe following table lists all versions, the number that is used in `emoji.EMOJI\\_DATA` in\nthe “Data File Comment” column:\n\nUnicode/Emoji Version (emoji/unicode\\_codes/data\\_dict.py)¶\n\n +----------------+-------------+------------------+-------------------+\n | Emoji Version  |    Date     | Unicode Version  | Data File Comment |\n +----------------+-------------+------------------+-------------------+\n | N/A            | 2010-10-11  | Unicode 6.0      | E0.6              |\n | N/A            | 2014-06-16  | Unicode 7.0      | E0.7              |\n | Emoji 1.0      | 2015-06-09  | Unicode 8.0      | E1.0              |\n | Emoji 2.0      | 2015-11-12  | Unicode 8.0      | E2.0              |\n | Emoji 3.0      | 2016-06-03  | Unicode 9.0      | E3.0              |\n | Emoji 4.0      | 2016-11-22  | Unicode 9.0      | E4.0              |\n | Emoji 5.0      | 2017-06-20  | Unicode 10.0     | E5.0              |\n | Emoji 11.0     | 2018-05-21  | Unicode 11.0     | E11.0             |\n | Emoji 12.0     | 2019-03-05  | Unicode 12.0     | E12.0             |\n | Emoji 12.1     | 2019-10-21  | Unicode 12.1     | E12.1             |\n | Emoji 13.0     | 2020-03-10  | Unicode 13.0     | E13.0             |\n | Emoji 13.1     | 2020-09-15  | Unicode 13.0     | E13.1             |\n | Emoji 14.0     | 2021-09-14  | Unicode 14.0     | E14.0             |\n | Emoji 15.0     | 2022-09-13  | Unicode 15.0     | E15.0             |\n\n                        http://www.unicode.org/reports/tr51/#Versioning\n\n==================\n Document 3 \n----------------\n# emoji for Python¶\n\n\nemoji terminal output for Python.\n\n```\n>>> import emoji\n>>> print(emoji.emojize('Python is :thumbsup:', language='alias'))\nPython is 👍\n>>> print(emoji.emojize('Python is :thumbs\\_up:'))\nPython is 👍\n\n```\n\n*class* emoji.EmojiMatch(*emoji: str*, *start: int*, *end: int*, *data: dict | None*)[source]¶\nRepresents a match of a “recommended for general interchange” (RGI)\nemoji in a string.\n\n\ndata¶\nThe entry from `EMOJI\\_DATA` for this emoji or `None` if the emoji is non-RGI\n\ndata\\_copy() → Dict[str, Any][source]¶\nReturns a copy of the data from `EMOJI\\_DATA` for this match\nwith the additional keys `match\\_start` and `match\\_end`.\n\nemoji¶\nThe emoji substring\n\nend¶\nThe end index of the match in the string\n\nis\\_zwj() → bool[source]¶\nChecks if this is a ZWJ-emoji.\n\nReturns:\nTrue if this is a ZWJ-emoji, False otherwise\n\nsplit() → EmojiMatchZWJ | EmojiMatch[source]¶\nSplits a ZWJ-emoji into its constituents.\n\nReturns:\nAn `EmojiMatchZWJ` containing the “sub-emoji” if this is a ZWJ-emoji, otherwise self\n\nstart¶\nThe start index of the match in the string\n\n\n*class* emoji.EmojiMatchZWJ(*match: EmojiMatch*)[source]¶\nRepresents a match of multiple emoji in a string that were joined by\nzero-width-joiners (ZWJ/`\\u200D`).\n\n\nemojis¶\nList of sub emoji as EmojiMatch objects\n\njoin() → str[source]¶\nJoins a ZWJ-emoji into a string\n\nsplit() → EmojiMatchZWJ[source]¶\nSplits a ZWJ-emoji into its constituents.\n\n\n*class* emoji.EmojiMatchZWJNonRGI(*first\\_emoji\\_match: EmojiMatch*, *second\\_emoji\\_match: EmojiMatch*)[source]¶\nRepresents a match of multiple emoji in a string that were joined by\nzero-width-joiners (ZWJ/`\\u200D`). This class is only used for emoji\nthat are not “recommended for general interchange” (non-RGI) by Unicode.org.\nThe data property of this class is always None.\n\n\n*class* emoji.Token(*chars: str*, *value: str | EmojiMatch*)[source]¶\nA named tuple containing the matched string and its `EmojiMatch` object if it is an emoji\nor a single character that is not a unicode emoji.\n\n\nchars*: str*¶\nAlias for field number 0\n\nvalue*: str | EmojiMatch*¶\nAlias for field number 1\n\n\nemoji.analyze(*string: str*, *non\\_emoji: bool = False*, *join\\_emoji: bool = True*) → Iterator[Token][source]¶\nFind unicode emoji in a string. Yield each emoji as a named tuple\n`Token` `(chars, EmojiMatch)` or :class:`Token `(chars, EmojiMatchZWJNonRGI)`.\nIf `non\\_emoji` is True, also yield all other characters as\n`Token` `(char, char)` .\n\nParameters:\n* **string** – String to analyze\n* **non\\_emoji** – If True also yield all non-emoji characters as Token(char, char)\n* **join\\_emoji** – If True, multiple EmojiMatch are merged into a single\nEmojiMatchZWJNonRGI if they are separated only by a ZWJ.\n\n*class* emoji.config[source]¶\nModule-wide configuration\n\n\ndemojize\\_keep\\_zwj *= True*¶\nChange the behavior of `emoji.demojize()` regarding\nzero-width-joiners (ZWJ/`\\u200D`) in emoji that are not\n“recommended for general interchange” (non-RGI).\nIt has no effect on RGI emoji.\n\n\nFor example this family emoji with different skin tones “👨‍👩🏿‍👧🏻‍👦🏾” contains four\nperson emoji that are joined together by three ZWJ characters:\n`👨\\u200D👩🏿\\u200D👧🏻\\u200D👦🏾`\n\n\nIf `True`, the zero-width-joiners will be kept and `emoji.emojize()` can\nreverse the `emoji.demojize()` operation:\n`emoji.emojize(emoji.demojize(s)) == s`\n\n\nThe example emoji would be converted to\n`:man:\\u200d:woman\\_dark\\_skin\\_tone:\\u200d:girl\\_light\\_skin\\_tone:\\u200d:boy\\_medium-dark\\_skin\\_tone:`\n\n\nIf `False`, the zero-width-joiners will be removed and `emoji.emojize()`\ncan only reverse the individual emoji: `emoji.emojize(emoji.demojize(s)) != s`\n\n\nThe example emoji would be converted to\n`:man::woman\\_dark\\_skin\\_tone::girl\\_light\\_skin\\_tone::boy\\_medium-dark\\_skin\\_tone:`\n\nreplace\\_emoji\\_keep\\_zwj *= False*¶\nChange the behavior of `emoji.replace\\_emoji()` regarding\nzero-width-joiners (ZWJ/`\\u200D`) in emoji that are not\n“recommended for general interchange” (non-RGI).\nIt has no effect on RGI emoji.\n\n\nSee `config.demojize\\_keep\\_zwj` for more information.\n\n\nemoji.demojize(*string*, *delimiters=(':', ':')*, *language='en'*, *version=None*, *handle\\_version=None*)[source]¶\n\nReplace Unicode emoji in a string with emoji shortcodes. Useful for storage.\n```\n>>> import emoji\n>>> print(emoji.emojize(\"Python is fun :thumbs\\_up:\"))\nPython is fun 👍\n>>> print(emoji.demojize(\"Python is fun 👍\"))\nPython is fun :thumbs\\_up:\n>>> print(emoji.demojize(\"icode is tricky 😯\", delimiters=(\"\\_\\_\", \"\\_\\_\")))\nUnicode is tricky \\_\\_hushed\\_face\\_\\_\n\nParameters:\n* **string** – String contains Unicode characters. MUST BE UNICODE.\n* **delimiters** – (optional) User delimiters other than `\\_DEFAULT\\_DELIMITER`\n* **language** – Choose language of emoji name: language code ‘es’, ‘de’, etc. or ‘alias’\nto use English aliases\n* **version** – (optional) Max version. If set to an Emoji Version,\nall emoji above this version will be removed.\n* **handle\\_version** – (optional) Replace the emoji above `version`\ninstead of removing it. handle\\_version can be either a string or a\ncallable `handle\\_version(emj: str, data: dict) -> str`; If it is\na callable, it’s passed the Unicode emoji and the data dict from\n`EMOJI\\_DATA` and must return a replacement string to be used.\nThe passed data is in the form of:\n\n```\nhandle\\_version('\\U0001F6EB', {\n    'en' : ':airplane\\_departure:',\n    'status' : fully\\_qualified,\n    'E' : 1,\n    'alias' : [':flight\\_departure:'],\n    'de': ':abflug:',\n    'es': ':avión\\_despegando:',\n    ...\n})\n\nemoji.distinct\\_emoji\\_list(*string*)[source]¶\nReturns distinct list of emojis from the string.\n\nemoji.emoji\\_count(*string*, *unique=False*)[source]¶\nReturns the count of emojis in a string.\n\nParameters:\n**unique** – (optional) True if count only unique emojis\n\nemoji.emoji\\_list(*string*)[source]¶\n\nReturns the location and emoji in list of dict format.\n```\n>>> emoji.emoji\\_list(\"Hi, I am fine. 😁\")\n[{'match\\_start': 15, 'match\\_end': 16, 'emoji': '😁'}]\n\nemoji.emojize(*string*, *delimiters=(':', ':')*, *variant=None*, *language='en'*, *version=None*, *handle\\_version=None*)[source]¶\n\nReplace emoji names in a string with Unicode codes.\n```\n>>> import emoji\n>>> print(emoji.emojize(\"Python is fun :thumbsup:\", language='alias'))\nPython is fun 👍\n>>> print(emoji.emojize(\"Python is fun :thumbs\\_up:\"))\nPython is fun 👍\n>>> print(emoji.emojize(\"Python is fun {thumbs\\_up}\", delimiters = (\"{\", \"}\")))\nPython is fun 👍\n>>> print(emoji.emojize(\"Python is fun :red\\_heart:\", variant=\"text\\_type\"))\nPython is fun ❤\n>>> print(emoji.emojize(\"Python is fun :red\\_heart:\", variant=\"emoji\\_type\"))\nPython is fun ❤️ # red heart, not black heart\n\nParameters:\n* **string** – String contains emoji names.\n* **delimiters** – (optional) Use delimiters other than \\_DEFAULT\\_DELIMITER. Each delimiter\nshould contain at least one character that is not part of a-zA-Z0-9 and `\\_-&.()!?#\\*+,`.\nSee `emoji.core.\\_EMOJI\\_NAME\\_PATTERN` for the regular expression of unsafe characters.\n* **variant** – (optional) Choose variation selector between “base”(None), VS-15 (“text\\_type”) and VS-16 (“emoji\\_type”)\n* **language** – Choose language of emoji name: language code ‘es’, ‘de’, etc. or ‘alias’\nto use English aliases\n* **version** – (optional) Max version. If set to an Emoji Version,\nall emoji above this version will be ignored.\n* **handle\\_version** – (optional) Replace the emoji above `version`\ninstead of ignoring it. handle\\_version can be either a string or a\ncallable; If it is a callable, it’s passed the Unicode emoji and the\ndata dict from `EMOJI\\_DATA` and must return a replacement string\nto be used:\n\nRaises:\n**ValueError** – if `variant` is neither None, ‘text\\_type’ or ‘emoji\\_type’\n\nemoji.is\\_emoji(*string*)[source]¶\nReturns True if the string is a single emoji, and it is “recommended for\ngeneral interchange” by Unicode.org.\n\nemoji.purely\\_emoji(*string: str*) → bool[source]¶\nReturns True if the string contains only emojis.\nThis might not imply that is\\_emoji for all the characters, for example,\nif the string contains variation selectors.\n\nemoji.replace\\_emoji(*string*, *replace=''*, *version=-1*)[source]¶\nReplace Unicode emoji in a customizable string.\n\nParameters:\n* **string** – String contains Unicode characters. MUST BE UNICODE.\n* **replace** – (optional) replace can be either a string or a callable;\nIf it is a callable, it’s passed the Unicode emoji and the data dict from\n`EMOJI\\_DATA` and must return a replacement string to be used.\nreplace(str, dict) -> str\n* **version** – (optional) Max version. If set to an Emoji Version,\nonly emoji above this version will be replaced.\n\nemoji.version(*string*)[source]¶\nReturns the Emoji Version of the emoji.\n\nSee https://www.unicode.org/reports/tr51/#Versioning for more information.\n```\n>>> emoji.version(\"😁\")\n0.6\n>>> emoji.version(\":butterfly:\")\n3\n\nParameters:\n**string** – An emoji or a text containing an emoji\n\nRaises:\n**ValueError** – if `string` does not contain an emoji\n\n\n## EMOJI\\_DATA¶\n\n\nemoji.EMOJI\\_DATA*: dict*¶\n\n> \n> Contains all emoji as keys and their names, Unicode version and status\n> \n> \n> \n> ```\n> EMOJI\\_DATA = {\n>   '🥇': {\n>       'en' : ':1st\\_place\\_medal:',\n>       'status' : emoji.STATUS[\"fully\\_qualified\"],\n>       'E' : 3,\n>       'de': ':goldmedaille:',\n>       'es': ':medalla\\_de\\_oro:',\n>       'fr': ':médaille\\_d’or:',\n>       'pt': ':medalha\\_de\\_ouro:',\n>       'it': ':medaglia\\_d’oro:'\n>   },\n>   ...\n> }\n> \n> ```\n> \n> \n> \n\n**Source code:** emoji/unicode\\_codes/data\\_dict.py **(2MB)**\n\n\n\n## Emoji status¶\n\n\nemoji.STATUS*: dict*¶\n\n> \n> The status values that are used in `emoji.EMOJI\\_DATA`.\n> \n> \n> For more information on the meaning of these values see http://www.unicode.org/reports/tr51/#Emoji\\_Implementation\\_Notes\n> \n> \n> \n\nemoji/unicode\\_codes/data\\_dict.py¶\n\n```\ncomponent = 1\nfully\\_qualified = 2\nminimally\\_qualified = 3\nunqualified = 4\n\nSTATUS = {\n    \"component\": component,\n    \"fully\\_qualified\": fully\\_qualified,\n    \"minimally\\_qualified\": minimally\\_qualified,\n    \"unqualified\": unqualified\n}\n\nLANGUAGES = ['en', 'es', 'ja', 'ko', 'pt', 'it', 'fr', 'de', 'fa', 'id', 'zh']\n\n\n```\n\n\n## Emoji version¶\n\n\nEvery emoji in `emoji.EMOJI\\_DATA` has a version number. The number refers to the release of\nthat emoji in the Unicode Standard.\nIt is stored in the key `'E'`. For example the emoji 🥇 `:1st\\_place\\_medal:` is version\n`E3.0` that is Emoji 3.0"
    },
    {
        "instruction": "Create a Python program that uses the 'emoji' API to check if a given text contains any specific emoji names. The program should take a text and a list of emoji names as input and return True if the text contains any emojis that match the specified names, and False otherwise. Run the program on a list of sample texts containing emojis and non-emojis, and specify a list of emoji names to check for each text.",
        "api": "emoji",
        "output": "#!pip install emoji\nimport emoji\n\ndef has_emoji_name(txt, names):\n  for char in txt:\n    for name in names:\n      if emoji.demojize(char).lower().find(name.lower()) != -1:\n        return True\n  return False\n\ntxts = [\"I am so happy 😀\", \n          \"Python is :thumbs_up:\", \n          \"Funny 😂\", \n          \"I liked it :red_heart:\",\n          \"I am so happy\",\n          \"This is a normal text\"]\n\nnames_to_check = [\"happy\", \"thumbs\", \"red\"]\n\nfor txt in txts:\n  print(has_emoji_name(txt, names_to_check))\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n API Reference¶\n\n\n| Table of Contents |  |\n| --- | --- |\n| **Functions:** |  |\n| `emojize()` | Replace emoji names with Unicode codes |\n| `demojize()` | Replace Unicode emoji with emoji shortcodes |\n| `analyze()` | Find Unicode emoji in a string |\n| `replace\\_emoji()` | Replace Unicode emoji with a customizable string |\n| `emoji\\_list()` | Location of all emoji in a string |\n| `distinct\\_emoji\\_list()` | Distinct list of emojis in the string |\n| `emoji\\_count()` | Number of emojis in a string |\n| `is\\_emoji()` | Check if a string/character is a single emoji |\n| `purely\\_emoji()` | Check if a string contains only emojis |\n| `version()` | Find Unicode/Emoji version of an emoji |\n| **Module variables:** |  |\n| `EMOJI\\_DATA` | Dict of all emoji |\n| `STATUS` | Dict of Unicode/Emoji status |\n| `config` | Module wide configuration |\n| **Classes:** |  |\n| `EmojiMatch` |  |\n| `EmojiMatchZWJ` |  |\n| `EmojiMatchZWJNonRGI` |  |\n| `Token` |  |\n## emoji for Python¶\n\n\nemoji terminal output for Python.\n\n```\n>>> import emoji\n>>> print(emoji.emojize('Python is :thumbsup:', language='alias'))\nPython is 👍\n>>> print(emoji.emojize('Python is :thumbs\\_up:'))\nPython is 👍\n\n```\n\n*class* emoji.EmojiMatch(*emoji: str*, *start: int*, *end: int*, *data: dict | None*)[source]¶\nRepresents a match of a “recommended for general interchange” (RGI)\nemoji\n\n==================\n Document 1 \n----------------\n\n# API Reference¶\n\n\n| Table of Contents |  |\n| --- | --- |\n| **Functions:** |  |\n| `emojize()` | Replace emoji names with Unicode codes |\n| `demojize()` | Replace Unicode emoji with emoji shortcodes |\n| `analyze()` | Find Unicode emoji\n\n==================\n Document 2 \n----------------\n# Emoji version¶\n\n\nEvery emoji in `emoji.EMOJI\\_DATA` has a version number. The number refers to the release of\nthat emoji in the Unicode Standard.\nIt is stored in the key `'E'`. For example the emoji 🥇 `:1st\\_place\\_medal:` is version\n`E3.0` that is Emoji 3.0 or Unicode 9.0:\n\n```\n>>> emoji.EMOJI\\_DATA['🥇']['E']\n3\n\n\nFor more information see http://www.unicode.org/reports/tr51/#Versioning\n\n\nThe following table lists all versions, the number that is used in `emoji.EMOJI\\_DATA` in\nthe “Data File Comment” column:\n\nUnicode/Emoji Version (emoji/unicode\\_codes/data\\_dict.py)¶\n\n +----------------+-------------+------------------+-------------------+\n | Emoji Version  |    Date     | Unicode Version  | Data File Comment |\n +----------------+-------------+------------------+-------------------+\n | N/A            | 2010-10-11  | Unicode 6.0      | E0.6              |\n | N/A            | 2014-06-16  | Unicode 7.0      | E0.7              |\n | Emoji 1.0      | 2015-06-09  | Unicode 8.0      | E1.0              |\n | Emoji 2.0      | 2015-11-12  | Unicode 8.0      | E2.0              |\n | Emoji 3.0      | 2016-06-03  | Unicode 9.0      | E3.0              |\n | Emoji 4.0      | 2016-11-22  | Unicode 9.0      | E4.0              |\n | Emoji 5.0      | 2017-06-20  | Unicode 10.0     | E5.0              |\n | Emoji 11.0     | 2018-05-21  | Unicode 11.0     | E11.0             |\n | Emoji 12.0     | 2019-03-05  | Unicode 12.0     | E12.0             |\n | Emoji 12.1     | 2019-10-21  | Unicode 12.1     | E12.1             |\n | Emoji 13.0     | 2020-03-10  | Unicode 13.0     | E13.0             |\n | Emoji 13.1     | 2020-09-15  | Unicode 13.0     | E13.1             |\n | Emoji 14.0     | 2021-09-14  | Unicode 14.0     | E14.0             |\n | Emoji 15.0     | 2022-09-13  | Unicode 15.0     | E15.0             |\n\n                        http://www.unicode.org/reports/tr51/#Versioning"
    },
    {
        "instruction": "Create a Python program that uses the 'evaluate' API to perform sentiment analysis on a dataset. The program should load the IMDb dataset, shuffle it, select the first 1000 examples, and then use the 'lvwerra/distilbert-imdb' model to classify the text as either 'NEGATIVE' or 'POSITIVE.' The results should be printed.",
        "api": "evaluate",
        "output": "#!pip install datasets\n#!pip install evaluate\nfrom datasets import load_dataset\nfrom evaluate import evaluator\n\ndata = load_dataset(\"imdb\", split=\"test\").shuffle(seed=42).select(range(1000))\ntask_evaluator = evaluator(\"text-classification\")\n\n# Pass a model name or path\neval_results = task_evaluator.compute(\n    model_or_pipeline=\"lvwerra/distilbert-imdb\",\n    data=data,\n    label_mapping={\"NEGATIVE\": 0, \"POSITIVE\": 1}\n)\n\nprint(eval_results)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n## class evaluate.AutomaticSpeechRecognitionEvaluator\n\n<\nsource\n>\n(\ntask = 'automatic-speech-recognition'\ndefault\\_metric\\_name = None\n\nAutomatic speech recognition evaluator.\nThis automatic speech recognition evaluator can currently be loaded from evaluator() using the default task name\n`automatic-speech-recognition`.\nMethods in this class assume a data format compatible with the `AutomaticSpeechRecognitionPipeline`.\n\n<\nsource\n>\n(\nmodel\\_or\\_pipeline: typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None\ndata: typing.Union[str, datasets.arrow\\_dataset.Dataset] = None\nsubset: typing.Optional[str] = None\nsplit: typing.Optional[str] = None\nmetric: typing.Union[str, evaluate.module.EvaluationModule] = None\ntokenizer: typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None\nstrategy: typing.Literal['simple', 'bootstrap'] = 'simple'\nconfidence\\_level: float = 0.95\nn\\_resamples: int = 9999\ndevice: int = None\nrandom\\_state: typing.Optional[int] = None\ninput\\_column: str = 'path'\nlabel\\_column: str = 'sentence'\ngeneration\\_kwargs: dict = None\n\n```\n>>> from evaluate import evaluator\n>>> from datasets import load_dataset\n>>> task_evaluator = evaluator(\"automatic-speech-recognition\")\n>>> data = load_dataset(\"mozilla-foundation/common\\_voice\\_11\\_0\", \"en\", split=\"validation[:40]\")\n>>> results = task_evaluator.compute(\n>>>     model_or_pipeline=\"https://huggingface.co/openai/whisper-tiny.en\",\n>>>     data=data,\n>>>     input_column=\"path\",\n>>>     label_column=\"sentence\",\n>>>     metric=\"wer\",\n>>> )\n```\n\n# \nVisualization methods\n\n\nMethods for visualizing evaluations results:\n\n\n\n## \nRadar Plot\n\n#### evaluate.visualization.radar\\_plot\n\n<\nsource\n>\n(\ndata\nmodel\\_names\ninvert\\_range = []\nconfig = None\nfig = None\n\n\n* **data** (`List[dict]`) — the results (list of metric + value pairs).\nE.g. data = [{“accuracy”: 0.9, “precision”:0.8},{“accuracy”: 0.7, “precision”:0.6}]\n* **names** (`List[dict]`) — model names.\nE.g. names = [“model1”, “model 2”, …]\n* **invert\\_range** (`List[dict]`, optional)\n\n==================\n Document 1 \n----------------\n## class evaluate.QuestionAnsweringEvaluator\n\n<\nsource\n>\n(\ntask = 'question-answering'\ndefault\\_metric\\_name = None\n\nQuestion answering evaluator. This evaluator handles\n**extractive** question answering,\nwhere the answer to the question is extracted from a context.\n\n\nThis question answering evaluator can currently be loaded from evaluator() using the default task name\n`question-answering`.\n\n\nMethods in this class assume a data format compatible with the\n`QuestionAnsweringPipeline`.\n\n<\nsource\n>\n(\nmodel\\_or\\_pipeline: typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None\ndata: typing.Union[str, datasets.arrow\\_dataset.Dataset] = None\nsubset: typing.Optional[str] = None\nsplit: typing.Optional[str] = None\nmetric: typing.Union[str, evaluate.module.EvaluationModule] = None\ntokenizer: typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None\nstrategy: typing.Literal['simple', 'bootstrap'] = 'simple'\nconfidence\\_level: float = 0.95\nn\\_resamples: int = 9999\ndevice: int = None\nrandom\\_state: typing.Optional[int] = None\nquestion\\_column: str = 'question'\ncontext\\_column: str = 'context'\nid\\_column: str = 'id'\nlabel\\_column: str = 'answers'\nsquad\\_v2\\_format: typing.Optional[bool] = None\n\n```\n>>> from evaluate import evaluator\n>>> from datasets import load_dataset\n>>> task_evaluator = evaluator(\"question-answering\")\n>>> data = load_dataset(\"squad\", split=\"validation[:2]\")\n>>> results = task_evaluator.compute(\n>>>     model_or_pipeline=\"sshleifer/tiny-distilbert-base-cased-distilled-squad\",\n>>>     data=data,\n>>>     metric=\"squad\",\n>>> )\n```\n\nDatasets where the answer may be missing in the context are supported, for example SQuAD v2 dataset. In this case, it is safer to pass `squad_v2_format=True` to\nthe compute() call.\n\n Copied\n\n```\n>>> from evaluate import evaluator\n>>> from datasets import load_dataset\n>>> task_evaluator = evaluator(\"question-answering\")\n>>> data = load_dataset(\"squad\\_v2\", split=\"validation[:2]\")\n>>> results = task_evaluator.compute(\n>>>     model_or_pipeline=\"mrm8488/bert-tiny-finetuned-squadv2\",\n>>>     data=data,\n>>>     metric=\"squad\\_v2\",\n>>>     squad_v2_format=True,\n>>> )\n```\n\n### \nTextClassificationEvaluator\n\n### class evaluate.TextClassificationEvaluator\n\n<\nsource\n>\n(\ntask = 'text-classification'\ndefault\\_metric\\_name = None\n\nText classification evaluator.\nThis text classification evaluator can currently be loaded from evaluator() using the default task name\n`text-classification` or with a `\"sentiment-analysis\"` alias.\nMethods in this class assume a data format compatible with the `TextClassificationPipeline` -\n\n==================\n Document 2 \n----------------\n## class evaluate.Text2TextGenerationEvaluator\n\n<\nsource\n>\n(\ntask = 'text2text-generation'\ndefault\\_metric\\_name = None\n\nText2Text generation evaluator.\nThis Text2Text generation evaluator can currently be loaded from evaluator() using the default task name\n`text2text-generation`.\nMethods in this class assume a data format compatible with the `Text2TextGenerationPipeline`.\n\n<\nsource\n>\n(\nmodel\\_or\\_pipeline: typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None\ndata: typing.Union[str, datasets.arrow\\_dataset.Dataset] = None\nsubset: typing.Optional[str] = None\nsplit: typing.Optional[str] = None\nmetric: typing.Union[str, evaluate.module.EvaluationModule] = None\ntokenizer: typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None\nstrategy: typing.Literal['simple', 'bootstrap'] = 'simple'\nconfidence\\_level: float = 0.95\nn\\_resamples: int = 9999\ndevice: int = None\nrandom\\_state: typing.Optional[int] = None\ninput\\_column: str = 'text'\nlabel\\_column: str = 'label'\ngeneration\\_kwargs: dict = None\n\n\n* **model\\_or\\_pipeline** (`str` or `Pipeline` or `Callable` or `PreTrainedModel` or `TFPreTrainedModel`, defaults to `None`) —\nIf the argument in not specified, we initialize the default pipeline for the task (in this case\n`text-classification` or its alias - `sentiment-analysis`). If the argument is of the type `str` or\nis a model instance, we use it to initialize a new `Pipeline` with the given model. Otherwise we assume the\nargument specifies a pre-initialized pipeline.\n* **data** (`str` or `Dataset`, defaults to `None`) —\nSpecifies the dataset we will run evaluation on. If it is of type `str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.\n* **subset** (`str`, defaults to `None`) —\nDefines which dataset subset to load. If `None` is passed the default subset is loaded.\n* **split** (`str`, defaults to `None`) —\nDefines which dataset split to load. If `None` is passed, infers based on the `choose_split` function.\n* **metric** (`str` or `EvaluationModule`, defaults to `None`) —\nSpecifies the metric we use in evaluator. If it is of type `str`, we treat it as the metric name, and\nload it. Otherwise we assume it represents a pre-loaded metric.\n* **tokenizer** (`str` or `PreTrainedTokenizer`, *optional*, defaults to `None`) —\nArgument can be used to overwrite a default tokenizer if `model_or_pipeline` represents a model for\nwhich we build a pipeline. If `model_or_pipeline` is `None` or a pre-initialized pipeline, we ignore\nthis argument.\n* **strategy** (`Literal[\"simple\", \"bootstrap\"]`, defaults to “simple”) —\nspecifies the evaluation strategy. Possible values are:\n\n\t+ `\"simple\"` - we evaluate the metric and return the scores.\n\t+ `\"bootstrap\"` - on top of computing the metric scores, we calculate the confidence interval for each\n\tof the returned metric keys, using `scipy`’s `bootstrap` method\n\thttps://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html.\n* **confidence\\_level** (`float`, defaults to `0.95`) —\nThe `confidence_level` value passed to `bootstrap` if `\"bootstrap\"` strategy is chosen.\n* **n\\_resamples** (`int`, defaults to `9999`) —\nThe `n_resamples` value passed to `bootstrap` if `\"bootstrap\"` strategy is chosen.\n* **device** (`int`, defaults to `None`) —\nDevice ordinal for CPU/GPU support of the pipeline. Setting this to -1 will leverage CPU, a positive\ninteger will run the model on the associated CUDA device ID. If `None` is provided it will be inferred and\nCUDA:0 used if available, CPU otherwise.\n* **random\\_state** (`int`, *optional*, defaults to `None`) —\nThe `random_state` value passed to `bootstrap` if `\"bootstrap\"` strategy is chosen. Useful for\ndebugging.\n* **input\\_column** (`str`, defaults to `\"text\"`) —\nthe name of the column containing the input text in the dataset specified by `data`.\n* **label\\_column** (`str`, defaults to `\"label\"`) —\nthe name of the column containing the labels in the dataset specified by `data`.\n* **generation\\_kwargs** (`Dict`, *optional*, defaults to `None`) —\nThe generation kwargs are passed to the pipeline and set the text generation strategy.\n\n\n### \nSummarizationEvaluator\n\n\n### class evaluate.SummarizationEvaluator\n\n<\nsource\n>\n(\ntask = 'summarization'\ndefault\\_metric\\_name = None\n\nText summarization evaluator.\nThis text summarization evaluator can currently be loaded from evaluator() using the default task name\n`summarization`.\nMethods in this class assume a data format compatible with the SummarizationEvaluator.\n\n\n\n### \nTranslationEvaluator\n\n\n### class evaluate.TranslationEvaluator\n\n<\nsource\n>\n(\ntask = 'translation'\ndefault\\_metric\\_name = None\n\nTranslation evaluator.\nThis translation generation evaluator can currently be loaded from evaluator() using the default task name\n`translation`.\nMethods in this class assume a data format compatible with the `TranslationPipeline`.\n\n\n\n### \nAutomaticSpeechRecognitionEvaluator\n\n### class evaluate.AutomaticSpeechRecognitionEvaluator\n\n<\nsource\n>\n(\ntask = 'automatic-speech-recognition'\ndefault\\_metric\\_name = None\n\nAutomatic speech recognition evaluator.\nThis automatic speech recognition evaluator can currently be loaded from evaluator() using the default task name\n`automatic-speech-recognition`.\nMethods in this class assume a data format compatible with the `AutomaticSpeechRecognitionPipeline`.\n\n<\nsource\n>\n(\nmodel\\_or\\_pipeline: typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'),\n\n==================\n Document 3 \n----------------\n## class evaluate.TextClassificationEvaluator\n\n<\nsource\n>\n(\ntask = 'text-classification'\ndefault\\_metric\\_name = None\n\nText classification evaluator.\nThis text classification evaluator can currently be loaded from evaluator() using the default task name\n`text-classification` or with a `\"sentiment-analysis\"` alias.\nMethods in this class assume a data format compatible with the `TextClassificationPipeline` - a single textual\nfeature as input and a categorical label as output.\n\n<\nsource\n>\n(\nmodel\\_or\\_pipeline: typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None\ndata: typing.Union[str, datasets.arrow\\_dataset.Dataset] = None\nsubset: typing.Optional[str] = None\nsplit: typing.Optional[str] = None\nmetric: typing.Union[str, evaluate.module.EvaluationModule] = None\ntokenizer: typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None\nfeature\\_extractor: typing.Union[str, ForwardRef('FeatureExtractionMixin'), NoneType] = None\nstrategy: typing.Literal['simple', 'bootstrap'] = 'simple'\nconfidence\\_level: float = 0.95\nn\\_resamples: int = 9999\ndevice: int = None\nrandom\\_state: typing.Optional[int] = None\ninput\\_column: str = 'text'\nsecond\\_input\\_column: typing.Optional[str] = None\nlabel\\_column: str = 'label'\nlabel\\_mapping: typing.Union[typing.Dict[str, numbers.Number], NoneType] = None\n\n```\n>>> from evaluate import evaluator\n>>> from datasets import load_dataset\n>>> task_evaluator = evaluator(\"text-classification\")\n>>> data = load_dataset(\"imdb\", split=\"test[:2]\")\n>>> results = task_evaluator.compute(\n>>>     model_or_pipeline=\"huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli\",\n>>>     data=data,\n>>>     metric=\"accuracy\",\n>>>     label_mapping={\"LABEL\\_0\": 0.0, \"LABEL\\_1\": 1.0},\n>>>     strategy=\"bootstrap\",\n>>>     n_resamples=10,\n>>>     random_state=0\n>>> )\n```\n\n### \nTokenClassificationEvaluator\n\n### class evaluate.TokenClassificationEvaluator\n\n<\nsource\n>\n(\ntask = 'token-classification'\ndefault\\_metric\\_name = None\n\nToken classification evaluator.\n\n\nThis token classification evaluator can currently be loaded from evaluator() using the default task name\n`token-classification`.\n\n\nMethods in this class assume a data format compatible with the `TokenClassificationPipeline`.\n\n<\nsource\n>\n(\nmodel\\_or\\_pipeline: typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] =\n\n==================\n Document 4 \n----------------\nLogging methods\n\n\n🤗 Evaluate strives to be transparent and explicit about how it works, but this can be quite verbose at times. We have included a series of logging methods which allow you to easily adjust the level of verbosity of the entire library. Currently the default verbosity of the library is set to `WARNING`.\n\n\nTo change the level of verbosity, use one of the direct setters. For instance, here is how to change the verbosity to the `INFO` level:\n\n```\nimport evaluate\nevaluate.logging.set_verbosity_info()\n```\n\nYou can also use the environment variable `EVALUATE_VERBOSITY` to override the default verbosity, and set it to one of the following: `debug`, `info`, `warning`, `error`, `critical`:\n\n```\nEVALUATE_VERBOSITY=error ./myprogram.py\n```\n\nAll the methods of this logging module are documented below. The main ones are:\n\n\n* logging.get\\_verbosity() to get the current level of verbosity in the logger\n* logging.set\\_verbosity() to set the verbosity to the level of your choice\n\n\nIn order from the least to the most verbose (with their corresponding `int` values):\n\n\n1. `logging.CRITICAL` or `logging.FATAL` (int value, 50): only report the most critical errors.\n2. `logging.ERROR` (int value, 40): only report errors.\n3. `logging.WARNING` or `logging.WARN` (int value, 30): only reports error and warnings. This the default level used by the library.\n4. `logging.INFO` (int value, 20): reports error, warnings and basic information.\n5. `logging.DEBUG` (int value, 10): report all information.\n\n\nBy default, `tqdm` progress bars will be displayed during evaluate download and processing. logging.disable\\_progress\\_bar() and logging.enable\\_progress\\_bar() can be used to suppress or unsuppress this behavior. \n\n\n## \nFunctions\n\n\n#### evaluate.utils.logging.get\\_verbosity\n\n<\nsource\n>\n(\n)\n\nReturn the current level for the HuggingFace datasets library’s root logger.\n\n\nHuggingFace datasets library has following logging levels:\n\n\n* *evaluate.logging.CRITICAL*, *evaluate.logging.FATAL*\n* *evaluate.logging.ERROR*\n* *evaluate.logging.WARNING*, *evaluate.logging.WARN*\n* *evaluate.logging.INFO*\n* *evaluate.logging.DEBUG*\n\n\n\n#### evaluate.utils.logging.set\\_verbosity\n\n<\nsource\n>\n(\nverbosity: int\n\nSet the level for the HuggingFace datasets library’s root logger.\n\n\n#### evaluate.utils.logging.set\\_verbosity\\_info\n\nSet the level for the HuggingFace datasets library’s root logger to INFO.\n\n\nThis will display most of the logging information and tqdm bars.\n\n\nShortcut to `evaluate.logging.set_verbosity(evaluate.logging.INFO)`\n\n\n#### evaluate.utils.logging.set\\_verbosity\\_warning\n\nSet the level for the HuggingFace datasets library’s root logger to WARNING.\n\n\nThis will display only the warning and errors logging information and tqdm bars.\n\n\nShortcut to `evaluate.logging.set_verbosity(evaluate.logging.WARNING)`\n\n\n#### evaluate.utils.logging.set\\_verbosity\\_debug\n\nSet the level for the HuggingFace datasets library’s root logger to DEBUG.\n\n\nThis will display all the logging information and tqdm bars.\n\n\nShortcut to `evaluate.logging.set_verbosity(evaluate.logging.DEBUG)`\n\n\n#### evaluate.utils.logging.set\\_verbosity\\_error\n\nSet the level for the HuggingFace datasets library’s root logger to ERROR.\n\n\nThis will display only the errors logging information and tqdm bars.\n\n\nShortcut to `evaluate.logging.set_verbosity(evaluate.logging.ERROR)`\n\n\n#### evaluate.utils.logging.disable\\_propagation\n\nDisable propagation of the library log outputs.\nNote that log propagation is disabled by default.\n\n\n#### evaluate.utils.logging.enable\\_propagation\n\nEnable propagation of the library log outputs.\nPlease disable the HuggingFace datasets library’s default handler to prevent double logging if the root logger has\nbeen configured.\n\n\n#### evaluate.utils.logging.get\\_logger\n\n<\nsource\n>\n(\nname: typing.Optional[str] = None\n\nReturn a logger with the specified name.\nThis function can be used in dataset and metrics scripts.\n\n\n#### evaluate.enable\\_progress\\_bar\n\nEnable tqdm progress bar.\n\n\n#### evaluate.disable\\_progress\\_bar\n\n\n\n## \nLevels\n\n\n\n### \nevaluate.logging.CRITICAL\n\n\nevaluate.logging.CRITICAL = 50\n\n\n\n### \nevaluate.logging.DEBUG\n\n\nevaluate.logging.DEBUG = 10\n\n\n\n### \nevaluate.logging.ERROR\n\n\nevaluate.logging.ERROR = 40\n\n\n\n### \nevaluate.logging.FATAL\n\n\nevaluate.logging.FATAL = 50\n\n\n\n### \nevaluate.logging.INFO\n\n\nevaluate.logging.INFO = 20\n\n\n\n### \nevaluate.logging.NOTSET\n\n\nevaluate.logging.NOTSET = 0\n\n\n\n### \nevaluate.logging.WARN\n\n\nevaluate.logging.WARN = 30\n\n\n\n### \nevaluate.logging.WARNING\n\n\nevaluate.logging.WARNING = 30"
    },
    {
        "instruction": "Create a Python program that uses the 'evaluate' API to perform text summarization on a given text. The program should take a text as input, use the 'facebook/bart-large-cnn' model for text summarization, and print the summarized text.",
        "api": "evaluate",
        "output": "#!pip install evaluate\nfrom evaluate import evaluator\n\n# Load the text summarization model\nsummarization_model = \"facebook/bart-large-cnn\"\n\ndef perform_summarization(text):\n    summarization_evaluator = evaluator(\"text-summarization\")\n\n    # Use the model for text summarization\n    summary = summarization_evaluator.compute(\n        model_or_pipeline=summarization_model,\n        data=text\n    )\n\n    return summary\n\n# User input\nuser_text = input(\"Enter the text for summarization: \")\nresult = perform_summarization(user_text)\n\nprint(\"Summarized Text:\")\nprint(result[0][\"summary_text\"])\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n## class evaluate.Text2TextGenerationEvaluator\n\n<\nsource\n>\n(\ntask = 'text2text-generation'\ndefault\\_metric\\_name = None\n\nText2Text generation evaluator.\nThis Text2Text generation evaluator can currently be loaded from evaluator() using the default task name\n`text2text-generation`.\nMethods in this class assume a data format compatible with the `Text2TextGenerationPipeline`.\n\n<\nsource\n>\n(\nmodel\\_or\\_pipeline: typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None\ndata: typing.Union[str, datasets.arrow\\_dataset.Dataset] = None\nsubset: typing.Optional[str] = None\nsplit: typing.Optional[str] = None\nmetric: typing.Union[str, evaluate.module.EvaluationModule] = None\ntokenizer: typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None\nstrategy: typing.Literal['simple', 'bootstrap'] = 'simple'\nconfidence\\_level: float = 0.95\nn\\_resamples: int = 9999\ndevice: int = None\nrandom\\_state: typing.Optional[int] = None\ninput\\_column: str = 'text'\nlabel\\_column: str = 'label'\ngeneration\\_kwargs: dict = None\n\n\n* **model\\_or\\_pipeline** (`str` or `Pipeline` or `Callable` or `PreTrainedModel` or `TFPreTrainedModel`, defaults to `None`) —\nIf the argument in not specified, we initialize the default pipeline for the task (in this case\n`text-classification` or its alias - `sentiment-analysis`). If the argument is of the type `str` or\nis a model instance, we use it to initialize a new `Pipeline` with the given model. Otherwise we assume the\nargument specifies a pre-initialized pipeline.\n* **data** (`str` or `Dataset`, defaults to `None`) —\nSpecifies the dataset we will run evaluation on. If it is of type `str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.\n* **subset** (`str`, defaults to `None`) —\nDefines which dataset subset to load. If `None` is passed the default subset is loaded.\n* **split** (`str`, defaults to `None`) —\nDefines which dataset split to load. If `None` is passed, infers based on the `choose_split` function.\n* **metric** (`str` or `EvaluationModule`, defaults to `None`) —\nSpecifies the metric we use in evaluator. If it is of type `str`, we treat it as the metric name, and\nload it. Otherwise we assume it represents a pre-loaded metric.\n* **tokenizer** (`str` or `PreTrainedTokenizer`, *optional*, defaults to `None`) —\nArgument can be used to overwrite a default tokenizer if `model_or_pipeline` represents a model for\nwhich we build a pipeline. If `model_or_pipeline` is `None` or a pre-initialized pipeline, we ignore\nthis argument.\n* **strategy** (`Literal[\"simple\", \"bootstrap\"]`, defaults to “simple”) —\nspecifies the evaluation strategy. Possible values are:\n\n\t+ `\"simple\"` - we evaluate the metric and return the scores.\n\t+ `\"bootstrap\"` - on top of computing the metric scores, we calculate the confidence interval for each\n\tof the returned metric keys, using `scipy`’s `bootstrap` method\n\thttps://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html.\n* **confidence\\_level** (`float`, defaults to `0.95`) —\nThe `confidence_level` value passed to `bootstrap` if `\"bootstrap\"` strategy is chosen.\n* **n\\_resamples** (`int`, defaults to `9999`) —\nThe `n_resamples` value passed to `bootstrap` if `\"bootstrap\"` strategy is chosen.\n* **device** (`int`, defaults to `None`) —\nDevice ordinal for CPU/GPU support of the pipeline. Setting this to -1 will leverage CPU, a positive\ninteger will run the model on the associated CUDA device ID. If `None` is provided it will be inferred and\nCUDA:0 used if available, CPU otherwise.\n* **random\\_state** (`int`, *optional*, defaults to `None`) —\nThe `random_state` value passed to `bootstrap` if `\"bootstrap\"` strategy is chosen. Useful for\ndebugging.\n* **input\\_column** (`str`, defaults to `\"text\"`) —\nthe name of the column containing the input text in the dataset specified by `data`.\n* **label\\_column** (`str`, defaults to `\"label\"`) —\nthe name of the column containing the labels in the dataset specified by `data`.\n* **generation\\_kwargs** (`Dict`, *optional*, defaults to `None`) —\nThe generation kwargs are passed to the pipeline and set the text generation strategy.\n\n\n### \nSummarizationEvaluator\n\n\n### class evaluate.SummarizationEvaluator\n\n<\nsource\n>\n(\ntask = 'summarization'\ndefault\\_metric\\_name = None\n\nText summarization evaluator.\nThis text summarization evaluator can currently be loaded from evaluator() using the default task name\n`summarization`.\nMethods in this class assume a data format compatible with the SummarizationEvaluator.\n\n\n\n### \nTranslationEvaluator\n\n\n### class evaluate.TranslationEvaluator\n\n<\nsource\n>\n(\ntask = 'translation'\ndefault\\_metric\\_name = None\n\nTranslation evaluator.\nThis translation generation evaluator can currently be loaded from evaluator() using the default task name\n`translation`.\nMethods in this class assume a data format compatible with the `TranslationPipeline`.\n\n\n\n### \nAutomaticSpeechRecognitionEvaluator\n\n### class evaluate.AutomaticSpeechRecognitionEvaluator\n\n<\nsource\n>\n(\ntask = 'automatic-speech-recognition'\ndefault\\_metric\\_name = None\n\nAutomatic speech recognition evaluator.\nThis automatic speech recognition evaluator can currently be loaded from evaluator() using the default task name\n`automatic-speech-recognition`.\nMethods in this class assume a data format compatible with the `AutomaticSpeechRecognitionPipeline`.\n\n<\nsource\n>\n(\nmodel\\_or\\_pipeline: typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'),\n\n==================\n Document 1 \n----------------\n## class evaluate.AutomaticSpeechRecognitionEvaluator\n\n<\nsource\n>\n(\ntask = 'automatic-speech-recognition'\ndefault\\_metric\\_name = None\n\nAutomatic speech recognition evaluator.\nThis automatic speech recognition evaluator can currently be loaded from evaluator() using the default task name\n`automatic-speech-recognition`.\nMethods in this class assume a data format compatible with the `AutomaticSpeechRecognitionPipeline`.\n\n<\nsource\n>\n(\nmodel\\_or\\_pipeline: typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None\ndata: typing.Union[str, datasets.arrow\\_dataset.Dataset] = None\nsubset: typing.Optional[str] = None\nsplit: typing.Optional[str] = None\nmetric: typing.Union[str, evaluate.module.EvaluationModule] = None\ntokenizer: typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None\nstrategy: typing.Literal['simple', 'bootstrap'] = 'simple'\nconfidence\\_level: float = 0.95\nn\\_resamples: int = 9999\ndevice: int = None\nrandom\\_state: typing.Optional[int] = None\ninput\\_column: str = 'path'\nlabel\\_column: str = 'sentence'\ngeneration\\_kwargs: dict = None\n\n```\n>>> from evaluate import evaluator\n>>> from datasets import load_dataset\n>>> task_evaluator = evaluator(\"automatic-speech-recognition\")\n>>> data = load_dataset(\"mozilla-foundation/common\\_voice\\_11\\_0\", \"en\", split=\"validation[:40]\")\n>>> results = task_evaluator.compute(\n>>>     model_or_pipeline=\"https://huggingface.co/openai/whisper-tiny.en\",\n>>>     data=data,\n>>>     input_column=\"path\",\n>>>     label_column=\"sentence\",\n>>>     metric=\"wer\",\n>>> )\n```\n\n# \nVisualization methods\n\n\nMethods for visualizing evaluations results:\n\n\n\n## \nRadar Plot\n\n#### evaluate.visualization.radar\\_plot\n\n<\nsource\n>\n(\ndata\nmodel\\_names\ninvert\\_range = []\nconfig = None\nfig = None\n\n\n* **data** (`List[dict]`) — the results (list of metric + value pairs).\nE.g. data = [{“accuracy”: 0.9, “precision”:0.8},{“accuracy”: 0.7, “precision”:0.6}]\n* **names** (`List[dict]`) — model names.\nE.g. names = [“model1”, “model 2”, …]\n* **invert\\_range** (`List[dict]`, optional)\n\n==================\n Document 2 \n----------------\n## class evaluate.TextClassificationEvaluator\n\n<\nsource\n>\n(\ntask = 'text-classification'\ndefault\\_metric\\_name = None\n\nText classification evaluator.\nThis text classification evaluator can currently be loaded from evaluator() using the default task name\n`text-classification` or with a `\"sentiment-analysis\"` alias.\nMethods in this class assume a data format compatible with the `TextClassificationPipeline` - a single textual\nfeature as input and a categorical label as output.\n\n<\nsource\n>\n(\nmodel\\_or\\_pipeline: typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None\ndata: typing.Union[str, datasets.arrow\\_dataset.Dataset] = None\nsubset: typing.Optional[str] = None\nsplit: typing.Optional[str] = None\nmetric: typing.Union[str, evaluate.module.EvaluationModule] = None\ntokenizer: typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None\nfeature\\_extractor: typing.Union[str, ForwardRef('FeatureExtractionMixin'), NoneType] = None\nstrategy: typing.Literal['simple', 'bootstrap'] = 'simple'\nconfidence\\_level: float = 0.95\nn\\_resamples: int = 9999\ndevice: int = None\nrandom\\_state: typing.Optional[int] = None\ninput\\_column: str = 'text'\nsecond\\_input\\_column: typing.Optional[str] = None\nlabel\\_column: str = 'label'\nlabel\\_mapping: typing.Union[typing.Dict[str, numbers.Number], NoneType] = None\n\n```\n>>> from evaluate import evaluator\n>>> from datasets import load_dataset\n>>> task_evaluator = evaluator(\"text-classification\")\n>>> data = load_dataset(\"imdb\", split=\"test[:2]\")\n>>> results = task_evaluator.compute(\n>>>     model_or_pipeline=\"huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli\",\n>>>     data=data,\n>>>     metric=\"accuracy\",\n>>>     label_mapping={\"LABEL\\_0\": 0.0, \"LABEL\\_1\": 1.0},\n>>>     strategy=\"bootstrap\",\n>>>     n_resamples=10,\n>>>     random_state=0\n>>> )\n```\n\n### \nTokenClassificationEvaluator\n\n### class evaluate.TokenClassificationEvaluator\n\n<\nsource\n>\n(\ntask = 'token-classification'\ndefault\\_metric\\_name = None\n\nToken classification evaluator.\n\n\nThis token classification evaluator can currently be loaded from evaluator() using the default task name\n`token-classification`.\n\n\nMethods in this class assume a data format compatible with the `TokenClassificationPipeline`.\n\n<\nsource\n>\n(\nmodel\\_or\\_pipeline: typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] =\n\n==================\n Document 3 \n----------------\n## class evaluate.QuestionAnsweringEvaluator\n\n<\nsource\n>\n(\ntask = 'question-answering'\ndefault\\_metric\\_name = None\n\nQuestion answering evaluator. This evaluator handles\n**extractive** question answering,\nwhere the answer to the question is extracted from a context.\n\n\nThis question answering evaluator can currently be loaded from evaluator() using the default task name\n`question-answering`.\n\n\nMethods in this class assume a data format compatible with the\n`QuestionAnsweringPipeline`.\n\n<\nsource\n>\n(\nmodel\\_or\\_pipeline: typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None\ndata: typing.Union[str, datasets.arrow\\_dataset.Dataset] = None\nsubset: typing.Optional[str] = None\nsplit: typing.Optional[str] = None\nmetric: typing.Union[str, evaluate.module.EvaluationModule] = None\ntokenizer: typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None\nstrategy: typing.Literal['simple', 'bootstrap'] = 'simple'\nconfidence\\_level: float = 0.95\nn\\_resamples: int = 9999\ndevice: int = None\nrandom\\_state: typing.Optional[int] = None\nquestion\\_column: str = 'question'\ncontext\\_column: str = 'context'\nid\\_column: str = 'id'\nlabel\\_column: str = 'answers'\nsquad\\_v2\\_format: typing.Optional[bool] = None\n\n```\n>>> from evaluate import evaluator\n>>> from datasets import load_dataset\n>>> task_evaluator = evaluator(\"question-answering\")\n>>> data = load_dataset(\"squad\", split=\"validation[:2]\")\n>>> results = task_evaluator.compute(\n>>>     model_or_pipeline=\"sshleifer/tiny-distilbert-base-cased-distilled-squad\",\n>>>     data=data,\n>>>     metric=\"squad\",\n>>> )\n```\n\nDatasets where the answer may be missing in the context are supported, for example SQuAD v2 dataset. In this case, it is safer to pass `squad_v2_format=True` to\nthe compute() call.\n\n Copied\n\n```\n>>> from evaluate import evaluator\n>>> from datasets import load_dataset\n>>> task_evaluator = evaluator(\"question-answering\")\n>>> data = load_dataset(\"squad\\_v2\", split=\"validation[:2]\")\n>>> results = task_evaluator.compute(\n>>>     model_or_pipeline=\"mrm8488/bert-tiny-finetuned-squadv2\",\n>>>     data=data,\n>>>     metric=\"squad\\_v2\",\n>>>     squad_v2_format=True,\n>>> )\n```\n\n### \nTextClassificationEvaluator\n\n### class evaluate.TextClassificationEvaluator\n\n<\nsource\n>\n(\ntask = 'text-classification'\ndefault\\_metric\\_name = None\n\nText classification evaluator.\nThis text classification evaluator can currently be loaded from evaluator() using the default task name\n`text-classification` or with a `\"sentiment-analysis\"` alias.\nMethods in this class assume a data format compatible with the `TextClassificationPipeline` -"
    },
    {
        "instruction": "Create a Python program that uses the 'evaluate' API to perform text summarization on a given text. The program should allow the user to input a text, then use the 'facebook/bart-large-cnn' model for text summarization and print the summarized text.",
        "api": "evaluate",
        "output": "#!pip install evaluate\nfrom evaluate import evaluator\n\n# Load the text summarization model\nsummarization_model = \"facebook/bart-large-cnn\"\n\ndef perform_summarization(text):\n    summarization_evaluator = evaluator(\"text-summarization\")\n\n    # Use the model for text summarization\n    summary = summarization_evaluator.compute(\n        model_or_pipeline=summarization_model,\n        data=text\n    )\n\n    return summary\n\n# User input\nuser_text = input(\"Enter the text for summarization: \")\nresult = perform_summarization(user_text)\n\nprint(\"Summarized Text:\")\nprint(result[0][\"summary_text\"])\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n## class evaluate.Text2TextGenerationEvaluator\n\n<\nsource\n>\n(\ntask = 'text2text-generation'\ndefault\\_metric\\_name = None\n\nText2Text generation evaluator.\nThis Text2Text generation evaluator can currently be loaded from evaluator() using the default task name\n`text2text-generation`.\nMethods in this class assume a data format compatible with the `Text2TextGenerationPipeline`.\n\n<\nsource\n>\n(\nmodel\\_or\\_pipeline: typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None\ndata: typing.Union[str, datasets.arrow\\_dataset.Dataset] = None\nsubset: typing.Optional[str] = None\nsplit: typing.Optional[str] = None\nmetric: typing.Union[str, evaluate.module.EvaluationModule] = None\ntokenizer: typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None\nstrategy: typing.Literal['simple', 'bootstrap'] = 'simple'\nconfidence\\_level: float = 0.95\nn\\_resamples: int = 9999\ndevice: int = None\nrandom\\_state: typing.Optional[int] = None\ninput\\_column: str = 'text'\nlabel\\_column: str = 'label'\ngeneration\\_kwargs: dict = None\n\n\n* **model\\_or\\_pipeline** (`str` or `Pipeline` or `Callable` or `PreTrainedModel` or `TFPreTrainedModel`, defaults to `None`) —\nIf the argument in not specified, we initialize the default pipeline for the task (in this case\n`text-classification` or its alias - `sentiment-analysis`). If the argument is of the type `str` or\nis a model instance, we use it to initialize a new `Pipeline` with the given model. Otherwise we assume the\nargument specifies a pre-initialized pipeline.\n* **data** (`str` or `Dataset`, defaults to `None`) —\nSpecifies the dataset we will run evaluation on. If it is of type `str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.\n* **subset** (`str`, defaults to `None`) —\nDefines which dataset subset to load. If `None` is passed the default subset is loaded.\n* **split** (`str`, defaults to `None`) —\nDefines which dataset split to load. If `None` is passed, infers based on the `choose_split` function.\n* **metric** (`str` or `EvaluationModule`, defaults to `None`) —\nSpecifies the metric we use in evaluator. If it is of type `str`, we treat it as the metric name, and\nload it. Otherwise we assume it represents a pre-loaded metric.\n* **tokenizer** (`str` or `PreTrainedTokenizer`, *optional*, defaults to `None`) —\nArgument can be used to overwrite a default tokenizer if `model_or_pipeline` represents a model for\nwhich we build a pipeline. If `model_or_pipeline` is `None` or a pre-initialized pipeline, we ignore\nthis argument.\n* **strategy** (`Literal[\"simple\", \"bootstrap\"]`, defaults to “simple”) —\nspecifies the evaluation strategy. Possible values are:\n\n\t+ `\"simple\"` - we evaluate the metric and return the scores.\n\t+ `\"bootstrap\"` - on top of computing the metric scores, we calculate the confidence interval for each\n\tof the returned metric keys, using `scipy`’s `bootstrap` method\n\thttps://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html.\n* **confidence\\_level** (`float`, defaults to `0.95`) —\nThe `confidence_level` value passed to `bootstrap` if `\"bootstrap\"` strategy is chosen.\n* **n\\_resamples** (`int`, defaults to `9999`) —\nThe `n_resamples` value passed to `bootstrap` if `\"bootstrap\"` strategy is chosen.\n* **device** (`int`, defaults to `None`) —\nDevice ordinal for CPU/GPU support of the pipeline. Setting this to -1 will leverage CPU, a positive\ninteger will run the model on the associated CUDA device ID. If `None` is provided it will be inferred and\nCUDA:0 used if available, CPU otherwise.\n* **random\\_state** (`int`, *optional*, defaults to `None`) —\nThe `random_state` value passed to `bootstrap` if `\"bootstrap\"` strategy is chosen. Useful for\ndebugging.\n* **input\\_column** (`str`, defaults to `\"text\"`) —\nthe name of the column containing the input text in the dataset specified by `data`.\n* **label\\_column** (`str`, defaults to `\"label\"`) —\nthe name of the column containing the labels in the dataset specified by `data`.\n* **generation\\_kwargs** (`Dict`, *optional*, defaults to `None`) —\nThe generation kwargs are passed to the pipeline and set the text generation strategy.\n\n\n### \nSummarizationEvaluator\n\n\n### class evaluate.SummarizationEvaluator\n\n<\nsource\n>\n(\ntask = 'summarization'\ndefault\\_metric\\_name = None\n\nText summarization evaluator.\nThis text summarization evaluator can currently be loaded from evaluator() using the default task name\n`summarization`.\nMethods in this class assume a data format compatible with the SummarizationEvaluator.\n\n\n\n### \nTranslationEvaluator\n\n\n### class evaluate.TranslationEvaluator\n\n<\nsource\n>\n(\ntask = 'translation'\ndefault\\_metric\\_name = None\n\nTranslation evaluator.\nThis translation generation evaluator can currently be loaded from evaluator() using the default task name\n`translation`.\nMethods in this class assume a data format compatible with the `TranslationPipeline`.\n\n\n\n### \nAutomaticSpeechRecognitionEvaluator\n\n### class evaluate.AutomaticSpeechRecognitionEvaluator\n\n<\nsource\n>\n(\ntask = 'automatic-speech-recognition'\ndefault\\_metric\\_name = None\n\nAutomatic speech recognition evaluator.\nThis automatic speech recognition evaluator can currently be loaded from evaluator() using the default task name\n`automatic-speech-recognition`.\nMethods in this class assume a data format compatible with the `AutomaticSpeechRecognitionPipeline`.\n\n<\nsource\n>\n(\nmodel\\_or\\_pipeline: typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'),\n\n==================\n Document 1 \n----------------\n## class evaluate.TextClassificationEvaluator\n\n<\nsource\n>\n(\ntask = 'text-classification'\ndefault\\_metric\\_name = None\n\nText classification evaluator.\nThis text classification evaluator can currently be loaded from evaluator() using the default task name\n`text-classification` or with a `\"sentiment-analysis\"` alias.\nMethods in this class assume a data format compatible with the `TextClassificationPipeline` - a single textual\nfeature as input and a categorical label as output.\n\n<\nsource\n>\n(\nmodel\\_or\\_pipeline: typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None\ndata: typing.Union[str, datasets.arrow\\_dataset.Dataset] = None\nsubset: typing.Optional[str] = None\nsplit: typing.Optional[str] = None\nmetric: typing.Union[str, evaluate.module.EvaluationModule] = None\ntokenizer: typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None\nfeature\\_extractor: typing.Union[str, ForwardRef('FeatureExtractionMixin'), NoneType] = None\nstrategy: typing.Literal['simple', 'bootstrap'] = 'simple'\nconfidence\\_level: float = 0.95\nn\\_resamples: int = 9999\ndevice: int = None\nrandom\\_state: typing.Optional[int] = None\ninput\\_column: str = 'text'\nsecond\\_input\\_column: typing.Optional[str] = None\nlabel\\_column: str = 'label'\nlabel\\_mapping: typing.Union[typing.Dict[str, numbers.Number], NoneType] = None\n\n```\n>>> from evaluate import evaluator\n>>> from datasets import load_dataset\n>>> task_evaluator = evaluator(\"text-classification\")\n>>> data = load_dataset(\"imdb\", split=\"test[:2]\")\n>>> results = task_evaluator.compute(\n>>>     model_or_pipeline=\"huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli\",\n>>>     data=data,\n>>>     metric=\"accuracy\",\n>>>     label_mapping={\"LABEL\\_0\": 0.0, \"LABEL\\_1\": 1.0},\n>>>     strategy=\"bootstrap\",\n>>>     n_resamples=10,\n>>>     random_state=0\n>>> )\n```\n\n### \nTokenClassificationEvaluator\n\n### class evaluate.TokenClassificationEvaluator\n\n<\nsource\n>\n(\ntask = 'token-classification'\ndefault\\_metric\\_name = None\n\nToken classification evaluator.\n\n\nThis token classification evaluator can currently be loaded from evaluator() using the default task name\n`token-classification`.\n\n\nMethods in this class assume a data format compatible with the `TokenClassificationPipeline`.\n\n<\nsource\n>\n(\nmodel\\_or\\_pipeline: typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] =\n\n==================\n Document 2 \n----------------\n## class evaluate.QuestionAnsweringEvaluator\n\n<\nsource\n>\n(\ntask = 'question-answering'\ndefault\\_metric\\_name = None\n\nQuestion answering evaluator. This evaluator handles\n**extractive** question answering,\nwhere the answer to the question is extracted from a context.\n\n\nThis question answering evaluator can currently be loaded from evaluator() using the default task name\n`question-answering`.\n\n\nMethods in this class assume a data format compatible with the\n`QuestionAnsweringPipeline`.\n\n<\nsource\n>\n(\nmodel\\_or\\_pipeline: typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None\ndata: typing.Union[str, datasets.arrow\\_dataset.Dataset] = None\nsubset: typing.Optional[str] = None\nsplit: typing.Optional[str] = None\nmetric: typing.Union[str, evaluate.module.EvaluationModule] = None\ntokenizer: typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None\nstrategy: typing.Literal['simple', 'bootstrap'] = 'simple'\nconfidence\\_level: float = 0.95\nn\\_resamples: int = 9999\ndevice: int = None\nrandom\\_state: typing.Optional[int] = None\nquestion\\_column: str = 'question'\ncontext\\_column: str = 'context'\nid\\_column: str = 'id'\nlabel\\_column: str = 'answers'\nsquad\\_v2\\_format: typing.Optional[bool] = None\n\n```\n>>> from evaluate import evaluator\n>>> from datasets import load_dataset\n>>> task_evaluator = evaluator(\"question-answering\")\n>>> data = load_dataset(\"squad\", split=\"validation[:2]\")\n>>> results = task_evaluator.compute(\n>>>     model_or_pipeline=\"sshleifer/tiny-distilbert-base-cased-distilled-squad\",\n>>>     data=data,\n>>>     metric=\"squad\",\n>>> )\n```\n\nDatasets where the answer may be missing in the context are supported, for example SQuAD v2 dataset. In this case, it is safer to pass `squad_v2_format=True` to\nthe compute() call.\n\n Copied\n\n```\n>>> from evaluate import evaluator\n>>> from datasets import load_dataset\n>>> task_evaluator = evaluator(\"question-answering\")\n>>> data = load_dataset(\"squad\\_v2\", split=\"validation[:2]\")\n>>> results = task_evaluator.compute(\n>>>     model_or_pipeline=\"mrm8488/bert-tiny-finetuned-squadv2\",\n>>>     data=data,\n>>>     metric=\"squad\\_v2\",\n>>>     squad_v2_format=True,\n>>> )\n```\n\n### \nTextClassificationEvaluator\n\n### class evaluate.TextClassificationEvaluator\n\n<\nsource\n>\n(\ntask = 'text-classification'\ndefault\\_metric\\_name = None\n\nText classification evaluator.\nThis text classification evaluator can currently be loaded from evaluator() using the default task name\n`text-classification` or with a `\"sentiment-analysis\"` alias.\nMethods in this class assume a data format compatible with the `TextClassificationPipeline` -"
    },
    {
        "instruction": "Develop a Python program using the 'gymnasium' API to play the 'Taxi-v3' environment. The program should control the taxi within the environment using keyboard inputs (W, A, S, D) to move up, left, down, and right, respectively.",
        "api": "gymnasium",
        "output": "#!pip install ale_py gymnasium\n#!pip install swig\n#!pip install gymnasium[box2d]\n#!pip install gymnasium[classic-control]\nimport gymnasium as gym\nfrom gymnasium.utils import play\n\nenv = gym.make(\"Taxi-v3\")\n\nmapping = {\"w\": 0,\n            \"a\": 3,\n            \"s\": 2,\n            \"d\": 1}\n\ndefault_action = 0\nplay.play(env, keys_to_action=mapping, noop=default_action)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# Env#\n\n## gymnasium.Env#\n\n\n*class* gymnasium.Env[source]#\nThe main Gymnasium class for implementing Reinforcement Learning Agents environments.\n\n\nThe class encapsulates an environment with arbitrary behind-the-scenes dynamics through the `step()` and `reset()` functions.\nAn environment can be partially or fully observed by single agents. For multi-agent environments, see\n\n==================\n Document 1 \n----------------\n## Attributes#\n\n\nEnv.action\\_space*: spaces.Space[ActType]*#\nThe Space object corresponding to valid actions, all valid actions should be contained with the space. For example, if the action space is of type Discrete and gives the value Discrete(2), this means there are two valid discrete actions: 0 & 1.\n\n```\n>>> env.action\\_space\nDiscrete(2)\n>>> env.observation\\_space\nBox(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n\n```\n\nEnv.observation\\_space*: spaces.Space[ObsType]*#\nThe Space object corresponding to valid observations, all valid observations should be contained with the space. For example, if the observation space is of type `Box` and the shape of the object is `(4,)`, this denotes a valid observation will be an array of 4 numbers. We can check the box bounds as well with attributes.\n\n```\n>>> env.observation\\_space.high\narray([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38], dtype=float32)\n>>> env.observation\\_space.low\narray([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38], dtype=float32)\n\nEnv.metadata*: dict[str, Any]* *= {'render\\_modes': []}*#\nThe metadata of the environment containing rendering modes, rendering fps, etc\n\nEnv.render\\_mode*: str | None* *= None*#\nThe render mode of the environment determined at initialisation\n\nEnv.reward\\_range *= (-inf, inf)*#\nA tuple corresponding to the minimum and maximum possible rewards for an agent over an episode. The default reward range is set to \\((-\\infty,+\\infty)\\).\n\nEnv.spec*: EnvSpec | None* *= None*#\nThe `EnvSpec` of the environment normally set during `gymnasium.make()`\n\n### Additional Methods#\n\n\ngymnasium.Env.close(*self*)#\nAfter the user has finished using the environment, close contains the code necessary to “clean up” the environment.\n\n\nThis is critical for closing rendering windows, database or HTTP connections.\nCalling `close` on an already closed environment has no effect and won’t raise an error.\n\n*property* Env.unwrapped*: Env[ObsType, ActType]*#\nReturns the base non-wrapped environment.\n\nReturns:\n**Env** – The base non-wrapped `gymnasium.Env` instance\n\n*property* Env.np\\_random*: Generator*#\nReturns the environment’s internal `\\_np\\_random` that if not set will initialise with a random seed.\n\nReturns:\n**Instances of `np.random.Generator`**\n\n\n### Implementing environments#\n\n\nWhen implementing an environment, the `Env.reset()` and `Env.step()` functions much be created describing the\ndynamics of the environment.\nFor more information see the environment creation tutorial.\n\n# Register and Make#\n\n\nGymnasium allows users to automatically load environments, pre-wrapped with several important wrappers through the `gymnasium.make()` function. To do this, the environment must be registered prior with `gymnasium.register()`. To get the environment specifications for a registered environment, use\n\n==================\n Document 2 \n----------------\n# gymnasium.Env#\n\n\n*class* gymnasium.Env[source]#\nThe main Gymnasium class for implementing Reinforcement Learning Agents environments.\n\n\nThe class encapsulates an environment with arbitrary behind-the-scenes dynamics through the `step()` and `reset()` functions.\nAn environment can be partially or fully observed by single agents. For multi-agent environments, see PettingZoo.\n\n\nThe main API methods that users of this class need to know are:\n\n\n* `step()` - Updates an environment with actions returning the next agent observation, the reward for taking that actions,\nif the environment has terminated or truncated due to the latest action and information from the environment about the step, i.e. metrics, debug info.\n* `reset()` - Resets the environment to an initial state, required before calling step.\nReturns the first agent observation for an episode and information, i.e. metrics, debug info.\n* `render()` - Renders the environments to help visualise what the agent see, examples modes are “human”, “rgb\\_array”, “ansi” for text.\n* `close()` - Closes the environment, important when external software is used, i.e. pygame for rendering, databases\n\n\nEnvironments have additional attributes for users to understand the implementation\n\n\n* `action\\_space` - The Space object corresponding to valid actions, all valid actions should be contained within the space.\n* `observation\\_space` - The Space object corresponding to valid observations, all valid observations should be contained within the space.\n* `reward\\_range` - A tuple corresponding to the minimum and maximum possible rewards for an agent over an episode.\nThe default reward range is set to \\((-\\infty,+\\infty)\\).\n* `spec` - An environment spec that contains the information used to initialize the environment from `gymnasium.make()`\n* `metadata` - The metadata of the environment, i.e. render modes, render fps\n* `np\\_random` - The random number generator for the environment. This is automatically assigned during\n`super().reset(seed=seed)` and when assessing `self.np\\_random`.\n\nSee also\n\n\nFor modifying or extending environments use the `gymnasium.Wrapper` class\n\n\nNote\n\n\nTo get reproducible sampling of actions, a seed can be set with `env.action\\_space.seed(123)`.\n### Methods#\n\n\ngymnasium.Env.step(*self*, *action: ActType*) → tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]#\nRun one timestep of the environment’s dynamics using the agent actions.\n\n\nWhen the end of an episode is reached (`terminated or truncated`), it is necessary to call `reset()` to\nreset this environment’s\n\n==================\n Document 3 \n----------------\n# Visualization#\n\n\ngymnasium.utils.play.play(*env: Env*, *transpose: bool | None = True*, *fps: int | None = None*, *zoom: float | None = None*, *callback: Callable | None = None*, *keys\\_to\\_action: Dict[Tuple[str | int] | str, ActType] | None = None*, *seed: int | None = None*, *noop: ActType = 0*)[source]#\nAllows one to play the game using keyboard.\n\nParameters:\n* **env** – Environment to use for playing.\n* **transpose** – If this is `True`, the output of observation is transposed. Defaults to `True`.\n* **fps** – Maximum number of steps of the environment executed every second. If `None` (the default),\n`env.metadata[\"render\\_fps\"\"]` (or 30, if the environment does not specify “render\\_fps”) is used.\n* **zoom** – Zoom the observation in, `zoom` amount, should be positive float\n* **callback** – If a callback is provided, it will be executed after every step. It takes the following input:\nobs\\_t: observation before performing action\nobs\\_tp1: observation after performing action\naction: action that was executed\nrew: reward that was received\nterminated: whether the environment is terminated or not\ntruncated: whether the environment is truncated or not\ninfo: debug info\n* **keys\\_to\\_action** – Mapping from keys pressed to action performed.\nDifferent formats are supported: Key combinations can either be expressed as a tuple of unicode code\npoints of the keys, as a tuple of characters, or as a string where each character of the string represents\none key.\nFor example if pressing ‘w’ and space at the same time is supposed\nto trigger action number 2 then `key\\_to\\_action` dict could look like this:\n\n```\n>>> key\\_to\\_action = {\n...    # ...\n...    (ord('w'), ord(' ')): 2\n...    # ...\n... }\n\n\nor like this:\n\n```\n>>> key\\_to\\_action = {\n...    # ...\n...    (\"w\", \" \"): 2\n...    # ...\n... }\n\n```\n>>> key\\_to\\_action = {\n...    # ...\n...    \"w \": 2\n...    # ...\n... }\n\n\nIf `None`, default `key\\_to\\_action` mapping for that environment is used, if provided.\n* **seed** – Random seed used when resetting the environment. If None, no seed is used.\n* **noop** – The action used when no key input has been entered, or the entered key combination is unknown.\n\n```\n>>> import gymnasium as gym\n>>> from gymnasium.utils.play import play\n>>> play(gym.make(\"CarRacing-v2\", render\\_mode=\"rgb\\_array\"), keys\\_to\\_action={  \n...                                                \"w\": np.array([0, 0.7, 0]),\n...                                                \"a\": np.array([-1, 0, 0]),\n...                                                \"s\": np.array([0, 0, 1]),\n...                                                \"d\": np.array([1, 0, 0]),\n...                                                \"wa\": np.array([-1, 0.7, 0]),\n...                                                \"dw\": np.array([1, 0.7, 0]),\n...                                                \"ds\": np.array([1, 0, 1]),\n...                                                \"as\": np.array([-1, 0, 1]),\n...                                               }, noop=np.array([0,0,0]))\n\n\nAbove code works also if the environment is wrapped, so it’s particularly useful in\nverifying that the frame-level preprocessing does not render the game\nunplayable.\n\n\nIf you wish to plot real time statistics as you play, you can use\n`gym.utils.play.PlayPlot`. Here’s a sample code for plotting the reward\nfor last 150 steps.\n\n```\n>>> import gymnasium as gym\n>>> from gymnasium.utils.play import PlayPlot, play\n>>> def callback(obs\\_t, obs\\_tp1, action, rew, terminated, truncated, info):\n...        return [rew,]\n>>> plotter = PlayPlot(callback, 150, [\"reward\"])             \n>>> play(gym.make(\"CartPole-v1\"), callback=plotter.callback)  \n\n*class* gymnasium.utils.play.PlayPlot(*callback: Callable*, *horizon\\_timesteps: int*, *plot\\_names: List[str]*)[source]#\nProvides a callback to create live plots of arbitrary metrics when using `play()`.\n\nThis class is instantiated with a function that accepts information about a single environment transition:* obs\\_t: observation before performing action\n* obs\\_tp1: observation after performing action\n* action: action that was executed\n* rew: reward that was received\n* terminated: whether the environment is terminated or not\n* truncated: whether the environment is truncated or not\n* info: debug info\n\n\nIt should return a list of metrics that are computed from this data.\nFor instance, the function may look like this:\n\n```\n>>> def compute\\_metrics(obs\\_t, obs\\_tp, action, reward, terminated, truncated, info):\n...     return [reward, info[\"cumulative\\_reward\"], np.linalg.norm(action)]\n\n\n`PlayPlot` provides the method `callback()` which will pass its arguments along to that function\nand uses the returned values to update live plots of the metrics.\n\n\nTypically, this `callback()` will be used in conjunction with `play()` to see how the metrics evolve as you play:\n\n```\n>>> plotter = PlayPlot(compute\\_metrics, horizon\\_timesteps=200,                               \n...                    plot\\_names=[\"Immediate Rew.\", \"Cumulative Rew.\", \"Action Magnitude\"])\n>>> play(your\\_env, callback=plotter.callback)                                                \n\nParameters:\n* **callback** – Function that computes metrics from environment transitions\n* **horizon\\_timesteps** – The time horizon used for the live plots\n* **plot\\_names** – List of plot titles\n\nRaises:\n**DependencyNotInstalled** – If matplotlib is not installed\n\n\ncallback(*obs\\_t: ObsType*, *obs\\_tp1: ObsType*, *action: ActType*, *rew: float*, *terminated: bool*, *truncated: bool*, *info: dict*)[source]#\nThe callback that calls the provided data callback and adds the data to the plots.\n\nParameters:\n* **obs\\_t** – The observation at time step t\n* **obs\\_tp1** – The observation at time step t+1\n* **action** – The action\n* **rew** – The reward\n* **terminated** – If the environment is terminated\n* **truncated** – If the environment is truncated\n* **info** – The information from the environment\n\n\n*class* gymnasium.utils.play.PlayableGame(*env: Env*, *keys\\_to\\_action: Dict[Tuple[int, ...], int] | None = None*, *zoom: float | None = None*)[source]#\nWraps an environment allowing keyboard inputs to interact with the environment.\n\nParameters:\n* **env** – The environment to play\n* **keys\\_to\\_action** – The dictionary of keyboard tuples and action value\n* **zoom** – If to zoom in on the environment render\n\n\nprocess\\_event(*event: Event*)[source]#\nProcesses a PyGame event.\n\n\nIn particular, this function is used to keep track of which buttons are currently pressed\nand to exit the `play()` function when the PyGame window is closed.\n\nParameters:\n**event** – The event to process\n\n## Save Rendering Videos#\n\n\ngymnasium.utils.save\\_video.save\\_video(*frames: list*, *video\\_folder: str*, *episode\\_trigger: Callable[[int], bool] | None = None*, *step\\_trigger: Callable[[int], bool] | None = None*, *video\\_length: int | None = None*, *name\\_prefix: str = 'rl-video'*, *episode\\_index: int = 0*, *step\\_starting\\_index: int = 0*, *\\*\\*kwargs*)[source]#\nSave"
    },
    {
        "instruction": "Develop a Python program using the 'gymnasium' API to play the 'RoboschoolInvertedPendulum-v1' environment. The program should control the inverted pendulum within the environment using continuous actions to apply torque to the joint.",
        "api": "gymnasium",
        "output": "#!pip install ale_py gymnasium\n#!pip install swig\n#!pip install gymnasium[box2d]\n#!pip install gymnasium[classic-control]\nimport gymnasium as gym\n\nenv = gym.make(\"RoboschoolInvertedPendulum-v1\")\n\ndone = False\nwhile not done:\n    action = env.action_space.sample()\n    observation, reward, done, info = env.step(action)\n    print(\"Action:\", action)\n    print(\"Observation:\", observation)\n    print(\"Reward:\", reward)\n    print(\"Done:\", done)\n    print(\"Info:\", info)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# Env#\n\n## gymnasium.Env#\n\n\n*class* gymnasium.Env[source]#\nThe main Gymnasium class for implementing Reinforcement Learning Agents environments.\n\n\nThe class encapsulates an environment with arbitrary behind-the-scenes dynamics through the `step()` and `reset()` functions.\nAn environment can be partially or fully observed by single agents. For multi-agent environments, see\n\n==================\n Document 1 \n----------------\n## Attributes#\n\n\nEnv.action\\_space*: spaces.Space[ActType]*#\nThe Space object corresponding to valid actions, all valid actions should be contained with the space. For example, if the action space is of type Discrete and gives the value Discrete(2), this means there are two valid discrete actions: 0 & 1.\n\n```\n>>> env.action\\_space\nDiscrete(2)\n>>> env.observation\\_space\nBox(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n\n```\n\nEnv.observation\\_space*: spaces.Space[ObsType]*#\nThe Space object corresponding to valid observations, all valid observations should be contained with the space. For example, if the observation space is of type `Box` and the shape of the object is `(4,)`, this denotes a valid observation will be an array of 4 numbers. We can check the box bounds as well with attributes.\n\n```\n>>> env.observation\\_space.high\narray([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38], dtype=float32)\n>>> env.observation\\_space.low\narray([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38], dtype=float32)\n\nEnv.metadata*: dict[str, Any]* *= {'render\\_modes': []}*#\nThe metadata of the environment containing rendering modes, rendering fps, etc\n\nEnv.render\\_mode*: str | None* *= None*#\nThe render mode of the environment determined at initialisation\n\nEnv.reward\\_range *= (-inf, inf)*#\nA tuple corresponding to the minimum and maximum possible rewards for an agent over an episode. The default reward range is set to \\((-\\infty,+\\infty)\\).\n\nEnv.spec*: EnvSpec | None* *= None*#\nThe `EnvSpec` of the environment normally set during `gymnasium.make()`\n\n### Additional Methods#\n\n\ngymnasium.Env.close(*self*)#\nAfter the user has finished using the environment, close contains the code necessary to “clean up” the environment.\n\n\nThis is critical for closing rendering windows, database or HTTP connections.\nCalling `close` on an already closed environment has no effect and won’t raise an error.\n\n*property* Env.unwrapped*: Env[ObsType, ActType]*#\nReturns the base non-wrapped environment.\n\nReturns:\n**Env** – The base non-wrapped `gymnasium.Env` instance\n\n*property* Env.np\\_random*: Generator*#\nReturns the environment’s internal `\\_np\\_random` that if not set will initialise with a random seed.\n\nReturns:\n**Instances of `np.random.Generator`**\n\n\n### Implementing environments#\n\n\nWhen implementing an environment, the `Env.reset()` and `Env.step()` functions much be created describing the\ndynamics of the environment.\nFor more information see the environment creation tutorial.\n\n# Register and Make#\n\n\nGymnasium allows users to automatically load environments, pre-wrapped with several important wrappers through the `gymnasium.make()` function. To do this, the environment must be registered prior with `gymnasium.register()`. To get the environment specifications for a registered environment, use\n\n==================\n Document 2 \n----------------\n# gymnasium.Env#\n\n\n*class* gymnasium.Env[source]#\nThe main Gymnasium class for implementing Reinforcement Learning Agents environments.\n\n\nThe class encapsulates an environment with arbitrary behind-the-scenes dynamics through the `step()` and `reset()` functions.\nAn environment can be partially or fully observed by single agents. For multi-agent environments, see PettingZoo.\n\n\nThe main API methods that users of this class need to know are:\n\n\n* `step()` - Updates an environment with actions returning the next agent observation, the reward for taking that actions,\nif the environment has terminated or truncated due to the latest action and information from the environment about the step, i.e. metrics, debug info.\n* `reset()` - Resets the environment to an initial state, required before calling step.\nReturns the first agent observation for an episode and information, i.e. metrics, debug info.\n* `render()` - Renders the environments to help visualise what the agent see, examples modes are “human”, “rgb\\_array”, “ansi” for text.\n* `close()` - Closes the environment, important when external software is used, i.e. pygame for rendering, databases\n\n\nEnvironments have additional attributes for users to understand the implementation\n\n\n* `action\\_space` - The Space object corresponding to valid actions, all valid actions should be contained within the space.\n* `observation\\_space` - The Space object corresponding to valid observations, all valid observations should be contained within the space.\n* `reward\\_range` - A tuple corresponding to the minimum and maximum possible rewards for an agent over an episode.\nThe default reward range is set to \\((-\\infty,+\\infty)\\).\n* `spec` - An environment spec that contains the information used to initialize the environment from `gymnasium.make()`\n* `metadata` - The metadata of the environment, i.e. render modes, render fps\n* `np\\_random` - The random number generator for the environment. This is automatically assigned during\n`super().reset(seed=seed)` and when assessing `self.np\\_random`.\n\nSee also\n\n\nFor modifying or extending environments use the `gymnasium.Wrapper` class\n\n\nNote\n\n\nTo get reproducible sampling of actions, a seed can be set with `env.action\\_space.seed(123)`.\n### Methods#\n\n\ngymnasium.Env.step(*self*, *action: ActType*) → tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]#\nRun one timestep of the environment’s dynamics using the agent actions.\n\n\nWhen the end of an episode is reached (`terminated or truncated`), it is necessary to call `reset()` to\nreset this environment’s"
    },
    {
        "instruction": "Develop a Python program using the 'gymnasium' API to play the 'FrozenLake-v0' environment. The program should control the agent within the environment using keyboard inputs (W, A, S, D) to move up, left, down, and right, respectively.",
        "api": "gymnasium",
        "output": "#!pip install ale_py gymnasium\n#!pip install swig\n#!pip install gymnasium[box2d]\n#!pip install gymnasium[classic-control]\nimport gymnasium as gym\nfrom gymnasium.utils import play\n\nenv = gym.make(\"FrozenLake-v0\")\n\nmapping = {\"w\": 0,\n            \"a\": 3,\n            \"s\": 2,\n            \"d\": 1}\n\ndefault_action = 0\nplay.play(env, keys_to_action=mapping, noop=default_action)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# Env#\n\n## gymnasium.Env#\n\n\n*class* gymnasium.Env[source]#\nThe main Gymnasium class for implementing Reinforcement Learning Agents environments.\n\n\nThe class encapsulates an environment with arbitrary behind-the-scenes dynamics through the `step()` and `reset()` functions.\nAn environment can be partially or fully observed by single agents. For multi-agent environments, see\n\n==================\n Document 1 \n----------------\n## Attributes#\n\n\nEnv.action\\_space*: spaces.Space[ActType]*#\nThe Space object corresponding to valid actions, all valid actions should be contained with the space. For example, if the action space is of type Discrete and gives the value Discrete(2), this means there are two valid discrete actions: 0 & 1.\n\n```\n>>> env.action\\_space\nDiscrete(2)\n>>> env.observation\\_space\nBox(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n\n```\n\nEnv.observation\\_space*: spaces.Space[ObsType]*#\nThe Space object corresponding to valid observations, all valid observations should be contained with the space. For example, if the observation space is of type `Box` and the shape of the object is `(4,)`, this denotes a valid observation will be an array of 4 numbers. We can check the box bounds as well with attributes.\n\n```\n>>> env.observation\\_space.high\narray([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38], dtype=float32)\n>>> env.observation\\_space.low\narray([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38], dtype=float32)\n\nEnv.metadata*: dict[str, Any]* *= {'render\\_modes': []}*#\nThe metadata of the environment containing rendering modes, rendering fps, etc\n\nEnv.render\\_mode*: str | None* *= None*#\nThe render mode of the environment determined at initialisation\n\nEnv.reward\\_range *= (-inf, inf)*#\nA tuple corresponding to the minimum and maximum possible rewards for an agent over an episode. The default reward range is set to \\((-\\infty,+\\infty)\\).\n\nEnv.spec*: EnvSpec | None* *= None*#\nThe `EnvSpec` of the environment normally set during `gymnasium.make()`\n\n### Additional Methods#\n\n\ngymnasium.Env.close(*self*)#\nAfter the user has finished using the environment, close contains the code necessary to “clean up” the environment.\n\n\nThis is critical for closing rendering windows, database or HTTP connections.\nCalling `close` on an already closed environment has no effect and won’t raise an error.\n\n*property* Env.unwrapped*: Env[ObsType, ActType]*#\nReturns the base non-wrapped environment.\n\nReturns:\n**Env** – The base non-wrapped `gymnasium.Env` instance\n\n*property* Env.np\\_random*: Generator*#\nReturns the environment’s internal `\\_np\\_random` that if not set will initialise with a random seed.\n\nReturns:\n**Instances of `np.random.Generator`**\n\n\n### Implementing environments#\n\n\nWhen implementing an environment, the `Env.reset()` and `Env.step()` functions much be created describing the\ndynamics of the environment.\nFor more information see the environment creation tutorial.\n\n# Register and Make#\n\n\nGymnasium allows users to automatically load environments, pre-wrapped with several important wrappers through the `gymnasium.make()` function. To do this, the environment must be registered prior with `gymnasium.register()`. To get the environment specifications for a registered environment, use\n\n==================\n Document 2 \n----------------\n# gymnasium.Env#\n\n\n*class* gymnasium.Env[source]#\nThe main Gymnasium class for implementing Reinforcement Learning Agents environments.\n\n\nThe class encapsulates an environment with arbitrary behind-the-scenes dynamics through the `step()` and `reset()` functions.\nAn environment can be partially or fully observed by single agents. For multi-agent environments, see PettingZoo.\n\n\nThe main API methods that users of this class need to know are:\n\n\n* `step()` - Updates an environment with actions returning the next agent observation, the reward for taking that actions,\nif the environment has terminated or truncated due to the latest action and information from the environment about the step, i.e. metrics, debug info.\n* `reset()` - Resets the environment to an initial state, required before calling step.\nReturns the first agent observation for an episode and information, i.e. metrics, debug info.\n* `render()` - Renders the environments to help visualise what the agent see, examples modes are “human”, “rgb\\_array”, “ansi” for text.\n* `close()` - Closes the environment, important when external software is used, i.e. pygame for rendering, databases\n\n\nEnvironments have additional attributes for users to understand the implementation\n\n\n* `action\\_space` - The Space object corresponding to valid actions, all valid actions should be contained within the space.\n* `observation\\_space` - The Space object corresponding to valid observations, all valid observations should be contained within the space.\n* `reward\\_range` - A tuple corresponding to the minimum and maximum possible rewards for an agent over an episode.\nThe default reward range is set to \\((-\\infty,+\\infty)\\).\n* `spec` - An environment spec that contains the information used to initialize the environment from `gymnasium.make()`\n* `metadata` - The metadata of the environment, i.e. render modes, render fps\n* `np\\_random` - The random number generator for the environment. This is automatically assigned during\n`super().reset(seed=seed)` and when assessing `self.np\\_random`.\n\nSee also\n\n\nFor modifying or extending environments use the `gymnasium.Wrapper` class\n\n\nNote\n\n\nTo get reproducible sampling of actions, a seed can be set with `env.action\\_space.seed(123)`.\n### Methods#\n\n\ngymnasium.Env.step(*self*, *action: ActType*) → tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]#\nRun one timestep of the environment’s dynamics using the agent actions.\n\n\nWhen the end of an episode is reached (`terminated or truncated`), it is necessary to call `reset()` to\nreset this environment’s\n\n==================\n Document 3 \n----------------\n# Visualization#\n\n\ngymnasium.utils.play.play(*env: Env*, *transpose: bool | None = True*, *fps: int | None = None*, *zoom: float | None = None*, *callback: Callable | None = None*, *keys\\_to\\_action: Dict[Tuple[str | int] | str, ActType] | None = None*, *seed: int | None = None*, *noop: ActType = 0*)[source]#\nAllows one to play the game using keyboard.\n\nParameters:\n* **env** – Environment to use for playing.\n* **transpose** – If this is `True`, the output of observation is transposed. Defaults to `True`.\n* **fps** – Maximum number of steps of the environment executed every second. If `None` (the default),\n`env.metadata[\"render\\_fps\"\"]` (or 30, if the environment does not specify “render\\_fps”) is used.\n* **zoom** – Zoom the observation in, `zoom` amount, should be positive float\n* **callback** – If a callback is provided, it will be executed after every step. It takes the following input:\nobs\\_t: observation before performing action\nobs\\_tp1: observation after performing action\naction: action that was executed\nrew: reward that was received\nterminated: whether the environment is terminated or not\ntruncated: whether the environment is truncated or not\ninfo: debug info\n* **keys\\_to\\_action** – Mapping from keys pressed to action performed.\nDifferent formats are supported: Key combinations can either be expressed as a tuple of unicode code\npoints of the keys, as a tuple of characters, or as a string where each character of the string represents\none key.\nFor example if pressing ‘w’ and space at the same time is supposed\nto trigger action number 2 then `key\\_to\\_action` dict could look like this:\n\n```\n>>> key\\_to\\_action = {\n...    # ...\n...    (ord('w'), ord(' ')): 2\n...    # ...\n... }\n\n\nor like this:\n\n```\n>>> key\\_to\\_action = {\n...    # ...\n...    (\"w\", \" \"): 2\n...    # ...\n... }\n\n```\n>>> key\\_to\\_action = {\n...    # ...\n...    \"w \": 2\n...    # ...\n... }\n\n\nIf `None`, default `key\\_to\\_action` mapping for that environment is used, if provided.\n* **seed** – Random seed used when resetting the environment. If None, no seed is used.\n* **noop** – The action used when no key input has been entered, or the entered key combination is unknown.\n\n```\n>>> import gymnasium as gym\n>>> from gymnasium.utils.play import play\n>>> play(gym.make(\"CarRacing-v2\", render\\_mode=\"rgb\\_array\"), keys\\_to\\_action={  \n...                                                \"w\": np.array([0, 0.7, 0]),\n...                                                \"a\": np.array([-1, 0, 0]),\n...                                                \"s\": np.array([0, 0, 1]),\n...                                                \"d\": np.array([1, 0, 0]),\n...                                                \"wa\": np.array([-1, 0.7, 0]),\n...                                                \"dw\": np.array([1, 0.7, 0]),\n...                                                \"ds\": np.array([1, 0, 1]),\n...                                                \"as\": np.array([-1, 0, 1]),\n...                                               }, noop=np.array([0,0,0]))\n\n\nAbove code works also if the environment is wrapped, so it’s particularly useful in\nverifying that the frame-level preprocessing does not render the game\nunplayable.\n\n\nIf you wish to plot real time statistics as you play, you can use\n`gym.utils.play.PlayPlot`. Here’s a sample code for plotting the reward\nfor last 150 steps.\n\n```\n>>> import gymnasium as gym\n>>> from gymnasium.utils.play import PlayPlot, play\n>>> def callback(obs\\_t, obs\\_tp1, action, rew, terminated, truncated, info):\n...        return [rew,]\n>>> plotter = PlayPlot(callback, 150, [\"reward\"])             \n>>> play(gym.make(\"CartPole-v1\"), callback=plotter.callback)  \n\n*class* gymnasium.utils.play.PlayPlot(*callback: Callable*, *horizon\\_timesteps: int*, *plot\\_names: List[str]*)[source]#\nProvides a callback to create live plots of arbitrary metrics when using `play()`.\n\nThis class is instantiated with a function that accepts information about a single environment transition:* obs\\_t: observation before performing action\n* obs\\_tp1: observation after performing action\n* action: action that was executed\n* rew: reward that was received\n* terminated: whether the environment is terminated or not\n* truncated: whether the environment is truncated or not\n* info: debug info\n\n\nIt should return a list of metrics that are computed from this data.\nFor instance, the function may look like this:\n\n```\n>>> def compute\\_metrics(obs\\_t, obs\\_tp, action, reward, terminated, truncated, info):\n...     return [reward, info[\"cumulative\\_reward\"], np.linalg.norm(action)]\n\n\n`PlayPlot` provides the method `callback()` which will pass its arguments along to that function\nand uses the returned values to update live plots of the metrics.\n\n\nTypically, this `callback()` will be used in conjunction with `play()` to see how the metrics evolve as you play:\n\n```\n>>> plotter = PlayPlot(compute\\_metrics, horizon\\_timesteps=200,                               \n...                    plot\\_names=[\"Immediate Rew.\", \"Cumulative Rew.\", \"Action Magnitude\"])\n>>> play(your\\_env, callback=plotter.callback)                                                \n\nParameters:\n* **callback** – Function that computes metrics from environment transitions\n* **horizon\\_timesteps** – The time horizon used for the live plots\n* **plot\\_names** – List of plot titles\n\nRaises:\n**DependencyNotInstalled** – If matplotlib is not installed\n\n\ncallback(*obs\\_t: ObsType*, *obs\\_tp1: ObsType*, *action: ActType*, *rew: float*, *terminated: bool*, *truncated: bool*, *info: dict*)[source]#\nThe callback that calls the provided data callback and adds the data to the plots.\n\nParameters:\n* **obs\\_t** – The observation at time step t\n* **obs\\_tp1** – The observation at time step t+1\n* **action** – The action\n* **rew** – The reward\n* **terminated** – If the environment is terminated\n* **truncated** – If the environment is truncated\n* **info** – The information from the environment\n\n\n*class* gymnasium.utils.play.PlayableGame(*env: Env*, *keys\\_to\\_action: Dict[Tuple[int, ...], int] | None = None*, *zoom: float | None = None*)[source]#\nWraps an environment allowing keyboard inputs to interact with the environment.\n\nParameters:\n* **env** – The environment to play\n* **keys\\_to\\_action** – The dictionary of keyboard tuples and action value\n* **zoom** – If to zoom in on the environment render\n\n\nprocess\\_event(*event: Event*)[source]#\nProcesses a PyGame event.\n\n\nIn particular, this function is used to keep track of which buttons are currently pressed\nand to exit the `play()` function when the PyGame window is closed.\n\nParameters:\n**event** – The event to process\n\n## Save Rendering Videos#\n\n\ngymnasium.utils.save\\_video.save\\_video(*frames: list*, *video\\_folder: str*, *episode\\_trigger: Callable[[int], bool] | None = None*, *step\\_trigger: Callable[[int], bool] | None = None*, *video\\_length: int | None = None*, *name\\_prefix: str = 'rl-video'*, *episode\\_index: int = 0*, *step\\_starting\\_index: int = 0*, *\\*\\*kwargs*)[source]#\nSave"
    },
    {
        "instruction": "Create a Python program using the 'holidays' API to check if a specified date is a public holiday in a given country or region.",
        "api": "holidays",
        "output": "#!pip install holidays\nimport holidays\n\n# Define the date to check\ndate_to_check = \"2023-07-04\" \n\n# Specify the country or region (e.g., 'US' for the United States)\ncountry_code = 'US'\n\n# Create a dictionary of holidays for the specified country or region\nholiday_dict = holidays.country_holidays(country_code)\n\n# Check if the date is a public holiday\nis_holiday = date_to_check in holiday_dict\n\n# Display the result\nif is_holiday:\n    print(f\"{date_to_check} is a public holiday in {country_code}.\")\nelse:\n    print(f\"{date_to_check} is not a public holiday in {country_code}.\")",
        "documentation": "\n\n==================\n Document 0 \n----------------\n For a specific subdivision (e.g. state or province):\n>>> calif\\_holidays = country\\_holidays('US', subdiv='CA')\n\n```\n\n\nThe below will cause 2015 holidays to be calculated on the fly:\n\n```\n>>> from datetime import date\n>>> assert date(2015, 1, 1) in us\\_holidays\n\n\nThis will be faster because 2015 holidays are already calculated:\n\n```\n>>> assert date(2015, 1, 2) not in us\\_holidays\n\n\nThe `HolidayBase` class also recognizes strings of many formats\nand numbers representing a POSIX timestamp:\n\n```\n>>> assert '2014-01-01' in us\\_holidays\n>>> assert '1/1/2014' in us\\_holidays\n>>> assert 1388597445 in us\\_holidays\n\n\nShow the holiday’s name:\n\n```\n>>> us\\_holidays.get('2014-01-01')\n\"New Year's Day\"\n\n\nCheck a range:\n\n```\n>>> us\\_holidays['2014-01-01': '2014-01-03']\n[datetime.date(2014, 1, 1)]\n\n\nList all 2020 holidays:\n\n```\n>>> us\\_holidays = country\\_holidays('US', years=2020)\n>>> for day in us\\_holidays.items():\n...     print(day)\n(datetime.date(2020, 1, 1), \"New Year's Day\")\n(datetime.date(2020, 1, 20), 'Martin Luther King Jr. Day')\n(datetime.date(2020, 2, 17), \"Washington's Birthday\")\n(datetime.date(2020, 5, 25), 'Memorial Day')\n(datetime.date(2020, 7, 4), 'Independence Day')\n(datetime.date(2020, 7, 3), 'Independence Day (Observed)')\n(datetime.date(2020, 9, 7), 'Labor Day')\n(datetime.date(2020, 10, 12), 'Columbus Day')\n(datetime.date(2020, 11, 11), 'Veterans Day')\n(datetime.date(2020, 11, 26), 'Thanksgiving')\n(datetime.date(2020, 12, 25), 'Christmas Day')\n\n\nSome holidays are only present in parts of a country:\n\n```\n>>> us\\_pr\\_holidays = country\\_holidays('US', subdiv='PR')\n>>> assert '2018-01-06' not in us\\_holidays\n>>> assert '2018-01-06' in us\\_pr\\_holidays\n\n\nAppend custom holiday dates by passing one of:\n\n\n* a `dict` with date/name key/value pairs (e.g.\n`{'2010-07-10': 'My birthday!'}`),\n* a list of dates (as a `datetime.date`, `datetime.datetime`,\n`str`, `int`, or `float`); `'Holiday'` will be\nused as a description,\n* or a single date item (of one of the types above); `'Holiday'` will be\nused as a description:\n\n```\n>>> custom\\_holidays = country\\_holidays('US', years=2015)\n>>> custom\\_holidays.update({'2015-01-01': \"New Year's Day\"})\n>>> custom\\_holidays.update(['2015-07-01', '07/04/2015'])\n>>> custom\\_holidays.update(date(2015, 12, 25))\n>>> assert date(2015, 1, 1) in custom\\_holidays\n>>> assert date(2015, 1, 2) not in custom\\_holidays\n>>> assert '12/25/2015' in custom\\_holidays\n\n\nFor more complex logic, like 4th Monday of January, you can inherit the\n`HolidayBase` class and define your own `\\_populate()` method.\nSee documentation for examples.\n\nholidays.utils.financial\\_holidays(*market*, *subdiv=None*, *years=None*, *expand=True*, *observed=True*, *language=None*)\nReturns a new dictionary-like `HolidayBase` object for the public\nholidays of the financial market matching **market** and other keyword\narguments.\n\nParameters:\n* **market** (*str*) – An ISO 3166-1 Alpha-2 market code.\n* **subdiv** (*str* *|* *None*) – Currently not implemented for markets (see documentation).\n* **years** (*int* *|* *Iterable**[**int**]* *|* *None*) – The year(s) to pre-calculate public holidays for at instantiation.\n* **expand** (*bool*) – Whether the entire year is calculated when one date from that year\nis requested.\n* **observed** (*bool*) – Whether to include the dates of when public holiday are observed\n(e.g. a holiday falling on a Sunday being observed the following\nMonday). False may not work for all countries.\n* **language** (*str* *|* *None*) – The language which the returned holiday names will be translated\ninto. It must be an ISO 639-1 (2-letter) language code. If the\nlanguage translation is not supported the original holiday names\nwill be used.\n\nReturns:\nA `HolidayBase` object matching the **market**.\n\n```\n>>> from holidays import financial\\_holidays\n>>> nyse\\_holidays = financial\\_holidays('NYSE')\n\n\nSee `country\\_holidays()` documentation for further details and\nexamples.\n\nholidays.utils.list\\_localized\\_countries(*include\\_aliases=True*)\nGet all localized countries and languages they support.\n\nParameters:\n**include\\_aliases** – Whether to include entity aliases (e.g. UK for GB).\n\nReturns:\nA dictionary where key is an ISO 3166-1 alpha-2 country code and\nvalue is a list of supported languages (either ISO 639-1 or a\ncombination of ISO 639-1 and ISO 3166-1 codes joined with “\\_”).\n\nReturn type:\n*Dict*[*str*, *List*[*str*]]\n\nholidays.utils.list\\_localized\\_financial(*include\\_aliases=True*)\nGet all localized financial markets and languages they support.\n\nParameters:\n**include\\_aliases** – Whether to include entity aliases(e.g. TAR for ECB, XNYS for NYSE).\n\nReturns:\nA dictionary where key is a market code and value is a list of\nsupported subdivision codes.\n\nholidays.utils.list\\_supported\\_countries(*include\\_aliases=True*)\nGet all supported countries and their subdivisions.\n\nReturns:\nA dictionary where key is an ISO 3166-1 alpha-2 country code and\nvalue is a list of supported subdivision codes.\n\nholidays.utils.list\\_supported\\_financial(*include\\_aliases=True*)\nGet all supported financial markets and their subdivisions.\n\n*class* holidays.holiday\\_base.HolidayBase(*years=None*, *expand=True*, *observed=True*, *subdiv=None*, *prov=None*, *state=None*, *language=None*, *categories=None*)\nBases: `Dict`[`date`, `str`]\n\n\nA dict-like object containing the holidays for a specific country (and\nprovince or state if so initiated); inherits the dict class (so behaves\nsimilarly to a dict). Dates without a key in the Holiday object are not\nholidays.\n\n\nThe key of the object is the date of the holiday and the value is the name\nof the holiday itself. When passing the date as a key, the date can be\nexpressed as one of the following formats:\n\n\n* datetime.datetime type;\n* datetime.date types;\n* a float representing a Unix timestamp;\n* or a string of any format (recognized by datetime.parse).\n\n\nThe key is always returned as a datetime.date object.\n\n\nTo maximize speed, the list of holidays is built as needed on the fly, one\ncalendar year at a time. When you instantiate the object, it is empty, but\nthe moment a key is accessed it will build that entire year’s list of\nholidays. To pre-populate holidays, instantiate the class with the years\nargument:\n\n\nus\\_holidays = holidays.US(years=2020)\n\n\nIt is generally instantiated using the `country\\_holidays()` function.\n\n```\n>>> from holidays import country\\_holidays\n>>> us\\_holidays = country\\_holidays('US')# For a specific subdivisions (e.g. state or province):\n>>> california\\_holidays = country\\_holidays('US', subdiv='CA')\n\n\nFor special (one-off) country-wide holidays handling use\n`special\\_holidays`:\n\n```\nspecial\\_holidays = {\n    1977: ((JUN, 7, \"Silver Jubilee of Elizabeth II\"),),\n    1981: ((JUL, 29, \"Wedding of\n\n==================\n Document 1 \n----------------\nFor a specific subdivisions (e.g. state or province):\n>>> california\\_holidays = country\\_holidays('US', subdiv='CA')\n\n\nFor special (one-off) country-wide holidays handling use\n`special\\_holidays`:\n\n```\nspecial\\_holidays = {\n    1977: ((JUN, 7, \"Silver Jubilee of Elizabeth II\"),),\n    1981: ((JUL, 29, \"Wedding of Charles and Diana\"),),\n    1999: ((DEC, 31, \"Millennium Celebrations\"),),\n    2002: ((JUN, 3, \"Golden Jubilee of Elizabeth II\"),),\n    2011: ((APR, 29, \"Wedding of William and Catherine\"),),\n    2012: ((JUN, 5, \"Diamond Jubilee of Elizabeth II\"),),\n    2022: (\n        (JUN, 3, \"Platinum Jubilee of Elizabeth II\"),\n        (SEP, 19, \"State Funeral of Queen Elizabeth II\"),\n    ),\n}\n\ndef \\_populate(self, year):\n    super().\\_populate(year)\n\n    ...\n\nParameters:\n* **years** (*Set**[**int**]*) – The year(s) to pre-calculate public holidays for at instantiation.\n* **expand** (*bool*) – Whether the entire year is calculated when one date from that year\nis requested.\n* **observed** (*bool*) – Whether to include the dates when public holiday are observed\n(e.g. a holiday falling on a Sunday being observed the\nfollowing Monday). This doesn’t work for all countries.\n* **subdiv** (*str* *|* *None*) – The subdivision (e.g. state or province); not implemented for all\ncountries (see documentation).\n* **prov** (*str* *|* *None*) – *deprecated* use subdiv instead.\n* **state** (*str* *|* *None*) – *deprecated* use subdiv instead.\n* **language** (*str* *|* *None*) – The language which the returned holiday names will be translated\ninto. It must be an ISO 639-1 (2-letter) language code. If the\nlanguage translation is not supported the original holiday names\nwill be used.\n* **categories** (*Set**[**str**]* *|* *None*) – Requested holiday categories.\n\n\ncountry*: str*\nThe country’s ISO 3166-1 alpha-2 code.\n\nmarket*: str*\nThe market’s ISO 3166-1 alpha-2 code.\n\nsubdivisions*: Tuple[str, ...]* *= ()*\nThe subdivisions supported for this country (see documentation).\n\nspecial\\_holidays*: Dict[int, Tuple[int, int, str] | Tuple[Tuple[int, int, str], ...]]* *= {}*\nA list of the country-wide special (as opposite to regular) holidays for\na specific year.\n\nsubstituted\\_holidays*: Dict[int, Tuple[int, int, int, int] | Tuple[int, int, int, int, int] | Tuple[Tuple[int, int, int, int] | Tuple[int, int, int, int, int], ...]]* *= {}*\nA list of the country-wide substituted holidays for a specific year.\n\nweekend*: Set[int]* *= {5, 6}*\nCountry weekend days.\n\ndefault\\_language*: str | None* *= None*\nThe entity language used by default.\n\nsupported\\_categories*: Set[str]* *= {}*\nAll holiday categories supported by this entity.\n\nsupported\\_languages*: Tuple[str, ...]* *= ()*\nAll languages supported by this entity.\n\nexpand*: bool*\nWhether the entire year is calculated when one date from that year\nis requested.\n\nobserved*: bool*\nWhether dates when public holiday are observed are included.\n\ncategories*: Set[str] | None* *= None*\nRequested holiday categories.\n\nsubdiv*: str | None* *= None*\nThe subdiv requested.\n\nyears*: Set[int]*\nThe years calculated.\n\nclear() → None.  Remove all items from D.\n\nfromkeys(*value=None*, */*)\nCreate a new dictionary with keys from iterable and values set to value.\n\nitems() → a set-like object providing a view on D's items\n\nkeys() → a set-like object providing a view on D's keys\n\npopitem()\nRemove and return a (key, value) pair as a 2-tuple.\n\n\nPairs are returned in LIFO (last-in, first-out) order.\nRaises KeyError if the dict is empty.\n\nsetdefault(*key*, *default=None*, */*)\nInsert key with a value of default if key is not in the dictionary.\n\n\nReturn the value for key if key is in the dictionary, else default.\n\nvalues() → an object providing a view on D's values\n\nappend(*\\*args*)\nAlias for `update()` to mimic list type.\n\nParameters:\n**args** (*Dict**[**date* *|* *datetime* *|* *str* *|* *float* *|* *int**,* *str**]* *|* *List**[**date* *|* *datetime* *|* *str* *|* *float* *|* *int**]* *|* *date* *|* *datetime* *|* *str* *|* *float* *|* *int*) – \n\nReturn type:\n*None*\n\ncopy()\nReturn a copy of the object.\n\nget(*key*, *default=None*)\nReturn the holiday name for a date if date is a holiday, else\ndefault. If default is not given, it defaults to None, so that this\nmethod never raises a KeyError. If more than one holiday is present,\nthey are separated by a comma.\n\nParameters:\n* **key** (*date* *|* *datetime* *|* *str* *|* *float* *|* *int*) – The date expressed in one of the following types:\n\n\t+ `datetime.date`,\n\t+ `datetime.datetime`,\n\t+ a `str` of any format recognized by\n\t`dateutil.parser.parse()`,\n\t+ or a `float` or `int` representing a POSIX\n\ttimestamp.\n* **default** (*str* *|* *Any*) – The default value to return if no value is found.\n\nReturn type:\n*str* | *Any*\n\nget\\_list(*key*)\nReturn a list of all holiday names for a date if date is a holiday,\nelse empty string.\n\nParameters:\n**key** (*date* *|* *datetime* *|* *str* *|* *float* *|* *int*) – The date expressed in one of the following types:\n\n\n* `datetime.date`,\n* `datetime.datetime`,\n* a `str` of any format recognized by\n`dateutil.parser.parse()`,\n* or a `float` or `int` representing a POSIX\ntimestamp.\n\n\nReturn type:\n*List*[*str*]\n\nget\\_named(*holiday\\_name*, *lookup='icontains'*, *split\\_multiple\\_names=True*)\nReturn a list of all holiday dates matching the provided holiday\nname. The match will be made case insensitively and partial matches\nwill be included by default.\n\nParameters:\n* **holiday\\_name** (*str*) – The holiday’s name to try to match.\n* **lookup** – \nThe holiday name lookup type:contains - case sensitive contains match;\nexact - case sensitive exact match;\nstartswith - case sensitive starts with match;\nicontains - case insensitive contains match;\niexact - case insensitive exact match;\nistartswith - case insensitive starts with match;\n* **split\\_multiple\\_names** – Either use the exact name for each date or split it by holiday\nname delimiter.\n\nReturns:\nA list of all holiday dates matching the provided holiday name.\n\nReturn type:\n*List*[*date*]\n\npop(*key*, *default=None*)\nIf date is a holiday, remove it and return its date, else return\ndefault.\n\n\t+ `datetime.date`,\n\t+ `datetime.datetime`,\n\t+ a `str` of any format recognized by\n\t`dateutil.parser.parse()`,\n\t+ or a `float` or `int` representing a POSIX\n\ttimestamp.\n* **default** (*str* *|* *Any*) – The default value to return if no match is found.\n\nReturns:\nThe date removed.\n\nRaise:\nKeyError if date is not a holiday and default is not given.\n\npop\\_named(*name*)\nRemove (no longer treat at as holiday) all dates matching the\nprovided holiday name. The match will be made case insensitively and\npartial matches will be removed.\n\nParameters:\n* **name** (*str*) – The holiday’s name to try to match.\n* **default** – The default value to return if no match is found.\n\nReturns:\nA list of dates removed.\n\nupdate(*\\*args*)\nUpdate the object, overwriting existing dates.\n\nParam:\nEither another dictionary object where keys are dates and values\nare holiday names, or a single date (or a list of dates) for which\nthe value will be set to “Holiday”.\n\n\nDates can be expressed in one or more of the following types:\n\n\n*class* holidays.holiday\\_base.HolidaySum(*h1*, *h2*)\nBases: `HolidayBase`\n\n\nReturns a `dict`-like object resulting from the addition of two or\nmore individual dictionaries of public holidays. The original dictionaries\nare available as a `list` in the attribute `holidays,` and\n`country` and `subdiv` attributes are added\ntogether and could become `list` s. Holiday names, when different,\nare merged. All years are calculated (expanded) for all operands.\n\nParameters:\n* **h1** (*HolidayBase* *|* *HolidaySum*) – The first HolidayBase object to add.\n* **h2** (*HolidayBase* *|* *HolidaySum*) – The other HolidayBase object to add.\n\n\nExample:\n\n```\n>>> from holidays import country\\_holidays\n>>> nafta\\_holidays = country\\_holidays('US', years=2020) + country\\_holidays('CA') + country\\_holidays('MX')\n>>> dates = sorted(nafta\\_holidays.items(), key=lambda x: x[0])\n>>> from pprint import pprint\n>>> pprint(dates[:10], width=72)\n[(datetime.date(2020, 1, 1), \"Año Nuevo\"),\n (datetime.date(2020, 1, 20), 'Martin Luther King Jr. Day'),\n (datetime.date(2020, 2, 3),\n 'Día de la Constitución'),\n (datetime.date(2020, 2, 17), \"Washington's Birthday, Family Day\"),\n (datetime.date(2020, 3, 16),\n \"Natalicio de Benito Juárez\"),\n (datetime.date(2020, 4, 10), 'Good Friday'),\n (datetime.date(2020, 5, 1), 'Día del Trabajo'),\n (datetime.date(2020, 5, 18), 'Victoria Day')]\n\n\nappend(*\\*args*)\nAlias for `update()` to mimic list type.\n\ncountry*: str | List[str]*\nCountries included in the addition.\n\nmarket*: str | List[str]*\nMarkets included in the addition.\n\nsubdiv*: str | List[str] | None* *= None*\nSubdivisions included in the addition.\n\nholidays*: List[HolidayBase]*\nThe original HolidayBase objects included in the addition.\n\n==================\n Document 2 \n----------------\n API Reference\n\n\nholidays.utils.country\\_holidays(*country*, *subdiv=None*, *years=None*, *expand=True*, *observed=True*, *prov=None*, *state=None*, *language=None*, *categories=None*)\nReturns a new dictionary-like `HolidayBase` object for the public\nholidays of the country matching **country** and other keyword arguments.\n\nParameters:\n* **country** (*str*) – An ISO 3166-1 Alpha-2 country code.\n* **subdiv** (*str* *|* *None*) – The subdivision (e.g. state or province); not implemented for all\ncountries (see documentation).\n* **years** (*int* *|* *Iterable**[**int**]* *|* *None*) – The year(s) to pre-calculate public holidays for at instantiation.\n* **expand** (*bool*) – Whether the entire year is calculated when one date from that year\nis requested.\n* **observed** (*bool*) – Whether to include the dates of when public holiday are observed\n(e.g. a holiday falling on a Sunday being observed the following\nMonday). False may not work for all countries.\n* **prov** (*str* *|* *None*) – *deprecated* use subdiv instead.\n* **state** (*str* *|* *None*) – *deprecated* use subdiv instead.\n* **language** (*str* *|* *None*) – The language which the returned holiday names will be translated\ninto. It must be an ISO 639-1 (2-letter) language code. If the\nlanguage translation is not supported the original holiday names\nwill be used.\n* **categories** (*Tuple**[**str**]* *|* *None*) – Requested holiday categories.\n\nReturns:\nA `HolidayBase` object matching the **country**.\n\nReturn type:\n*HolidayBase*\n\n\nThe key of the `dict`-like `HolidayBase` object is the\ndate of the holiday, and the value is the name of the holiday itself.\nDates where a key is not present are not public holidays (or, if\n**observed** is False, days when a public holiday is observed).\n\n\nWhen passing the date as a key, the date can be expressed in one of the\nfollowing types:\n\n\n* `datetime.date`,\n* `datetime.datetime`,\n* a `str` of any format recognized by `dateutil.parser.parse()`,\n* or a `float` or `int` representing a POSIX timestamp.\n\n\nThe key is always returned as a `datetime.date` object.\n\n\nTo maximize speed, the list of public holidays is built on the fly as\nneeded, one calendar year at a time. When the object is instantiated\nwithout a **years** parameter, it is empty, but, unless **expand** is set\nto False, as soon as a key is accessed the class will calculate that entire\nyear’s list of holidays and set the keys with them.\n\n\nIf you need to list the holidays as opposed to querying individual dates,\ninstantiate the class with the **years** parameter.\n\n\nExample usage:\n\n```\n>>> from holidays import country\\_holidays\n>>> us\\_holidays = country\\_holidays('US')# For a specific subdivision (e.g. state or province):\n>>> calif\\_holidays = country\\_holidays('US', subdiv='CA')\n\n```\n\n\nThe below will cause 2015 holidays to be calculated on the fly:\n\n```\n>>> from datetime import date\n>>> assert date(2015, 1, 1) in us\\_holidays\n\n\nThis will be faster because 2015 holidays"
    },
    {
        "instruction": "Create a Python program using the 'holidays' API to list all the public holidays in a specific region (e.g., a city or district) within a country and state for a given year.",
        "api": "holidays",
        "output": "#!pip install holidays\nimport holidays\n\n# Specify the country or region (e.g., 'US' for the United States)\ncountry_code = 'US'\n\n# Specify the state or province code (e.g., 'CA' for California)\nstate_code = 'CA'\n\n# Specify the region (e.g., 'LA' for Los Angeles)\nregion = 'LA'\n\n# Specify the year for which you want to list holidays\nyear = 2023\n\n# Create a dictionary of holidays for the specified region and year\nholiday_dict = holidays.RegionHoliday(country_code, region, observed=True, years=year)\n\n# List all the public holidays\nholiday_list = sorted(holiday_dict.keys())\n\n# Display the list of holidays\nprint(f\"Public holidays in {region}, {state_code}, {country_code} for the year {year}:\")\nfor holiday in holiday_list:\n    print(holiday)",
        "documentation": "\n\n==================\n Document 0 \n----------------\nFor a specific subdivisions (e.g. state or province):\n>>> california\\_holidays = country\\_holidays('US', subdiv='CA')\n\n\nFor special (one-off) country-wide holidays handling use\n`special\\_holidays`:\n\n```\nspecial\\_holidays = {\n    1977: ((JUN, 7, \"Silver Jubilee of Elizabeth II\"),),\n    1981: ((JUL, 29, \"Wedding of Charles and Diana\"),),\n    1999: ((DEC, 31, \"Millennium Celebrations\"),),\n    2002: ((JUN, 3, \"Golden Jubilee of Elizabeth II\"),),\n    2011: ((APR, 29, \"Wedding of William and Catherine\"),),\n    2012: ((JUN, 5, \"Diamond Jubilee of Elizabeth II\"),),\n    2022: (\n        (JUN, 3, \"Platinum Jubilee of Elizabeth II\"),\n        (SEP, 19, \"State Funeral of Queen Elizabeth II\"),\n    ),\n}\n\ndef \\_populate(self, year):\n    super().\\_populate(year)\n\n    ...\n\nParameters:\n* **years** (*Set**[**int**]*) – The year(s) to pre-calculate public holidays for at instantiation.\n* **expand** (*bool*) – Whether the entire year is calculated when one date from that year\nis requested.\n* **observed** (*bool*) – Whether to include the dates when public holiday are observed\n(e.g. a holiday falling on a Sunday being observed the\nfollowing Monday). This doesn’t work for all countries.\n* **subdiv** (*str* *|* *None*) – The subdivision (e.g. state or province); not implemented for all\ncountries (see documentation).\n* **prov** (*str* *|* *None*) – *deprecated* use subdiv instead.\n* **state** (*str* *|* *None*) – *deprecated* use subdiv instead.\n* **language** (*str* *|* *None*) – The language which the returned holiday names will be translated\ninto. It must be an ISO 639-1 (2-letter) language code. If the\nlanguage translation is not supported the original holiday names\nwill be used.\n* **categories** (*Set**[**str**]* *|* *None*) – Requested holiday categories.\n\n\ncountry*: str*\nThe country’s ISO 3166-1 alpha-2 code.\n\nmarket*: str*\nThe market’s ISO 3166-1 alpha-2 code.\n\nsubdivisions*: Tuple[str, ...]* *= ()*\nThe subdivisions supported for this country (see documentation).\n\nspecial\\_holidays*: Dict[int, Tuple[int, int, str] | Tuple[Tuple[int, int, str], ...]]* *= {}*\nA list of the country-wide special (as opposite to regular) holidays for\na specific year.\n\nsubstituted\\_holidays*: Dict[int, Tuple[int, int, int, int] | Tuple[int, int, int, int, int] | Tuple[Tuple[int, int, int, int] | Tuple[int, int, int, int, int], ...]]* *= {}*\nA list of the country-wide substituted holidays for a specific year.\n\nweekend*: Set[int]* *= {5, 6}*\nCountry weekend days.\n\ndefault\\_language*: str | None* *= None*\nThe entity language used by default.\n\nsupported\\_categories*: Set[str]* *= {}*\nAll holiday categories supported by this entity.\n\nsupported\\_languages*: Tuple[str, ...]* *= ()*\nAll languages supported by this entity.\n\nexpand*: bool*\nWhether the entire year is calculated when one date from that year\nis requested.\n\nobserved*: bool*\nWhether dates when public holiday are observed are included.\n\ncategories*: Set[str] | None* *= None*\nRequested holiday categories.\n\nsubdiv*: str | None* *= None*\nThe subdiv requested.\n\nyears*: Set[int]*\nThe years calculated.\n\nclear() → None.  Remove all items from D.\n\nfromkeys(*value=None*, */*)\nCreate a new dictionary with keys from iterable and values set to value.\n\nitems() → a set-like object providing a view on D's items\n\nkeys() → a set-like object providing a view on D's keys\n\npopitem()\nRemove and return a (key, value) pair as a 2-tuple.\n\n\nPairs are returned in LIFO (last-in, first-out) order.\nRaises KeyError if the dict is empty.\n\nsetdefault(*key*, *default=None*, */*)\nInsert key with a value of default if key is not in the dictionary.\n\n\nReturn the value for key if key is in the dictionary, else default.\n\nvalues() → an object providing a view on D's values\n\nappend(*\\*args*)\nAlias for `update()` to mimic list type.\n\nParameters:\n**args** (*Dict**[**date* *|* *datetime* *|* *str* *|* *float* *|* *int**,* *str**]* *|* *List**[**date* *|* *datetime* *|* *str* *|* *float* *|* *int**]* *|* *date* *|* *datetime* *|* *str* *|* *float* *|* *int*) – \n\nReturn type:\n*None*\n\ncopy()\nReturn a copy of the object.\n\nget(*key*, *default=None*)\nReturn the holiday name for a date if date is a holiday, else\ndefault. If default is not given, it defaults to None, so that this\nmethod never raises a KeyError. If more than one holiday is present,\nthey are separated by a comma.\n\nParameters:\n* **key** (*date* *|* *datetime* *|* *str* *|* *float* *|* *int*) – The date expressed in one of the following types:\n\n\t+ `datetime.date`,\n\t+ `datetime.datetime`,\n\t+ a `str` of any format recognized by\n\t`dateutil.parser.parse()`,\n\t+ or a `float` or `int` representing a POSIX\n\ttimestamp.\n* **default** (*str* *|* *Any*) – The default value to return if no value is found.\n\nReturn type:\n*str* | *Any*\n\nget\\_list(*key*)\nReturn a list of all holiday names for a date if date is a holiday,\nelse empty string.\n\nParameters:\n**key** (*date* *|* *datetime* *|* *str* *|* *float* *|* *int*) – The date expressed in one of the following types:\n\n\n* `datetime.date`,\n* `datetime.datetime`,\n* a `str` of any format recognized by\n`dateutil.parser.parse()`,\n* or a `float` or `int` representing a POSIX\ntimestamp.\n\n\nReturn type:\n*List*[*str*]\n\nget\\_named(*holiday\\_name*, *lookup='icontains'*, *split\\_multiple\\_names=True*)\nReturn a list of all holiday dates matching the provided holiday\nname. The match will be made case insensitively and partial matches\nwill be included by default.\n\nParameters:\n* **holiday\\_name** (*str*) – The holiday’s name to try to match.\n* **lookup** – \nThe holiday name lookup type:contains - case sensitive contains match;\nexact - case sensitive exact match;\nstartswith - case sensitive starts with match;\nicontains - case insensitive contains match;\niexact - case insensitive exact match;\nistartswith - case insensitive starts with match;\n* **split\\_multiple\\_names** – Either use the exact name for each date or split it by holiday\nname delimiter.\n\nReturns:\nA list of all holiday dates matching the provided holiday name.\n\nReturn type:\n*List*[*date*]\n\npop(*key*, *default=None*)\nIf date is a holiday, remove it and return its date, else return\ndefault.\n\n\t+ `datetime.date`,\n\t+ `datetime.datetime`,\n\t+ a `str` of any format recognized by\n\t`dateutil.parser.parse()`,\n\t+ or a `float` or `int` representing a POSIX\n\ttimestamp.\n* **default** (*str* *|* *Any*) – The default value to return if no match is found.\n\nReturns:\nThe date removed.\n\nRaise:\nKeyError if date is not a holiday and default is not given.\n\npop\\_named(*name*)\nRemove (no longer treat at as holiday) all dates matching the\nprovided holiday name. The match will be made case insensitively and\npartial matches will be removed.\n\nParameters:\n* **name** (*str*) – The holiday’s name to try to match.\n* **default** – The default value to return if no match is found.\n\nReturns:\nA list of dates removed.\n\nupdate(*\\*args*)\nUpdate the object, overwriting existing dates.\n\nParam:\nEither another dictionary object where keys are dates and values\nare holiday names, or a single date (or a list of dates) for which\nthe value will be set to “Holiday”.\n\n\nDates can be expressed in one or more of the following types:\n\n\n*class* holidays.holiday\\_base.HolidaySum(*h1*, *h2*)\nBases: `HolidayBase`\n\n\nReturns a `dict`-like object resulting from the addition of two or\nmore individual dictionaries of public holidays. The original dictionaries\nare available as a `list` in the attribute `holidays,` and\n`country` and `subdiv` attributes are added\ntogether and could become `list` s. Holiday names, when different,\nare merged. All years are calculated (expanded) for all operands.\n\nParameters:\n* **h1** (*HolidayBase* *|* *HolidaySum*) – The first HolidayBase object to add.\n* **h2** (*HolidayBase* *|* *HolidaySum*) – The other HolidayBase object to add.\n\n\nExample:\n\n```\n>>> from holidays import country\\_holidays\n>>> nafta\\_holidays = country\\_holidays('US', years=2020) + country\\_holidays('CA') + country\\_holidays('MX')\n>>> dates = sorted(nafta\\_holidays.items(), key=lambda x: x[0])\n>>> from pprint import pprint\n>>> pprint(dates[:10], width=72)\n[(datetime.date(2020, 1, 1), \"Año Nuevo\"),\n (datetime.date(2020, 1, 20), 'Martin Luther King Jr. Day'),\n (datetime.date(2020, 2, 3),\n 'Día de la Constitución'),\n (datetime.date(2020, 2, 17), \"Washington's Birthday, Family Day\"),\n (datetime.date(2020, 3, 16),\n \"Natalicio de Benito Juárez\"),\n (datetime.date(2020, 4, 10), 'Good Friday'),\n (datetime.date(2020, 5, 1), 'Día del Trabajo'),\n (datetime.date(2020, 5, 18), 'Victoria Day')]\n\n\nappend(*\\*args*)\nAlias for `update()` to mimic list type.\n\ncountry*: str | List[str]*\nCountries included in the addition.\n\nmarket*: str | List[str]*\nMarkets included in the addition.\n\nsubdiv*: str | List[str] | None* *= None*\nSubdivisions included in the addition.\n\nholidays*: List[HolidayBase]*\nThe original HolidayBase objects included in the addition.\n\n==================\n Document 1 \n----------------\n For a specific subdivision (e.g. state or province):\n>>> calif\\_holidays = country\\_holidays('US', subdiv='CA')\n\n```\n\n\nThe below will cause 2015 holidays to be calculated on the fly:\n\n```\n>>> from datetime import date\n>>> assert date(2015, 1, 1) in us\\_holidays\n\n\nThis will be faster because 2015 holidays are already calculated:\n\n```\n>>> assert date(2015, 1, 2) not in us\\_holidays\n\n\nThe `HolidayBase` class also recognizes strings of many formats\nand numbers representing a POSIX timestamp:\n\n```\n>>> assert '2014-01-01' in us\\_holidays\n>>> assert '1/1/2014' in us\\_holidays\n>>> assert 1388597445 in us\\_holidays\n\n\nShow the holiday’s name:\n\n```\n>>> us\\_holidays.get('2014-01-01')\n\"New Year's Day\"\n\n\nCheck a range:\n\n```\n>>> us\\_holidays['2014-01-01': '2014-01-03']\n[datetime.date(2014, 1, 1)]\n\n\nList all 2020 holidays:\n\n```\n>>> us\\_holidays = country\\_holidays('US', years=2020)\n>>> for day in us\\_holidays.items():\n...     print(day)\n(datetime.date(2020, 1, 1), \"New Year's Day\")\n(datetime.date(2020, 1, 20), 'Martin Luther King Jr. Day')\n(datetime.date(2020, 2, 17), \"Washington's Birthday\")\n(datetime.date(2020, 5, 25), 'Memorial Day')\n(datetime.date(2020, 7, 4), 'Independence Day')\n(datetime.date(2020, 7, 3), 'Independence Day (Observed)')\n(datetime.date(2020, 9, 7), 'Labor Day')\n(datetime.date(2020, 10, 12), 'Columbus Day')\n(datetime.date(2020, 11, 11), 'Veterans Day')\n(datetime.date(2020, 11, 26), 'Thanksgiving')\n(datetime.date(2020, 12, 25), 'Christmas Day')\n\n\nSome holidays are only present in parts of a country:\n\n```\n>>> us\\_pr\\_holidays = country\\_holidays('US', subdiv='PR')\n>>> assert '2018-01-06' not in us\\_holidays\n>>> assert '2018-01-06' in us\\_pr\\_holidays\n\n\nAppend custom holiday dates by passing one of:\n\n\n* a `dict` with date/name key/value pairs (e.g.\n`{'2010-07-10': 'My birthday!'}`),\n* a list of dates (as a `datetime.date`, `datetime.datetime`,\n`str`, `int`, or `float`); `'Holiday'` will be\nused as a description,\n* or a single date item (of one of the types above); `'Holiday'` will be\nused as a description:\n\n```\n>>> custom\\_holidays = country\\_holidays('US', years=2015)\n>>> custom\\_holidays.update({'2015-01-01': \"New Year's Day\"})\n>>> custom\\_holidays.update(['2015-07-01', '07/04/2015'])\n>>> custom\\_holidays.update(date(2015, 12, 25))\n>>> assert date(2015, 1, 1) in custom\\_holidays\n>>> assert date(2015, 1, 2) not in custom\\_holidays\n>>> assert '12/25/2015' in custom\\_holidays\n\n\nFor more complex logic, like 4th Monday of January, you can inherit the\n`HolidayBase` class and define your own `\\_populate()` method.\nSee documentation for examples.\n\nholidays.utils.financial\\_holidays(*market*, *subdiv=None*, *years=None*, *expand=True*, *observed=True*, *language=None*)\nReturns a new dictionary-like `HolidayBase` object for the public\nholidays of the financial market matching **market** and other keyword\narguments.\n\nParameters:\n* **market** (*str*) – An ISO 3166-1 Alpha-2 market code.\n* **subdiv** (*str* *|* *None*) – Currently not implemented for markets (see documentation).\n* **years** (*int* *|* *Iterable**[**int**]* *|* *None*) – The year(s) to pre-calculate public holidays for at instantiation.\n* **expand** (*bool*) – Whether the entire year is calculated when one date from that year\nis requested.\n* **observed** (*bool*) – Whether to include the dates of when public holiday are observed\n(e.g. a holiday falling on a Sunday being observed the following\nMonday). False may not work for all countries.\n* **language** (*str* *|* *None*) – The language which the returned holiday names will be translated\ninto. It must be an ISO 639-1 (2-letter) language code. If the\nlanguage translation is not supported the original holiday names\nwill be used.\n\nReturns:\nA `HolidayBase` object matching the **market**.\n\n```\n>>> from holidays import financial\\_holidays\n>>> nyse\\_holidays = financial\\_holidays('NYSE')\n\n\nSee `country\\_holidays()` documentation for further details and\nexamples.\n\nholidays.utils.list\\_localized\\_countries(*include\\_aliases=True*)\nGet all localized countries and languages they support.\n\nParameters:\n**include\\_aliases** – Whether to include entity aliases (e.g. UK for GB).\n\nReturns:\nA dictionary where key is an ISO 3166-1 alpha-2 country code and\nvalue is a list of supported languages (either ISO 639-1 or a\ncombination of ISO 639-1 and ISO 3166-1 codes joined with “\\_”).\n\nReturn type:\n*Dict*[*str*, *List*[*str*]]\n\nholidays.utils.list\\_localized\\_financial(*include\\_aliases=True*)\nGet all localized financial markets and languages they support.\n\nParameters:\n**include\\_aliases** – Whether to include entity aliases(e.g. TAR for ECB, XNYS for NYSE).\n\nReturns:\nA dictionary where key is a market code and value is a list of\nsupported subdivision codes.\n\nholidays.utils.list\\_supported\\_countries(*include\\_aliases=True*)\nGet all supported countries and their subdivisions.\n\nReturns:\nA dictionary where key is an ISO 3166-1 alpha-2 country code and\nvalue is a list of supported subdivision codes.\n\nholidays.utils.list\\_supported\\_financial(*include\\_aliases=True*)\nGet all supported financial markets and their subdivisions.\n\n*class* holidays.holiday\\_base.HolidayBase(*years=None*, *expand=True*, *observed=True*, *subdiv=None*, *prov=None*, *state=None*, *language=None*, *categories=None*)\nBases: `Dict`[`date`, `str`]\n\n\nA dict-like object containing the holidays for a specific country (and\nprovince or state if so initiated); inherits the dict class (so behaves\nsimilarly to a dict). Dates without a key in the Holiday object are not\nholidays.\n\n\nThe key of the object is the date of the holiday and the value is the name\nof the holiday itself. When passing the date as a key, the date can be\nexpressed as one of the following formats:\n\n\n* datetime.datetime type;\n* datetime.date types;\n* a float representing a Unix timestamp;\n* or a string of any format (recognized by datetime.parse).\n\n\nThe key is always returned as a datetime.date object.\n\n\nTo maximize speed, the list of holidays is built as needed on the fly, one\ncalendar year at a time. When you instantiate the object, it is empty, but\nthe moment a key is accessed it will build that entire year’s list of\nholidays. To pre-populate holidays, instantiate the class with the years\nargument:\n\n\nus\\_holidays = holidays.US(years=2020)\n\n\nIt is generally instantiated using the `country\\_holidays()` function.\n\n```\n>>> from holidays import country\\_holidays\n>>> us\\_holidays = country\\_holidays('US')# For a specific subdivisions (e.g. state or province):\n>>> california\\_holidays = country\\_holidays('US', subdiv='CA')\n\n\nFor special (one-off) country-wide holidays handling use\n`special\\_holidays`:\n\n```\nspecial\\_holidays = {\n    1977: ((JUN, 7, \"Silver Jubilee of Elizabeth II\"),),\n    1981: ((JUL, 29, \"Wedding of\n\n==================\n Document 2 \n----------------\n API Reference\n\n\nholidays.utils.country\\_holidays(*country*, *subdiv=None*, *years=None*, *expand=True*, *observed=True*, *prov=None*, *state=None*, *language=None*, *categories=None*)\nReturns a new dictionary-like `HolidayBase` object for the public\nholidays of the country matching **country** and other keyword arguments.\n\nParameters:\n* **country** (*str*) – An ISO 3166-1 Alpha-2 country code.\n* **subdiv** (*str* *|* *None*) – The subdivision (e.g. state or province); not implemented for all\ncountries (see documentation).\n* **years** (*int* *|* *Iterable**[**int**]* *|* *None*) – The year(s) to pre-calculate public holidays for at instantiation.\n* **expand** (*bool*) – Whether the entire year is calculated when one date from that year\nis requested.\n* **observed** (*bool*) – Whether to include the dates of when public holiday are observed\n(e.g. a holiday falling on a Sunday being observed the following\nMonday). False may not work for all countries.\n* **prov** (*str* *|* *None*) – *deprecated* use subdiv instead.\n* **state** (*str* *|* *None*) – *deprecated* use subdiv instead.\n* **language** (*str* *|* *None*) – The language which the returned holiday names will be translated\ninto. It must be an ISO 639-1 (2-letter) language code. If the\nlanguage translation is not supported the original holiday names\nwill be used.\n* **categories** (*Tuple**[**str**]* *|* *None*) – Requested holiday categories.\n\nReturns:\nA `HolidayBase` object matching the **country**.\n\nReturn type:\n*HolidayBase*\n\n\nThe key of the `dict`-like `HolidayBase` object is the\ndate of the holiday, and the value is the name of the holiday itself.\nDates where a key is not present are not public holidays (or, if\n**observed** is False, days when a public holiday is observed).\n\n\nWhen passing the date as a key, the date can be expressed in one of the\nfollowing types:\n\n\n* `datetime.date`,\n* `datetime.datetime`,\n* a `str` of any format recognized by `dateutil.parser.parse()`,\n* or a `float` or `int` representing a POSIX timestamp.\n\n\nThe key is always returned as a `datetime.date` object.\n\n\nTo maximize speed, the list of public holidays is built on the fly as\nneeded, one calendar year at a time. When the object is instantiated\nwithout a **years** parameter, it is empty, but, unless **expand** is set\nto False, as soon as a key is accessed the class will calculate that entire\nyear’s list of holidays and set the keys with them.\n\n\nIf you need to list the holidays as opposed to querying individual dates,\ninstantiate the class with the **years** parameter.\n\n\nExample usage:\n\n```\n>>> from holidays import country\\_holidays\n>>> us\\_holidays = country\\_holidays('US')# For a specific subdivision (e.g. state or province):\n>>> calif\\_holidays = country\\_holidays('US', subdiv='CA')\n\n```\n\n\nThe below will cause 2015 holidays to be calculated on the fly:\n\n```\n>>> from datetime import date\n>>> assert date(2015, 1, 1) in us\\_holidays\n\n\nThis will be faster because 2015 holidays"
    },
    {
        "instruction": "Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any valid dates in the format 'YYYY-MM-DD'. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the date checking function.",
        "api": "hypothesis",
        "output": "#!pip install hypothesis\nimport re\nimport unittest\nfrom hypothesis import given, strategies as st\n\ndef is_valid_date(date):\n    pattern = r\"^\\d{4}-\\d{2}-\\d{2}$\"\n    return re.match(pattern, date) is not None\n\ndef has_valid_dates(lst):\n    return any(is_valid_date(s) for s in lst)\n\nclass HasValidDatesTest(unittest.TestCase):\n    @given(st.lists(st.text(min_size=10, max_size=10, alphabet=st.characters(whitelist_categories=('Nd', 'Pd')))))\n    def test_has_valid_dates(self, lst):\n        result = has_valid_dates(lst)\n        self.assertEqual(result, any(is_valid_date(s) for s in lst))\n\nif __name__ == \"__main__\":\n    unittest.main()",
        "documentation": "\n\n==================\n Document 0 \n----------------\n Welcome to Hypothesis!¶\n\n\nHypothesis is a Python library for\ncreating unit tests which are simpler to write and more powerful when run,\nfinding edge cases in your code you wouldn’t have thought to look for. It is\nstable, powerful and easy to add to any existing test suite.\n\n\nIt works by letting you write tests that assert that something should be true\nfor every case, not just the ones you happen to think of.\n\n\nThink of a normal unit test as being something like the following:\n\n\n1. Set up some data.\n2. Perform some operations on the data.\n3. Assert something about the result.\n\n\nHypothesis lets you write tests which instead look like this:\n\n\n1. For all data matching some specification.\n2. Perform some operations on the data.\n3. Assert something about the result.\n\n\nThis is often called property-based testing, and was popularised by the\nHaskell library Quickcheck.\n\n\nIt works by generating arbitrary data matching your specification and checking\nthat your guarantee still holds in that case. If it finds an example where it doesn’t,\nit takes that example and cuts it down to size, simplifying it until it finds a\nmuch smaller example that still causes the problem. It then saves that example\nfor later, so that once it has found a problem with your code it will not forget\nit in the future.\n\n\nWriting tests of this form usually consists of deciding on guarantees that\nyour code should make - properties that should always hold true,\nregardless of what the world throws at you. Examples of such guarantees\nmight be:\n\n\n* Your code shouldn’t throw an exception, or should only throw a particular\ntype of exception (this works particularly well if you have a lot of internal\nassertions).\n* If you delete an object, it is no longer visible.\n* If you serialize and then deserialize a value, then you get the same value back.\n\n\nNow you know the basics of what Hypothesis does, the rest of this\ndocumentation will take you through how and why. It’s divided into a\nnumber of sections, which you can see in the sidebar (or the\nmenu at the top if you’re on mobile), but you probably want to begin with\nthe Quick start guide, which will give you a worked\nexample of how to use Hypothesis and a detailed outline\nof the things you need to know to begin testing your code with it, or\ncheck out some of the\nintroductory articles.\n\n# Quick start guide¶\n\n\nThis document should talk you through everything you need to get started with\nHypothesis.\n\n## An example¶\n\n\nSuppose we’ve written a run length encoding\nsystem and we want to test it out.\n\n\nWe have the following code which I took straight from the\nRosetta Code wiki (OK, I\nremoved some commented out code and fixed the formatting, but there\n\n==================\n Document 1 \n----------------\n\n\n# Welcome to Hypothesis!¶\n\n\nHypothesis is a Python library for\ncreating unit tests which are simpler to write and more powerful when run,\nfinding edge cases in your code you wouldn’t have thought to look for. It is\nstable, powerful and easy to add\n\n==================\n Document 2 \n----------------\n# Fuzzing an HTTP API¶\n\n\nHypothesis’s support for testing HTTP services is somewhat nascent. There are\nplans for some fully featured things around this, but right now they’re\nprobably quite far down the line.\n\n\nBut you can do a lot yourself without any explicit support! Here’s a script\nI wrote to throw arbitrary data against the API for an entirely fictitious service\ncalled Waspfinder (this is only lightly obfuscated and you can easily figure\nout who I’m actually talking about, but I don’t want you to run this code and\nhammer their API without their permission).\n\n\nAll this does is use Hypothesis to generate arbitrary JSON data matching the\nformat their API asks for and check for 500 errors. More advanced tests which\nthen use the result and go on to do other things are definitely also possible.\nThe schemathesis package provides an excellent example of this!\n\n```\nimport math\nimport os\nimport random\nimport time\nimport unittest\nfrom collections import namedtuple\n\nimport requests\n\nfrom hypothesis import assume, given, strategies as st\n\nGoal = namedtuple(\"Goal\", (\"slug\",))\n\n# We just pass in our API credentials via environment variables.\nwaspfinder\\_token = os.getenv(\"WASPFINDER\\_TOKEN\")\nwaspfinder\\_user = os.getenv(\"WASPFINDER\\_USER\")\nassert waspfinder\\_token is not None\nassert waspfinder\\_user is not None\n\nGoalData = st.fixed\\_dictionaries(\n    {\n        \"title\": st.text(),\n\n==================\n Document 3 \n----------------\n## Other Python libraries¶\n\n\nHypothesis has *mandatory* dependencies on the following libraries:\n\n\n* attrs\n* sortedcontainers\n\n\nHypothesis has *optional* dependencies on the following libraries:\n\n```\nextras\\_require = {\n    \"cli\": [\"click>=7.0\", \"black>=19.10b0\", \"rich>=9.0.0\"],\n    \"codemods\": [\"libcst>=0.3.16\"],\n    \"ghostwriter\": [\"black>=19.10b0\"],\n    \"pytz\": [\"pytz>=2014.1\"],\n    \"dateutil\": [\"python-dateutil>=1.4\"],\n    \"lark\": [\"lark>=0.10.1\"],  # probably still works with old `lark-parser` too\n    \"numpy\": [\"numpy>=1.17.3\"],  # oldest with wheels for non-EOL Python (for now)\n    \"pandas\": [\"pandas>=1.1\"],\n    \"pytest\": [\"pytest>=4.6\"],\n    \"dpcontracts\": [\"dpcontracts>=0.4\"],\n    \"redis\": [\"redis>=3.0.0\"],\n    # zoneinfo is an odd one: every dependency is conditional, because they're\n    # only necessary on old versions of Python or Windows systems.\n    \"zoneinfo\": [\n        \"tzdata>=2023.3 ; sys\\_platform == 'win32'\",\n        \"backports.zoneinfo>=0.2.1 ; python\\_version<'3.9'\",\n    ],\n    # We only support Django versions with upstream support - see\n    # https://www.djangoproject.com/download/#supported-versions\n    # We also leave the choice of timezone library to the user, since it\n    # might be zoneinfo or pytz depending on version and configuration.\n    \"django\": [\"django>=3.2\"],\n}\n\n\nThe way this works when installing Hypothesis normally is that these features become available if the relevant\nlibrary is installed.\n\n\nSpecifically for pytest, our plugin supports versions of pytest which\nhave been out of upstream support for some time. Hypothesis tests can still\nbe executed by even older versions of pytest - you just won’t have the plugin\nto provide automatic marks, helpful usage warnings, and per-test statistics.\n\n## Testing Hypothesis¶\n\n\nIf you want to test Hypothesis as part of your packaging you will probably not want to use the mechanisms\nHypothesis itself uses for running its tests, because it has a lot of logic for installing and testing against\ndifferent versions of Python.\n\n\nThe tests must be run with fairly recent tooling; check the tree/master/requirements/\ndirectory for details.\n\n\nThe organisation of the tests is described in the hypothesis-python/tests/README.rst.\n\n\n\n## Examples¶\n\n\n* arch linux\n* fedora\n* gentoo\n\n\n# Reproducing failures¶\n\n\nOne of the things that is often concerning for people using randomized testing\nis the question of how to reproduce failing test cases.\n\n\nIt is better to think about the data Hypothesis generates as being\n*arbitrary*, rather than *random*. We deliberately"
    },
    {
        "instruction": "Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given number is a prime number. The 'hypothesis' library will be used to generate various input numbers and verify the correctness of the prime number checking function.",
        "api": "hypothesis",
        "output": "#!pip install hypothesis\nimport math\nimport unittest\nfrom hypothesis import given, strategies as st\n\ndef is_prime(n):\n    if n < 2:\n        return False\n    for i in range(2, int(math.sqrt(n))+1):\n        if n % i == 0:\n            return False\n    return True\n\nclass PrimeNumberTest(unittest.TestCase):\n    @given(st.integers(min_value=0, max_value=100))\n    def test_is_prime(self, n):\n        result = is_prime(n)\n        self.assertEqual(result, all(n % i != 0 for i in range(2, int(math.sqrt(n))+1)))\n\nif __name__ == \"__main__\":\n    unittest.main()",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# Welcome to Hypothesis!¶\n\n\nHypothesis is a Python library for\ncreating unit tests which are simpler to write and more powerful when run,\nfinding edge cases in your code you wouldn’t have thought to look for. It is\nstable, powerful and easy to add\n\n==================\n Document 1 \n----------------\n Welcome to Hypothesis!¶\n\n\nHypothesis is a Python library for\ncreating unit tests which are simpler to write and more powerful when run,\nfinding edge cases in your code you wouldn’t have thought to look for. It is\nstable, powerful and easy to add to any existing test suite.\n\n\nIt works by letting you write tests that assert that something should be true\nfor every case, not just the ones you happen to think of.\n\n\nThink of a normal unit test as being something like the following:\n\n\n1. Set up some data.\n2. Perform some operations on the data.\n3. Assert something about the result.\n\n\nHypothesis lets you write tests which instead look like this:\n\n\n1. For all data matching some specification.\n2. Perform some operations on the data.\n3. Assert something about the result.\n\n\nThis is often called property-based testing, and was popularised by the\nHaskell library Quickcheck.\n\n\nIt works by generating arbitrary data matching your specification and checking\nthat your guarantee still holds in that case. If it finds an example where it doesn’t,\nit takes that example and cuts it down to size, simplifying it until it finds a\nmuch smaller example that still causes the problem. It then saves that example\nfor later, so that once it has found a problem with your code it will not forget\nit in the future.\n\n\nWriting tests of this form usually consists of deciding on guarantees that\nyour code should make - properties that should always hold true,\nregardless of what the world throws at you. Examples of such guarantees\nmight be:\n\n\n* Your code shouldn’t throw an exception, or should only throw a particular\ntype of exception (this works particularly well if you have a lot of internal\nassertions).\n* If you delete an object, it is no longer visible.\n* If you serialize and then deserialize a value, then you get the same value back.\n\n\nNow you know the basics of what Hypothesis does, the rest of this\ndocumentation will take you through how and why. It’s divided into a\nnumber of sections, which you can see in the sidebar (or the\nmenu at the top if you’re on mobile), but you probably want to begin with\nthe Quick start guide, which will give you a worked\nexample of how to use Hypothesis and a detailed outline\nof the things you need to know to begin testing your code with it, or\ncheck out some of the\nintroductory articles.\n\n# Quick start guide¶\n\n\nThis document should talk you through everything you need to get started with\nHypothesis.\n\n## An example¶\n\n\nSuppose we’ve written a run length encoding\nsystem and we want to test it out.\n\n\nWe have the following code which I took straight from the\nRosetta Code wiki (OK, I\nremoved some commented out code and fixed the formatting, but there\n\n==================\n Document 2 \n----------------\n# Where to start¶\n\n\nYou should now know enough of the basics to write some tests for your code\nusing Hypothesis. The best way to learn is by doing, so go have a try.\n\n\nIf you’re stuck for ideas for how to use this sort of test for your code, here\nare some good starting points:\n\n\n1. Try just calling functions with appropriate arbitrary data and see if they\ncrash. You may be surprised how often this works. e.g. note that the first\nbug we found in the encoding example didn’t even get as far as our\nassertion: It crashed because it couldn’t handle the data we gave it, not\nbecause it did the wrong thing.\n2. Look for duplication in your tests. Are there any cases where you’re testing\nthe same thing with multiple different examples? Can you generalise that to\na single test using Hypothesis?\n3. This piece is designed for an F# implementation, but\nis still very good advice which you may find helps give you good ideas for\nusing Hypothesis.\n\n\nIf you have any trouble getting started, don’t feel shy about\nasking for help.\n\n\n# Details and advanced features¶\n\n\nThis is an account of slightly less common Hypothesis features that you don’t need\nto get started but will nevertheless make your life easier.\n\n## Additional test output¶\n\n\nNormally the output of a failing test will look something like:\n\n```\nFalsifying example: test\\_a\\_thing(x=1, y=\"foo\")\n\n\nWith the `repr` of each keyword argument being printed.\n\n\nSometimes this isn’t enough, either because you have a value with a\n`\\_\\_repr\\_\\_()` method that isn’t very\n\n==================\n Document 3 \n----------------\n## Other Python libraries¶\n\n\nHypothesis has *mandatory* dependencies on the following libraries:\n\n\n* attrs\n* sortedcontainers\n\n\nHypothesis has *optional* dependencies on the following libraries:\n\n```\nextras\\_require = {\n    \"cli\": [\"click>=7.0\", \"black>=19.10b0\", \"rich>=9.0.0\"],\n    \"codemods\": [\"libcst>=0.3.16\"],\n    \"ghostwriter\": [\"black>=19.10b0\"],\n    \"pytz\": [\"pytz>=2014.1\"],\n    \"dateutil\": [\"python-dateutil>=1.4\"],\n    \"lark\": [\"lark>=0.10.1\"],  # probably still works with old `lark-parser` too\n    \"numpy\": [\"numpy>=1.17.3\"],  # oldest with wheels for non-EOL Python (for now)\n    \"pandas\": [\"pandas>=1.1\"],\n    \"pytest\": [\"pytest>=4.6\"],\n    \"dpcontracts\": [\"dpcontracts>=0.4\"],\n    \"redis\": [\"redis>=3.0.0\"],\n    # zoneinfo is an odd one: every dependency is conditional, because they're\n    # only necessary on old versions of Python or Windows systems.\n    \"zoneinfo\": [\n        \"tzdata>=2023.3 ; sys\\_platform == 'win32'\",\n        \"backports.zoneinfo>=0.2.1 ; python\\_version<'3.9'\",\n    ],\n    # We only support Django versions with upstream support - see\n    # https://www.djangoproject.com/download/#supported-versions\n    # We also leave the choice of timezone library to the user, since it\n    # might be zoneinfo or pytz depending on version and configuration.\n    \"django\": [\"django>=3.2\"],\n}\n\n\nThe way this works when installing Hypothesis normally is that these features become available if the relevant\nlibrary is installed.\n\n\nSpecifically for pytest, our plugin supports versions of pytest which\nhave been out of upstream support for some time. Hypothesis tests can still\nbe executed by even older versions of pytest - you just won’t have the plugin\nto provide automatic marks, helpful usage warnings, and per-test statistics.\n\n## Testing Hypothesis¶\n\n\nIf you want to test Hypothesis as part of your packaging you will probably not want to use the mechanisms\nHypothesis itself uses for running its tests, because it has a lot of logic for installing and testing against\ndifferent versions of Python.\n\n\nThe tests must be run with fairly recent tooling; check the tree/master/requirements/\ndirectory for details.\n\n\nThe organisation of the tests is described in the hypothesis-python/tests/README.rst.\n\n\n\n## Examples¶\n\n\n* arch linux\n* fedora\n* gentoo\n\n\n# Reproducing failures¶\n\n\nOne of the things that is often concerning for people using randomized testing\nis the question of how to reproduce failing test cases.\n\n\nIt is better to think about the data Hypothesis generates as being\n*arbitrary*, rather than *random*. We deliberately"
    },
    {
        "instruction": "Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given string is a valid IPv4 address. The 'hypothesis' library will be used to generate various input strings and verify the correctness of the IPv4 address validation function.",
        "api": "hypothesis",
        "output": "#!pip install hypothesis\nimport re\nimport unittest\nfrom hypothesis import given, strategies as st\n\ndef is_valid_ipv4_address(ip_address):\n    pattern = r\"^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$\"\n    return re.match(pattern, ip_address) is not None\n\nclass ValidIPv4AddressTest(unittest.TestCase):\n    @given(st.text(min_size=7, max_size=15, alphabet=st.characters(whitelist_categories=('Nd', 'Pd'))))\n    def test_is_valid_ipv4_address(self, ip_address):\n        result = is_valid_ipv4_address(ip_address)\n        self.assertEqual(result, re.match(r\"^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$\", ip_address) is not None)\n\nif __name__ == \"__main__\":\n    unittest.main()",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# Welcome to Hypothesis!¶\n\n\nHypothesis is a Python library for\ncreating unit tests which are simpler to write and more powerful when run,\nfinding edge cases in your code you wouldn’t have thought to look for. It is\nstable, powerful and easy to add\n\n==================\n Document 1 \n----------------\n Welcome to Hypothesis!¶\n\n\nHypothesis is a Python library for\ncreating unit tests which are simpler to write and more powerful when run,\nfinding edge cases in your code you wouldn’t have thought to look for. It is\nstable, powerful and easy to add to any existing test suite.\n\n\nIt works by letting you write tests that assert that something should be true\nfor every case, not just the ones you happen to think of.\n\n\nThink of a normal unit test as being something like the following:\n\n\n1. Set up some data.\n2. Perform some operations on the data.\n3. Assert something about the result.\n\n\nHypothesis lets you write tests which instead look like this:\n\n\n1. For all data matching some specification.\n2. Perform some operations on the data.\n3. Assert something about the result.\n\n\nThis is often called property-based testing, and was popularised by the\nHaskell library Quickcheck.\n\n\nIt works by generating arbitrary data matching your specification and checking\nthat your guarantee still holds in that case. If it finds an example where it doesn’t,\nit takes that example and cuts it down to size, simplifying it until it finds a\nmuch smaller example that still causes the problem. It then saves that example\nfor later, so that once it has found a problem with your code it will not forget\nit in the future.\n\n\nWriting tests of this form usually consists of deciding on guarantees that\nyour code should make - properties that should always hold true,\nregardless of what the world throws at you. Examples of such guarantees\nmight be:\n\n\n* Your code shouldn’t throw an exception, or should only throw a particular\ntype of exception (this works particularly well if you have a lot of internal\nassertions).\n* If you delete an object, it is no longer visible.\n* If you serialize and then deserialize a value, then you get the same value back.\n\n\nNow you know the basics of what Hypothesis does, the rest of this\ndocumentation will take you through how and why. It’s divided into a\nnumber of sections, which you can see in the sidebar (or the\nmenu at the top if you’re on mobile), but you probably want to begin with\nthe Quick start guide, which will give you a worked\nexample of how to use Hypothesis and a detailed outline\nof the things you need to know to begin testing your code with it, or\ncheck out some of the\nintroductory articles.\n\n# Quick start guide¶\n\n\nThis document should talk you through everything you need to get started with\nHypothesis.\n\n## An example¶\n\n\nSuppose we’ve written a run length encoding\nsystem and we want to test it out.\n\n\nWe have the following code which I took straight from the\nRosetta Code wiki (OK, I\nremoved some commented out code and fixed the formatting, but there\n\n==================\n Document 2 \n----------------\n# Fuzzing an HTTP API¶\n\n\nHypothesis’s support for testing HTTP services is somewhat nascent. There are\nplans for some fully featured things around this, but right now they’re\nprobably quite far down the line.\n\n\nBut you can do a lot yourself without any explicit support! Here’s a script\nI wrote to throw arbitrary data against the API for an entirely fictitious service\ncalled Waspfinder (this is only lightly obfuscated and you can easily figure\nout who I’m actually talking about, but I don’t want you to run this code and\nhammer their API without their permission).\n\n\nAll this does is use Hypothesis to generate arbitrary JSON data matching the\nformat their API asks for and check for 500 errors. More advanced tests which\nthen use the result and go on to do other things are definitely also possible.\nThe schemathesis package provides an excellent example of this!\n\n```\nimport math\nimport os\nimport random\nimport time\nimport unittest\nfrom collections import namedtuple\n\nimport requests\n\nfrom hypothesis import assume, given, strategies as st\n\nGoal = namedtuple(\"Goal\", (\"slug\",))\n\n# We just pass in our API credentials via environment variables.\nwaspfinder\\_token = os.getenv(\"WASPFINDER\\_TOKEN\")\nwaspfinder\\_user = os.getenv(\"WASPFINDER\\_USER\")\nassert waspfinder\\_token is not None\nassert waspfinder\\_user is not None\n\nGoalData = st.fixed\\_dictionaries(\n    {\n        \"title\": st.text(),\n\n==================\n Document 3 \n----------------\n# Where to start¶\n\n\nYou should now know enough of the basics to write some tests for your code\nusing Hypothesis. The best way to learn is by doing, so go have a try.\n\n\nIf you’re stuck for ideas for how to use this sort of test for your code, here\nare some good starting points:\n\n\n1. Try just calling functions with appropriate arbitrary data and see if they\ncrash. You may be surprised how often this works. e.g. note that the first\nbug we found in the encoding example didn’t even get as far as our\nassertion: It crashed because it couldn’t handle the data we gave it, not\nbecause it did the wrong thing.\n2. Look for duplication in your tests. Are there any cases where you’re testing\nthe same thing with multiple different examples? Can you generalise that to\na single test using Hypothesis?\n3. This piece is designed for an F# implementation, but\nis still very good advice which you may find helps give you good ideas for\nusing Hypothesis.\n\n\nIf you have any trouble getting started, don’t feel shy about\nasking for help.\n\n\n# Details and advanced features¶\n\n\nThis is an account of slightly less common Hypothesis features that you don’t need\nto get started but will nevertheless make your life easier.\n\n## Additional test output¶\n\n\nNormally the output of a failing test will look something like:\n\n```\nFalsifying example: test\\_a\\_thing(x=1, y=\"foo\")\n\n\nWith the `repr` of each keyword argument being printed.\n\n\nSometimes this isn’t enough, either because you have a value with a\n`\\_\\_repr\\_\\_()` method that isn’t very\n\n==================\n Document 4 \n----------------\n Community¶\n\n\nThe Hypothesis community is small for the moment but is full of excellent people\nwho can answer your questions and help you out. Please do join us.\nThe major place for community discussion is the mailing list.\n\n\nFeel free to use it to ask for help, provide feedback, or discuss anything remotely\nHypothesis related at all. If you post a question on Stack Overflow, please use the\npython-hypothesis tag!\n\n\nPlease note that the Hypothesis code of conduct\napplies in all Hypothesis community spaces.\n\n\nIf you would like to cite Hypothesis, please consider our suggested citation.\n\n\nIf you like repo badges, we suggest the following badge, which you can add\nwith reStructuredText or Markdown, respectively:\n\n\n\n```\n.. image:: https://img.shields.io/badge/hypothesis-tested-brightgreen.svg\n   :alt: Tested with Hypothesis\n   :target: https://hypothesis.readthedocs.io\n\n```\n[](https://hypothesis.readthedocs.io/)\n\n\nFinally, we have a beautiful logo which appears online, and often on stickers:\n\n\n\nAs well as being beautiful, dragonflies actively hunt down bugs for a living!\nYou can find the images and a usage guide in the brand directory on\nGitHub, or find us at conferences where we often have stickers and sometimes\nother swag.\n# The purpose of Hypothesis¶\n\n\nWhat is Hypothesis for?\n\n\nFrom the perspective of a user, the purpose of Hypothesis is to make it easier for\nyou to write better tests.\n\n\nFrom my perspective as the author, that is of course also a purpose of"
    },
    {
        "instruction": "Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any email addresses. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the email address checking function.",
        "api": "hypothesis",
        "output": "#!pip install hypothesis\nimport re\nimport unittest\nfrom hypothesis import given, strategies as st\n\ndef has_email_addresses(lst):\n    pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\"\n    return any(re.match(pattern, s) for s in lst)\n\nclass HasEmailAddressesTest(unittest.TestCase):\n    @given(st.lists(st.text()))\n    def test_has_email_addresses(self, lst):\n        result = has_email_addresses(lst)\n        self.assertEqual(result, any(re.match(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", s) for s in lst))\n\nif __name__ == \"__main__\":\n    unittest.main()",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# Welcome to Hypothesis!¶\n\n\nHypothesis is a Python library for\ncreating unit tests which are simpler to write and more powerful when run,\nfinding edge cases in your code you wouldn’t have thought to look for. It is\nstable, powerful and easy to add\n\n==================\n Document 1 \n----------------\n Welcome to Hypothesis!¶\n\n\nHypothesis is a Python library for\ncreating unit tests which are simpler to write and more powerful when run,\nfinding edge cases in your code you wouldn’t have thought to look for. It is\nstable, powerful and easy to add to any existing test suite.\n\n\nIt works by letting you write tests that assert that something should be true\nfor every case, not just the ones you happen to think of.\n\n\nThink of a normal unit test as being something like the following:\n\n\n1. Set up some data.\n2. Perform some operations on the data.\n3. Assert something about the result.\n\n\nHypothesis lets you write tests which instead look like this:\n\n\n1. For all data matching some specification.\n2. Perform some operations on the data.\n3. Assert something about the result.\n\n\nThis is often called property-based testing, and was popularised by the\nHaskell library Quickcheck.\n\n\nIt works by generating arbitrary data matching your specification and checking\nthat your guarantee still holds in that case. If it finds an example where it doesn’t,\nit takes that example and cuts it down to size, simplifying it until it finds a\nmuch smaller example that still causes the problem. It then saves that example\nfor later, so that once it has found a problem with your code it will not forget\nit in the future.\n\n\nWriting tests of this form usually consists of deciding on guarantees that\nyour code should make - properties that should always hold true,\nregardless of what the world throws at you. Examples of such guarantees\nmight be:\n\n\n* Your code shouldn’t throw an exception, or should only throw a particular\ntype of exception (this works particularly well if you have a lot of internal\nassertions).\n* If you delete an object, it is no longer visible.\n* If you serialize and then deserialize a value, then you get the same value back.\n\n\nNow you know the basics of what Hypothesis does, the rest of this\ndocumentation will take you through how and why. It’s divided into a\nnumber of sections, which you can see in the sidebar (or the\nmenu at the top if you’re on mobile), but you probably want to begin with\nthe Quick start guide, which will give you a worked\nexample of how to use Hypothesis and a detailed outline\nof the things you need to know to begin testing your code with it, or\ncheck out some of the\nintroductory articles.\n\n# Quick start guide¶\n\n\nThis document should talk you through everything you need to get started with\nHypothesis.\n\n## An example¶\n\n\nSuppose we’ve written a run length encoding\nsystem and we want to test it out.\n\n\nWe have the following code which I took straight from the\nRosetta Code wiki (OK, I\nremoved some commented out code and fixed the formatting, but there\n\n==================\n Document 2 \n----------------\n# Fuzzing an HTTP API¶\n\n\nHypothesis’s support for testing HTTP services is somewhat nascent. There are\nplans for some fully featured things around this, but right now they’re\nprobably quite far down the line.\n\n\nBut you can do a lot yourself without any explicit support! Here’s a script\nI wrote to throw arbitrary data against the API for an entirely fictitious service\ncalled Waspfinder (this is only lightly obfuscated and you can easily figure\nout who I’m actually talking about, but I don’t want you to run this code and\nhammer their API without their permission).\n\n\nAll this does is use Hypothesis to generate arbitrary JSON data matching the\nformat their API asks for and check for 500 errors. More advanced tests which\nthen use the result and go on to do other things are definitely also possible.\nThe schemathesis package provides an excellent example of this!\n\n```\nimport math\nimport os\nimport random\nimport time\nimport unittest\nfrom collections import namedtuple\n\nimport requests\n\nfrom hypothesis import assume, given, strategies as st\n\nGoal = namedtuple(\"Goal\", (\"slug\",))\n\n# We just pass in our API credentials via environment variables.\nwaspfinder\\_token = os.getenv(\"WASPFINDER\\_TOKEN\")\nwaspfinder\\_user = os.getenv(\"WASPFINDER\\_USER\")\nassert waspfinder\\_token is not None\nassert waspfinder\\_user is not None\n\nGoalData = st.fixed\\_dictionaries(\n    {\n        \"title\": st.text(),\n\n==================\n Document 3 \n----------------\n# Where to start¶\n\n\nYou should now know enough of the basics to write some tests for your code\nusing Hypothesis. The best way to learn is by doing, so go have a try.\n\n\nIf you’re stuck for ideas for how to use this sort of test for your code, here\nare some good starting points:\n\n\n1. Try just calling functions with appropriate arbitrary data and see if they\ncrash. You may be surprised how often this works. e.g. note that the first\nbug we found in the encoding example didn’t even get as far as our\nassertion: It crashed because it couldn’t handle the data we gave it, not\nbecause it did the wrong thing.\n2. Look for duplication in your tests. Are there any cases where you’re testing\nthe same thing with multiple different examples? Can you generalise that to\na single test using Hypothesis?\n3. This piece is designed for an F# implementation, but\nis still very good advice which you may find helps give you good ideas for\nusing Hypothesis.\n\n\nIf you have any trouble getting started, don’t feel shy about\nasking for help.\n\n\n# Details and advanced features¶\n\n\nThis is an account of slightly less common Hypothesis features that you don’t need\nto get started but will nevertheless make your life easier.\n\n## Additional test output¶\n\n\nNormally the output of a failing test will look something like:\n\n```\nFalsifying example: test\\_a\\_thing(x=1, y=\"foo\")\n\n\nWith the `repr` of each keyword argument being printed.\n\n\nSometimes this isn’t enough, either because you have a value with a\n`\\_\\_repr\\_\\_()` method that isn’t very\n\n==================\n Document 4 \n----------------\n The purpose of Hypothesis¶\n\n\nWhat is Hypothesis for?\n\n\nFrom the perspective of a user, the purpose of Hypothesis is to make it easier for\nyou to write better tests.\n\n\nFrom my perspective as the author, that is of course also a purpose of Hypothesis,\nbut (if you will permit me to indulge in a touch of megalomania for a moment), the\nlarger purpose of Hypothesis is to drag the world kicking and screaming into a new\nand terrifying age of high quality software.\n\n\nSoftware is, as they say, eating the world. Software is also terrible. It’s buggy,\ninsecure and generally poorly thought out. This combination is clearly a recipe for\ndisaster.\n\n\nAnd the state of software testing is even worse. Although it’s fairly uncontroversial\nat this point that you *should* be testing your code, can you really say with a straight\nface that most projects you’ve worked on are adequately tested?\n\n\nA lot of the problem here is that it’s too hard to write good tests. Your tests encode\nexactly the same assumptions and fallacies that you had when you wrote the code, so they\nmiss exactly the same bugs that you missed when you wrote the code.\n\n\nMeanwhile, there are all sorts of tools for making testing better that are basically\nunused. The original Quickcheck is from *1999* and the majority of developers have\nnot even heard of it, let alone used it. There are a bunch of half-baked implementations\nfor most languages, but very few of them are worth using.\n\n\nThe goal of Hypothesis is to bring advanced testing techniques to the masses, and to\nprovide an implementation that is so high quality that it is easier to use them than\nit is not to use them. Where I can, I will beg, borrow and steal every good idea\nI can find that someone has had to make software testing better. Where I can’t, I will\ninvent new ones.\n\n\nQuickcheck is the start, but I also plan to integrate ideas from fuzz testing (a\nplanned future feature is to use coverage information to drive example selection, and\nthe example saving database is already inspired by the workflows people use for fuzz\ntesting), and am open to and actively seeking out other suggestions and ideas.\n\n\nThe plan is to treat the social problem of people not using these ideas as a bug to\nwhich there is a technical solution: Does property-based testing not match your workflow?\nThat’s a bug, let’s fix it by figuring out how to integrate Hypothesis into it.\nToo hard to generate custom data for your application? That’s a bug. Let’s fix it by\nfiguring out how to make it easier, or how to take something you’re already using to\nspecify your data and derive a generator from that automatically. Find the explanations\nof these advanced ideas hopelessly obtuse and hard to follow? That’s a bug. Let’s provide\nyou with an easy API that lets you test your code better without a PhD in software\nverification.\n\n\nGrand ambitions, I know, and I expect ultimately the reality will be somewhat less\ngrand, but so far in about three months of development, Hypothesis has become the most\nsolid implementation of Quickcheck ever seen in a mainstream language (as long as we don’t\ncount Scala as mainstream yet), and at the same time managed to\nsignificantly push forward the state of the art, so I think there’s\nreason to be optimistic.\n\n# Testimonials¶\n\n\nThis is a page for listing people who are using Hypothesis and how excited they\nare about that. If that’s you and your name is not on the list,\nthis file is in Git\nand I’d love it if you sent me a pull request to fix that.\n\n## Stripe¶\n\n\nAt Stripe we use Hypothesis to test every piece of our machine\nlearning model training pipeline (powered by scikit). Before we\nmigrated, our tests were filled with hand-crafted pandas Dataframes\nthat weren’t representative at all of our actual very complex\ndata. Because we\n\n==================\n Document 5 \n----------------\n Community¶\n\n\nThe Hypothesis community is small for the moment but is full of excellent people\nwho can answer your questions and help you out. Please do join us.\nThe major place for community discussion is the mailing list.\n\n\nFeel free to use it to ask for help, provide feedback, or discuss anything remotely\nHypothesis related at all. If you post a question on Stack Overflow, please use the\npython-hypothesis tag!\n\n\nPlease note that the Hypothesis code of conduct\napplies in all Hypothesis community spaces.\n\n\nIf you would like to cite Hypothesis, please consider our suggested citation.\n\n\nIf you like repo badges, we suggest the following badge, which you can add\nwith reStructuredText or Markdown, respectively:\n\n\n\n```\n.. image:: https://img.shields.io/badge/hypothesis-tested-brightgreen.svg\n   :alt: Tested with Hypothesis\n   :target: https://hypothesis.readthedocs.io\n\n```\n[](https://hypothesis.readthedocs.io/)\n\n\nFinally, we have a beautiful logo which appears online, and often on stickers:\n\n\n\nAs well as being beautiful, dragonflies actively hunt down bugs for a living!\nYou can find the images and a usage guide in the brand directory on\nGitHub, or find us at conferences where we often have stickers and sometimes\nother swag.\n# The purpose of Hypothesis¶\n\n\nWhat is Hypothesis for?\n\n\nFrom the perspective of a user, the purpose of Hypothesis is to make it easier for\nyou to write better tests.\n\n\nFrom my perspective as the author, that is of course also a purpose of"
    },
    {
        "instruction": "Create a Python program using the 'ibis-framework' API to perform data transformation operations on a data table. The program should define a sample data table using a Pandas DataFrame and convert it to an 'ibis' table. You need to interact with the 'ibis-framework' library to load the data, transform the table by adding a new column, and rename existing columns.",
        "api": "ibis-framework",
        "output": "#!pip install 'ibis-framework[duckdb]'\nimport ibis\nimport pandas as pd\n\nibis.options.interactive = True\n\ndf = pd.DataFrame(\n    [[\"a\", 1, 2], [\"b\", 3, 4], [\"c\", 5, 6]],\n    columns=[\"one\", \"two\", \"three\"],\n    index=[5, 6, 7],\n)\nt = ibis.memtable(df, name=\"t\")\n\n# Add a new column to the table\ntransformed_table = t.mutate(four=t.two + t.three)\n\n# Rename existing columns\ntransformed_table = transformed_table.rename({\"one\": \"new_one\", \"two\": \"new_two\", \"three\": \"new_three\", \"four\": \"new_four\"})\n\nprint(\"Transformed table: \")\nprint(transformed_table)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n\n1. Expression API\n2. Table expressions\n\n\n# Table expressions\n\n\nTables are one of the core data structures in Ibis.\n\n\n# Table\n\n\n`Table()`\n\n\n## Attributes\n\n\n| Name | Description |\n| --- | --- |\n| columns | The list of columns in this table. |\n\n\n## Methods\n\n\n| Name | Description |\n| --- | --- |\n| aggregate | Aggregate a table with a given set of reductions grouping by `by`. |\n| alias | Create a table expression with a specific name `alias`. |\n| as\\_table | Promote\n\n==================\n Document 1 \n----------------\n table\n\n\n`ibis.table(schema=None, name=None)`\n\n\nCreate a table literal or an abstract table without data.\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `schema` | `SupportsSchema` | None | A schema for the table | `None` |\n| `name` | str | None | Name for the table. One is generated if this value is `None`. | `None` |\n\n\n| Type | Description |\n| --- | --- |\n| `Table` | A table expression |\n\n\nCreate a table with no data backing it\n\n\n```\n>>> import ibis\n>>> ibis.options.interactive\n```\n\n```\n>>> t = ibis.table(schema=dict(a=\"int\", b=\"string\"), name=\"t\")\n>>> t\n```\n\n```\nIbisError: Expression contains unbound tables and therefore cannot be executed. Use ibis.<backend>.execute(expr) or assign a backend instance to `ibis.options.default_backend`.\n```\n\n\n# difference\n\n\n`ibis.difference(table, *rest, distinct=True)`\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `table` | `ir`.`Table` | A table expression | *required* |\n| `*rest` | `ir`.`Table` | Additional table expressions | `()` |\n| `distinct` | bool | Only diff distinct rows not occurring in the calling table | `True` |\n\n| Type | Description |\n| --- | --- |\n| `Table` | The rows present in `self` that are not present in `tables`. |\n\n```\n>>> ibis.difference(t1, t2)\n```\n\n\n\n# intersect\n\n\n`ibis.intersect(table, *rest, distinct=True)`\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `table` | `ir`.`Table` | A table expression | *required* |\n| `*rest` | `ir`.`Table` | Additional table expressions | `()` |\n| `distinct` | bool | Only return distinct rows | `True` |\n\n| Type | Description |\n| --- | --- |\n| `Table` | A new table containing the intersection of all input tables. |\n\n```\n>>> ibis.intersect(t1, t2)\n```\n\n\n\n# union\n\n\n`ibis.union(table, *rest, distinct=False)`\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `table` | `ir`.`Table` | A table expression | *required* |\n| `*rest` | `ir`.`Table` | Additional table expressions | `()` |\n| `distinct` | bool | Only return distinct rows | `False` |\n\n| Type | Description |\n| --- | --- |\n| `Table` | A new table containing the union of all input tables. |\n\n```\n>>> ibis.union(t1, t2) # union all by default\n```\n\n```\n>>> ibis.union(t1, t2, distinct=True).order\\_by(\"a\")\n```\n\n\n\n# row\\_number\n\n\n`ibis.row_number()`\n\n\nReturn an analytic function expression for the current row number.\n\n\n## Returns\n\n| Type | Description |\n| --- | --- |\n| `IntegerColumn` | A column expression enumerating rows |\n\n# window\n\n\n`ibis.window(preceding=None, following=None, order_by=None, group_by=None, *, rows=None, range=None, between=None)`\n\n\nCreate a window clause for use with window functions.\n\n\nThe `ROWS` window clause includes peer rows based on differences in row **number** whereas `RANGE` includes rows based on the differences in row **value**\n\n==================\n Document 2 \n----------------\n memtable\n\n\n`ibis.memtable(data, *, columns=None, schema=None, name=None)`\n\n\nConstruct an ibis table expression from in-memory data.\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `data` |  | Any data accepted by the `pandas.DataFrame` constructor or a `pyarrow.Table`. Examples of acceptable objects are a `pandas.DataFrame`, a `pyarrow.Table`, a list of dicts of non-ibis Python objects, etc. `ibis` objects, like `MapValue`, will result in an error. Do not depend on the underlying storage type (e.g., pyarrow.Table), it’s subject to change across non-major releases. | *required* |\n| `columns` | Iterable[str] | None | Optional `typing.Iterable` of `str` column names. | `None` |\n| `schema` | `SupportsSchema` | None | Optional `Schema`. The functions use `data` to infer a schema if not passed. | `None` |\n| `name` | str | None | Optional name of the table. | `None` |\n\n| Type | Description |\n| --- | --- |\n| `Table` | A table expression backed by in-memory data. |\n\n\n```\n>>> import ibis\n>>> t = ibis.memtable([{\"a\": 1}, {\"a\": 2}])\n>>> t\n```\n\n```\n>>> t = ibis.memtable([{\"a\": 1, \"b\": \"foo\"}, {\"a\": 2, \"b\": \"baz\"}])\n>>> t\n```\n\n```\n┏━━━━━━━┳━━━━━━━━┓\n┃ a ┃ b ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     1 │ foo  │\n│     2 │ baz  │\n└───────┴────────┘\n\nCreate a table literal without column names embedded in the data and pass `columns`\n\n\n```\n>>> t = ibis.memtable([(1, \"foo\"), (2, \"baz\")], columns=[\"a\", \"b\"])\n>>> t\n```\n\nCreate a table literal without column names embedded in the data. Ibis generates column names if none are provided.\n\n\n```\n>>> t = ibis.memtable([(1, \"foo\"), (2, \"baz\")])\n>>> t\n```\n\n```\n┏━━━━━━━┳━━━━━━━━┓\n┃ col0 ┃ col1 ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     1 │ foo  │\n│     2 │ baz  │\n└───────┴────────┘\n\n# table\n\n\n`ibis.table(schema=None, name=None)`\n\n\nCreate a table literal or an abstract table without data.\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `schema` | `SupportsSchema` | None | A schema for the table |\n\n==================\n Document 3 \n----------------\n## pandas\n\n\n`pandas(cls, fn=None, *args, name=None, schema=None, **kwargs)`\n\n\nConstruct a **vectorized** scalar user-defined function that accepts pandas Series’ as inputs.\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `fn` | Callable | None | The The function to wrap. | `None` |\n| `args` | Any | Configuration arguments for the UDF. | `()` |\n| `name` | str | None | The name of the UDF in the backend if different from the function name. | `None` |\n| `schema` | str | None | The schema in which to create the UDF. | `None` |\n| `kwargs` | Any | Additional configuration arguments for the UDF. | `{}` |\n\n```\n>>> import ibis\n>>> @ibis.udf.scalar.pandas\n... def add\\_one(x: int) -> int:\n... return x + 1\n>>> expr = add\\_one(2)\n>>> con = ibis.connect(os.environ[\"SNOWFLAKE\\_URL\"]) # doctest: +SKIP\n>>> con.execute(expr) # doctest: +SKIP\n3\n```\n\n\n* `python`\n* `pyarrow`\n\n### pyarrow\n\n\n`pyarrow(cls, fn=None, *args, name=None, schema=None, **kwargs)`\n\n\nConstruct a **vectorized** scalar user-defined function that accepts PyArrow Arrays as input.\n\n\n```\n>>> import ibis\n>>> import pyarrow.compute as pc\n>>> @ibis.udf.scalar.pyarrow\n... def add\\_one(x: int) -> int:\n... return pc.add(x, 1)\n>>> expr = add\\_one(2)\n>>> con = ibis.connect(\"duckdb://\")\n>>> con.execute(expr)\n```\n\n\n* `python`\n* `pandas`\n\n\n### python\n\n\n`python(cls, fn=None, *args, name=None, schema=None, **kwargs)`\n\n\nConstruct a **non-vectorized** scalar user-defined function that accepts Python scalar values as inputs.\n\n\n`python` UDFs are likely to be slow\n\n`python` UDFs are not vectorized: they are executed row by row with one Python function call per row\n\n\nThis calling pattern tends to be **much** slower than `pandas` or `pyarrow`-based vectorized UDFs.\n\n\n```\n>>> import ibis\n>>> @ibis.udf.scalar.python\n... def add\\_one(x: int) -> int:\n... return x + 1\n>>> expr = add\\_one(2)\n>>> con = ibis.connect(\"duckdb://\")\n>>> con.execute(expr)\n```\n\n\n* `pandas`\n* `pyarrow`\n\n\n1. Connection APIs\n2. Top-level connection APIs\n\n\n\n# Top-level connection APIs\n\n\nCreate and manage backend connections.\n\n# connect\n\n\n`ibis.connect(resource, **kwargs)`\n\n\nConnect to `resource`, inferring the backend automatically.\n\n\nThe general pattern for `ibis.connect` is\n\n```\ncon = ibis.connect(\"backend://connection-parameters\")\n```\n\nWith many backends that looks like\n\n```\ncon = ibis.connect(\"backend://user:password@host:port/database\")\n```\n\nSee the connection syntax for each backend for details about URL connection requirements.\n\n\n| Name | Type | Description\n\n==================\n Document 4 \n----------------\n# Parameters\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `sources` | str | Path | Sequence[str | Path] | A filesystem path or URL or list of same. Supports CSV and TSV files. | *required* |\n| `table_name` | str | None | A name to refer to the table. If not provided, a name will be generated. | `None` |\n| `kwargs` | Any | Backend-specific keyword arguments for the file type. For the DuckDB backend used by default, please refer to: \\* CSV/TSV: https://duckdb.org/docs/data/csv#parameters. | `{}` |\n\n\n## Returns\n\n| Type | Description |\n| --- | --- |\n| `ir`.`Table` | Table expression representing a file |\n\n\n\n## Examples\n\n\n```\n>>> import ibis\n>>> ibis.options.interactive = True\n>>> lines = '''a,b\n... 1,d\n... 2,\n... ,f\n... '''\n>>> with open(\"/tmp/lines.csv\", mode=\"w\") as f:\n... \\_ = f.write(lines)\n...\n>>> t = ibis.read\\_csv(\"/tmp/lines.csv\")\n>>> t\n```\n\n```\n┏━━━━━━━┳━━━━━━━━┓\n┃ a ┃ b ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     1 │ d  │\n│     2 │ NULL   │\n│  NULL │ f  │\n└───────┴────────┘\n\n\n# read\\_delta\n\n\n`ibis.read_delta(source, table_name=None, **kwargs)`\n\n\nLazily load a Delta Lake table.\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `source` | str | Path | A filesystem path or URL. | *required* |\n| `table_name`"
    },
    {
        "instruction": "Create a Python program using the 'ibis-framework' API to work with data tables and schemas. The program should define a sample data table using a Pandas DataFrame and convert it to an 'ibis' table. You need to interact with the 'ibis-framework' library to load the data, define a schema, and display the table and its schema.",
        "api": "ibis-framework",
        "output": "#!pip install 'ibis-framework[duckdb]'\nimport ibis\nimport pandas as pd\n\nibis.options.interactive = True\n\ndf = pd.DataFrame(\n    [[\"a\", 1, 2], [\"b\", 3, 4]],\n    columns=[\"one\", \"two\", \"three\"],\n    index=[5, 6],\n)\nt = ibis.memtable(df, name=\"t\")\nprint(\"Ibis data table: \")\nprint(t)\nprint(\"Ibis data schema: \")\nprint(t.schema())",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n\n1. Expression API\n2. Table expressions\n\n\n# Table expressions\n\n\nTables are one of the core data structures in Ibis.\n\n\n# Table\n\n\n`Table()`\n\n\n## Attributes\n\n\n| Name | Description |\n| --- | --- |\n| columns | The list of columns in this table. |\n\n\n## Methods\n\n\n| Name | Description |\n| --- | --- |\n| aggregate | Aggregate a table with a given set of reductions grouping by `by`. |\n| alias | Create a table expression with a specific name `alias`. |\n| as\\_table | Promote\n\n==================\n Document 1 \n----------------\n table\n\n\n`ibis.table(schema=None, name=None)`\n\n\nCreate a table literal or an abstract table without data.\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `schema` | `SupportsSchema` | None | A schema for the table | `None` |\n| `name` | str | None | Name for the table. One is generated if this value is `None`. | `None` |\n\n\n| Type | Description |\n| --- | --- |\n| `Table` | A table expression |\n\n\nCreate a table with no data backing it\n\n\n```\n>>> import ibis\n>>> ibis.options.interactive\n```\n\n```\n>>> t = ibis.table(schema=dict(a=\"int\", b=\"string\"), name=\"t\")\n>>> t\n```\n\n```\nIbisError: Expression contains unbound tables and therefore cannot be executed. Use ibis.<backend>.execute(expr) or assign a backend instance to `ibis.options.default_backend`.\n```\n\n\n# difference\n\n\n`ibis.difference(table, *rest, distinct=True)`\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `table` | `ir`.`Table` | A table expression | *required* |\n| `*rest` | `ir`.`Table` | Additional table expressions | `()` |\n| `distinct` | bool | Only diff distinct rows not occurring in the calling table | `True` |\n\n| Type | Description |\n| --- | --- |\n| `Table` | The rows present in `self` that are not present in `tables`. |\n\n```\n>>> ibis.difference(t1, t2)\n```\n\n\n\n# intersect\n\n\n`ibis.intersect(table, *rest, distinct=True)`\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `table` | `ir`.`Table` | A table expression | *required* |\n| `*rest` | `ir`.`Table` | Additional table expressions | `()` |\n| `distinct` | bool | Only return distinct rows | `True` |\n\n| Type | Description |\n| --- | --- |\n| `Table` | A new table containing the intersection of all input tables. |\n\n```\n>>> ibis.intersect(t1, t2)\n```\n\n\n\n# union\n\n\n`ibis.union(table, *rest, distinct=False)`\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `table` | `ir`.`Table` | A table expression | *required* |\n| `*rest` | `ir`.`Table` | Additional table expressions | `()` |\n| `distinct` | bool | Only return distinct rows | `False` |\n\n| Type | Description |\n| --- | --- |\n| `Table` | A new table containing the union of all input tables. |\n\n```\n>>> ibis.union(t1, t2) # union all by default\n```\n\n```\n>>> ibis.union(t1, t2, distinct=True).order\\_by(\"a\")\n```\n\n\n\n# row\\_number\n\n\n`ibis.row_number()`\n\n\nReturn an analytic function expression for the current row number.\n\n\n## Returns\n\n| Type | Description |\n| --- | --- |\n| `IntegerColumn` | A column expression enumerating rows |\n\n# window\n\n\n`ibis.window(preceding=None, following=None, order_by=None, group_by=None, *, rows=None, range=None, between=None)`\n\n\nCreate a window clause for use with window functions.\n\n\nThe `ROWS` window clause includes peer rows based on differences in row **number** whereas `RANGE` includes rows based on the differences in row **value**\n\n==================\n Document 2 \n----------------\n connect\n\n\n`ibis.connect(resource, **kwargs)`\n\n\nConnect to `resource`, inferring the backend automatically.\n\n\nThe general pattern for `ibis.connect` is\n\n```\ncon = ibis.connect(\"backend://connection-parameters\")\n```\n\nWith many backends that looks like\n\n```\ncon = ibis.connect(\"backend://user:password@host:port/database\")\n```\n\nSee the connection syntax for each backend for details about URL connection requirements.\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `resource` | Path | str | A URL or path to the resource to be connected to. | *required* |\n| `kwargs` | Any | Backend specific keyword arguments | `{}` |\n\n\nConnect to an in-memory DuckDB database:\n\n\n```\n>>> import ibis\n>>> con = ibis.connect(\"duckdb://\")\n```\n\n\nConnect to an on-disk SQLite database:\n\n\n```\n>>> con = ibis.connect(\"sqlite://relative.db\")\n>>> con = ibis.connect(\"sqlite:///absolute/path/to/data.db\")\n```\n\n\nConnect to a PostgreSQL server:\n\n```\n>>> con = ibis.connect(\n... \"postgres://user:password@hostname:5432\"\n... ) # quartodoc: +SKIP\n```\n\nConnect to BigQuery:\n\n```\n>>> con = ibis.connect(\n... \"bigquery://my-project/my-dataset\"\n... ) # quartodoc: +SKIP\n```\n\n\n# get\\_backend\n\n\n`ibis.get_backend(expr=None)`\n\n\nGet the current Ibis backend to use for a given expression.\n\n\nexpr An expression to get the backend from. If not passed, the default backend is returned.\n\n\n| Type | Description |\n| --- | --- |\n| `BaseBackend` | The Ibis backend. |\n\n\n# set\\_backend\n\n\n`ibis.set_backend(backend)`\n\n\nSet the default Ibis backend.\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `backend` | str | `BaseBackend` | May be a backend name or URL, or an existing backend instance. | *required* |\n\n\nYou can pass the backend as a name:\n\n\n```\n>>> import ibis\n>>> ibis.set\\_backend(\"polars\")\n```\n\n\nOr as a URI\n\n```\n>>> ibis.set\\_backend(\n... \"postgres://user:password@hostname:5432\"\n... ) # quartodoc: +SKIP\n```\n\nOr as an existing backend instance\n\n\n```\n>>> ibis.set\\_backend(ibis.duckdb.connect())\n```\n\n\n# ContextAdjustment\n\n\n`ContextAdjustment()`\n\n\nOptions related to time context adjustment.\n\n\n| Name | Type | Description |\n| --- | --- | --- |\n| time\\_col | str | Name of the timestamp column for execution with a `timecontext`. See `ibis/expr/timecontext.py` for details. |\n\n# Interactive\n\n\n`Interactive()`\n\n\nOptions controlling the interactive repr.\n\n\n| Name | Type | Description |\n| --- | --- | --- |\n| max\\_rows | int | Maximum rows to pretty print. |\n| max\\_columns | int | None | The maximum number of columns to\n\n==================\n Document 3 \n----------------\n### Examples\n\n\n```\n>>> import ibis\n>>> from ibis import \\_\n>>> ibis.options.interactive = True\n>>> t = ibis.memtable(\n... {\n... \"fruit\": [\"apple\", \"apple\", \"banana\", \"orange\"],\n... \"price\": [0.5, 0.5, 0.25, 0.33],\n... }\n... )\n>>> t\n```\n\n```\n┏━━━━━━━━┳━━━━━━━━━┓\n┃ fruit ┃ price ┃\n┡━━━━━━━━╇━━━━━━━━━┩\n│ string │ float64 │\n├────────┼─────────┤\n│ apple  │    0.50 │\n│ apple  │    0.50 │\n│ banana │    0.25 │\n│ orange │    0.33 │\n└────────┴─────────┘\n\n```\n\n```\n>>> t.aggregate(\n... by=[\"fruit\"],\n... total\\_cost=\\_.price.sum(),\n... avg\\_cost=\\_.price.mean(),\n... having=\\_.price.sum() < 0.5,\n... )\n```\n\n```\n┏━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ fruit ┃ total\\_cost ┃ avg\\_cost ┃\n┡━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ string │ float64    │ float64  │\n├────────┼────────────┼──────────┤\n│ banana │       0.25 │     0.25 │\n│ orange │       0.33 │     0.33 │\n└────────┴────────────┴──────────┘\n\n### alias\n\n\n`alias(self, alias)`\n\n\nCreate a table expression with a specific name `alias`.\n\n\nThis method is useful for exposing an ibis expression to the underlying backend for use in the `Table.sql` method.\n\n\n`.alias` will create a temporary view\n\n`.alias` creates a temporary view in the\n\n==================\n Document 4 \n----------------\n memtable\n\n\n`ibis.memtable(data, *, columns=None, schema=None, name=None)`\n\n\nConstruct an ibis table expression from in-memory data.\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `data` |  | Any data accepted by the `pandas.DataFrame` constructor or a `pyarrow.Table`. Examples of acceptable objects are a `pandas.DataFrame`, a `pyarrow.Table`, a list of dicts of non-ibis Python objects, etc. `ibis` objects, like `MapValue`, will result in an error. Do not depend on the underlying storage type (e.g., pyarrow.Table), it’s subject to change across non-major releases. | *required* |\n| `columns` | Iterable[str] | None | Optional `typing.Iterable` of `str` column names. | `None` |\n| `schema` | `SupportsSchema` | None | Optional `Schema`. The functions use `data` to infer a schema if not passed. | `None` |\n| `name` | str | None | Optional name of the table. | `None` |\n\n| Type | Description |\n| --- | --- |\n| `Table` | A table expression backed by in-memory data. |\n\n\n```\n>>> import ibis\n>>> t = ibis.memtable([{\"a\": 1}, {\"a\": 2}])\n>>> t\n```\n\n```\n>>> t = ibis.memtable([{\"a\": 1, \"b\": \"foo\"}, {\"a\": 2, \"b\": \"baz\"}])\n>>> t\n```\n\n```\n┏━━━━━━━┳━━━━━━━━┓\n┃ a ┃ b ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     1 │ foo  │\n│     2 │ baz  │\n└───────┴────────┘\n\nCreate a table literal without column names embedded in the data and pass `columns`\n\n\n```\n>>> t = ibis.memtable([(1, \"foo\"), (2, \"baz\")], columns=[\"a\", \"b\"])\n>>> t\n```\n\nCreate a table literal without column names embedded in the data. Ibis generates column names if none are provided.\n\n\n```\n>>> t = ibis.memtable([(1, \"foo\"), (2, \"baz\")])\n>>> t\n```\n\n```\n┏━━━━━━━┳━━━━━━━━┓\n┃ col0 ┃ col1 ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     1 │ foo  │\n│     2 │ baz  │\n└───────┴────────┘\n\n# table\n\n\n`ibis.table(schema=None, name=None)`\n\n\nCreate a table literal or an abstract table without data.\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `schema` | `SupportsSchema` | None | A schema for the table |"
    },
    {
        "instruction": "Create a Python program that uses 'json-tricks' to handle JSON data with custom serialization of NaN and Infinity. Define a dictionary with floating-point values that include NaN and Infinity. Serialize the dictionary into a JSON string using 'dumps' and ensure that the custom serialization of NaN and Infinity is applied.",
        "api": "json-tricks",
        "output": "#!pip install json_tricks\nfrom json_tricks import dumps\nimport math\n\ndata = {\n    'pi': math.pi,\n    'nan_value': math.nan,\n    'infinity_value': math.inf\n}\n\njson_str = dumps(data, special_values={'NaN': 'NaN', 'Infinity': 'Infinity'}, indent=4)\n\nprint(json_str)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n JSON tricks (python)¶\n\n\nThe pyjson-tricks package brings several pieces of functionality to python handling of json files:\n\n\n1. **Store and load numpy arrays** in human-readable format.\n2. **Store and load class instances** both generic and customized.\n3. **Store and load date/times** as a dictionary (including timezone).\n4. **Preserve map order** `{}` using `OrderedDict`.\n5. **Allow for comments** in json files by starting lines with `#`.\n6. Sets, complex numbers, Decimal, Fraction, enums, compression, duplicate keys, …\n\n\nAs well as compression and disallowing duplicate keys.\n\n\n* Code: https://github.com/mverleg/pyjson\\_tricks\n* Documentation: http://json-tricks.readthedocs.org/en/latest/\n* PIP: https://pypi.python.org/pypi/json\\_tricks\n\n\nThe 2.0 series added some of the above features and broke backward compatibility. The version 3.0 series is a more readable rewrite that also makes it easier to combine encoders, again not fully backward compatible.\n\n\nSeveral keys of the format `\\_\\_keyname\\_\\_` have special meanings, and more might be added in future releases.\n\n\nIf you’re considering JSON-but-with-comments as a config file format, have a look at HJSON, it might be more appropriate. For other purposes, keep reading!\n\n\nThanks for all the Github stars!\n\n# Installation and use¶\n\n\nYou can install using\n\n```\npip install json-tricks  # or e.g. 'json-tricks<3.0' for older versions\n\n```\n\n\nDecoding of some data types needs the corresponding package to be installed, e.g. `numpy` for arrays, `pandas` for dataframes and `pytz` for timezone-aware datetimes.\n\n\nYou\n\n==================\n Document 1 \n----------------\n\n# JSON tricks (python)¶\n\n\nThe pyjson-tricks package brings several pieces of functionality to python handling of json files:\n\n\n1. **Store and load numpy arrays** in human-readable format.\n2. **Store and load class instances** both generic and customized.\n3. **Store and load date/times** as a\n\n==================\n Document 2 \n----------------\n Installation and use¶\n\n\nYou can install using\n\n```\npip install json-tricks  # or e.g. 'json-tricks<3.0' for older versions\n\n```\n\n\nDecoding of some data types needs the corresponding package to be installed, e.g. `numpy` for arrays, `pandas` for dataframes and `pytz` for timezone-aware datetimes.\n\n\nYou can import the usual json functions dump(s) and load(s), as well as a separate comment removal function, as follows:\n\n```\nfrom json_tricks import dump, dumps, load, loads, strip_comments\n\n\nThe exact signatures of these and other functions are in the documentation.\n\n\n`json-tricks` supports Python 2.7, and Python 3.4 and later, and is automatically tested on 2.7, 3.4, 3.5 and 3.6. Pypy is supported without numpy and pandas. Pandas doesn’t support 3.4.\n\n# Preserve type vs use primitive¶\n\n\nBy default, types are encoded such that they can be restored to their original type when loaded with `json-tricks`. Example encodings in this documentation refer to that format.\n\n\nYou can also choose to store things as\n\n==================\n Document 3 \n----------------\n Preserve type vs use primitive¶\n\n\nBy default, types are encoded such that they can be restored to their original type when loaded with `json-tricks`. Example encodings in this documentation refer to that format.\n\n\nYou can also choose to store things as their closest primitive type (e.g. arrays and sets as lists, decimals as floats). This may be desirable if you don’t care about the exact type, or you are loading the json in another language (which doesn’t restore python types). It’s also smaller.\n\n\nTo forego meta data and store primitives instead, pass `primitives` to `dump(s)`. This is available in version `3.8` and later. Example:\n\n```\ndata = [\n        arange(0, 10, 1, dtype=int).reshape((2, 5)),\n        datetime(year=2017, month=1, day=19, hour=23, minute=00, second=00),\n        1 + 2j,\n        Decimal(42),\n        Fraction(1, 3),\n        MyTestCls(s='ub', dct={'7': 7}),  # see later\n        set(range(7)),\n]# Encode with metadata to preserve types when decoding\nprint(dumps(data))\n\n```\n// (comments added and indenting changed)\n[\n        // numpy array\n        {\n\n==================\n Document 4 \n----------------\n json\\_tricks/test\\_class.py\nclass MyTestCls:\n        def \\_\\_init\\_\\_(self, \\*\\*kwargs):\n                for k, v in kwargs.items():\n                        setattr(self, k, v)\n\ncls\\_instance = MyTestCls(s='ub', dct={'7': 7})\n\njson = dumps(cls\\_instance, indent=4)\ncls\\_instance\\_again = loads(json)\n\n\nYou’ll get your instance back. Here the json looks like this:\n\n```\n{\n        \"\\_\\_instance\\_type\\_\\_\": [\n                \"json\\_tricks.test\\_class\",\n                \"MyTestCls\"\n        ],\n        \"attributes\": {\n                \"s\": \"ub\",\n                \"dct\": {\n                        \"7\": 7\n                }\n        }\n}\n\n\nAs you can see, this stores the module and class name. The class must be importable from the same module when decoding (and should not have changed).\nIf it isn’t, you have to manually provide a dictionary to `cls\\_lookup\\_map` when loading in which the class name can be looked up. Note that if the class is imported, then `globals()` is such a dictionary (so try `loads(json, cls\\_lookup\\_map=glboals())`).\nAlso note that if the class is defined in the ‘top’ script (that you’re calling directly), then this isn’t a module and the import part cannot be extracted. Only the class name will be stored; it can then only be deserialized in the same script, or if you provide `cls\\_lookup\\_map`.\n\n\nNote that this also works with `slots` without having to do anything (thanks to `koffie`), which encodes like this (custom indentation):\n\n```\n{\n        \"\\_\\_instance\\_type\\_\\_\": [\"module.path\", \"ClassName\"],\n        \"slots\": {\"slotattr\": 37},\n        \"attributes\": {\"dictattr\": 42}\n}\n\n\nIf the instance doesn’t serialize automatically, or if you want custom behaviour, then you can implement `\\_\\_json\\_\\_encode\\_\\_(self)` and `\\_\\_json\\_decode\\_\\_(self, \\*\\*attributes)` methods, like so:\n\n```\nclass CustomEncodeCls:\n        def \\_\\_init\\_\\_(self):\n                self.relevant = 42\n                self.irrelevant = 37\n\n        def \\_\\_json\\_encode\\_\\_(self):\n                # should return primitive, serializable types like dict, list, int, string, float...\n                return {'relevant': self.relevant}\n\n        def \\_\\_json\\_decode\\_\\_(self, \\*\\*attrs):\n                # should initialize all properties; note that \\_\\_init\\_\\_ is not called implicitly\n                self.relevant = attrs['relevant']\n                self.irrelevant = 12\n\n\nAs you’ve seen, this uses the magic key `\\_\\_instance\\_type\\_\\_`. Don’t use `\\_\\_instance\\_type\\_\\_` as a dictionary key unless you know what you’re doing.\n\n## Date, time, datetime and timedelta¶\n\n\nDate, time, datetime and timedelta objects are stored as dictionaries of “day”, “hour”, “millisecond” etc keys, for each nonzero property.\n\n\nTimezone name is also stored in case it is set. You’ll need to have `pytz` installed"
    },
    {
        "instruction": "Develop a Python program that showcases the use of 'json-tricks' to handle JSON data with nested objects. Create a dictionary containing nested dictionaries and lists. Serialize the dictionary into a JSON string using 'dumps' and ensure that the nested structure is preserved.",
        "api": "json-tricks",
        "output": "#!pip install json_tricks\nfrom json_tricks import dumps\n\nnested_data = {\n    'name': 'John',\n    'info': {\n        'age': 30,\n        'address': {\n            'street': '123 Main St',\n            'city': 'Cityville'\n        },\n        'hobbies': ['Reading', 'Traveling']\n    }\n}\n\njson_str = dumps(nested_data, indent=4)\n\nprint(json_str)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n# JSON tricks (python)¶\n\n\nThe pyjson-tricks package brings several pieces of functionality to python handling of json files:\n\n\n1. **Store and load numpy arrays** in human-readable format.\n2. **Store and load class instances** both generic and customized.\n3. **Store and load date/times** as a\n\n==================\n Document 1 \n----------------\n JSON tricks (python)¶\n\n\nThe pyjson-tricks package brings several pieces of functionality to python handling of json files:\n\n\n1. **Store and load numpy arrays** in human-readable format.\n2. **Store and load class instances** both generic and customized.\n3. **Store and load date/times** as a dictionary (including timezone).\n4. **Preserve map order** `{}` using `OrderedDict`.\n5. **Allow for comments** in json files by starting lines with `#`.\n6. Sets, complex numbers, Decimal, Fraction, enums, compression, duplicate keys, …\n\n\nAs well as compression and disallowing duplicate keys.\n\n\n* Code: https://github.com/mverleg/pyjson\\_tricks\n* Documentation: http://json-tricks.readthedocs.org/en/latest/\n* PIP: https://pypi.python.org/pypi/json\\_tricks\n\n\nThe 2.0 series added some of the above features and broke backward compatibility. The version 3.0 series is a more readable rewrite that also makes it easier to combine encoders, again not fully backward compatible.\n\n\nSeveral keys of the format `\\_\\_keyname\\_\\_` have special meanings, and more might be added in future releases.\n\n\nIf you’re considering JSON-but-with-comments as a config file format, have a look at HJSON, it might be more appropriate. For other purposes, keep reading!\n\n\nThanks for all the Github stars!\n\n# Installation and use¶\n\n\nYou can install using\n\n```\npip install json-tricks  # or e.g. 'json-tricks<3.0' for older versions\n\n```\n\n\nDecoding of some data types needs the corresponding package to be installed, e.g. `numpy` for arrays, `pandas` for dataframes and `pytz` for timezone-aware datetimes.\n\n\nYou\n\n==================\n Document 2 \n----------------\n Installation and use¶\n\n\nYou can install using\n\n```\npip install json-tricks  # or e.g. 'json-tricks<3.0' for older versions\n\n```\n\n\nDecoding of some data types needs the corresponding package to be installed, e.g. `numpy` for arrays, `pandas` for dataframes and `pytz` for timezone-aware datetimes.\n\n\nYou can import the usual json functions dump(s) and load(s), as well as a separate comment removal function, as follows:\n\n```\nfrom json_tricks import dump, dumps, load, loads, strip_comments\n\n\nThe exact signatures of these and other functions are in the documentation.\n\n\n`json-tricks` supports Python 2.7, and Python 3.4 and later, and is automatically tested on 2.7, 3.4, 3.5 and 3.6. Pypy is supported without numpy and pandas. Pandas doesn’t support 3.4.\n\n# Preserve type vs use primitive¶\n\n\nBy default, types are encoded such that they can be restored to their original type when loaded with `json-tricks`. Example encodings in this documentation refer to that format.\n\n\nYou can also choose to store things as\n\n==================\n Document 3 \n----------------\n Preserve type vs use primitive¶\n\n\nBy default, types are encoded such that they can be restored to their original type when loaded with `json-tricks`. Example encodings in this documentation refer to that format.\n\n\nYou can also choose to store things as their closest primitive type (e.g. arrays and sets as lists, decimals as floats). This may be desirable if you don’t care about the exact type, or you are loading the json in another language (which doesn’t restore python types). It’s also smaller.\n\n\nTo forego meta data and store primitives instead, pass `primitives` to `dump(s)`. This is available in version `3.8` and later. Example:\n\n```\ndata = [\n        arange(0, 10, 1, dtype=int).reshape((2, 5)),\n        datetime(year=2017, month=1, day=19, hour=23, minute=00, second=00),\n        1 + 2j,\n        Decimal(42),\n        Fraction(1, 3),\n        MyTestCls(s='ub', dct={'7': 7}),  # see later\n        set(range(7)),\n]# Encode with metadata to preserve types when decoding\nprint(dumps(data))\n\n```\n// (comments added and indenting changed)\n[\n        // numpy array\n        {\n\n==================\n Document 4 \n----------------\n# Numpy arrays¶\n\n\nThe array is encoded in sort-of-readable and very flexible and portable format, like so:\n\n```\narr = arange(0, 10, 1, dtype=uint8).reshape((2, 5))\nprint(dumps({'mydata': arr}))\n\n\nthis yields:\n\n```\n{\n        \"mydata\": {\n                \"dtype\": \"uint8\",\n                \"shape\": [2, 5],\n                \"Corder\": true,\n                \"\\_\\_ndarray\\_\\_\": [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\n        }\n}\n\n\nwhich will be converted back to a numpy array when using `json\\_tricks.loads`. Note that the memory order (`Corder`) is only stored in v3.1 and later and for arrays with at least 2 dimensions.\n\n\nAs you’ve seen, this uses the magic key `\\_\\_ndarray\\_\\_`. Don’t use `\\_\\_ndarray\\_\\_` as a dictionary key unless you’re trying to make a numpy array (and know what you’re doing).\n\n\nNumpy scalars are also serialized (v3.5+). They are represented by the closest python primitive type. A special representation was not feasible, because Python’s json implementation serializes some numpy types as primitives, without consulting custom encoders. If you want to preverse the exact numpy type, use encode\\_scalars\\_inplace.\n\n\n**Performance**: this method has slow write times similar to other human-readable formats, although read time is worse than csv. File size (with compression) is high on a relative scale, but it’s only around 30% above binary. See this benchmark (it’s called JSONGzip). A binary alternative might be added, but is not yet available.\n\n\nThis implementation is inspired by an answer by tlausch on stackoverflow that you could read for details.\n\n\n## Class instances¶\n\n\n`json\\_tricks` can serialize class instances.\n\n\nIf the class behaves normally (not generated dynamic, no `\\_\\_new\\_\\_` or `\\_\\_metaclass\\_\\_` magic, etc) *and* all it’s attributes are serializable, then this should work by default.\n\n```\n# json\\_tricks/test\\_class.py\nclass MyTestCls:\n        def \\_\\_init\\_\\_(self, \\*\\*kwargs):\n                for k, v in kwargs.items():"
    },
    {
        "instruction": "Create a Python program that showcases the use of 'json-tricks' for handling JSON data with custom encoder and decoder functions. Define custom functions for encoding and decoding data. Use these functions to serialize and deserialize a dictionary containing special data types. Ensure that the custom encoding and decoding functions are applied to the data.",
        "api": "json-tricks",
        "output": "#!pip install json_tricks\nfrom json_tricks import dumps, loads\n\ndef custom_encoder(obj):\n    if isinstance(obj, complex):\n        return {'__complex__': True, 'real': obj.real, 'imag': obj.imag}\n    raise TypeError(repr(obj) + ' is not JSON serializable')\n\ndef custom_decoder(dct):\n    if '__complex__' in dct:\n        return complex(dct['real'], dct['imag'])\n    return dct\n\ndata = {\n    'complex_number': 2 + 3j,\n    'regular_number': 42\n}\n\njson_str = dumps(data, default=custom_encoder, indent=4)\nparsed_data = loads(json_str, object_hook=custom_decoder)\n\nprint(parsed_data['complex_number'])\nprint(parsed_data['regular_number'])",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n# JSON tricks (python)¶\n\n\nThe pyjson-tricks package brings several pieces of functionality to python handling of json files:\n\n\n1. **Store and load numpy arrays** in human-readable format.\n2. **Store and load class instances** both generic and customized.\n3. **Store and load date/times** as a\n\n==================\n Document 1 \n----------------\n JSON tricks (python)¶\n\n\nThe pyjson-tricks package brings several pieces of functionality to python handling of json files:\n\n\n1. **Store and load numpy arrays** in human-readable format.\n2. **Store and load class instances** both generic and customized.\n3. **Store and load date/times** as a dictionary (including timezone).\n4. **Preserve map order** `{}` using `OrderedDict`.\n5. **Allow for comments** in json files by starting lines with `#`.\n6. Sets, complex numbers, Decimal, Fraction, enums, compression, duplicate keys, …\n\n\nAs well as compression and disallowing duplicate keys.\n\n\n* Code: https://github.com/mverleg/pyjson\\_tricks\n* Documentation: http://json-tricks.readthedocs.org/en/latest/\n* PIP: https://pypi.python.org/pypi/json\\_tricks\n\n\nThe 2.0 series added some of the above features and broke backward compatibility. The version 3.0 series is a more readable rewrite that also makes it easier to combine encoders, again not fully backward compatible.\n\n\nSeveral keys of the format `\\_\\_keyname\\_\\_` have special meanings, and more might be added in future releases.\n\n\nIf you’re considering JSON-but-with-comments as a config file format, have a look at HJSON, it might be more appropriate. For other purposes, keep reading!\n\n\nThanks for all the Github stars!\n\n# Installation and use¶\n\n\nYou can install using\n\n```\npip install json-tricks  # or e.g. 'json-tricks<3.0' for older versions\n\n```\n\n\nDecoding of some data types needs the corresponding package to be installed, e.g. `numpy` for arrays, `pandas` for dataframes and `pytz` for timezone-aware datetimes.\n\n\nYou\n\n==================\n Document 2 \n----------------\n Installation and use¶\n\n\nYou can install using\n\n```\npip install json-tricks  # or e.g. 'json-tricks<3.0' for older versions\n\n```\n\n\nDecoding of some data types needs the corresponding package to be installed, e.g. `numpy` for arrays, `pandas` for dataframes and `pytz` for timezone-aware datetimes.\n\n\nYou can import the usual json functions dump(s) and load(s), as well as a separate comment removal function, as follows:\n\n```\nfrom json_tricks import dump, dumps, load, loads, strip_comments\n\n\nThe exact signatures of these and other functions are in the documentation.\n\n\n`json-tricks` supports Python 2.7, and Python 3.4 and later, and is automatically tested on 2.7, 3.4, 3.5 and 3.6. Pypy is supported without numpy and pandas. Pandas doesn’t support 3.4.\n\n# Preserve type vs use primitive¶\n\n\nBy default, types are encoded such that they can be restored to their original type when loaded with `json-tricks`. Example encodings in this documentation refer to that format.\n\n\nYou can also choose to store things as"
    },
    {
        "instruction": "Develop a Python program that demonstrates the use of 'json-tricks' for handling JSON data with custom object serialization. Define a dictionary that includes custom Python objects and serialize it into a JSON string using 'dumps'. Specify custom serialization options for the objects.",
        "api": "json-tricks",
        "output": "#!pip install json_tricks\nfrom json_tricks import dumps\n\nclass CustomObject:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\ndata = {\n    'person': CustomObject('Frank', 45)\n}\n\njson_str = dumps(data, object_serialization={\n    'person': lambda obj: {'name': obj.name, 'age': obj.age}\n}, indent=4)\n\nprint(json_str)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n# JSON tricks (python)¶\n\n\nThe pyjson-tricks package brings several pieces of functionality to python handling of json files:\n\n\n1. **Store and load numpy arrays** in human-readable format.\n2. **Store and load class instances** both generic and customized.\n3. **Store and load date/times** as a\n\n==================\n Document 1 \n----------------\n JSON tricks (python)¶\n\n\nThe pyjson-tricks package brings several pieces of functionality to python handling of json files:\n\n\n1. **Store and load numpy arrays** in human-readable format.\n2. **Store and load class instances** both generic and customized.\n3. **Store and load date/times** as a dictionary (including timezone).\n4. **Preserve map order** `{}` using `OrderedDict`.\n5. **Allow for comments** in json files by starting lines with `#`.\n6. Sets, complex numbers, Decimal, Fraction, enums, compression, duplicate keys, …\n\n\nAs well as compression and disallowing duplicate keys.\n\n\n* Code: https://github.com/mverleg/pyjson\\_tricks\n* Documentation: http://json-tricks.readthedocs.org/en/latest/\n* PIP: https://pypi.python.org/pypi/json\\_tricks\n\n\nThe 2.0 series added some of the above features and broke backward compatibility. The version 3.0 series is a more readable rewrite that also makes it easier to combine encoders, again not fully backward compatible.\n\n\nSeveral keys of the format `\\_\\_keyname\\_\\_` have special meanings, and more might be added in future releases.\n\n\nIf you’re considering JSON-but-with-comments as a config file format, have a look at HJSON, it might be more appropriate. For other purposes, keep reading!\n\n\nThanks for all the Github stars!\n\n# Installation and use¶\n\n\nYou can install using\n\n```\npip install json-tricks  # or e.g. 'json-tricks<3.0' for older versions\n\n```\n\n\nDecoding of some data types needs the corresponding package to be installed, e.g. `numpy` for arrays, `pandas` for dataframes and `pytz` for timezone-aware datetimes.\n\n\nYou\n\n==================\n Document 2 \n----------------\n Installation and use¶\n\n\nYou can install using\n\n```\npip install json-tricks  # or e.g. 'json-tricks<3.0' for older versions\n\n```\n\n\nDecoding of some data types needs the corresponding package to be installed, e.g. `numpy` for arrays, `pandas` for dataframes and `pytz` for timezone-aware datetimes.\n\n\nYou can import the usual json functions dump(s) and load(s), as well as a separate comment removal function, as follows:\n\n```\nfrom json_tricks import dump, dumps, load, loads, strip_comments\n\n\nThe exact signatures of these and other functions are in the documentation.\n\n\n`json-tricks` supports Python 2.7, and Python 3.4 and later, and is automatically tested on 2.7, 3.4, 3.5 and 3.6. Pypy is supported without numpy and pandas. Pandas doesn’t support 3.4.\n\n# Preserve type vs use primitive¶\n\n\nBy default, types are encoded such that they can be restored to their original type when loaded with `json-tricks`. Example encodings in this documentation refer to that format.\n\n\nYou can also choose to store things as\n\n==================\n Document 3 \n----------------\n json\\_tricks/test\\_class.py\nclass MyTestCls:\n        def \\_\\_init\\_\\_(self, \\*\\*kwargs):\n                for k, v in kwargs.items():\n                        setattr(self, k, v)\n\ncls\\_instance = MyTestCls(s='ub', dct={'7': 7})\n\njson = dumps(cls\\_instance, indent=4)\ncls\\_instance\\_again = loads(json)\n\n\nYou’ll get your instance back. Here the json looks like this:\n\n```\n{\n        \"\\_\\_instance\\_type\\_\\_\": [\n                \"json\\_tricks.test\\_class\",\n                \"MyTestCls\"\n        ],\n        \"attributes\": {\n                \"s\": \"ub\",\n                \"dct\": {\n                        \"7\": 7\n                }\n        }\n}\n\n\nAs you can see, this stores the module and class name. The class must be importable from the same module when decoding (and should not have changed).\nIf it isn’t, you have to manually provide a dictionary to `cls\\_lookup\\_map` when loading in which the class name can be looked up. Note that if the class is imported, then `globals()` is such a dictionary (so try `loads(json, cls\\_lookup\\_map=glboals())`).\nAlso note that if the class is defined in the ‘top’ script (that you’re calling directly), then this isn’t a module and the import part cannot be extracted. Only the class name will be stored; it can then only be deserialized in the same script, or if you provide `cls\\_lookup\\_map`.\n\n\nNote that this also works with `slots` without having to do anything (thanks to `koffie`), which encodes like this (custom indentation):\n\n```\n{\n        \"\\_\\_instance\\_type\\_\\_\": [\"module.path\", \"ClassName\"],\n        \"slots\": {\"slotattr\": 37},\n        \"attributes\": {\"dictattr\": 42}\n}\n\n\nIf the instance doesn’t serialize automatically, or if you want custom behaviour, then you can implement `\\_\\_json\\_\\_encode\\_\\_(self)` and `\\_\\_json\\_decode\\_\\_(self, \\*\\*attributes)` methods, like so:\n\n```\nclass CustomEncodeCls:\n        def \\_\\_init\\_\\_(self):\n                self.relevant = 42\n                self.irrelevant = 37\n\n        def \\_\\_json\\_encode\\_\\_(self):\n                # should return primitive, serializable types like dict, list, int, string, float...\n                return {'relevant': self.relevant}\n\n        def \\_\\_json\\_decode\\_\\_(self, \\*\\*attrs):\n                # should initialize all properties; note that \\_\\_init\\_\\_ is not called implicitly\n                self.relevant = attrs['relevant']\n                self.irrelevant = 12\n\n\nAs you’ve seen, this uses the magic key `\\_\\_instance\\_type\\_\\_`. Don’t use `\\_\\_instance\\_type\\_\\_` as a dictionary key unless you know what you’re doing.\n\n## Date, time, datetime and timedelta¶\n\n\nDate, time, datetime and timedelta objects are stored as dictionaries of “day”, “hour”, “millisecond” etc keys, for each nonzero property.\n\n\nTimezone name is also stored in case it is set. You’ll need to have `pytz` installed"
    },
    {
        "instruction": "Create a Python program that demonstrates the use of 'json-tricks' for handling JSON arrays. Define a list of dictionaries, each representing a person's information, such as name, age, and address. Serialize this list of dictionaries into a JSON string using the 'dumps' function. Then, deserialize the JSON string back into a list of dictionaries using the 'loads' function.",
        "api": "json-tricks",
        "output": "#!pip install json_tricks\nfrom json_tricks import dumps, loads\n\npeople = [\n    {'name': 'Alice', 'age': 25, 'address': '123 Main St'},\n    {'name': 'Bob', 'age': 30, 'address': '456 Elm St'},\n    {'name': 'Charlie', 'age': 35, 'address': '789 Oak St'}\n]\n\njson_str = dumps(people, indent=4)\nparsed_people = loads(json_str)\n\nprint(parsed_people)\nprint(parsed_people[0]['name'])  # Should print 'Alice'.",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n# JSON tricks (python)¶\n\n\nThe pyjson-tricks package brings several pieces of functionality to python handling of json files:\n\n\n1. **Store and load numpy arrays** in human-readable format.\n2. **Store and load class instances** both generic and customized.\n3. **Store and load date/times** as a\n\n==================\n Document 1 \n----------------\n JSON tricks (python)¶\n\n\nThe pyjson-tricks package brings several pieces of functionality to python handling of json files:\n\n\n1. **Store and load numpy arrays** in human-readable format.\n2. **Store and load class instances** both generic and customized.\n3. **Store and load date/times** as a dictionary (including timezone).\n4. **Preserve map order** `{}` using `OrderedDict`.\n5. **Allow for comments** in json files by starting lines with `#`.\n6. Sets, complex numbers, Decimal, Fraction, enums, compression, duplicate keys, …\n\n\nAs well as compression and disallowing duplicate keys.\n\n\n* Code: https://github.com/mverleg/pyjson\\_tricks\n* Documentation: http://json-tricks.readthedocs.org/en/latest/\n* PIP: https://pypi.python.org/pypi/json\\_tricks\n\n\nThe 2.0 series added some of the above features and broke backward compatibility. The version 3.0 series is a more readable rewrite that also makes it easier to combine encoders, again not fully backward compatible.\n\n\nSeveral keys of the format `\\_\\_keyname\\_\\_` have special meanings, and more might be added in future releases.\n\n\nIf you’re considering JSON-but-with-comments as a config file format, have a look at HJSON, it might be more appropriate. For other purposes, keep reading!\n\n\nThanks for all the Github stars!\n\n# Installation and use¶\n\n\nYou can install using\n\n```\npip install json-tricks  # or e.g. 'json-tricks<3.0' for older versions\n\n```\n\n\nDecoding of some data types needs the corresponding package to be installed, e.g. `numpy` for arrays, `pandas` for dataframes and `pytz` for timezone-aware datetimes.\n\n\nYou\n\n==================\n Document 2 \n----------------\n Installation and use¶\n\n\nYou can install using\n\n```\npip install json-tricks  # or e.g. 'json-tricks<3.0' for older versions\n\n```\n\n\nDecoding of some data types needs the corresponding package to be installed, e.g. `numpy` for arrays, `pandas` for dataframes and `pytz` for timezone-aware datetimes.\n\n\nYou can import the usual json functions dump(s) and load(s), as well as a separate comment removal function, as follows:\n\n```\nfrom json_tricks import dump, dumps, load, loads, strip_comments\n\n\nThe exact signatures of these and other functions are in the documentation.\n\n\n`json-tricks` supports Python 2.7, and Python 3.4 and later, and is automatically tested on 2.7, 3.4, 3.5 and 3.6. Pypy is supported without numpy and pandas. Pandas doesn’t support 3.4.\n\n# Preserve type vs use primitive¶\n\n\nBy default, types are encoded such that they can be restored to their original type when loaded with `json-tricks`. Example encodings in this documentation refer to that format.\n\n\nYou can also choose to store things as\n\n==================\n Document 3 \n----------------\n# Numpy arrays¶\n\n\nThe array is encoded in sort-of-readable and very flexible and portable format, like so:\n\n```\narr = arange(0, 10, 1, dtype=uint8).reshape((2, 5))\nprint(dumps({'mydata': arr}))\n\n\nthis yields:\n\n```\n{\n        \"mydata\": {\n                \"dtype\": \"uint8\",\n                \"shape\": [2, 5],\n                \"Corder\": true,\n                \"\\_\\_ndarray\\_\\_\": [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\n        }\n}\n\n\nwhich will be converted back to a numpy array when using `json\\_tricks.loads`. Note that the memory order (`Corder`) is only stored in v3.1 and later and for arrays with at least 2 dimensions.\n\n\nAs you’ve seen, this uses the magic key `\\_\\_ndarray\\_\\_`. Don’t use `\\_\\_ndarray\\_\\_` as a dictionary key unless you’re trying to make a numpy array (and know what you’re doing).\n\n\nNumpy scalars are also serialized (v3.5+). They are represented by the closest python primitive type. A special representation was not feasible, because Python’s json implementation serializes some numpy types as primitives, without consulting custom encoders. If you want to preverse the exact numpy type, use encode\\_scalars\\_inplace.\n\n\n**Performance**: this method has slow write times similar to other human-readable formats, although read time is worse than csv. File size (with compression) is high on a relative scale, but it’s only around 30% above binary. See this benchmark (it’s called JSONGzip). A binary alternative might be added, but is not yet available.\n\n\nThis implementation is inspired by an answer by tlausch on stackoverflow that you could read for details.\n\n\n## Class instances¶\n\n\n`json\\_tricks` can serialize class instances.\n\n\nIf the class behaves normally (not generated dynamic, no `\\_\\_new\\_\\_` or `\\_\\_metaclass\\_\\_` magic, etc) *and* all it’s attributes are serializable, then this should work by default.\n\n```\n# json\\_tricks/test\\_class.py\nclass MyTestCls:\n        def \\_\\_init\\_\\_(self, \\*\\*kwargs):\n                for k, v in kwargs.items():"
    },
    {
        "instruction": "Create a Python program that uses 'jsonschema' to validate a JSON object against a schema with conditional validation based on properties.",
        "api": "jsonschema",
        "output": "#!pip install jsonschema\nimport jsonschema\n\n# Define a JSON schema with conditional validation based on properties\nschema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"isAdult\": {\"type\": \"boolean\"},\n        \"age\": {\"type\": \"integer\"}\n    },\n    \"if\": {\"properties\": {\"isAdult\": {\"const\": True}}, \"then\": {\"required\": [\"age\"]}}\n}\n\n# Create a JSON object to validate\njson_data = {\n    \"isAdult\": True,\n    \"age\": 25\n}\n\n# Validate the JSON data against the schema with conditional validation\ntry:\n    jsonschema.validate(instance=json_data, schema=schema)\n    print(\"JSON data is valid according to the schema with conditional validation.\")\nexcept jsonschema.exceptions.ValidationError as e:\n    print(f\"JSON data is not valid according to the schema with conditional validation. Error: {e}\")\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# API Reference#\n\n\n## Submodules#\n\n* `jsonschema.validators`\n* `jsonschema.exceptions`\n* `jsonschema.protocols`\n\n## `jsonschema`#\n\n\nAn implementation of JSON Schema for Python.\n\n\nThe main functionality is provided by the validator classes for each of the\nsupported JSON Schema versions.\n\n\nMost commonly, `jsonschema.validators.validate` is the quickest way to simply\nvalidate a given instance under a schema, and will create\n\n==================\n Document 1 \n----------------\n# `jsonschema`#\n\n\nAn implementation of JSON Schema for Python.\n\n\nThe main functionality is provided by the validator classes for each of the\nsupported JSON Schema versions.\n\n\nMost commonly, `jsonschema.validators.validate` is the quickest way to simply\nvalidate a given instance under a schema, and will create a validator\nfor you.\n\n\n*class* jsonschema.FormatChecker(*formats: Iterable[str] | None = None*)[source]#\nA `format` property checker.\n\n\nJSON Schema does not mandate that the `format` property actually do any\nvalidation. If validation is desired however, instances of this class can\nbe hooked into validators to enable format validation.\n\n\n`FormatChecker` objects always return `True` when asked about\nformats that they do not know how to validate.\n\n\nTo add a check for a custom format use the `FormatChecker.checks`\ndecorator.\n\nParameters:\n**formats** – The known formats to validate. This argument can be used to\nlimit which formats will be used during validation.\n\n\ncheck(*instance: object*, *format: str*) → None[source]#\nCheck whether the instance conforms to the given format.\n\nParameters:\n* **instance** (*any primitive type*, i.e. str, number, bool) – The instance to check\n* **format** – The format that instance should conform to\n\nRaises:\n**FormatError** – if the instance does not conform to `format`\n\nchecks(*format: str*, *raises: Type[Exception] | Tuple[Type[Exception], ...] = ()*) → Callable[[\\_F], \\_F][source]#\nRegister a decorated function as validating a new format.\n\nParameters:\n* **format** – The format that the decorated function will check.\n* **raises** – The exception(s) raised by the decorated function when an\ninvalid instance is found.\n\n\nThe exception object will be accessible as the\n`jsonschema.exceptions.ValidationError.cause` attribute of the\nresulting validation error.\n\nconforms(*instance: object*, *format: str*) → bool[source]#\nCheck whether the instance conforms to the given format.\n\nReturns:\nwhether it conformed\n\nReturn type:\nbool\n\n\n*exception* jsonschema.SchemaError(*message: str*, *validator=<unset>*, *path=()*, *cause=None*, *context=()*, *validator\\_value=<unset>*, *instance=<unset>*, *schema=<unset>*, *schema\\_path=()*, *parent=None*, *type\\_checker=<unset>*)[source]#\nA schema was invalid under its corresponding metaschema.\n\n*class* jsonschema.TypeChecker(*type\\_checkers: Mapping[str, Callable[[TypeChecker, Any], bool]] = HashTrieMap({})*)[source]#\nA type property checker.\n\n\nA `TypeChecker` performs type checking for a `Validator`, converting\nbetween the defined JSON Schema types and some associated Python types or\nobjects.\n\n\nModifying the behavior just mentioned by redefining which Python objects\nare considered to be of which JSON Schema types can be done using\n`TypeChecker.redefine` or `TypeChecker.redefine\\_many`, and types can be\nremoved via `TypeChecker.remove`. Each of these return a new `TypeChecker`.\n\nParameters:\n**type\\_checkers** – The initial mapping of types to their checking functions.\n\n\nis\\_type(*instance*, *type: str*) → bool[source]#\nCheck if the instance is of the appropriate type.\n\nParameters:\n* **instance** – The instance to check\n* **type** – The name of the type that is expected.\n\nRaises:\n**jsonschema.exceptions.UndefinedTypeCheck** – if `type` is unknown to this object.\n\nredefine(*type: str*, *fn*) → TypeChecker[source]#\nProduce a new checker with the given type redefined.\n\nParameters:\n* **type** – The name of the type to check.\n* **fn** (*collections.abc.Callable*) – A callable taking exactly two parameters - the type\nchecker calling the function and the instance to check.\nThe function should return true if instance is of this\ntype and false otherwise.\n\nredefine\\_many(*definitions=()*) → TypeChecker[source]#\nProduce a new checker with the given types redefined.\n\nParameters:\n**definitions** (*dict*) – A dictionary mapping types to their checking functions.\n\nremove(*\\*types*) → TypeChecker[source]#\nProduce a new checker with the given types forgotten.\n\nParameters:\n**types** – the names of the types to remove.\n\nRaises:\n**jsonschema.exceptions.UndefinedTypeCheck** – if any given type is unknown to this object\n\n\n*exception* jsonschema.ValidationError(*message: str*, *validator=<unset>*, *path=()*, *cause=None*, *context=()*, *validator\\_value=<unset>*, *instance=<unset>*, *schema=<unset>*, *schema\\_path=()*, *parent=None*, *type\\_checker=<unset>*)[source]#\nAn instance was invalid under a provided schema.\n\njsonschema.validate(*instance*, *schema*, *cls=None*, *\\*args*, *\\*\\*kwargs*)[source]#\nValidate an instance under the given schema.\n\n```\n>>> validate([2, 3, 4], {\"maxItems\": 2})\nTraceback (most recent call last):\n ...\nValidationError: [2, 3, 4] is too long\n\n```\n\n\n`validate()` will first verify that the\nprovided schema is itself valid, since not doing so can lead to less\nobvious error messages and fail in less obvious or consistent ways.\n\n\nIf you know you have a valid schema already, especially\nif you intend to validate multiple instances with\nthe same schema, you likely would prefer using the\n`jsonschema.protocols.Validator.validate` method directly on a\nspecific validator (e.g. `Draft202012Validator.validate`).\n\nParameters:\n* **instance** – The instance to validate\n* **schema** – The schema to validate with\n* **cls** (*jsonschema.protocols.Validator*) – The class that will be used to validate the instance.\n\n\nIf the `cls` argument is not provided, two things will happen\nin accordance with the specification. First, if the schema has a\n$schema keyword containing a known meta-schema [1] then the\nproper validator will be used. The specification recommends that\nall schemas contain $schema properties for this reason. If no\n$schema property is found, the default validator class is the\nlatest released draft.\n\n\nAny other provided positional and keyword arguments will be passed\non when instantiating the `cls`.\n\nRaises:\n* **jsonschema.exceptions.ValidationError** – if the instance is invalid\n* **jsonschema.exceptions.SchemaError** – if the schema itself is invalid\n\n\nFootnotes\n\njsonschema.\\_format.\\_F *= ~\\_F*#\nA format checker callable.\n\njsonschema.\\_typing.id\\_of#\nalias of `Callable`[[`Union`[`bool`, `Mapping`[`str`, `Any`]]], `Optional`[`str`]]\n# `jsonschema.validators`#\n\n\nCreation and extension of validators, with implementations for existing drafts.\n\n\n*class* jsonschema.validators.Draft201909Validator(*schema: bool | ~collections.abc.Mapping[str, ~typing.Any], resolver=None, format\\_checker: ~jsonschema.\\_format.FormatChecker | None = None, \\*, registry: ~referencing.\\_core.Registry[bool | ~collections.abc.Mapping[str, ~typing.Any]] = <Registry (20 resources)>, \\_resolver=None*)#\n\n\nFORMAT\\_CHECKER *= <FormatChecker checkers=['date', 'email', 'idn-email', 'idn-hostname',\n\n==================\n Document 2 \n----------------\n`jsonschema.protocols`#\n\n\ntyping.Protocol classes for jsonschema interfaces.\n\n\n*class* jsonschema.protocols.Validator(*schema: Mapping | bool*, *registry: referencing.jsonschema.SchemaRegistry*, *format\\_checker: jsonschema.FormatChecker | None = None*)[source]#\nThe protocol to which all validator classes adhere.\n\nParameters:\n* **schema** – The schema that the validator object will validate with.\nIt is assumed to be valid, and providing\nan invalid schema can lead to undefined behavior. See\n`Validator.check\\_schema` to validate a schema first.\n* **registry** – a schema registry that will be used for looking up JSON references\n* **resolver** – a resolver that will be used to resolve $ref\nproperties (JSON references). If unprovided, one will be created.\n\nDeprecated since version v4.18.0: `RefResolver` has been deprecated in favor of\n`referencing`, and with it, this argument.\n* **format\\_checker** – if provided, a checker which will be used to assert about\nformat properties present in the schema. If unprovided,\n*no* format validation is done, and the presence of format\nwithin schemas is strictly informational. Certain formats\nrequire additional packages to be installed in order to assert\nagainst instances. Ensure you’ve installed `jsonschema` with\nits extra (optional) dependencies when\ninvoking `pip`.\n\nDeprecated since version v4.12.0: Subclassing validator classes now explicitly warns this is not part of\ntheir public API.\n\nFORMAT\\_CHECKER*: ClassVar[jsonschema.FormatChecker]*#\nA `jsonschema.FormatChecker` that will be used when validating\nformat keywords in JSON schemas.\n\nID\\_OF*: \\_typing.id\\_of*#\nA function which given a schema returns its ID.\n\nMETA\\_SCHEMA*: ClassVar[Mapping]*#\nAn object representing the validator’s meta schema (the schema that\ndescribes valid schemas in the given version).\n\nTYPE\\_CHECKER*: ClassVar[jsonschema.TypeChecker]*#\nA `jsonschema.TypeChecker` that will be used when validating\ntype keywords in JSON schemas.\n\nVALIDATORS*: ClassVar[Mapping]*#\nA mapping of validation keywords (`str`s) to functions that\nvalidate the keyword with that name. For more information see\nCreating or Extending Validator Classes.\n\n*classmethod* check\\_schema(*schema: Mapping | bool*) → None[source]#\nValidate the given schema against the validator’s `META\\_SCHEMA`.\n\nRaises:\n**jsonschema.exceptions.SchemaError** – if the schema is invalid\n\nevolve(*\\*\\*kwargs*) → Validator[source]#\nCreate a new validator like this one, but with given changes.\n\n\nPreserves all other attributes, so can be used to e.g. create a\nvalidator with a different schema but with the same $ref\nresolution behavior.\n\n```\n>>> validator = Draft202012Validator({})\n>>> validator.evolve(schema={\"type\": \"number\"})\nDraft202012Validator(schema={'type': 'number'}, format\\_checker=None)\n\n\nThe returned object satisfies the validator protocol, but may not\nbe of the same concrete class! In particular this occurs\nwhen a $ref occurs to a schema with a different\n$schema than this one (i.e. for a different draft).\n\n```\n>>> validator.evolve(\n...     schema={\"$schema\": Draft7Validator.META\\_SCHEMA[\"$id\"]}\n... )\nDraft7Validator(schema=..., format\\_checker=None)\n\nis\\_type(*instance: Any*, *type: str*) → bool[source]#\nCheck if the instance is of the given (JSON Schema) type.\n\nParameters:\n* **instance** – the value to check\n* **type** – the name of a known (JSON Schema) type\n\nReturns:\nwhether the instance is of the given type\n\nRaises:\n**jsonschema.exceptions.UnknownType** – if `type` is not a known type\n\nis\\_valid(*instance: Any*) → bool[source]#\nCheck if the instance is valid under the current `schema`.\n\nReturns:\nwhether the instance is valid or not\n\n```\n>>> schema = {\"maxItems\" : 2}\n>>> Draft202012Validator(schema).is\\_valid([2, 3, 4])\nFalse\n\niter\\_errors(*instance: Any*) → Iterable[ValidationError][source]#\nLazily yield each of the validation errors in the given instance.\n\n```\n>>> schema = {\n...     \"type\" : \"array\",\n...     \"items\" : {\"enum\" : [1, 2, 3]},\n...     \"maxItems\" : 2,\n... }\n>>> v = Draft202012Validator(schema)\n>>> for error in sorted(v.iter\\_errors([2, 3, 4]), key=str):\n...     print(error.message)\n4 is not one of [1, 2, 3]\n[2, 3, 4] is too long\n\nDeprecated since version v4.0.0: Calling this function with a second schema argument is deprecated.\nUse `Validator.evolve` instead.\n\n\nschema*: Mapping | bool*#\nThe schema that will be used to validate instances\n\nvalidate(*instance: Any*) → None[source]#\nCheck if the instance is valid under the current `schema`.\n\nRaises:\n**jsonschema.exceptions.ValidationError** – if the instance is invalid\n\n```\n>>> schema = {\"maxItems\" : 2}\n>>> Draft202012Validator(schema).validate([2, 3, 4])\nTraceback (most recent call last):\n ...\nValidationError: [2, 3, 4] is too long"
    },
    {
        "instruction": "Create a Python program that utilizes 'jsonschema' to validate a JSON object against a schema with format-based date validation for a property, custom keyword validation, conditional validation based on property values, and pattern-based validation for another property.",
        "api": "jsonschema",
        "output": "#!pip install jsonschema\nimport jsonschema\n\n# Define a JSON schema with format-based date validation for a property, custom keyword validation, conditional validation based on property values, and pattern-based validation for another property\nschema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"birthDate\": {\"type\": \"string\", \"format\": \"date\"},\n        \"isStudent\": {\"type\": \"boolean\"},\n        \"studentID\": {\"type\": \"string\"},\n        \"email\": {\"type\": \"string\", \"pattern\": \"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\"}\n    },\n    \"custom_keywords\": {\n        \"student_required\": {\n            \"if\": {\"properties\": {\"isStudent\": {\"const\": True}}, \"then\": {\"required\": [\"studentID\"]}}\n        },\n        \"student_email_required\": {\n            \"if\": {\n                \"properties\": {\"isStudent\": {\"const\": True}, \"studentID\": {\"pattern\": \"^[A-Z]{3}[0-9]{3}$\"}},\n                \"then\": {\"required\": [\"email\"]}\n            }\n        }\n    }\n}\n\n# Create a JSON object to validate\njson_data = {\n    \"birthDate\": \"1995-01-15\",\n    \"isStudent\": True,\n    \"studentID\": \"ABC123\",\n    \"email\": \"student@example.com\"\n}\n\n# Custom keyword validation function\ndef validate_custom_keywords(validator, custom_keywords, instance, schema):\n    if instance[\"isStudent\"] and \"studentID\" in instance and not instance[\"studentID\"].startswith(\"ABC\"):\n        yield jsonschema.exceptions.ValidationError(\"Student ID must start with 'ABC' for students.\")\n    if instance[\"isStudent\"] and \"studentID\" in instance and not instance[\"email\"].endswith(\"student.com\"):\n        yield jsonschema.exceptions.ValidationError(\"Invalid email format for student with ID starting with 'ABC'.\")\n\n# Add the custom keyword validator to the validator registry\njsonschema.validators.validator_for(schema).VALIDATORS[\"custom_keywords\"] = validate_custom_keywords\n\n# Validate the JSON data against the schema with format-based date validation, custom keywords, conditional validation, and pattern-based validation\ntry:\n    jsonschema.validate(instance=json_data, schema=schema, format_checker=jsonschema.FormatChecker())\n    print(\"JSON data is valid according to the schema with format-based date validation, custom keywords, conditional validation, and pattern-based validation.\")\nexcept jsonschema.exceptions.ValidationError as e:\n    print(f\"JSON data is not valid according to the schema. Error: {e}\")\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# API Reference#\n\n\n## Submodules#\n\n* `jsonschema.validators`\n* `jsonschema.exceptions`\n* `jsonschema.protocols`\n\n## `jsonschema`#\n\n\nAn implementation of JSON Schema for Python.\n\n\nThe main functionality is provided by the validator classes for each of the\nsupported JSON Schema versions.\n\n\nMost commonly, `jsonschema.validators.validate` is the quickest way to simply\nvalidate a given instance under a schema, and will create\n\n==================\n Document 1 \n----------------\n# `jsonschema`#\n\n\nAn implementation of JSON Schema for Python.\n\n\nThe main functionality is provided by the validator classes for each of the\nsupported JSON Schema versions.\n\n\nMost commonly, `jsonschema.validators.validate` is the quickest way to simply\nvalidate a given instance under a schema, and will create a validator\nfor you.\n\n\n*class* jsonschema.FormatChecker(*formats: Iterable[str] | None = None*)[source]#\nA `format` property checker.\n\n\nJSON Schema does not mandate that the `format` property actually do any\nvalidation. If validation is desired however, instances of this class can\nbe hooked into validators to enable format validation.\n\n\n`FormatChecker` objects always return `True` when asked about\nformats that they do not know how to validate.\n\n\nTo add a check for a custom format use the `FormatChecker.checks`\ndecorator.\n\nParameters:\n**formats** – The known formats to validate. This argument can be used to\nlimit which formats will be used during validation.\n\n\ncheck(*instance: object*, *format: str*) → None[source]#\nCheck whether the instance conforms to the given format.\n\nParameters:\n* **instance** (*any primitive type*, i.e. str, number, bool) – The instance to check\n* **format** – The format that instance should conform to\n\nRaises:\n**FormatError** – if the instance does not conform to `format`\n\nchecks(*format: str*, *raises: Type[Exception] | Tuple[Type[Exception], ...] = ()*) → Callable[[\\_F], \\_F][source]#\nRegister a decorated function as validating a new format.\n\nParameters:\n* **format** – The format that the decorated function will check.\n* **raises** – The exception(s) raised by the decorated function when an\ninvalid instance is found.\n\n\nThe exception object will be accessible as the\n`jsonschema.exceptions.ValidationError.cause` attribute of the\nresulting validation error.\n\nconforms(*instance: object*, *format: str*) → bool[source]#\nCheck whether the instance conforms to the given format.\n\nReturns:\nwhether it conformed\n\nReturn type:\nbool\n\n\n*exception* jsonschema.SchemaError(*message: str*, *validator=<unset>*, *path=()*, *cause=None*, *context=()*, *validator\\_value=<unset>*, *instance=<unset>*, *schema=<unset>*, *schema\\_path=()*, *parent=None*, *type\\_checker=<unset>*)[source]#\nA schema was invalid under its corresponding metaschema.\n\n*class* jsonschema.TypeChecker(*type\\_checkers: Mapping[str, Callable[[TypeChecker, Any], bool]] = HashTrieMap({})*)[source]#\nA type property checker.\n\n\nA `TypeChecker` performs type checking for a `Validator`, converting\nbetween the defined JSON Schema types and some associated Python types or\nobjects.\n\n\nModifying the behavior just mentioned by redefining which Python objects\nare considered to be of which JSON Schema types can be done using\n`TypeChecker.redefine` or `TypeChecker.redefine\\_many`, and types can be\nremoved via `TypeChecker.remove`. Each of these return a new `TypeChecker`.\n\nParameters:\n**type\\_checkers** – The initial mapping of types to their checking functions.\n\n\nis\\_type(*instance*, *type: str*) → bool[source]#\nCheck if the instance is of the appropriate type.\n\nParameters:\n* **instance** – The instance to check\n* **type** – The name of the type that is expected.\n\nRaises:\n**jsonschema.exceptions.UndefinedTypeCheck** – if `type` is unknown to this object.\n\nredefine(*type: str*, *fn*) → TypeChecker[source]#\nProduce a new checker with the given type redefined.\n\nParameters:\n* **type** – The name of the type to check.\n* **fn** (*collections.abc.Callable*) – A callable taking exactly two parameters - the type\nchecker calling the function and the instance to check.\nThe function should return true if instance is of this\ntype and false otherwise.\n\nredefine\\_many(*definitions=()*) → TypeChecker[source]#\nProduce a new checker with the given types redefined.\n\nParameters:\n**definitions** (*dict*) – A dictionary mapping types to their checking functions.\n\nremove(*\\*types*) → TypeChecker[source]#\nProduce a new checker with the given types forgotten.\n\nParameters:\n**types** – the names of the types to remove.\n\nRaises:\n**jsonschema.exceptions.UndefinedTypeCheck** – if any given type is unknown to this object\n\n\n*exception* jsonschema.ValidationError(*message: str*, *validator=<unset>*, *path=()*, *cause=None*, *context=()*, *validator\\_value=<unset>*, *instance=<unset>*, *schema=<unset>*, *schema\\_path=()*, *parent=None*, *type\\_checker=<unset>*)[source]#\nAn instance was invalid under a provided schema.\n\njsonschema.validate(*instance*, *schema*, *cls=None*, *\\*args*, *\\*\\*kwargs*)[source]#\nValidate an instance under the given schema.\n\n```\n>>> validate([2, 3, 4], {\"maxItems\": 2})\nTraceback (most recent call last):\n ...\nValidationError: [2, 3, 4] is too long\n\n```\n\n\n`validate()` will first verify that the\nprovided schema is itself valid, since not doing so can lead to less\nobvious error messages and fail in less obvious or consistent ways.\n\n\nIf you know you have a valid schema already, especially\nif you intend to validate multiple instances with\nthe same schema, you likely would prefer using the\n`jsonschema.protocols.Validator.validate` method directly on a\nspecific validator (e.g. `Draft202012Validator.validate`).\n\nParameters:\n* **instance** – The instance to validate\n* **schema** – The schema to validate with\n* **cls** (*jsonschema.protocols.Validator*) – The class that will be used to validate the instance.\n\n\nIf the `cls` argument is not provided, two things will happen\nin accordance with the specification. First, if the schema has a\n$schema keyword containing a known meta-schema [1] then the\nproper validator will be used. The specification recommends that\nall schemas contain $schema properties for this reason. If no\n$schema property is found, the default validator class is the\nlatest released draft.\n\n\nAny other provided positional and keyword arguments will be passed\non when instantiating the `cls`.\n\nRaises:\n* **jsonschema.exceptions.ValidationError** – if the instance is invalid\n* **jsonschema.exceptions.SchemaError** – if the schema itself is invalid\n\n\nFootnotes\n\njsonschema.\\_format.\\_F *= ~\\_F*#\nA format checker callable.\n\njsonschema.\\_typing.id\\_of#\nalias of `Callable`[[`Union`[`bool`, `Mapping`[`str`, `Any`]]], `Optional`[`str`]]\n# `jsonschema.validators`#\n\n\nCreation and extension of validators, with implementations for existing drafts.\n\n\n*class* jsonschema.validators.Draft201909Validator(*schema: bool | ~collections.abc.Mapping[str, ~typing.Any], resolver=None, format\\_checker: ~jsonschema.\\_format.FormatChecker | None = None, \\*, registry: ~referencing.\\_core.Registry[bool | ~collections.abc.Mapping[str, ~typing.Any]] = <Registry (20 resources)>, \\_resolver=None*)#\n\n\nFORMAT\\_CHECKER *= <FormatChecker checkers=['date', 'email', 'idn-email', 'idn-hostname',\n\n==================\n Document 2 \n----------------\n`jsonschema.protocols`#\n\n\ntyping.Protocol classes for jsonschema interfaces.\n\n\n*class* jsonschema.protocols.Validator(*schema: Mapping | bool*, *registry: referencing.jsonschema.SchemaRegistry*, *format\\_checker: jsonschema.FormatChecker | None = None*)[source]#\nThe protocol to which all validator classes adhere.\n\nParameters:\n* **schema** – The schema that the validator object will validate with.\nIt is assumed to be valid, and providing\nan invalid schema can lead to undefined behavior. See\n`Validator.check\\_schema` to validate a schema first.\n* **registry** – a schema registry that will be used for looking up JSON references\n* **resolver** – a resolver that will be used to resolve $ref\nproperties (JSON references). If unprovided, one will be created.\n\nDeprecated since version v4.18.0: `RefResolver` has been deprecated in favor of\n`referencing`, and with it, this argument.\n* **format\\_checker** – if provided, a checker which will be used to assert about\nformat properties present in the schema. If unprovided,\n*no* format validation is done, and the presence of format\nwithin schemas is strictly informational. Certain formats\nrequire additional packages to be installed in order to assert\nagainst instances. Ensure you’ve installed `jsonschema` with\nits extra (optional) dependencies when\ninvoking `pip`.\n\nDeprecated since version v4.12.0: Subclassing validator classes now explicitly warns this is not part of\ntheir public API.\n\nFORMAT\\_CHECKER*: ClassVar[jsonschema.FormatChecker]*#\nA `jsonschema.FormatChecker` that will be used when validating\nformat keywords in JSON schemas.\n\nID\\_OF*: \\_typing.id\\_of*#\nA function which given a schema returns its ID.\n\nMETA\\_SCHEMA*: ClassVar[Mapping]*#\nAn object representing the validator’s meta schema (the schema that\ndescribes valid schemas in the given version).\n\nTYPE\\_CHECKER*: ClassVar[jsonschema.TypeChecker]*#\nA `jsonschema.TypeChecker` that will be used when validating\ntype keywords in JSON schemas.\n\nVALIDATORS*: ClassVar[Mapping]*#\nA mapping of validation keywords (`str`s) to functions that\nvalidate the keyword with that name. For more information see\nCreating or Extending Validator Classes.\n\n*classmethod* check\\_schema(*schema: Mapping | bool*) → None[source]#\nValidate the given schema against the validator’s `META\\_SCHEMA`.\n\nRaises:\n**jsonschema.exceptions.SchemaError** – if the schema is invalid\n\nevolve(*\\*\\*kwargs*) → Validator[source]#\nCreate a new validator like this one, but with given changes.\n\n\nPreserves all other attributes, so can be used to e.g. create a\nvalidator with a different schema but with the same $ref\nresolution behavior.\n\n```\n>>> validator = Draft202012Validator({})\n>>> validator.evolve(schema={\"type\": \"number\"})\nDraft202012Validator(schema={'type': 'number'}, format\\_checker=None)\n\n\nThe returned object satisfies the validator protocol, but may not\nbe of the same concrete class! In particular this occurs\nwhen a $ref occurs to a schema with a different\n$schema than this one (i.e. for a different draft).\n\n```\n>>> validator.evolve(\n...     schema={\"$schema\": Draft7Validator.META\\_SCHEMA[\"$id\"]}\n... )\nDraft7Validator(schema=..., format\\_checker=None)\n\nis\\_type(*instance: Any*, *type: str*) → bool[source]#\nCheck if the instance is of the given (JSON Schema) type.\n\nParameters:\n* **instance** – the value to check\n* **type** – the name of a known (JSON Schema) type\n\nReturns:\nwhether the instance is of the given type\n\nRaises:\n**jsonschema.exceptions.UnknownType** – if `type` is not a known type\n\nis\\_valid(*instance: Any*) → bool[source]#\nCheck if the instance is valid under the current `schema`.\n\nReturns:\nwhether the instance is valid or not\n\n```\n>>> schema = {\"maxItems\" : 2}\n>>> Draft202012Validator(schema).is\\_valid([2, 3, 4])\nFalse\n\niter\\_errors(*instance: Any*) → Iterable[ValidationError][source]#\nLazily yield each of the validation errors in the given instance.\n\n```\n>>> schema = {\n...     \"type\" : \"array\",\n...     \"items\" : {\"enum\" : [1, 2, 3]},\n...     \"maxItems\" : 2,\n... }\n>>> v = Draft202012Validator(schema)\n>>> for error in sorted(v.iter\\_errors([2, 3, 4]), key=str):\n...     print(error.message)\n4 is not one of [1, 2, 3]\n[2, 3, 4] is too long\n\nDeprecated since version v4.0.0: Calling this function with a second schema argument is deprecated.\nUse `Validator.evolve` instead.\n\n\nschema*: Mapping | bool*#\nThe schema that will be used to validate instances\n\nvalidate(*instance: Any*) → None[source]#\nCheck if the instance is valid under the current `schema`.\n\nRaises:\n**jsonschema.exceptions.ValidationError** – if the instance is invalid\n\n```\n>>> schema = {\"maxItems\" : 2}\n>>> Draft202012Validator(schema).validate([2, 3, 4])\nTraceback (most recent call last):\n ...\nValidationError: [2, 3, 4] is too long"
    },
    {
        "instruction": "Create a Python program that utilizes 'jsonschema' to validate a JSON object against a schema that includes custom keyword validation.",
        "api": "jsonschema",
        "output": "#!pip install jsonschema\nimport jsonschema\n\n# Define a JSON schema with custom keyword validation\nschema = {\n    \"type\": \"object\",\n    \"custom_keyword\": \"my_custom_value\"\n}\n\n# Create a JSON object to validate\njson_data = {\n    \"custom_keyword\": \"my_custom_value\"\n}\n\n# Custom keyword validation function\ndef validate_custom_keyword(validator, custom_keyword, instance, schema):\n    if instance != custom_keyword:\n        yield jsonschema.exceptions.ValidationError(f\"Value does not match the custom keyword: {custom_keyword}\")\n\n# Add the custom keyword validator to the validator registry\njsonschema.validators.validator_for(schema).VALIDATORS[\"custom_keyword\"] = validate_custom_keyword\n\n# Validate the JSON data against the schema with custom keyword validation\ntry:\n    jsonschema.validate(instance=json_data, schema=schema)\n    print(\"JSON data is valid according to the schema with custom keyword validation.\")\nexcept jsonschema.exceptions.ValidationError as e:\n    print(f\"JSON data is not valid according to the schema with custom keyword validation. Error: {e}\")\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# API Reference#\n\n\n## Submodules#\n\n* `jsonschema.validators`\n* `jsonschema.exceptions`\n* `jsonschema.protocols`\n\n## `jsonschema`#\n\n\nAn implementation of JSON Schema for Python.\n\n\nThe main functionality is provided by the validator classes for each of the\nsupported JSON Schema versions.\n\n\nMost commonly, `jsonschema.validators.validate` is the quickest way to simply\nvalidate a given instance under a schema, and will create\n\n==================\n Document 1 \n----------------\n# `jsonschema`#\n\n\nAn implementation of JSON Schema for Python.\n\n\nThe main functionality is provided by the validator classes for each of the\nsupported JSON Schema versions.\n\n\nMost commonly, `jsonschema.validators.validate` is the quickest way to simply\nvalidate a given instance under a schema, and will create a validator\nfor you.\n\n\n*class* jsonschema.FormatChecker(*formats: Iterable[str] | None = None*)[source]#\nA `format` property checker.\n\n\nJSON Schema does not mandate that the `format` property actually do any\nvalidation. If validation is desired however, instances of this class can\nbe hooked into validators to enable format validation.\n\n\n`FormatChecker` objects always return `True` when asked about\nformats that they do not know how to validate.\n\n\nTo add a check for a custom format use the `FormatChecker.checks`\ndecorator.\n\nParameters:\n**formats** – The known formats to validate. This argument can be used to\nlimit which formats will be used during validation.\n\n\ncheck(*instance: object*, *format: str*) → None[source]#\nCheck whether the instance conforms to the given format.\n\nParameters:\n* **instance** (*any primitive type*, i.e. str, number, bool) – The instance to check\n* **format** – The format that instance should conform to\n\nRaises:\n**FormatError** – if the instance does not conform to `format`\n\nchecks(*format: str*, *raises: Type[Exception] | Tuple[Type[Exception], ...] = ()*) → Callable[[\\_F], \\_F][source]#\nRegister a decorated function as validating a new format.\n\nParameters:\n* **format** – The format that the decorated function will check.\n* **raises** – The exception(s) raised by the decorated function when an\ninvalid instance is found.\n\n\nThe exception object will be accessible as the\n`jsonschema.exceptions.ValidationError.cause` attribute of the\nresulting validation error.\n\nconforms(*instance: object*, *format: str*) → bool[source]#\nCheck whether the instance conforms to the given format.\n\nReturns:\nwhether it conformed\n\nReturn type:\nbool\n\n\n*exception* jsonschema.SchemaError(*message: str*, *validator=<unset>*, *path=()*, *cause=None*, *context=()*, *validator\\_value=<unset>*, *instance=<unset>*, *schema=<unset>*, *schema\\_path=()*, *parent=None*, *type\\_checker=<unset>*)[source]#\nA schema was invalid under its corresponding metaschema.\n\n*class* jsonschema.TypeChecker(*type\\_checkers: Mapping[str, Callable[[TypeChecker, Any], bool]] = HashTrieMap({})*)[source]#\nA type property checker.\n\n\nA `TypeChecker` performs type checking for a `Validator`, converting\nbetween the defined JSON Schema types and some associated Python types or\nobjects.\n\n\nModifying the behavior just mentioned by redefining which Python objects\nare considered to be of which JSON Schema types can be done using\n`TypeChecker.redefine` or `TypeChecker.redefine\\_many`, and types can be\nremoved via `TypeChecker.remove`. Each of these return a new `TypeChecker`.\n\nParameters:\n**type\\_checkers** – The initial mapping of types to their checking functions.\n\n\nis\\_type(*instance*, *type: str*) → bool[source]#\nCheck if the instance is of the appropriate type.\n\nParameters:\n* **instance** – The instance to check\n* **type** – The name of the type that is expected.\n\nRaises:\n**jsonschema.exceptions.UndefinedTypeCheck** – if `type` is unknown to this object.\n\nredefine(*type: str*, *fn*) → TypeChecker[source]#\nProduce a new checker with the given type redefined.\n\nParameters:\n* **type** – The name of the type to check.\n* **fn** (*collections.abc.Callable*) – A callable taking exactly two parameters - the type\nchecker calling the function and the instance to check.\nThe function should return true if instance is of this\ntype and false otherwise.\n\nredefine\\_many(*definitions=()*) → TypeChecker[source]#\nProduce a new checker with the given types redefined.\n\nParameters:\n**definitions** (*dict*) – A dictionary mapping types to their checking functions.\n\nremove(*\\*types*) → TypeChecker[source]#\nProduce a new checker with the given types forgotten.\n\nParameters:\n**types** – the names of the types to remove.\n\nRaises:\n**jsonschema.exceptions.UndefinedTypeCheck** – if any given type is unknown to this object\n\n\n*exception* jsonschema.ValidationError(*message: str*, *validator=<unset>*, *path=()*, *cause=None*, *context=()*, *validator\\_value=<unset>*, *instance=<unset>*, *schema=<unset>*, *schema\\_path=()*, *parent=None*, *type\\_checker=<unset>*)[source]#\nAn instance was invalid under a provided schema.\n\njsonschema.validate(*instance*, *schema*, *cls=None*, *\\*args*, *\\*\\*kwargs*)[source]#\nValidate an instance under the given schema.\n\n```\n>>> validate([2, 3, 4], {\"maxItems\": 2})\nTraceback (most recent call last):\n ...\nValidationError: [2, 3, 4] is too long\n\n```\n\n\n`validate()` will first verify that the\nprovided schema is itself valid, since not doing so can lead to less\nobvious error messages and fail in less obvious or consistent ways.\n\n\nIf you know you have a valid schema already, especially\nif you intend to validate multiple instances with\nthe same schema, you likely would prefer using the\n`jsonschema.protocols.Validator.validate` method directly on a\nspecific validator (e.g. `Draft202012Validator.validate`).\n\nParameters:\n* **instance** – The instance to validate\n* **schema** – The schema to validate with\n* **cls** (*jsonschema.protocols.Validator*) – The class that will be used to validate the instance.\n\n\nIf the `cls` argument is not provided, two things will happen\nin accordance with the specification. First, if the schema has a\n$schema keyword containing a known meta-schema [1] then the\nproper validator will be used. The specification recommends that\nall schemas contain $schema properties for this reason. If no\n$schema property is found, the default validator class is the\nlatest released draft.\n\n\nAny other provided positional and keyword arguments will be passed\non when instantiating the `cls`.\n\nRaises:\n* **jsonschema.exceptions.ValidationError** – if the instance is invalid\n* **jsonschema.exceptions.SchemaError** – if the schema itself is invalid\n\n\nFootnotes\n\njsonschema.\\_format.\\_F *= ~\\_F*#\nA format checker callable.\n\njsonschema.\\_typing.id\\_of#\nalias of `Callable`[[`Union`[`bool`, `Mapping`[`str`, `Any`]]], `Optional`[`str`]]\n# `jsonschema.validators`#\n\n\nCreation and extension of validators, with implementations for existing drafts.\n\n\n*class* jsonschema.validators.Draft201909Validator(*schema: bool | ~collections.abc.Mapping[str, ~typing.Any], resolver=None, format\\_checker: ~jsonschema.\\_format.FormatChecker | None = None, \\*, registry: ~referencing.\\_core.Registry[bool | ~collections.abc.Mapping[str, ~typing.Any]] = <Registry (20 resources)>, \\_resolver=None*)#\n\n\nFORMAT\\_CHECKER *= <FormatChecker checkers=['date', 'email', 'idn-email', 'idn-hostname',\n\n==================\n Document 2 \n----------------\n`jsonschema.protocols`#\n\n\ntyping.Protocol classes for jsonschema interfaces.\n\n\n*class* jsonschema.protocols.Validator(*schema: Mapping | bool*, *registry: referencing.jsonschema.SchemaRegistry*, *format\\_checker: jsonschema.FormatChecker | None = None*)[source]#\nThe protocol to which all validator classes adhere.\n\nParameters:\n* **schema** – The schema that the validator object will validate with.\nIt is assumed to be valid, and providing\nan invalid schema can lead to undefined behavior. See\n`Validator.check\\_schema` to validate a schema first.\n* **registry** – a schema registry that will be used for looking up JSON references\n* **resolver** – a resolver that will be used to resolve $ref\nproperties (JSON references). If unprovided, one will be created.\n\nDeprecated since version v4.18.0: `RefResolver` has been deprecated in favor of\n`referencing`, and with it, this argument.\n* **format\\_checker** – if provided, a checker which will be used to assert about\nformat properties present in the schema. If unprovided,\n*no* format validation is done, and the presence of format\nwithin schemas is strictly informational. Certain formats\nrequire additional packages to be installed in order to assert\nagainst instances. Ensure you’ve installed `jsonschema` with\nits extra (optional) dependencies when\ninvoking `pip`.\n\nDeprecated since version v4.12.0: Subclassing validator classes now explicitly warns this is not part of\ntheir public API.\n\nFORMAT\\_CHECKER*: ClassVar[jsonschema.FormatChecker]*#\nA `jsonschema.FormatChecker` that will be used when validating\nformat keywords in JSON schemas.\n\nID\\_OF*: \\_typing.id\\_of*#\nA function which given a schema returns its ID.\n\nMETA\\_SCHEMA*: ClassVar[Mapping]*#\nAn object representing the validator’s meta schema (the schema that\ndescribes valid schemas in the given version).\n\nTYPE\\_CHECKER*: ClassVar[jsonschema.TypeChecker]*#\nA `jsonschema.TypeChecker` that will be used when validating\ntype keywords in JSON schemas.\n\nVALIDATORS*: ClassVar[Mapping]*#\nA mapping of validation keywords (`str`s) to functions that\nvalidate the keyword with that name. For more information see\nCreating or Extending Validator Classes.\n\n*classmethod* check\\_schema(*schema: Mapping | bool*) → None[source]#\nValidate the given schema against the validator’s `META\\_SCHEMA`.\n\nRaises:\n**jsonschema.exceptions.SchemaError** – if the schema is invalid\n\nevolve(*\\*\\*kwargs*) → Validator[source]#\nCreate a new validator like this one, but with given changes.\n\n\nPreserves all other attributes, so can be used to e.g. create a\nvalidator with a different schema but with the same $ref\nresolution behavior.\n\n```\n>>> validator = Draft202012Validator({})\n>>> validator.evolve(schema={\"type\": \"number\"})\nDraft202012Validator(schema={'type': 'number'}, format\\_checker=None)\n\n\nThe returned object satisfies the validator protocol, but may not\nbe of the same concrete class! In particular this occurs\nwhen a $ref occurs to a schema with a different\n$schema than this one (i.e. for a different draft).\n\n```\n>>> validator.evolve(\n...     schema={\"$schema\": Draft7Validator.META\\_SCHEMA[\"$id\"]}\n... )\nDraft7Validator(schema=..., format\\_checker=None)\n\nis\\_type(*instance: Any*, *type: str*) → bool[source]#\nCheck if the instance is of the given (JSON Schema) type.\n\nParameters:\n* **instance** – the value to check\n* **type** – the name of a known (JSON Schema) type\n\nReturns:\nwhether the instance is of the given type\n\nRaises:\n**jsonschema.exceptions.UnknownType** – if `type` is not a known type\n\nis\\_valid(*instance: Any*) → bool[source]#\nCheck if the instance is valid under the current `schema`.\n\nReturns:\nwhether the instance is valid or not\n\n```\n>>> schema = {\"maxItems\" : 2}\n>>> Draft202012Validator(schema).is\\_valid([2, 3, 4])\nFalse\n\niter\\_errors(*instance: Any*) → Iterable[ValidationError][source]#\nLazily yield each of the validation errors in the given instance.\n\n```\n>>> schema = {\n...     \"type\" : \"array\",\n...     \"items\" : {\"enum\" : [1, 2, 3]},\n...     \"maxItems\" : 2,\n... }\n>>> v = Draft202012Validator(schema)\n>>> for error in sorted(v.iter\\_errors([2, 3, 4]), key=str):\n...     print(error.message)\n4 is not one of [1, 2, 3]\n[2, 3, 4] is too long\n\nDeprecated since version v4.0.0: Calling this function with a second schema argument is deprecated.\nUse `Validator.evolve` instead.\n\n\nschema*: Mapping | bool*#\nThe schema that will be used to validate instances\n\nvalidate(*instance: Any*) → None[source]#\nCheck if the instance is valid under the current `schema`.\n\nRaises:\n**jsonschema.exceptions.ValidationError** – if the instance is invalid\n\n```\n>>> schema = {\"maxItems\" : 2}\n>>> Draft202012Validator(schema).validate([2, 3, 4])\nTraceback (most recent call last):\n ...\nValidationError: [2, 3, 4] is too long"
    },
    {
        "instruction": "Create a Python program that utilizes the \"langchain\" API to interact with the GPT-3.5 Turbo model. Define a chat prompt template that includes a variable {country}. Use this template to inquire about the capital of a specific country, e.g., \"What is the capital of {country}?\" Replace {country} with \"Germany\" and retrieve the model's response. Save the response.",
        "api": "langchain",
        "output": "#!pip install langchain\n# Import necessary libraries\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.chains import LLMChain\n\n# Define the GPT-3.5 Turbo model\nllm_model = \"gpt-3.5-turbo\"\n\n# Initialize a ChatOpenAI instance with the specified model and temperature\nllm = ChatOpenAI(temperature=0.5, model=llm_model)\n\n# Define a template for chat prompts, where {country} will be replaced with the actual country name\nprompt = ChatPromptTemplate.from_template(\"What is the capital of {country}?\")\n\n# Initialize an LLMChain with the ChatOpenAI instance and the prompt template\nchain = LLMChain(llm=llm, prompt=prompt)\n\n# Specify the country you want to inquire about\ncountry = \"Germany\"\n\n# Run the chain to get a response about the capital of the specified country\nchain.run(country)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n Retrieval\n\nMany LLM applications require user-specific data that is not part of the model's training set.\nThe primary way of accomplishing this is through Retrieval Augmented Generation (RAG).\nIn this process, external data is *retrieved* and then passed to the LLM when doing the *generation* step.\n\nLangChain provides all the building blocks for RAG applications - from simple to complex.\nThis section of the documentation covers everything related to the *retrieval* step - e.g. the fetching of the data.\nAlthough this sounds simple, it can be subtly complex.\nThis encompasses several key modules.\n\n\n\n**Document loaders**\n\nLoad documents from many different sources.\nLangChain provides over 100 different document loaders as well as integrations with other major providers in the space,\nlike AirByte and Unstructured.\nWe provide integrations to load all types of documents (HTML, PDF, code) from all types of locations (private s3 buckets, public websites).\n\n**Document transformers**\n\nA key part of retrieval is fetching only the relevant parts of documents.\nThis involves several transformation steps in order to best prepare the documents for retrieval.\nOne of the primary ones here is splitting (or chunking) a large document into smaller chunks.\nLangChain provides several different algorithms for doing this, as well as logic optimized for specific document types (code, markdown, etc).\n\n**Text embedding models**\n\nAnother key part of retrieval has become creating embeddings for documents.\nEmbeddings capture the semantic meaning of the text, allowing you to quickly and\nefficiently find other pieces of text that are similar.\nLangChain provides integrations with over 25 different embedding providers and methods,\nfrom open-source to proprietary API,\nallowing you to choose the one best suited for your needs.\nLangChain provides a standard interface, allowing you to easily swap between models.\n\n**Vector stores**\n\nWith the rise of embeddings, there has emerged a need for databases to support efficient storage and searching of these embeddings.\nLangChain provides integrations with over 50 different vectorstores, from open-source local ones to cloud-hosted proprietary ones,\nallowing you to choose the one best suited for your needs.\nLangChain exposes a standard interface, allowing you to easily swap between vector stores.\n\n**Retrievers**\n\nOnce the data is in the database, you still need to retrieve it.\nLangChain supports many different retrieval algorithms and is one of the places where we add the most value.\nWe support basic methods that are easy to get started - namely simple semantic search.\nHowever, we have also added a collection of algorithms on top of this to increase performance.\nThese include:\n\n* Parent Document Retriever: This allows you to create multiple embeddings per parent document, allowing you to look up smaller chunks but return larger context.\n* Self Query Retriever: User questions often contain a reference to something that isn't just semantic but rather expresses some logic that can best be represented as a metadata filter. Self-query allows you to parse out the *semantic* part of a query from other *metadata filters* present in the query.\n* Ensemble Retriever: Sometimes you may want to retrieve documents from multiple different sources, or using multiple different algorithms. The ensemble retriever allows you to easily do this.\n* And more!\n\n\n* \n* Modules\n* Chains\nOn this page# Chains\n\nUsing an LLM in isolation is fine for simple applications,\nbut more complex applications require chaining LLMs - either with each other or with other components.\n\nLangChain provides the **Chain** interface for such \"chained\" applications. We define a Chain very generically as a sequence of calls to components, which can include other chains. The base interface is simple:\n\n\n```\nclass Chain(BaseModel, ABC):  \n \"\"\"Base interface that all chains should implement.\"\"\"  \n  \n memory: BaseMemory  \n callbacks: Callbacks  \n  \n def \\_\\_call\\_\\_(  \n self,  \n inputs: Any,  \n return\\_only\\_outputs: bool = False,  \n callbacks: Callbacks = None,  \n ) -> Dict[str, Any]:  \n ...  \n\n```\nThis idea of composing components together in a chain is simple but powerful. It drastically simplifies and makes more modular the implementation of complex applications, which in turn makes it much easier to debug, maintain, and improve your applications.\n\nFor more specifics check out:\n\n* How-to for walkthroughs of different chain features\n* Foundational to get acquainted with core building block chains\n* Document to learn how to incorporate documents into chains\n\n## Why do we need chains?​\n\nChains allow us to combine multiple components together to create a single, coherent application. For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM. We can build more complex chains by combining multiple chains together, or by combining chains with other components.\n\n\n## Get started​\n\n#### Using `LLMChain`​\n\nThe `LLMChain` is most basic building block chain. It takes in a prompt template, formats it with the user input and returns the response from an LLM.\n\nTo use the `LLMChain`, first create a prompt template.\n\n\n```\nfrom langchain.llms import OpenAI\n\n==================\n Document 1 \n----------------\n\n\n* \n* Modules\nOn this page# Modules\n\nLangChain provides standard, extendable interfaces and external integrations for the following modules, listed from least to most complex:\n\n#### Model I/O​\n\nInterface with language models\n\n\n#### Retrieval​\n\nInterface with application-specific data\n\n\n#### Chains​\n\nConstruct sequences of calls\n\n\n#### Agents​\n\nLet chains choose which tools to use given high-level directives\n\n\n#### Memory​\n\nPersist application state between runs of a chain\n\n\n#### Callbacks​\n\nLog and stream intermediate steps of any chain\n\n* \n* Modules\n* Model I/​O\n\n# Model I/O\n\nThe core element of any language model application is...the model. LangChain gives you the building blocks to interface with any language model.\n\n* Prompts: Templatize, dynamically select, and manage model inputs\n* Language models: Make calls to language models through common interfaces\n* Output parsers: Extract information from model outputs\n\n\n\n* \n* Modules\n* Retrieval\n# Retrieval\n\nMany LLM applications require user-specific data that is not part of the model's training set.\nThe primary way of accomplishing this is through Retrieval Augmented Generation (RAG).\nIn this process, external data is *retrieved* and then passed to the LLM when\n\n==================\n Document 2 \n----------------\n### Using `LLMChain`​\n\nThe `LLMChain` is most basic building block chain. It takes in a prompt template, formats it with the user input and returns the response from an LLM.\n\nTo use the `LLMChain`, first create a prompt template.\n\n\n```\nfrom langchain.llms import OpenAI  \nfrom langchain.prompts import PromptTemplate  \n  \nllm = OpenAI(temperature=0.9)  \nprompt = PromptTemplate(  \n input\\_variables=[\"product\"],  \n template=\"What is a good name for a company that makes {product}?\",  \n)  \n\n```\nWe can now create a very simple chain that will take user input, format the prompt with it, and then send it to the LLM.\n\n\n```\nfrom langchain.chains import LLMChain  \nchain = LLMChain(llm=llm, prompt=prompt)  \n  # Run the chain only specifying the input variable.  \nprint(chain.run(\"colorful socks\"))  \n\n```\n\n```\n Colorful Toes Co.  \n\n```\nIf there are multiple variables, you can input them all at once using a dictionary.\n\n\n```\nprompt = PromptTemplate(  \n input\\_variables=[\"company\", \"product\"],\n\n==================\n Document 3 \n----------------\n Run the chain only specifying the input variable.  \nprint(chain.run(\"colorful socks\"))  \n\n```\n\n```\n Colorful Toes Co.  \n\n```\nIf there are multiple variables, you can input them all at once using a dictionary.\n\n\n```\nprompt = PromptTemplate(  \n input\\_variables=[\"company\", \"product\"],  \n template=\"What is a good name for {company} that makes {product}?\",  \n)  \nchain = LLMChain(llm=llm, prompt=prompt)  \nprint(chain.run({  \n 'company': \"ABC Startup\",  \n 'product': \"colorful socks\"  \n }))  \n\n```\n Socktopia Colourful Creations.  \n\n```\nYou can use a chat model in an `LLMChain` as well:\n\n\n```\nfrom langchain.chat\\_models import ChatOpenAI  \nfrom langchain.prompts.chat import (  \n ChatPromptTemplate,  \n HumanMessagePromptTemplate,  \n)  \nhuman\\_message\\_prompt = HumanMessagePromptTemplate(  \n prompt=PromptTemplate(  \n template=\"What is a good name for a company that makes {product}?\",  \n input\\_variables=[\"product\"],  \n )  \n )  \nchat\\_prompt\\_template = ChatPromptTemplate.from\\_messages([human\\_message\\_prompt])  \nchat = ChatOpenAI(temperature=0.9)  \nchain = LLMChain(llm=chat, prompt=chat\\_prompt\\_template)  \nprint(chain.run(\"colorful socks\"))  \n\n```\n Rainbow Socks Co.  \n\n\n* \n* Modules\n* Memory\nOn this page# Memory\n\nMost LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation.\nAt bare minimum, a conversational system should be able to access some window of past messages directly.\nA more complex system will need to have a world model that it is constantly updating, which allows it to do things like maintain information about entities and their relationships.\n\nWe call this ability to store information about past interactions \"memory\".\nLangChain provides a lot of utilities for adding memory to a system.\nThese utilities can be used by themselves or incorporated seamlessly into a chain.\n\nA memory system needs to support two basic actions: reading and writing.\nRecall that every chain defines some core execution logic that expects certain inputs.\nSome of these inputs come directly from the user, but some of these inputs can come from memory.\nA chain will interact with its memory system twice in a given run.\n\n1. AFTER receiving the initial user inputs but BEFORE executing the core logic, a chain will READ from its memory system and augment the user inputs.\n2. AFTER executing the core logic but BEFORE returning the answer, a chain will WRITE the inputs and outputs of the current run to memory, so that they can be referred to in future runs.\n\n\n\n## Building memory into a system​\n\nThe two core design decisions in any memory system are:\n\n* How state is stored\n* How state is queried\n\n\n### Storing: List of chat messages​\n\nUnderlying any memory is a history of all chat interactions.\nEven if these are not all used directly, they need to be stored in some form.\nOne of the key parts of the LangChain memory module is a series of integrations for storing these chat messages,\nfrom in-memory lists to persistent databases.\n\n* Chat message storage: How to work with Chat Messages, and the various integrations offered.\n\n### Querying: Data structures and algorithms on top of chat messages​\n\nKeeping a list of chat messages is fairly straight-forward.\nWhat is less straight-forward are the data structures and algorithms built on top of chat messages that serve a view of those"
    },
    {
        "instruction": "Create a Python program that utilizes the 'linear-operator' API to compute the eigenvalues and eigenvectors of a given square matrix. The program should print the computed eigenvalues and eigenvectors.",
        "api": "linear-operator",
        "output": "#!pip install linear-operator\nimport torch\nfrom linear_operator import to_linear_operator\nfrom scipy.linalg import eigh\n\n# Define a square matrix M\nM = torch.Tensor([[4, 2], [2, 5]])\n\n# Compute the eigenvalues and eigenvectors\nM_op = to_linear_operator(M)\neigenvalues, eigenvectors = eigh(M_op.to_dense())\n\n# Print the eigenvalues and eigenvectors\nprint(\"Eigenvalues:\", eigenvalues)\nprint(\"Eigenvectors:\", eigenvectors)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n What is a Linear Operator?\n\n\nA linear operator is a generalization of a matrix.\nIt is a linear function that is defined in by its application to a vector.\nThe most common linear operators are (potentially structured) matrices,\nwhere the function applying them to a vector are (potentially efficient)\nmatrix-vector multiplication routines.\n\n\nIn code, a `LinearOperator` is a class that\n\n\n1. specifies the tensor(s) needed to define the LinearOperator,\n2. specifies a `\\_matmul` function (how the LinearOperator is applied to a vector),\n3. specifies a `\\_size` function (how big is the LinearOperator if it is represented as a matrix, or batch of matrices), and\n4. specifies a `\\_transpose\\_nonbatch` function (the adjoint of the LinearOperator).\n5. (optionally) defines other functions (e.g. `logdet`, `eigh`, etc.) to accelerate computations for which efficient sturcture-exploiting routines exist.\n\n\nFor example:\n\n```\nclass DiagLinearOperator(linear\\_operator.LinearOperator):\n r\"\"\"\n A LinearOperator representing a diagonal matrix.\n \"\"\"\n    def \\_\\_init\\_\\_(self, diag):\n        # diag: the vector that defines the diagonal of the matrix\n        self.diag = diag\n\n    def \\_matmul(self, v):\n        return self.diag.unsqueeze(-1) \\* v\n\n    def \\_size(self):\n        return torch.Size([\\*self.diag.shape, self.diag.size(-1)])\n\n    def \\_transpose\\_nonbatch(self):\n        return self  # Diagonal matrices are symmetric\n\n    # this function is optional, but it will accelerate computation\n    def logdet(self):\n        return self.diag.log().sum(dim=-1)\n# ...\n\nD = DiagLinearOperator(torch.tensor([1., 2., 3.])\n\n# Represents the matrix\n\n# [[1., 0., 0.],\n\n# [0., 2., 0.],\n\n# [0., 0., 3.]]\ntorch.matmul(D, torch.tensor([4., 5., 6.])\n# Returns [4., 10., 18.]\n\n\nWhile `\\_matmul`, `\\_size`, and `\\_transpose\\_nonbatch` might seem like a limited set of functions,\nit turns out that most functions on the `torch` and `torch.linalg` namespaces can be efficiently implemented\nusing only these three primitative functions.\n\n\nMoreover, because `\\_matmul` is\n\n==================\n Document 1 \n----------------\n\n\n linear\\_operator\n \n\n latest\n \n\n\nGetting Started\n\n\n* LinearOperator\n* Installation\n\n\nBasic Concepts\n\n\n* What is a Linear Operator?\n* Using LinearOperator Objects\n* Converting Between LinearOperators and torch.Tensor\n* Writing Your Own LinearOpeators\n\n\nLinear Operator Objects\n\n\n* LinearOperator\n* Data-Sparse LinearOperators\n* Composition/Decoration LinearOperators\n* Structured LinearOperators\n\n\nAdvanced Package Reference\n\n\n* Settings\n* Functions\n* Utilities\n\nlinear\\_operator\n\n\n* \n* LinearOperator\n* Edit on GitHub\n\n\n---\n\n\n# LinearOperator\n\n\nLinearOperator is a PyTorch package for abstracting away the linear algebra routines needed for structured matrices (or operators).\n\n\n**This package is in beta.**\nCurrently, most of the functionality only supports positive semi-definite and triangular matrices.\nPackage development TODOs:\n\n\n* Support PSD operators\n* Support triangular operators\n* Interface to specify structure (i.e. symmetric, triangular, PSD, etc.)\n* Add algebraic routines for symmetric operators\n* Add algebraic routines for generic square operators\n* Add algebraic routines for generic rectangular operators\n* Add sparse operators\n\n\n## Why LinearOperator\n\n\n### Why LinearOperator\n\n\nBefore describing what linear operators are and why they make a useful abstraction, it’s easiest to see an example.\nLet’s say you wanted to compute a matrix solve:\n\n\\[\\boldsymbol A^{-1} \\boldsymbol b.\\]\nIf you didn’t know anything about the matrix \\(\\boldsymbol A\\), the simplest (and best) way to accomplish this in code is:\n\n```\n\n# A = torch.randn(1000, 1000)\n# b = torch.randn(1000)\ntorch.linalg.solve(A, b)  # computes A^{-1} b\n\n```\n\n\nWhile this is easy, the `solve` routine is \\(\\mathcal O(N^3)\\), which gets very slow as \\(N\\) grows large.\n\n\nHowever, let’s imagine that we knew that \\(\\boldsymbol A\\) was equal to a low\n\n==================\n Document 2 \n----------------\n Using LinearOperator Objects\n\n\nLinearOperator objects share (mostly) the same API as `torch.Tensor` objects.\nUnder the hood, these objects use `\\_\\_torch\\_function\\_\\_` to dispatch all efficient linear algebra operations\nto the `torch` and `torch.linalg` namespaces.\nThis includes\n\n\n* `torch.add`\n* `torch.cat`\n* `torch.clone`\n* `torch.diagonal`\n* `torch.dim`\n* `torch.div`\n* `torch.expand`\n* `torch.logdet`\n* `torch.matmul`\n* `torch.numel`\n* `torch.permute`\n* `torch.prod`\n* `torch.squeeze`\n* `torch.sub`\n* `torch.sum`\n* `torch.transpose`\n* `torch.unsqueeze`\n* `torch.linalg.cholesky`\n* `torch.linalg.eigh`\n* `torch.linalg.eigvalsh`\n* `torch.linalg.solve`\n* `torch.linalg.svd`\n\n\nEach of these functions will either return a `torch.Tensor`, or a new `LinearOperator` object,\ndepending on the function.\nFor example:\n\n```\n# A = RootLinearOperator(...)\n\n# B = ToeplitzLinearOperator(...)\n\n# d = vec\n\nC = torch.matmul(A, B)  # A new LienearOperator representing the product of A and B\ntorch.linalg.solve(C, d)  # A torch.Tensor\n\n\nFor more examples, see the examples folder.\n\n\n## Batch Support and Broadcasting\n\n\n`LinearOperator` objects operate naturally in batch mode.\nFor example, to represent a batch of 3 `100 x 100` diagonal matrices:\n\n```\n\n# d = torch.randn(3, 100)\nD = DiagLinearOperator(d)  # Reprents an operator of size 3 x 100 x 100\n\n\nThese objects fully support broadcasted operations:\n\n```\nD @ torch.randn(100, 2)  # Returns a tensor of size 3 x 100 x 2\n\nD2 = DiagLinearOperator(torch.randn([2, 1, 100]))  # Represents an operator of size 2 x 1 x 100 x 100\nD2 + D  # Represents an operator of size 2 x 3 x 100 x 100\n\n\n## Indexing\n\n\n`LinearOperator` objects can be indexed in ways similar to torch Tensors. This includes:\n\n\n* Integer indexing (get a row, column, or batch)\n* Slice indexing (get a subset of rows, columns, or batches)\n* LongTensor indexing (get a set of individual entries\n\n==================\n Document 3 \n----------------\n# Indexing\n\n\n`LinearOperator` objects can be indexed in ways similar to torch Tensors. This includes:\n\n\n* Integer indexing (get a row, column, or batch)\n* Slice indexing (get a subset of rows, columns, or batches)\n* LongTensor indexing (get a set of individual entries by index)\n* Ellipses (support indexing operations with arbitrary batch dimensions)\n\n```\nD = DiagLinearOperator(torch.randn(2, 3, 100))  # Represents an operator of size 2 x 3 x 100 x 100\nD[-1]  # Returns a 3 x 100 x 100 operator\nD[..., :10, -5:]  # Returns a 2 x 3 x 10 x 5 operator\nD[..., torch.LongTensor([0, 1, 2, 3]), torch.LongTensor([0, 1, 2, 3])]  # Returns a 2 x 3 x 4 tensor\n\n## Composition and Decoration\n\n\nLinearOperators can be composed with one another in various ways.\nThis includes\n\n\n* Addition (`LinearOpA + LinearOpB`)\n* Matrix multiplication (`LinearOpA @ LinearOpB`)\n* Concatenation (`torch.cat([LinearOpA, LinearOpB], dim=-2)`)\n* Kronecker product (`torch.kron(LinearOpA, LinearOpB)`)\n\n\nIn addition, there are many ways to “decorate” LinearOperator objects.\nThis"
    },
    {
        "instruction": "Create a Python program that utilizes the 'linear-operator' API to compute the matrix sine of a given matrix. The program should print the computed matrix sine.",
        "api": "linear-operator",
        "output": "#!pip install linear-operator\nimport torch\nfrom linear_operator import to_linear_operator\nfrom scipy.linalg import sinm\n\n# Define a matrix M\nM = torch.Tensor([[1, 2], [3, 4]])\n\n# Compute the matrix sine\nM_op = to_linear_operator(M)\nmatrix_sine = sinm(M_op.to_dense())\n\n# Print the matrix sine\nprint(\"Matrix Sine:\", matrix_sine)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n What is a Linear Operator?\n\n\nA linear operator is a generalization of a matrix.\nIt is a linear function that is defined in by its application to a vector.\nThe most common linear operators are (potentially structured) matrices,\nwhere the function applying them to a vector are (potentially efficient)\nmatrix-vector multiplication routines.\n\n\nIn code, a `LinearOperator` is a class that\n\n\n1. specifies the tensor(s) needed to define the LinearOperator,\n2. specifies a `\\_matmul` function (how the LinearOperator is applied to a vector),\n3. specifies a `\\_size` function (how big is the LinearOperator if it is represented as a matrix, or batch of matrices), and\n4. specifies a `\\_transpose\\_nonbatch` function (the adjoint of the LinearOperator).\n5. (optionally) defines other functions (e.g. `logdet`, `eigh`, etc.) to accelerate computations for which efficient sturcture-exploiting routines exist.\n\n\nFor example:\n\n```\nclass DiagLinearOperator(linear\\_operator.LinearOperator):\n r\"\"\"\n A LinearOperator representing a diagonal matrix.\n \"\"\"\n    def \\_\\_init\\_\\_(self, diag):\n        # diag: the vector that defines the diagonal of the matrix\n        self.diag = diag\n\n    def \\_matmul(self, v):\n        return self.diag.unsqueeze(-1) \\* v\n\n    def \\_size(self):\n        return torch.Size([\\*self.diag.shape, self.diag.size(-1)])\n\n    def \\_transpose\\_nonbatch(self):\n        return self  # Diagonal matrices are symmetric\n\n    # this function is optional, but it will accelerate computation\n    def logdet(self):\n        return self.diag.log().sum(dim=-1)\n# ...\n\nD = DiagLinearOperator(torch.tensor([1., 2., 3.])\n\n# Represents the matrix\n\n# [[1., 0., 0.],\n\n# [0., 2., 0.],\n\n# [0., 0., 3.]]\ntorch.matmul(D, torch.tensor([4., 5., 6.])\n# Returns [4., 10., 18.]\n\n\nWhile `\\_matmul`, `\\_size`, and `\\_transpose\\_nonbatch` might seem like a limited set of functions,\nit turns out that most functions on the `torch` and `torch.linalg` namespaces can be efficiently implemented\nusing only these three primitative functions.\n\n\nMoreover, because `\\_matmul` is\n\n==================\n Document 1 \n----------------\n\n\n linear\\_operator\n \n\n latest\n \n\n\nGetting Started\n\n\n* LinearOperator\n* Installation\n\n\nBasic Concepts\n\n\n* What is a Linear Operator?\n* Using LinearOperator Objects\n* Converting Between LinearOperators and torch.Tensor\n* Writing Your Own LinearOpeators\n\n\nLinear Operator Objects\n\n\n* LinearOperator\n* Data-Sparse LinearOperators\n* Composition/Decoration LinearOperators\n* Structured LinearOperators\n\n\nAdvanced Package Reference\n\n\n* Settings\n* Functions\n* Utilities\n\nlinear\\_operator\n\n\n* \n* LinearOperator\n* Edit on GitHub\n\n\n---\n\n\n# LinearOperator\n\n\nLinearOperator is a PyTorch package for abstracting away the linear algebra routines needed for structured matrices (or operators).\n\n\n**This package is in beta.**\nCurrently, most of the functionality only supports positive semi-definite and triangular matrices.\nPackage development TODOs:\n\n\n* Support PSD operators\n* Support triangular operators\n* Interface to specify structure (i.e. symmetric, triangular, PSD, etc.)\n* Add algebraic routines for symmetric operators\n* Add algebraic routines for generic square operators\n* Add algebraic routines for generic rectangular operators\n* Add sparse operators\n\n\n## Why LinearOperator\n\n\n### Why LinearOperator\n\n\nBefore describing what linear operators are and why they make a useful abstraction, it’s easiest to see an example.\nLet’s say you wanted to compute a matrix solve:\n\n\\[\\boldsymbol A^{-1} \\boldsymbol b.\\]\nIf you didn’t know anything about the matrix \\(\\boldsymbol A\\), the simplest (and best) way to accomplish this in code is:\n\n```\n\n# A = torch.randn(1000, 1000)\n# b = torch.randn(1000)\ntorch.linalg.solve(A, b)  # computes A^{-1} b\n\n```\n\n\nWhile this is easy, the `solve` routine is \\(\\mathcal O(N^3)\\), which gets very slow as \\(N\\) grows large.\n\n\nHowever, let’s imagine that we knew that \\(\\boldsymbol A\\) was equal to a low\n\n==================\n Document 2 \n----------------\n Returns [4., 10., 18.]\n\n\nWhile `\\_matmul`, `\\_size`, and `\\_transpose\\_nonbatch` might seem like a limited set of functions,\nit turns out that most functions on the `torch` and `torch.linalg` namespaces can be efficiently implemented\nusing only these three primitative functions.\n\n\nMoreover, because `\\_matmul` is a linear function, it is very easy to compose linear operators in various ways.\nFor example: adding two linear operators (`SumLinearOperator`) just requires adding the output of their `\\_matmul` functions.\nThis makes it possible to define very complex compositional structures that still yield efficient linear algebraic routines.\n\n\nFinally, `LinearOperator` objects can be composed with one another, yielding new `LinearOperator` objects and automatically keeping track of algebraic structure after each computation.\nAs a result, users never need to reason about what efficient linear algebra routines to use (so long as the input elements defined by the user encode known input structure).\n\n\n Previous\nNext \n\n\n* What is a Linear Operator?\n* Using LinearOperator Objects\n\t+ Batch Support and Broadcasting\n\t+ Indexing\n\t+ Composition and Decoration\n* Converting Between LinearOperators and torch.Tensor\n* Writing Your Own LinearOpeators\n\n\n* \n* Using LinearOperator Objects\n* Edit on GitHub\n\n# Using LinearOperator Objects\n\n\nLinearOperator objects share (mostly) the same API as `torch.Tensor` objects.\nUnder the hood, these objects use `\\_\\_torch\\_function\\_\\_` to dispatch all efficient linear algebra operations\nto the `torch` and `torch.linalg` namespaces.\nThis includes\n\n\n* `torch.add`\n* `torch.cat`\n* `torch.clone`\n* `torch.diagonal`\n* `torch.dim`\n* `torch.div`\n* `torch.expand`\n* `torch.logdet`\n*\n\n==================\n Document 3 \n----------------\n# Indexing\n\n\n`LinearOperator` objects can be indexed in ways similar to torch Tensors. This includes:\n\n\n* Integer indexing (get a row, column, or batch)\n* Slice indexing (get a subset of rows, columns, or batches)\n* LongTensor indexing (get a set of individual entries by index)\n* Ellipses (support indexing operations with arbitrary batch dimensions)\n\n```\nD = DiagLinearOperator(torch.randn(2, 3, 100))  # Represents an operator of size 2 x 3 x 100 x 100\nD[-1]  # Returns a 3 x 100 x 100 operator\nD[..., :10, -5:]  # Returns a 2 x 3 x 10 x 5 operator\nD[..., torch.LongTensor([0, 1, 2, 3]), torch.LongTensor([0, 1, 2, 3])]  # Returns a 2 x 3 x 4 tensor\n\n## Composition and Decoration\n\n\nLinearOperators can be composed with one another in various ways.\nThis includes\n\n\n* Addition (`LinearOpA + LinearOpB`)\n* Matrix multiplication (`LinearOpA @ LinearOpB`)\n* Concatenation (`torch.cat([LinearOpA, LinearOpB], dim=-2)`)\n* Kronecker product (`torch.kron(LinearOpA, LinearOpB)`)\n\n\nIn addition, there are many ways to “decorate” LinearOperator objects.\nThis"
    },
    {
        "instruction": "Create a Python program that utilizes the 'linear-operator' API to compute the matrix square root of a given matrix. The program should print the computed matrix square root.",
        "api": "linear-operator",
        "output": "#!pip install linear-operator\nimport torch\nfrom linear_operator import to_linear_operator\nfrom scipy.linalg import sqrtm\n\n# Define a matrix M\nM = torch.Tensor([[1, 2], [3, 4]])\n\n# Compute the matrix square root\nM_op = to_linear_operator(M)\nmatrix_sqrt = sqrtm(M_op.to_dense())\n\n# Print the matrix square root\nprint(\"Matrix Square Root:\", matrix_sqrt)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n linear\\_operator\n \n\n latest\n \n\n\nGetting Started\n\n\n* LinearOperator\n* Installation\n\n\nBasic Concepts\n\n\n* What is a Linear Operator?\n* Using LinearOperator Objects\n* Converting Between LinearOperators and torch.Tensor\n* Writing Your Own LinearOpeators\n\n\nLinear Operator Objects\n\n\n* LinearOperator\n* Data-Sparse LinearOperators\n* Composition/Decoration LinearOperators\n* Structured LinearOperators\n\n\nAdvanced Package Reference\n\n\n* Settings\n* Functions\n* Utilities\n\nlinear\\_operator\n\n\n* \n* LinearOperator\n* Edit on GitHub\n\n\n---\n\n\n# LinearOperator\n\n\nLinearOperator is a PyTorch package for abstracting away the linear algebra routines needed for structured matrices (or operators).\n\n\n**This package is in beta.**\nCurrently, most of the functionality only supports positive semi-definite and triangular matrices.\nPackage development TODOs:\n\n\n* Support PSD operators\n* Support triangular operators\n* Interface to specify structure (i.e. symmetric, triangular, PSD, etc.)\n* Add algebraic routines for symmetric operators\n* Add algebraic routines for generic square operators\n* Add algebraic routines for generic rectangular operators\n* Add sparse operators\n\n\n## Why LinearOperator\n\n\n### Why LinearOperator\n\n\nBefore describing what linear operators are and why they make a useful abstraction, it’s easiest to see an example.\nLet’s say you wanted to compute a matrix solve:\n\n\\[\\boldsymbol A^{-1} \\boldsymbol b.\\]\nIf you didn’t know anything about the matrix \\(\\boldsymbol A\\), the simplest (and best) way to accomplish this in code is:\n\n```\n\n# A = torch.randn(1000, 1000)\n# b = torch.randn(1000)\ntorch.linalg.solve(A, b)  # computes A^{-1} b\n\n```\n\n\nWhile this is easy, the `solve` routine is \\(\\mathcal O(N^3)\\), which gets very slow as \\(N\\) grows large.\n\n\nHowever, let’s imagine that we knew that \\(\\boldsymbol A\\) was equal to a low\n\n==================\n Document 1 \n----------------\n What is a Linear Operator?\n\n\nA linear operator is a generalization of a matrix.\nIt is a linear function that is defined in by its application to a vector.\nThe most common linear operators are (potentially structured) matrices,\nwhere the function applying them to a vector are (potentially efficient)\nmatrix-vector multiplication routines.\n\n\nIn code, a `LinearOperator` is a class that\n\n\n1. specifies the tensor(s) needed to define the LinearOperator,\n2. specifies a `\\_matmul` function (how the LinearOperator is applied to a vector),\n3. specifies a `\\_size` function (how big is the LinearOperator if it is represented as a matrix, or batch of matrices), and\n4. specifies a `\\_transpose\\_nonbatch` function (the adjoint of the LinearOperator).\n5. (optionally) defines other functions (e.g. `logdet`, `eigh`, etc.) to accelerate computations for which efficient sturcture-exploiting routines exist.\n\n\nFor example:\n\n```\nclass DiagLinearOperator(linear\\_operator.LinearOperator):\n r\"\"\"\n A LinearOperator representing a diagonal matrix.\n \"\"\"\n    def \\_\\_init\\_\\_(self, diag):\n        # diag: the vector that defines the diagonal of the matrix\n        self.diag = diag\n\n    def \\_matmul(self, v):\n        return self.diag.unsqueeze(-1) \\* v\n\n    def \\_size(self):\n        return torch.Size([\\*self.diag.shape, self.diag.size(-1)])\n\n    def \\_transpose\\_nonbatch(self):\n        return self  # Diagonal matrices are symmetric\n\n    # this function is optional, but it will accelerate computation\n    def logdet(self):\n        return self.diag.log().sum(dim=-1)\n# ...\n\nD = DiagLinearOperator(torch.tensor([1., 2., 3.])\n\n# Represents the matrix\n\n# [[1., 0., 0.],\n\n# [0., 2., 0.],\n\n# [0., 0., 3.]]\ntorch.matmul(D, torch.tensor([4., 5., 6.])\n# Returns [4., 10., 18.]\n\n\nWhile `\\_matmul`, `\\_size`, and `\\_transpose\\_nonbatch` might seem like a limited set of functions,\nit turns out that most functions on the `torch` and `torch.linalg` namespaces can be efficiently implemented\nusing only these three primitative functions.\n\n\nMoreover, because `\\_matmul` is\n\n==================\n Document 2 \n----------------\n Using LinearOperator Objects\n\n\nLinearOperator objects share (mostly) the same API as `torch.Tensor` objects.\nUnder the hood, these objects use `\\_\\_torch\\_function\\_\\_` to dispatch all efficient linear algebra operations\nto the `torch` and `torch.linalg` namespaces.\nThis includes\n\n\n* `torch.add`\n* `torch.cat`\n* `torch.clone`\n* `torch.diagonal`\n* `torch.dim`\n* `torch.div`\n* `torch.expand`\n* `torch.logdet`\n* `torch.matmul`\n* `torch.numel`\n* `torch.permute`\n* `torch.prod`\n* `torch.squeeze`\n* `torch.sub`\n* `torch.sum`\n* `torch.transpose`\n* `torch.unsqueeze`\n* `torch.linalg.cholesky`\n* `torch.linalg.eigh`\n* `torch.linalg.eigvalsh`\n* `torch.linalg.solve`\n* `torch.linalg.svd`\n\n\nEach of these functions will either return a `torch.Tensor`, or a new `LinearOperator` object,\ndepending on the function.\nFor example:\n\n```\n# A = RootLinearOperator(...)\n\n# B = ToeplitzLinearOperator(...)\n\n# d = vec\n\nC = torch.matmul(A, B)  # A new LienearOperator representing the product of A and B\ntorch.linalg.solve(C, d)  # A torch.Tensor\n\n\nFor more examples, see the examples folder.\n\n\n## Batch Support and Broadcasting\n\n\n`LinearOperator` objects operate naturally in batch mode.\nFor example, to represent a batch of 3 `100 x 100` diagonal matrices:\n\n```\n\n# d = torch.randn(3, 100)\nD = DiagLinearOperator(d)  # Reprents an operator of size 3 x 100 x 100\n\n\nThese objects fully support broadcasted operations:\n\n```\nD @ torch.randn(100, 2)  # Returns a tensor of size 3 x 100 x 2\n\nD2 = DiagLinearOperator(torch.randn([2, 1, 100]))  # Represents an operator of size 2 x 1 x 100 x 100\nD2 + D  # Represents an operator of size 2 x 3 x 100 x 100\n\n\n## Indexing\n\n\n`LinearOperator` objects can be indexed in ways similar to torch Tensors. This includes:\n\n\n* Integer indexing (get a row, column, or batch)\n* Slice indexing (get a subset of rows, columns, or batches)\n* LongTensor indexing (get a set of individual entries\n\n==================\n Document 3 \n----------------\n b = torch.randn(1000)\nA = LowRankRootLinearOperator(C) + DiagLinearOperator(d)  # represents C C^T + diag(d)\n\n\nit provides an interface that lets us treat \\(\\boldsymbol A\\) as if it were a generic tensor,\nusing the standard PyTorch API:\n\n```\ntorch.linalg.solve(A, b)  # computes A^{-1} b efficiently!\n\n\nUnder-the-hood, the `LinearOperator` object keeps track of the algebraic structure of \\(\\boldsymbol A\\) (low rank plus diagonal)\nand determines the most efficient routine to use (the Woodbury formula).\nThis way, we can get a efficient \\(\\mathcal O(N)\\) solve while abstracting away all of the details.\n\n\nCrucially, \\(\\boldsymbol A\\) is never explicitly instantiated as a matrix, which makes it possible to scale\nto very large operators without running out of memory:\n\n```\n# C = torch.randn(10000000, 20)\n\n# d = torch.randn(10000000)\n\n# b = torch.randn(10000000)\nA = LowRankRootLinearOperator(C) + DiagLinearOperator(d)  # represents a 10M x 10M matrix!\ntorch.linalg.solve(A, b)  # computes A^{-1} b efficiently!\n\n\n\n## Use Cases\n\n\nThere are several use cases for the LinearOperator package.\nHere we highlight two general themes:\n\n\n### Modular Code for Structured Matrices\n\n\nFor example, let’s say that you have a generative model that involves\nsampling from a high-dimensional multivariate Gaussian.\nThis sampling operation will require storing and manipulating a large covariance matrix,\nso to speed things up you might want to experiment with different structured\napproximations of that covariance matrix.\nThis is easy with the LinearOperator package.\n\n```\nfrom gpytorch.distributions import MultivariateNormal\n\n\n# variance = torch.randn(10000)\ncov = DiagLinearOperator(variance)\n\n# or\n\n# cov = LowRankRootLinearOperator(...) + DiagLinearOperator(...)\n\n# or\n\n# cov = KroneckerProductLinearOperator(...)\n\n# or\n\n# cov = ToeplitzLinearOperator(...)\n\n# or\n\n# ...\n\nmvn = MultivariateNormal(torch.zeros(cov.size(-1), cov) # 10000-dimensional MVN\nmvn.rsample()  # returns a 10000-dimensional vector\n\n\n\n### Efficient Routines for Complex Operators\n\n\nMany of the efficient linear algebra routines in LinearOperator are iterative algorithms\nbased on matrix-vector multiplication.\nSince matrix-vector multiplication obeys many nice compositional properties\nit is possible to obtain efficient routines for extremely complex compositional LienarOperators:\n\n```\nfrom linear\\_operator.operators import KroneckerProductLinearOperator, RootLinearOperator, ToeplitzLinearOperator\n\n\n# mat1 = 200 x 200 PSD matrix\n\n# mat2 = 100 x 100 PSD matrix\n\n# vec3 = 20000 vector\n\nA = KroneckerProductLinearOperator(mat1, mat2) + RootLinearOperator(ToeplitzLinearOperator(vec3))\n\n# represents a 20000 x 20000 matrix\n\ntorch.linalg.solve(A, torch.randn(20000))  # Sub O(N^3) routine!\n\n\n\n# Indices and Tables\n\n\n* Index\n* Module Index\n* Search Page\n\n\nNext \n\n---\n\n© Copyright 2022, Cornellius GP.\n Revision `54962429`.\n \n\n\n Built with Sphinx using a\n theme\n provided by Read the Docs.\n \n\n\n Read the Docs\n v: latest\n \n\nVersions\nlatest\nstable\n\n\nDownloads\n\n\nOn Read the Docs\n\nProject Home\n\n\nBuilds\n\n\n linear\\_operator\n \n\n\n* LinearOperator\n* Installation\n\t+ Standard Installation (Most Recent Stable Version)\n\t+ Installing from the `main` Branch (Latest Unsable Version)\n\n\n* \n* Installation\n* Edit on GitHub\n\n\n\n# Installation\n\n\nLinearOperator requires Python >= 3.8.\n\n\n## Standard Installation (Most Recent Stable Version)\n\n\nWe recommend installing via `pip` or Anaconda:\n\n```\npip install linear_operator\n\n# or\nconda install linear_operator -c gpytorch\n\n\nThe installation requires the following packages:\n\n\n* PyTorch >= 1.11\n* Scipy\n\n\nYou can customize your PyTorch installation (i.e. CUDA version, CPU only option)\nby following the PyTorch installation instructions.\n\n\n\n## Installing from the `main` Branch (Latest Unsable Version)\n\n\nTo install what is currently on the `main` branch (potentially buggy and unstable):\n\n```\npip install --upgrade git+https://github.com/cornellius-gp/linear_operator.git\n\n Previous\nNext \n\n\n* \n* What is a Linear Operator?\n* Edit on GitHub\n\n\n# What is a Linear Operator?\n\n\nA linear operator is a generalization of a matrix.\nIt is a linear function that is defined in by its application to a vector.\nThe most common linear operators are (potentially structured) matrices,\nwhere the function applying them\n\n==================\n Document 4 \n----------------\n Returns [4., 10., 18.]\n\n\nWhile `\\_matmul`, `\\_size`, and `\\_transpose\\_nonbatch` might seem like a limited set of functions,\nit turns out that most functions on the `torch` and `torch.linalg` namespaces can be efficiently implemented\nusing only these three primitative functions.\n\n\nMoreover, because `\\_matmul` is a linear function, it is very easy to compose linear operators in various ways.\nFor example: adding two linear operators (`SumLinearOperator`) just requires adding the output of their `\\_matmul` functions.\nThis makes it possible to define very complex compositional structures that still yield efficient linear algebraic routines.\n\n\nFinally, `LinearOperator` objects can be composed with one another, yielding new `LinearOperator` objects and automatically keeping track of algebraic structure after each computation.\nAs a result, users never need to reason about what efficient linear algebra routines to use (so long as the input elements defined by the user encode known input structure).\n\n\n Previous\nNext \n\n\n* What is a Linear Operator?\n* Using LinearOperator Objects\n\t+ Batch Support and Broadcasting\n\t+ Indexing\n\t+ Composition and Decoration\n* Converting Between LinearOperators and torch.Tensor\n* Writing Your Own LinearOpeators\n\n\n* \n* Using LinearOperator Objects\n* Edit on GitHub\n\n# Using LinearOperator Objects\n\n\nLinearOperator objects share (mostly) the same API as `torch.Tensor` objects.\nUnder the hood, these objects use `\\_\\_torch\\_function\\_\\_` to dispatch all efficient linear algebra operations\nto the `torch` and `torch.linalg` namespaces.\nThis includes\n\n\n* `torch.add`\n* `torch.cat`\n* `torch.clone`\n* `torch.diagonal`\n* `torch.dim`\n* `torch.div`\n* `torch.expand`\n* `torch.logdet`\n*"
    },
    {
        "instruction": "Develop a Python program using the 'llama-index' API to scrape and index data from 'https://en.wikipedia.org/wiki/Robotics.' Extract information about the history and types of robotics. Print the extracted information.",
        "api": "llama-index",
        "output": "#!pip install llama-cpp-python\n#!pip install llama-index html2text trafilatura\n\nfrom llama_index import GPTListIndex\nfrom llama_index import TrafilaturaWebReader\n\ndef scrape_and_index_robotics_info(url, *topics):\n    documents = TrafilaturaWebReader().load_data([url])\n    index = GPTListIndex.from_documents(documents)\n    engine = index.as_query_engine()\n\n    results = {}\n    for topic in topics:\n        results[topic] = engine.query(topic)\n\n    return results\n\nif __name__ == \"__main__\":\n    url = \"https://en.wikipedia.org/wiki/Robotics\"\n    topics = [\"History of robotics\", \"Types of robotics\"]\n    extracted_data = scrape_and_index_robotics_info(url, *topics)\n    \n    for topic, data in extracted_data.items():\n        print(f\"{topic}:\\n{data}\")",
        "documentation": "\n\n==================\n Document 0 \n----------------\nPlayground\n\n\nThe Playground module in LlamaIndex is a way to automatically test your data (i.e. documents) across a diverse combination of indices, models, embeddings, modes, etc. to decide which ones are best for your purposes. More options will continue to be added.\n\n\nFor each combination, you’ll be able to compare the results for any query and compare the answers, latency, tokens used, and so on.\n\n\nYou may initialize a Playground with a list of pre-built indices, or initialize one from a list of Documents using the preset indices.\n\n\nA sample usage is given below.\n\n```\nfrom llama\\_index import download\\_loader\nfrom llama\\_index.indices.vector\\_store import VectorStoreIndex\nfrom llama\\_index.indices.tree.base import TreeIndex\nfrom llama\\_index.playground import Playground\n\n# load data \nWikipediaReader = download\\_loader(\"WikipediaReader\")\nloader = WikipediaReader()\ndocuments = loader.load\\_data(pages=['Berlin'])\n\n\n# define multiple index data structures (vector index, summary index)\nindices = [VectorStoreIndex(documents), TreeIndex(documents)]\n\n\n# initialize playground\nplayground = Playground(indices=indices)\n\n\n# playground compare\nplayground.compare(\"What is the population of Berlin?\")\n\n* Playground\n\n\n# Token Counting - Migration Guide\n\n\n\n# Cost Analysis\n\n\n# Playground\n\n==================\n Document 1 \n----------------\n Query and print response\nquery\\_engine = index.as\\_query\\_engine()\nresponse = query\\_engine.query(\"<query\\_text>\")\nprint(response)\n\n\nUsing this method, you can use any LLM. Maybe you have one running locally, or running on your own server. As long as the class is implemented and the generated tokens are returned, it should work out. Note that we need to use the prompt helper to customize the prompt sizes, since every model has a slightly different context length.\n\n\nThe decorator is optional, but provides observability via callbacks on the LLM calls.\n\n\nNote that you may have to adjust the internal prompts to get good performance. Even then, you should be using a sufficiently large LLM to ensure it’s capable of handling the complex queries that LlamaIndex uses internally, so your mileage may vary.\n\n\nA list of all default internal prompts is available here, and chat-specific prompts are listed here. You can also implement your own custom prompts, as described here.\n\n\nWe support integrations with OpenAI, Anthropic, Hugging Face, PaLM, and more.\n\n## OpenAI\n\n* OpenAI\n* Azure OpenAI\n\n\n## Anthropic\n\n* Anthropic\n\n\n## Hugging Face\n\n* HuggingFace LLM - Camel-5b\n* HuggingFace LLM - StableLM\n* Local Llama2 + VectorStoreIndex\n\n\n## LiteLLM\n\n* LiteLLM\n\n\n## PaLM\n\n* PaLM\n\n\n## Predibase\n\n* Predibase\n\n\n## Replicate\n\n* Replicate - Llama 2 13B\n* Replicate - Vicuna 13B\n* Llama2 + VectorStoreIndex\n\n\n## LangChain\n\n* LangChain LLM\n\n\n## Llama API\n\n* Llama API\n\n\n## Llama CPP\n\n* LlamaCPP\n\n\n## Xorbits Inference\n\n* Xorbits Inference\n\n\n## MonsterAPI\n\n* Set Monster API Key env variable\n* Basic Usage Pattern\n\n\n## RunGPT\n\n* RunGPT\n* Setup\n\n\n## Portkey\n\n* Portkey\n\n\n## AnyScale\n\n* Anyscale\n\n\n## Ollama\n\n* Ollama - Llama 2 7B\n\n\n# Customizing LLMs within LlamaIndex Abstractions\n\n\n# LLM\n\n\n# Embeddings\n\n\nEmbeddings are used in LlamaIndex to represent your documents using a sophisticated numerical representation. Embedding models take text as input, and return a long list of numbers used to capture the semantics of the text. These embedding models have\n\n==================\n Document 2 \n----------------\n LLM\n\n\nPicking the proper Large Language Model (LLM) is one of the first steps you need to consider when building any LLM application over your data.\n\n\nLLMs are a core component of LlamaIndex. They can be used as standalone modules or plugged into other core LlamaIndex modules (indices, retrievers, query engines). They are always used during the response synthesis step (e.g. after retrieval). Depending on the type of index being used, LLMs may also be used during index construction, insertion, and query traversal.\n\n\nLlamaIndex provides a unified interface for defining LLM modules, whether it’s from OpenAI, Hugging Face, or LangChain, so that you\ndon’t have to write the boilerplate code of defining the LLM interface yourself. This interface consists of the following (more details below):\n\n\n* Support for **text completion** and **chat** endpoints (details below)\n* Support for **streaming** and **non-streaming** endpoints\n* Support for **synchronous** and **asynchronous** endpoints\n\n\nThe following code snippet shows how you can get started using LLMs.\n\n```\nfrom llama\\_index.llms import OpenAI\n\n# non-streaming\nresp = OpenAI().complete('Paul Graham is ')\nprint(resp)\n\n* Using LLMs as standalone modules\n* Customizing LLMs within LlamaIndex Abstractions\n\n\nWe support integrations with OpenAI, Hugging Face, PaLM, and more.\n\n* Modules\n\t+ OpenAI\n\t+ Anthropic\n\t+ Hugging Face\n\t+ LiteLLM\n\t+ PaLM\n\t+ Predibase\n\t+ Replicate\n\t+ LangChain\n\t+ Llama API\n\t+ Llama CPP\n\t+ Xorbits Inference\n\t+ MonsterAPI\n\t+ RunGPT\n\t+ Portkey\n\t+ AnyScale\n\t+ Ollama\n\n\n# Using LLMs as standalone modules\n\n\nYou can use our LLM modules on their own.\n\n\n## Text Completion Example\n\n\n# using streaming endpoint\nfrom llama\\_index.llms import OpenAI\nllm = OpenAI()\nresp = llm.stream\\_complete('Paul Graham is ')\nfor delta in resp:\n    print(delta, end='')\n\n\n\n## Chat Example\n\n```\nfrom llama\\_index.llms import ChatMessage, OpenAI\n\nmessages = [\n    ChatMessage(role=\"system\", content=\"You are a pirate with a colorful personality\"),\n    ChatMessage(role=\"user\", content=\"What is your name\"),\n]\nresp = OpenAI().chat(messages)\nprint(resp)\n\n\nCheck out our modules section for usage guides for each LLM.\n\n\n\n# Customizing LLMs within LlamaIndex Abstractions\n\n\nYou can plugin these LLM abstractions within our other modules in LlamaIndex (indexes, retrievers, query engines, agents) which allow you to build advanced workflows over your data.\n\n\nBy default, we use OpenAI’s `gpt-3.5-turbo` model. But you may choose to customize\nthe underlying LLM being used.\n\n\nBelow we show a few examples of LLM customization. This includes\n\n\n* changing the underlying LLM\n* changing the number of output tokens (for OpenAI, Cohere, or AI21)\n* having more fine-grained control over all parameters for any LLM, from context window to chunk overlap\n\n\n## Example: Changing the underlying LLM\n\n\nAn example snippet of customizing the LLM being used is shown below.\nIn this example, we use `gpt-4` instead of `gpt-3.5-turbo`. Available models include `gpt-3.5-turbo`, `gpt-3.5-turbo-instruct`, `gpt-3.5-turbo-16k`, `gpt-4`, `gpt-4-32k`, `text-davinci-003`, and `text-davinci-002`.\n\n\nNote that\nyou may also plug in any LLM shown on Langchain’s\nLLM page.\n\nfrom llama\\_index import (\n    KeywordTableIndex,\n    SimpleDirectoryReader,\n    LLMPredictor,\n    ServiceContext\n)\nfrom llama\\_index.llms import OpenAI\n\n# alternatively\n\n# from langchain.llms import ...\n\ndocuments = SimpleDirectoryReader('data').load\\_data()\n\n\n# define LLM\nllm = OpenAI(temperature=0.1, model=\"gpt-4\")\nservice\\_context = ServiceContext.from\\_defaults(llm=llm)\n\n\n# build index\nindex = KeywordTableIndex.from\\_documents(documents, service\\_context=service\\_context)\n\n\n# get response from query\nquery\\_engine = index.as\\_query\\_engine()\nresponse = query\\_engine.query(\"What did the author do after his time at Y Combinator?\")\n\n\n\n## Example: Changing the number of output tokens (for OpenAI, Cohere, AI21)\n\n\nThe number of output tokens is usually set to some low number by default (for instance,\nwith OpenAI the default is 256).\n\n\nFor OpenAI, Cohere, AI21, you just need to set the `max\\_tokens` parameter\n(or maxTokens for AI21). We will handle text chunking/calculations under the hood.\n\nfrom llama\\_index import (\n    KeywordTableIndex,\n    SimpleDirectoryReader,\n    ServiceContext\n)\nfrom llama\\_index.llms import OpenAI\n\n\n# define LLM\nllm = OpenAI(temperature=0, model=\"text-davinci-002\", max\\_tokens=512)\nservice\\_context = ServiceContext.from\\_defaults(llm=llm)\n\n\n\n## Example: Explicitly configure `context\\_window` and `num\\_output`\n\n\nIf you are using other LLM classes from langchain, you may need to explicitly configure the `context\\_window` and `num\\_output` via the `ServiceContext` since the information is not available by default.\n\nfrom llama\\_index import (\n    KeywordTableIndex,\n    SimpleDirectoryReader,\n    ServiceContext\n)\nfrom llama\\_index.llms import OpenAI\n\n# alternatively\n\n# from langchain.llms import ...\n\n\n\n# set context window\ncontext\\_window = 4096\n\n# set number of output tokens\nnum\\_output = 256\n\n\n# define LLM\nllm = OpenAI(\n    temperature=0, \n    model=\"text-davinci-002\", \n    max\\_tokens=num\\_output,\n)\n\nservice\\_context = ServiceContext.from\\_defaults(\n    llm=llm,\n    context\\_window=context\\_window,\n    num\\_output=num\\_output,\n)\n\n\n## Example: Using a HuggingFace LLM\n\n\nLlamaIndex supports using LLMs from HuggingFace directly. Note that for a completely private experience, also setup a local embedding model as in this example.\n\n\nMany open-source models from HuggingFace require either some preamble before each prompt,\n\n==================\n Document 3 \n----------------\n Data Agents\n\n\nData Agents are LLM-powered knowledge workers in LlamaIndex that can intelligently perform various tasks over your data, in both a “read” and “write” function. They are capable of the following:\n\n\n* Perform automated search and retrieval over different types of data - unstructured, semi-structured, and structured.\n* Calling any external service API in a structured fashion, and processing the response + storing it for later.\n\n\nIn that sense, agents are a step beyond our query engines in that they can not only “read” from a static source of data, but can dynamically ingest and modify data from a variety of different tools.\n\n\nBuilding a data agent requires the following core components:\n\n\n* A reasoning loop\n* Tool abstractions\n\n\nA data agent is initialized with set of APIs, or Tools, to interact with; these APIs can be called by the agent to return information or modify state. Given an input task, the data agent uses a reasoning loop to decide which tools to use, in which sequence, and the parameters to call each tool.\n\n### Reasoning Loop\n\n\nThe reasoning loop depends on the type of agent. We have support for the following agents:\n\n\n* OpenAI Function agent (built on top of the OpenAI Function API)\n* a ReAct agent (which works across any chat/text completion endpoint).\n\n\n\n### Tool Abstractions\n\n\nYou can learn more about our Tool abstractions in our Tools section.\n\n\n\n### Blog Post\n\n\nFor full details, please check out our detailed blog post.\n\n\nData agents can be used in the following manner (the example uses the OpenAI Function API)\n\n```\nfrom llama\\_index.agent import OpenAIAgent\nfrom llama\\_index.llms import OpenAI\n\n\n# import and define tools\n...\n\n\n# initialize llm\nllm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n\n\n# initialize openai agent\nagent = OpenAIAgent.from\\_tools(tools, llm=llm, verbose=True)\n\n\nSee our usage pattern guide for more details.\n\n* Usage Pattern\n\n\nLearn more about our different agent types in our module guides below.\n\n\nAlso take a look at our tools section!\n\n* Module Guides\n\t+ OpenAI Agent\n\t+ ReAct Agent\n\n\nAn agent is initialized from a set of Tools. Here’s an example of instantiating a ReAct\nagent from a set of Tools.\n\n```\nfrom llama\\_index.tools import FunctionTool\nfrom llama\\_index.llms import OpenAI\nfrom llama\\_index.agent import ReActAgent\n\n\n# define sample Tool\ndef multiply(a: int, b: int) -> int:\n \"\"\"Multiple two integers and returns the result integer\"\"\"\n    return a \\* b\n\nmultiply\\_tool = FunctionTool.from\\_defaults(fn=multiply)\n\n\n# initialize ReAct agent\nagent = ReActAgent.from\\_tools([multiply\\_tool], llm=llm, verbose=True)\n\n\nAn agent supports both `chat` and `query` endpoints, inheriting from our `ChatEngine` and `QueryEngine` respectively.\n\n\nExample usage:\n\n```\nagent.chat(\"What is 2123 \\* 215123\")\n\n\n\n## Query Engine Tools\n\n\nIt is easy to wrap query engines as tools for an agent as well. Simply do the following:\n\nfrom llama\\_index.agent import ReActAgent\nfrom llama\\_index.tools import QueryEngineTool\n\n\n# NOTE: lyft\\_index and uber\\_index are both SimpleVectorIndex instances\nlyft\\_engine = lyft\\_index.as\\_query\\_engine(similarity\\_top\\_k=3)\nuber\\_engine = uber\\_index.as\\_query\\_engine(similarity\\_top\\_k=3)\n\nquery\\_engine\\_tools = [\n    QueryEngineTool(\n        query\\_engine=lyft\\_engine,\n        metadata=ToolMetadata(\n            name=\"lyft\\_10k\",\n            description=\"Provides information about Lyft financials for year 2021. \"\n            \"Use a detailed plain text question as input to the tool.\",\n        ),\n    ),\n    QueryEngineTool(\n        query\\_engine=uber\\_engine,\n        metadata=ToolMetadata(\n            name=\"uber\\_10k\",\n            description=\"Provides information about Uber financials for year 2021. \"\n            \"Use a detailed plain text question as input to the tool.\",\n        ),\n    ),\n]\n\n\n# initialize ReAct agent\nagent = ReActAgent.from\\_tools(query\\_engine\\_tools, llm=llm, verbose=True)\n\n\n\n## Use other agents as Tools\n\n\nA nifty feature of our agents is that since they inherit from `BaseQueryEngine`, you can easily define other agents as tools\nthrough our `QueryEngineTool`.\n\n```\nfrom llama\\_index.tools import QueryEngineTool\n\nquery\\_engine\\_tools = [\n    QueryEngineTool(\n        query\\_engine=sql\\_agent,\n        metadata=ToolMetadata(\n            name=\"sql\\_agent\",\n            description=\"Agent that can execute SQL queries.\"\n        ),\n    ),\n    QueryEngineTool(\n        query\\_engine=gmail\\_agent,\n        metadata=ToolMetadata(\n            name=\"gmail\\_agent\",\n            description=\"Tool that can send emails on Gmail.\"\n        ),\n    ),\n]\n\nouter\\_agent = ReActAgent.from\\_tools(query\\_engine\\_tools, llm=llm, verbose=True)\n\n\n\n## Advanced Concepts (for `OpenAIAgent`, in beta)\n\n\nYou can also use agents in more advanced settings. For instance, being able to retrieve tools from an index during query-time, and\nbeing able to perform query planning over an existing set of Tools.\n\n\nThese are largely implemented with our `OpenAIAgent` classes (which depend on the OpenAI Function API). Support\nfor our more general `ReActAgent` is something we’re actively investigating.\n\n\nNOTE: these are largely still in beta. The abstractions may change and become more general over time.\n\n\n### Function Retrieval Agents\n\n\nIf the set of Tools is very large, you can create an `ObjectIndex` to index the tools, and then pass in an `ObjectRetriever` to the agent during query-time, to first dynamically retrieve the relevant tools before having the agent pick from the candidate tools.\n\n\nWe first build an `ObjectIndex` over an existing set of Tools.\n\n```\n\n# define an \"object\" index over these tools\nfrom llama\\_index import VectorStoreIndex\nfrom llama\\_index.objects import ObjectIndex, SimpleToolNodeMapping\n\ntool\\_mapping = SimpleToolNodeMapping.from\\_objects(all\\_tools)\nobj\\_index = ObjectIndex.from\\_objects(\n    all\\_tools,\n    tool\\_mapping,\n    VectorStoreIndex,\n)\n\n\nWe then define our `FnRetrieverOpenAIAgent`:\n\n```\nfrom llama\\_index.agent import FnRetrieverOpenAIAgent\n\nagent = FnRetrieverOpenAIAgent.from\\_retriever(obj\\_index.as\\_retriever(), verbose=True)\n\n\n\n### Context Retrieval Agents\n\n\nOur context-augmented OpenAI Agent will always perform retrieval before calling any tools.\n\n\nThis helps to provide additional context that can help the agent better pick Tools, versus\njust trying to make a decision without any context.\n\n```\nfrom llama\\_index.schema import Document\nfrom llama\\_index.agent import ContextRetrieverOpenAIAgent\n\n\n\n# toy index - stores a list of abbreviations\ntexts = [\n    \"Abbrevation: X = Revenue\",\n    \"Abbrevation: YZ = Risk Factors\",\n    \"Abbreviation: Z = Costs\",\n]\ndocs = [Document(text=t) for t in texts]\ncontext\\_index = VectorStoreIndex.from\\_documents(docs)\n\n\n# add context agent\ncontext\\_agent = ContextRetrieverOpenAIAgent.from\\_tools\\_and\\_retriever(\n    query\\_engine\\_tools, context\\_index.as\\_retriever(similarity\\_top\\_k=1), verbose=True\n)\nresponse = context\\_agent.chat(\"What is the YZ of March 2022?\")\n\n\n\n### Query Planning\n\n\nOpenAI Function Agents can be capable of advanced query planning. The trick is to provide the agent\nwith a `QueryPlanTool` - if the agent calls the QueryPlanTool, it is forced to infer a full Pydantic schema representing a query\nplan over a set of subtools.\n\n```\n\n# define query plan tool\nfrom llama\\_index.tools import QueryPlanTool\nfrom llama\\_index import get\\_response\\_synthesizer\n\nresponse\\_synthesizer = get\\_response\\_synthesizer(service\\_context=service\\_context)\nquery\\_plan\\_tool = QueryPlanTool.from\\_defaults(\n    query\\_engine\\_tools=[query\\_tool\\_sept, query\\_tool\\_june, query\\_tool\\_march],\n    response\\_synthesizer=response\\_synthesizer,\n)\n\n\n# initialize agent\nagent = OpenAIAgent.from\\_tools(\n    [query\\_plan\\_tool],\n    max\\_function\\_calls=10,\n    llm=OpenAI(temperature=0, model=\"gpt-4-0613\"),\n    verbose=True,\n)\n\n\n# should output a query plan to call march, june, and september tools\nresponse = agent.query(\"Analyze Uber revenue growth in March, June, and September\")\n\n\nThese guide provide an overview of how to use our agent classes.\n\n\nFor more detailed guides on how to use specific tools, check out our tools module guides.\n\n\n## OpenAI Agent\n\n* Build your own OpenAI Agent\n* OpenAI Agent with Query Engine Tools\n* Retrieval-Augmented OpenAI Agent\n* OpenAI Agent + Query Engine Experimental Cookbook\n* OpenAI Agent Query Planning\n* Context-Augmented OpenAI Agent\n* Recursive Retriever + Document Agents\n\n\n## ReAct Agent\n\n* ReAct Agent with Query Engine Tools\n\n\n# Data Agents\n\n\n# Evaluation\n\n\nEvaluation and benchmarking are crucial concepts in LLM development. To improve the performance of an LLM app (RAG, agents), you must have a way to measure it.\n\n\nLlamaIndex offers key modules to measure the quality of generated results. We also offer key modules to measure retrieval quality.\n\n\n* **Response Evaluation**: Does the response match the retrieved context? Does it also match the query? Does it match the reference answer or guidelnes?\n* **Retrieval Evaluation**: Are the retrieved sources relevant to the query?\n\n\nThis section describes how the evaluation components within LlamaIndex work.\n\n### Response Evaluation\n\n\nEvaluation of generated results can be difficult, since unlike traditional machine learning the predicted result isn’t a single number, and it can be hard to define quantitative metrics for this problem.\n\n\nLlamaIndex offers **LLM-based** evaluation modules to measure the\n\n==================\n Document 4 \n----------------\n# Example: Using a HuggingFace LLM\n\n\nLlamaIndex supports using LLMs from HuggingFace directly. Note that for a completely private experience, also setup a local embedding model as in this example.\n\n\nMany open-source models from HuggingFace require either some preamble before each prompt, which is a `system\\_prompt`. Additionally, queries themselves may need an additional wrapper around the `query\\_str` itself. All this information is usually available from the HuggingFace model card for the model you are using.\n\n\nBelow, this example uses both the `system\\_prompt` and `query\\_wrapper\\_prompt`, using specific prompts from the model card found here.\n\n```\nfrom llama\\_index.prompts import PromptTemplate\n\nsystem\\_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n- StableLM will refuse to participate in anything that could harm a human.\n\"\"\"\n# This will wrap the default prompts that are internal to llama-index\nquery\\_wrapper\\_prompt = PromptTemplate(\"<|USER|>{query\\_str}<|ASSISTANT|>\")\n\nimport torch\nfrom llama\\_index.llms import HuggingFaceLLM\nllm = HuggingFaceLLM(\n    context\\_window=4096, \n    max\\_new\\_tokens=256,\n    generate\\_kwargs={\"temperature\": 0.7, \"do\\_sample\": False},\n    system\\_prompt=system\\_prompt,"
    },
    {
        "instruction": "Write a Python program using the 'llama-index' API to scrape and index data from 'https://en.wikipedia.org/wiki/Machine_vision.' Extract information about the history and applications of machine vision. Print the extracted information.",
        "api": "llama-index",
        "output": "#!pip install llama-cpp-python\n#!pip install llama-index html2text trafilatura\n\nfrom llama_index import GPTListIndex\nfrom llama_index import TrafilaturaWebReader\n\ndef scrape_and_index_machine_vision_info(url, *topics):\n    documents = TrafilaturaWebReader().load_data([url])\n    index = GPTListIndex.from_documents(documents)\n    engine = index.as_query_engine()\n\n    results = {}\n    for topic in topics:\n        results[topic] = engine.query(topic)\n\n    return results\n\nif __name__ == \"__main__\":\n    url = \"https://en.wikipedia.org/wiki/Machine_vision\"\n    topics = [\"History of machine vision\", \"Applications of machine vision\"]\n    extracted_data = scrape_and_index_machine_vision_info(url, *topics)\n\n    for topic, data in extracted_data.items():\n        print(f\"{topic}:\\n{data}\")",
        "documentation": "\n\n==================\n Document 0 \n----------------\nPlayground\n\n\nThe Playground module in LlamaIndex is a way to automatically test your data (i.e. documents) across a diverse combination of indices, models, embeddings, modes, etc. to decide which ones are best for your purposes. More options will continue to be added.\n\n\nFor each combination, you’ll be able to compare the results for any query and compare the answers, latency, tokens used, and so on.\n\n\nYou may initialize a Playground with a list of pre-built indices, or initialize one from a list of Documents using the preset indices.\n\n\nA sample usage is given below.\n\n```\nfrom llama\\_index import download\\_loader\nfrom llama\\_index.indices.vector\\_store import VectorStoreIndex\nfrom llama\\_index.indices.tree.base import TreeIndex\nfrom llama\\_index.playground import Playground\n\n# load data \nWikipediaReader = download\\_loader(\"WikipediaReader\")\nloader = WikipediaReader()\ndocuments = loader.load\\_data(pages=['Berlin'])\n\n\n# define multiple index data structures (vector index, summary index)\nindices = [VectorStoreIndex(documents), TreeIndex(documents)]\n\n\n# initialize playground\nplayground = Playground(indices=indices)\n\n\n# playground compare\nplayground.compare(\"What is the population of Berlin?\")\n\n* Playground\n\n\n# Token Counting - Migration Guide\n\n\n\n# Cost Analysis\n\n\n# Playground\n\n==================\n Document 1 \n----------------\n LLM\n\n\nPicking the proper Large Language Model (LLM) is one of the first steps you need to consider when building any LLM application over your data.\n\n\nLLMs are a core component of LlamaIndex. They can be used as standalone modules or plugged into other core LlamaIndex modules (indices, retrievers, query engines). They are always used during the response synthesis step (e.g. after retrieval). Depending on the type of index being used, LLMs may also be used during index construction, insertion, and query traversal.\n\n\nLlamaIndex provides a unified interface for defining LLM modules, whether it’s from OpenAI, Hugging Face, or LangChain, so that you\ndon’t have to write the boilerplate code of defining the LLM interface yourself. This interface consists of the following (more details below):\n\n\n* Support for **text completion** and **chat** endpoints (details below)\n* Support for **streaming** and **non-streaming** endpoints\n* Support for **synchronous** and **asynchronous** endpoints\n\n\nThe following code snippet shows how you can get started using LLMs.\n\n```\nfrom llama\\_index.llms import OpenAI\n\n# non-streaming\nresp = OpenAI().complete('Paul Graham is ')\nprint(resp)\n\n* Using LLMs as standalone modules\n* Customizing LLMs within LlamaIndex Abstractions\n\n\nWe support integrations with OpenAI, Hugging Face, PaLM, and more.\n\n* Modules\n\t+ OpenAI\n\t+ Anthropic\n\t+ Hugging Face\n\t+ LiteLLM\n\t+ PaLM\n\t+ Predibase\n\t+ Replicate\n\t+ LangChain\n\t+ Llama API\n\t+ Llama CPP\n\t+ Xorbits Inference\n\t+ MonsterAPI\n\t+ RunGPT\n\t+ Portkey\n\t+ AnyScale\n\t+ Ollama\n\n\n# Using LLMs as standalone modules\n\n\nYou can use our LLM modules on their own.\n\n\n## Text Completion Example\n\n\n# using streaming endpoint\nfrom llama\\_index.llms import OpenAI\nllm = OpenAI()\nresp = llm.stream\\_complete('Paul Graham is ')\nfor delta in resp:\n    print(delta, end='')\n\n\n\n## Chat Example\n\n```\nfrom llama\\_index.llms import ChatMessage, OpenAI\n\nmessages = [\n    ChatMessage(role=\"system\", content=\"You are a pirate with a colorful personality\"),\n    ChatMessage(role=\"user\", content=\"What is your name\"),\n]\nresp = OpenAI().chat(messages)\nprint(resp)\n\n\nCheck out our modules section for usage guides for each LLM.\n\n\n\n# Customizing LLMs within LlamaIndex Abstractions\n\n\nYou can plugin these LLM abstractions within our other modules in LlamaIndex (indexes, retrievers, query engines, agents) which allow you to build advanced workflows over your data.\n\n\nBy default, we use OpenAI’s `gpt-3.5-turbo` model. But you may choose to customize\nthe underlying LLM being used.\n\n\nBelow we show a few examples of LLM customization. This includes\n\n\n* changing the underlying LLM\n* changing the number of output tokens (for OpenAI, Cohere, or AI21)\n* having more fine-grained control over all parameters for any LLM, from context window to chunk overlap\n\n\n## Example: Changing the underlying LLM\n\n\nAn example snippet of customizing the LLM being used is shown below.\nIn this example, we use `gpt-4` instead of `gpt-3.5-turbo`. Available models include `gpt-3.5-turbo`, `gpt-3.5-turbo-instruct`, `gpt-3.5-turbo-16k`, `gpt-4`, `gpt-4-32k`, `text-davinci-003`, and `text-davinci-002`.\n\n\nNote that\nyou may also plug in any LLM shown on Langchain’s\nLLM page.\n\nfrom llama\\_index import (\n    KeywordTableIndex,\n    SimpleDirectoryReader,\n    LLMPredictor,\n    ServiceContext\n)\nfrom llama\\_index.llms import OpenAI\n\n# alternatively\n\n# from langchain.llms import ...\n\ndocuments = SimpleDirectoryReader('data').load\\_data()\n\n\n# define LLM\nllm = OpenAI(temperature=0.1, model=\"gpt-4\")\nservice\\_context = ServiceContext.from\\_defaults(llm=llm)\n\n\n# build index\nindex = KeywordTableIndex.from\\_documents(documents, service\\_context=service\\_context)\n\n\n# get response from query\nquery\\_engine = index.as\\_query\\_engine()\nresponse = query\\_engine.query(\"What did the author do after his time at Y Combinator?\")\n\n\n\n## Example: Changing the number of output tokens (for OpenAI, Cohere, AI21)\n\n\nThe number of output tokens is usually set to some low number by default (for instance,\nwith OpenAI the default is 256).\n\n\nFor OpenAI, Cohere, AI21, you just need to set the `max\\_tokens` parameter\n(or maxTokens for AI21). We will handle text chunking/calculations under the hood.\n\nfrom llama\\_index import (\n    KeywordTableIndex,\n    SimpleDirectoryReader,\n    ServiceContext\n)\nfrom llama\\_index.llms import OpenAI\n\n\n# define LLM\nllm = OpenAI(temperature=0, model=\"text-davinci-002\", max\\_tokens=512)\nservice\\_context = ServiceContext.from\\_defaults(llm=llm)\n\n\n\n## Example: Explicitly configure `context\\_window` and `num\\_output`\n\n\nIf you are using other LLM classes from langchain, you may need to explicitly configure the `context\\_window` and `num\\_output` via the `ServiceContext` since the information is not available by default.\n\nfrom llama\\_index import (\n    KeywordTableIndex,\n    SimpleDirectoryReader,\n    ServiceContext\n)\nfrom llama\\_index.llms import OpenAI\n\n# alternatively\n\n# from langchain.llms import ...\n\n\n\n# set context window\ncontext\\_window = 4096\n\n# set number of output tokens\nnum\\_output = 256\n\n\n# define LLM\nllm = OpenAI(\n    temperature=0, \n    model=\"text-davinci-002\", \n    max\\_tokens=num\\_output,\n)\n\nservice\\_context = ServiceContext.from\\_defaults(\n    llm=llm,\n    context\\_window=context\\_window,\n    num\\_output=num\\_output,\n)\n\n\n## Example: Using a HuggingFace LLM\n\n\nLlamaIndex supports using LLMs from HuggingFace directly. Note that for a completely private experience, also setup a local embedding model as in this example.\n\n\nMany open-source models from HuggingFace require either some preamble before each prompt,\n\n==================\n Document 2 \n----------------\n Query and print response\nquery\\_engine = index.as\\_query\\_engine()\nresponse = query\\_engine.query(\"<query\\_text>\")\nprint(response)\n\n\nUsing this method, you can use any LLM. Maybe you have one running locally, or running on your own server. As long as the class is implemented and the generated tokens are returned, it should work out. Note that we need to use the prompt helper to customize the prompt sizes, since every model has a slightly different context length.\n\n\nThe decorator is optional, but provides observability via callbacks on the LLM calls.\n\n\nNote that you may have to adjust the internal prompts to get good performance. Even then, you should be using a sufficiently large LLM to ensure it’s capable of handling the complex queries that LlamaIndex uses internally, so your mileage may vary.\n\n\nA list of all default internal prompts is available here, and chat-specific prompts are listed here. You can also implement your own custom prompts, as described here.\n\n\nWe support integrations with OpenAI, Anthropic, Hugging Face, PaLM, and more.\n\n## OpenAI\n\n* OpenAI\n* Azure OpenAI\n\n\n## Anthropic\n\n* Anthropic\n\n\n## Hugging Face\n\n* HuggingFace LLM - Camel-5b\n* HuggingFace LLM - StableLM\n* Local Llama2 + VectorStoreIndex\n\n\n## LiteLLM\n\n* LiteLLM\n\n\n## PaLM\n\n* PaLM\n\n\n## Predibase\n\n* Predibase\n\n\n## Replicate\n\n* Replicate - Llama 2 13B\n* Replicate - Vicuna 13B\n* Llama2 + VectorStoreIndex\n\n\n## LangChain\n\n* LangChain LLM\n\n\n## Llama API\n\n* Llama API\n\n\n## Llama CPP\n\n* LlamaCPP\n\n\n## Xorbits Inference\n\n* Xorbits Inference\n\n\n## MonsterAPI\n\n* Set Monster API Key env variable\n* Basic Usage Pattern\n\n\n## RunGPT\n\n* RunGPT\n* Setup\n\n\n## Portkey\n\n* Portkey\n\n\n## AnyScale\n\n* Anyscale\n\n\n## Ollama\n\n* Ollama - Llama 2 7B\n\n\n# Customizing LLMs within LlamaIndex Abstractions\n\n\n# LLM\n\n\n# Embeddings\n\n\nEmbeddings are used in LlamaIndex to represent your documents using a sophisticated numerical representation. Embedding models take text as input, and return a long list of numbers used to capture the semantics of the text. These embedding models have\n\n==================\n Document 3 \n----------------\n Indexes\n\n\nAn `Index` is a data structure that allows us to quickly retrieve relevant context for a user query.\nFor LlamaIndex, it’s the core foundation for retrieval-augmented generation (RAG) use-cases.\n\n\nAt a high-level, `Indices` are built from Documents.\nThey are used to build Query Engines and Chat Engines\nwhich enables question & answer and chat over your data.\n\n\nUnder the hood, `Indices` store data in `Node` objects (which represent chunks of the original documents), and expose a Retriever interface that supports additional configuration and automation.\n\n\nFor a more in-depth explanation, check out our guide below:\n\n* How Each Index Works\n\n```\nfrom llama\\_index import VectorStoreIndex\n\nindex = VectorStoreIndex.from\\_documents(docs)\n\n* Usage Pattern\n\t+ Get Started\n\t+ Configuring Document Parsing\n\t+ Handling Document Update\n\n* Module Guides\n\t+ Vector Store Index\n\t+ Summary Index\n\t+ Tree Index\n\t+ Keyword Table Index\n\t+ Knowledge Graph Index\n\t+ Custom Retriever combining KG Index and VectorStore Index\n\t+ Knowledge Graph Query Engine\n\t+ Knowledge Graph RAG Query Engine\n\t+ REBEL + Knowledge Graph Index\n\t+ SQL Index\n\t+ SQL Query Engine with LlamaIndex + DuckDB\n\t+ Document Summary Index\n\n## Advanced Concepts\n\n* Composability\n\n\n# How Each Index Works\n\n\nThis guide describes how each index works with diagrams.\n\n\nSome terminology:\n\n\n* **Node**: Corresponds to a chunk of text from a Document. LlamaIndex takes in Document objects and internally parses/chunks them into Node objects.\n* **Response Synthesis**: Our module which synthesizes a response given the retrieved Node. You can see how to\nspecify different response modes here.\n\n\n## Summary Index (formerly List Index)\n\n\nThe summary index simply stores Nodes as a sequential chain.\n\n\n\n\n\n### Querying\n\n\nDuring query time, if no other query parameters are specified, LlamaIndex simply loads all Nodes in the list into\nour Response Synthesis module.\n\n\n\n\n\nThe summary index does offer numerous ways of querying a summary index, from an embedding-based query which\nwill fetch the top-k neighbors, or with the addition of a keyword filter, as seen below:\n\n\n\n\n\n## Vector Store Index\n\n\nThe vector store index stores each Node and a corresponding embedding in a Vector Store.\n\n\n\n\n\nQuerying a vector store index involves fetching the top-k most similar Nodes, and passing\nthose into our Response Synthesis module.\n\n\n\n\n\n## Tree Index\n\n\nThe tree index builds a hierarchical tree from a set of Nodes (which become leaf nodes in this tree).\n\n\n\n\n\nQuerying a tree index involves traversing from root nodes down\nto leaf nodes. By default, (`child\\_branch\\_factor=1`), a query\nchooses one child node given a parent node. If `child\\_branch\\_factor=2`, a query\nchooses two child nodes per level.\n\n\n\n\n\n## Keyword Table Index\n\n\nThe keyword table index extracts keywords from each Node and builds a mapping from\neach keyword to the corresponding Nodes of that keyword.\n\n\n\n\n\nDuring query time, we extract relevant keywords from the query, and match those with pre-extracted\nNode keywords to fetch the corresponding Nodes. The extracted Nodes are passed to our\nResponse Synthesis module.\n\n\n\n\n\nBuild an index from documents:\n\n\nTo learn how to load documents, see Data Connectors\n\n\n\n### What is happening under the hood?\n\n\n1. Documents are chunked up and parsed into `Node` objects (which are lightweight abstractions over text str that additionally keep track of metadata and relationships).\n2. Additional computation is performed to add `Node` into index data structure\n\n> \n> Note: the computation is index-specific.\n> \n> \n> \n> \t* For a vector store index, this means calling an embedding model (via API or locally) to compute embedding for the `Node` objects\n> \t* For a document summary index, this means calling an LLM to generate a summary\n>\n\n\n## Configuring Document Parsing\n\n\nThe most common configuration you might want to change is how to parse document into `Node` objects.\n\n\nWe can configure our service context to use the desired chunk size and set `show\\_progress` to display a progress bar during index construction.\n\n```\nfrom llama\\_index import ServiceContext, VectorStoreIndex\n\nservice\\_context = ServiceContext.from\\_defaults(chunk\\_size=512)\nindex = VectorStoreIndex.from\\_documents(\n    docs,\n    service\\_context=service\\_context,\n    show\\_progress=True\n)\n\n\n\n### Low-Level API\n\n\nYou can use the low-level composition API if you need more granular control.\n\n\nHere we show an example where you want to both modify the text chunk size, disable injecting metadata, and disable creating `Node` relationships.  \n\nThe steps are:\n\n\n1. Configure a node parser\n\nparser = SimpleNodeParser.from\\_defaults(\n    chunk\\_size=512,\n    include\\_extra\\_info=False,\n    include\\_prev\\_next\\_rel=False,\n)\n\n\n2. Parse document into `Node` objects\n\n```\nnodes = parser.get\\_nodes\\_from\\_documents(documents)\n\n\n3. build index from `Node` objects\n\n```\nindex = VectorStoreIndex(nodes)\n\n\n## Handling Document Update\n\n\nRead more about how to deal with data sources that change over time with `Index` **insertion**, **deletion**, **update**, and **refresh** operations.\n\n* Metadata Extraction\n* Document Management\n\n\n# Metadata Extraction\n\n\n## Introduction\n\n\nIn many cases, especially with long documents, a chunk of text may lack the context necessary to disambiguate the chunk from other similar chunks of text.\n\n\nTo combat this, we use LLMs to extract certain contextual information relevant to the document to better help the retrieval and language models disambiguate similar-looking passages.\n\n\nWe show this in an example notebook and demonstrate its effectiveness in processing long documents.\n\n\n## Usage\n\n\nFirst, we define a metadata extractor that takes in a list of feature extractors that will be processed in sequence.\n\n\nWe then feed this to the node parser, which will add the additional metadata to each node.\n\n```\nfrom llama\\_index.node\\_parser import SimpleNodeParser\nfrom\n\n==================\n Document 4 \n----------------\n# Utility Tools\n\n\nOftentimes, directly querying an API can return a massive volume of data, which on its own may overflow the context window of the LLM (or at the very least unnecessarily increase the number of tokens that you are using).\n\n\nTo tackle this, we’ve provided an initial set of “utility tools” in LlamaHub Tools - utility tools are not conceptually tied to a given service (e.g. Gmail, Notion), but rather can augment the capabilities of existing Tools. In this particular case, utility tools help to abstract away common patterns of needing to cache/index and query data that’s returned from any API request.\n\n\nLet’s walk through our two main utility tools below.\n### OnDemandLoaderTool\n\n\nThis tool turns any existing LlamaIndex data loader ( `BaseReader` class) into a tool that an agent can use. The tool can be called with all the parameters needed to trigger `load\\_data` from the data loader, along with a\n\n==================\n Document 5 \n----------------\n Data Agents\n\n\nData Agents are LLM-powered knowledge workers in LlamaIndex that can intelligently perform various tasks over your data, in both a “read” and “write” function. They are capable of the following:\n\n\n* Perform automated search and retrieval over different types of data - unstructured, semi-structured, and structured.\n* Calling any external service API in a structured fashion, and processing the response + storing it for later.\n\n\nIn that sense, agents are a step beyond our query engines in that they can not only “read” from a static source of data, but can dynamically ingest and modify data from a variety of different tools.\n\n\nBuilding a data agent requires the following core components:\n\n\n* A reasoning loop\n* Tool abstractions\n\n\nA data agent is initialized with set of APIs, or Tools, to interact with; these APIs can be called by the agent to return information or modify state. Given an input task, the data agent uses a reasoning loop to decide which tools to use, in which sequence, and the parameters to call each tool.\n\n### Reasoning Loop\n\n\nThe reasoning loop depends on the type of agent. We have support for the following agents:\n\n\n* OpenAI Function agent (built on top of the OpenAI Function API)\n* a ReAct agent (which works across any chat/text completion endpoint).\n\n\n\n### Tool Abstractions\n\n\nYou can learn more about our Tool abstractions in our Tools section.\n\n\n\n### Blog Post\n\n\nFor full details, please check out our detailed blog post.\n\n\nData agents can be used in the following manner (the example uses the OpenAI Function API)\n\n```\nfrom llama\\_index.agent import OpenAIAgent\nfrom llama\\_index.llms import OpenAI\n\n\n# import and define tools\n...\n\n\n# initialize llm\nllm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n\n\n# initialize openai agent\nagent = OpenAIAgent.from\\_tools(tools, llm=llm, verbose=True)\n\n\nSee our usage pattern guide for more details.\n\n* Usage Pattern\n\n\nLearn more about our different agent types in our module guides below.\n\n\nAlso take a look at our tools section!\n\n* Module Guides\n\t+ OpenAI Agent\n\t+ ReAct Agent\n\n\nAn agent is initialized from a set of Tools. Here’s an example of instantiating a ReAct\nagent from a set of Tools.\n\n```\nfrom llama\\_index.tools import FunctionTool\nfrom llama\\_index.llms import OpenAI\nfrom llama\\_index.agent import ReActAgent\n\n\n# define sample Tool\ndef multiply(a: int, b: int) -> int:\n \"\"\"Multiple two integers and returns the result integer\"\"\"\n    return a \\* b\n\nmultiply\\_tool = FunctionTool.from\\_defaults(fn=multiply)\n\n\n# initialize ReAct agent\nagent = ReActAgent.from\\_tools([multiply\\_tool], llm=llm, verbose=True)\n\n\nAn agent supports both `chat` and `query` endpoints, inheriting from our `ChatEngine` and `QueryEngine` respectively.\n\n\nExample usage:\n\n```\nagent.chat(\"What is 2123 \\* 215123\")\n\n\n\n## Query Engine Tools\n\n\nIt is easy to wrap query engines as tools for an agent as well. Simply do the following:\n\nfrom llama\\_index.agent import ReActAgent\nfrom llama\\_index.tools import QueryEngineTool\n\n\n# NOTE: lyft\\_index and uber\\_index are both SimpleVectorIndex instances\nlyft\\_engine = lyft\\_index.as\\_query\\_engine(similarity\\_top\\_k=3)\nuber\\_engine = uber\\_index.as\\_query\\_engine(similarity\\_top\\_k=3)\n\nquery\\_engine\\_tools = [\n    QueryEngineTool(\n        query\\_engine=lyft\\_engine,\n        metadata=ToolMetadata(\n            name=\"lyft\\_10k\",\n            description=\"Provides information about Lyft financials for year 2021. \"\n            \"Use a detailed plain text question as input to the tool.\",\n        ),\n    ),\n    QueryEngineTool(\n        query\\_engine=uber\\_engine,\n        metadata=ToolMetadata(\n            name=\"uber\\_10k\",\n            description=\"Provides information about Uber financials for year 2021. \"\n            \"Use a detailed plain text question as input to the tool.\",\n        ),\n    ),\n]\n\n\n# initialize ReAct agent\nagent = ReActAgent.from\\_tools(query\\_engine\\_tools, llm=llm, verbose=True)\n\n\n\n## Use other agents as Tools\n\n\nA nifty feature of our agents is that since they inherit from `BaseQueryEngine`, you can easily define other agents as tools\nthrough our `QueryEngineTool`.\n\n```\nfrom llama\\_index.tools import QueryEngineTool\n\nquery\\_engine\\_tools = [\n    QueryEngineTool(\n        query\\_engine=sql\\_agent,\n        metadata=ToolMetadata(\n            name=\"sql\\_agent\",\n            description=\"Agent that can execute SQL queries.\"\n        ),\n    ),\n    QueryEngineTool(\n        query\\_engine=gmail\\_agent,\n        metadata=ToolMetadata(\n            name=\"gmail\\_agent\",\n            description=\"Tool that can send emails on Gmail.\"\n        ),\n    ),\n]\n\nouter\\_agent = ReActAgent.from\\_tools(query\\_engine\\_tools, llm=llm, verbose=True)\n\n\n\n## Advanced Concepts (for `OpenAIAgent`, in beta)\n\n\nYou can also use agents in more advanced settings. For instance, being able to retrieve tools from an index during query-time, and\nbeing able to perform query planning over an existing set of Tools.\n\n\nThese are largely implemented with our `OpenAIAgent` classes (which depend on the OpenAI Function API). Support\nfor our more general `ReActAgent` is something we’re actively investigating.\n\n\nNOTE: these are largely still in beta. The abstractions may change and become more general over time.\n\n\n### Function Retrieval Agents\n\n\nIf the set of Tools is very large, you can create an `ObjectIndex` to index the tools, and then pass in an `ObjectRetriever` to the agent during query-time, to first dynamically retrieve the relevant tools before having the agent pick from the candidate tools.\n\n\nWe first build an `ObjectIndex` over an existing set of Tools.\n\n```\n\n# define an \"object\" index over these tools\nfrom llama\\_index import VectorStoreIndex\nfrom llama\\_index.objects import ObjectIndex, SimpleToolNodeMapping\n\ntool\\_mapping = SimpleToolNodeMapping.from\\_objects(all\\_tools)\nobj\\_index = ObjectIndex.from\\_objects(\n    all\\_tools,\n    tool\\_mapping,\n    VectorStoreIndex,\n)\n\n\nWe then define our `FnRetrieverOpenAIAgent`:\n\n```\nfrom llama\\_index.agent import FnRetrieverOpenAIAgent\n\nagent = FnRetrieverOpenAIAgent.from\\_retriever(obj\\_index.as\\_retriever(), verbose=True)\n\n\n\n### Context Retrieval Agents\n\n\nOur context-augmented OpenAI Agent will always perform retrieval before calling any tools.\n\n\nThis helps to provide additional context that can help the agent better pick Tools, versus\njust trying to make a decision without any context.\n\n```\nfrom llama\\_index.schema import Document\nfrom llama\\_index.agent import ContextRetrieverOpenAIAgent\n\n\n\n# toy index - stores a list of abbreviations\ntexts = [\n    \"Abbrevation: X = Revenue\",\n    \"Abbrevation: YZ = Risk Factors\",\n    \"Abbreviation: Z = Costs\",\n]\ndocs = [Document(text=t) for t in texts]\ncontext\\_index = VectorStoreIndex.from\\_documents(docs)\n\n\n# add context agent\ncontext\\_agent = ContextRetrieverOpenAIAgent.from\\_tools\\_and\\_retriever(\n    query\\_engine\\_tools, context\\_index.as\\_retriever(similarity\\_top\\_k=1), verbose=True\n)\nresponse = context\\_agent.chat(\"What is the YZ of March 2022?\")\n\n\n\n### Query Planning\n\n\nOpenAI Function Agents can be capable of advanced query planning. The trick is to provide the agent\nwith a `QueryPlanTool` - if the agent calls the QueryPlanTool, it is forced to infer a full Pydantic schema representing a query\nplan over a set of subtools.\n\n```\n\n# define query plan tool\nfrom llama\\_index.tools import QueryPlanTool\nfrom llama\\_index import get\\_response\\_synthesizer\n\nresponse\\_synthesizer = get\\_response\\_synthesizer(service\\_context=service\\_context)\nquery\\_plan\\_tool = QueryPlanTool.from\\_defaults(\n    query\\_engine\\_tools=[query\\_tool\\_sept, query\\_tool\\_june, query\\_tool\\_march],\n    response\\_synthesizer=response\\_synthesizer,\n)\n\n\n# initialize agent\nagent = OpenAIAgent.from\\_tools(\n    [query\\_plan\\_tool],\n    max\\_function\\_calls=10,\n    llm=OpenAI(temperature=0, model=\"gpt-4-0613\"),\n    verbose=True,\n)\n\n\n# should output a query plan to call march, june, and september tools\nresponse = agent.query(\"Analyze Uber revenue growth in March, June, and September\")\n\n\nThese guide provide an overview of how to use our agent classes.\n\n\nFor more detailed guides on how to use specific tools, check out our tools module guides.\n\n\n## OpenAI Agent\n\n* Build your own OpenAI Agent\n* OpenAI Agent with Query Engine Tools\n* Retrieval-Augmented OpenAI Agent\n* OpenAI Agent + Query Engine Experimental Cookbook\n* OpenAI Agent Query Planning\n* Context-Augmented OpenAI Agent\n* Recursive Retriever + Document Agents\n\n\n## ReAct Agent\n\n* ReAct Agent with Query Engine Tools\n\n\n# Data Agents\n\n\n# Evaluation\n\n\nEvaluation and benchmarking are crucial concepts in LLM development. To improve the performance of an LLM app (RAG, agents), you must have a way to measure it.\n\n\nLlamaIndex offers key modules to measure the quality of generated results. We also offer key modules to measure retrieval quality.\n\n\n* **Response Evaluation**: Does the response match the retrieved context? Does it also match the query? Does it match the reference answer or guidelnes?\n* **Retrieval Evaluation**: Are the retrieved sources relevant to the query?\n\n\nThis section describes how the evaluation components within LlamaIndex work.\n\n### Response Evaluation\n\n\nEvaluation of generated results can be difficult, since unlike traditional machine learning the predicted result isn’t a single number, and it can be hard to define quantitative metrics for this problem.\n\n\nLlamaIndex offers **LLM-based** evaluation modules to measure the\n\n==================\n Document 6 \n----------------\n# Example: Using a HuggingFace LLM\n\n\nLlamaIndex supports using LLMs from HuggingFace directly. Note that for a completely private experience, also setup a local embedding model as in this example.\n\n\nMany open-source models from HuggingFace require either some preamble before each prompt, which is a `system\\_prompt`. Additionally, queries themselves may need an additional wrapper around the `query\\_str` itself. All this information is usually available from the HuggingFace model card for the model you are using.\n\n\nBelow, this example uses both the `system\\_prompt` and `query\\_wrapper\\_prompt`, using specific prompts from the model card found here.\n\n```\nfrom llama\\_index.prompts import PromptTemplate\n\nsystem\\_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n- StableLM will refuse to participate in anything that could harm a human.\n\"\"\"\n# This will wrap the default prompts that are internal to llama-index\nquery\\_wrapper\\_prompt = PromptTemplate(\"<|USER|>{query\\_str}<|ASSISTANT|>\")\n\nimport torch\nfrom llama\\_index.llms import HuggingFaceLLM\nllm = HuggingFaceLLM(\n    context\\_window=4096, \n    max\\_new\\_tokens=256,\n    generate\\_kwargs={\"temperature\": 0.7, \"do\\_sample\": False},\n    system\\_prompt=system\\_prompt,"
    },
    {
        "instruction": "Develop a program that uses the MLflow API to train a K-means clustering model on a synthetic dataset. Log the model and its hyperparameters.",
        "api": "mlflow",
        "output": "#!pip install mlflow\nimport mlflow\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\n# Generate a synthetic dataset\nX = np.random.rand(300, 2)\n\n# Start an MLflow run\nwith mlflow.start_run() as run:\n    # Train a K-means clustering model\n    kmeans = KMeans(n_clusters=3, random_state=42)\n    kmeans.fit(X)\n    \n    # Log hyperparameters\n    mlflow.log_params(kmeans.get_params())\n    \n    # Log the trained model\n    mlflow.sklearn.log_model(kmeans, \"kmeans_model\")\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n## Workflows\n\n\n`save\\_model()` and `log\\_model()` support the following workflows:\n\n\n1. Programmatically defining a new MLflow model, including its attributes and artifacts.\n\n\nGiven a set of artifact URIs, `save\\_model()` and `log\\_model()` can\nautomatically download artifacts from their URIs and create an MLflow model directory.\n\n\nIn this case, you must define a Python class which inherits from `PythonModel`,\ndefining `predict()` and, optionally, `load\\_context()`. An instance of this class is\nspecified via the `python\\_model` parameter; it is automatically serialized and deserialized\nas a Python class, including all of its attributes.\n2. Interpreting pre-existing data as an MLflow model.\n\n\nIf you already have a directory containing model data, `save\\_model()` and\n`log\\_model()` can import the data as an MLflow model. The `data\\_path` parameter\nspecifies the local filesystem path to the directory containing model data.\n\n\nIn this case, you must provide a Python module, called a loader module. The\nloader module defines a `\\_load\\_pyfunc()` method that performs the following tasks:\n\n\n\t* Load data from the specified `data\\_path`. For example, this process may include\n\tdeserializing pickled Python objects or models or parsing CSV files.\n\t* Construct and return a pyfunc-compatible model wrapper. As in the first\n\tuse case, this wrapper must define a `predict()` method that is used to evaluate\n\tqueries. `predict()` must adhere to the Inference API.The `loader\\_module` parameter specifies the name of your loader module.\n\n\nFor an example loader module implementation, refer to the loader module\nimplementation in mlflow.sklearn.\n\n### Which workflow is right for my use case?\n\n\nWe consider the first workflow to be more user-friendly and generally recommend it for the\nfollowing reasons:\n\n\n* It automatically resolves and collects specified model artifacts.\n* It automatically serializes and deserializes the `python\\_model` instance\n\n==================\n Document 1 \n----------------\n mlflow.sklearn\n\n\nThe `mlflow.sklearn` module provides an API for logging and loading scikit-learn models. This\nmodule exports scikit-learn models with the following flavors:\n\nPython (native) pickle formatThis is the main flavor that can be loaded back into scikit-learn.\n\n`mlflow.pyfunc`Produced for use by generic pyfunc-based deployment tools and batch inference.\nNOTE: The mlflow.pyfunc flavor is only added for scikit-learn models that define predict(),\nsince predict() is required for pyfunc model inference.\n\n\n`mlflow.sklearn.``autolog`(*log\\_input\\_examples=False*, *log\\_model\\_signatures=True*, *log\\_models=True*, *log\\_datasets=True*, *disable=False*, *exclusive=False*, *disable\\_for\\_unsupported\\_versions=False*, *silent=False*, *max\\_tuning\\_runs=5*, *log\\_post\\_training\\_metrics=True*, *serialization\\_format='cloudpickle'*, *registered\\_model\\_name=None*, *pos\\_label=None*, *extra\\_tags=None*)[source] \n\n\nAutologging is known to be compatible with the following package versions: `0.22.1` <= `scikit-learn` <= `1.3.0`. Autologging may not succeed when used with package versions outside of this range.\n\nEnables (or disables) and configures autologging for scikit-learn estimators.\n\n**When is autologging performed?**Autologging is performed when you call:\n\n\n* `estimator.fit()`\n* `estimator.fit\\_predict()`\n* `estimator.fit\\_transform()`\n\n**Logged information**\n**Parameters*** Parameters obtained by `estimator.get\\_params(deep=True)`. Note that `get\\_params`\nis called with `deep=True`. This means when you fit a meta estimator that chains\na series of estimators, the parameters of these child estimators are also logged.\n\n**Training metrics*** A training score obtained by `estimator.score`. Note that the training score is\ncomputed using parameters given to `fit()`.\n* Common metrics for classifier:\n\n\n\t+ precision score\n\t+ recall score\n\t+ f1 score\n\t+ accuracy scoreIf the classifier has method `predict\\_proba`, we additionally log:\n\n\n\t+ log loss\n\t+ roc auc score\n* Common metrics for regressor:\n\n\n\t+ mean squared error\n\t+ root mean squared error\n\t+ mean absolute error\n\t+ r2 score\n\n**Post training metrics**When users call metric APIs after model training, MLflow tries to capture the metric API\nresults and log them as MLflow metrics to the Run associated with the model. The following\ntypes of scikit-learn metric APIs are supported:\n\n\n* model.score\n* metric APIs defined in the sklearn.metrics module\n\n\n* If the metric function is from sklearn.metrics, the MLflow “metric\\_name” is the\nmetric function name. If the metric function is model.score, then “metric\\_name” is\n“{model\\_class\\_name}\\_score”.\n* If multiple calls are made to the same scikit-learn metric API, each subsequent call\nadds a “call\\_index” (starting from 2) to the metric key.\n* MLflow uses the prediction input dataset variable name as the “dataset\\_name” in the\nmetric key. The “prediction input dataset variable” refers to the variable which was\nused as the first argument of the associated model.predict or model.score call.\nNote: MLflow captures the “prediction input dataset” instance in the outermost call\nframe and fetches the variable name in the outermost call frame. If the “prediction\ninput dataset” instance is an intermediate expression without a defined variable\nname, the dataset name is set to “unknown\\_dataset”. If multiple “prediction input\ndataset” instances have the same variable name, then subsequent ones will append an\nindex (starting from 2) to the inspected dataset name.\n\n**Limitations*** MLflow can only map the original prediction result object returned by a model\nprediction API (including predict / predict\\_proba / predict\\_log\\_proba / transform,\nbut excluding fit\\_predict / fit\\_transform.) to an MLflow run.\nMLflow cannot find run information\nfor other objects derived from a given prediction result (e.g. by copying or selecting\na subset of the prediction result). scikit-learn metric APIs invoked on derived objects\ndo not log metrics to MLflow.\n* Autologging must be enabled before scikit-learn metric APIs are imported from\nsklearn.metrics. Metric APIs imported before autologging is enabled do not log\nmetrics to MLflow runs.\n* If user define a scorer which is not based on metric APIs in sklearn.metrics, then\nthen post training metric autologging for the scorer is invalid.\n\n**Tags*** An estimator class name (e.g. “LinearRegression”).\n* A fully qualified estimator class name\n(e.g. “sklearn.linear\\_model.\\_base.LinearRegression”).\n\n**Artifacts*** An MLflow Model with the `mlflow.sklearn` flavor containing a fitted estimator\n(logged by `mlflow.sklearn.log\\_model()`). The Model also contains the\n`mlflow.pyfunc` flavor when the scikit-learn estimator defines predict().\n* For post training metrics API calls, a “metric\\_info.json” artifact is logged. This is a\nJSON object whose keys are MLflow post training metric names\n(see “Post training metrics” section for the key format) and whose values are the\ncorresponding metric call commands that produced the metrics, e.g.\n`accuracy\\_score(y\\_true=test\\_iris\\_y, y\\_pred=pred\\_iris\\_y, normalize=False)`.\n\n**How does autologging work for meta estimators?**When a meta estimator (e.g. Pipeline, GridSearchCV) calls `fit()`, it internally calls\n`fit()` on its child estimators. Autologging does NOT perform logging on these constituent\n`fit()` calls.\n\n**Parameter search**In addition to recording the information discussed above, autologging for parameter\nsearch meta estimators (GridSearchCV and RandomizedSearchCV) records child runs\nwith metrics for each set of explored parameters, as well as artifacts and parameters\nfor the best model (if available).\n\n**Supported estimators*** All estimators obtained by sklearn.utils.all\\_estimators (including meta estimators).\n* Pipeline\n* Parameter search estimators (GridSearchCV and RandomizedSearchCV)\n\n\n**Example**\n\n\nSee more examples\n\n```\nfrom pprint import pprint\nimport numpy as np\nfrom sklearn.linear\\_model import LinearRegression\nimport mlflow\nfrom mlflow import MlflowClient\n\n\ndef fetch\\_logged\\_data(run\\_id):\n    client = MlflowClient()\n    data = client.get\\_run(run\\_id).data\n    tags = {k: v for k, v in data.tags.items() if not k.startswith(\"mlflow.\")}\n    artifacts = [f.path for f in client.list\\_artifacts(run\\_id, \"model\")]\n    return data.params, data.metrics, tags, artifacts\n\n\n# enable autologging\nmlflow.sklearn.autolog()\n\n\n# prepare training data\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\ny = np.dot(X, np.array([1, 2])) + 3\n\n\n# train a model\nmodel = LinearRegression()\nwith mlflow.start\\_run() as run:\n    model.fit(X, y)\n\n\n# fetch logged data\nparams, metrics, tags, artifacts = fetch\\_logged\\_data(run.info.run\\_id)\n\npprint(params)\n\n# {'copy\\_X': 'True',\n\n# 'fit\\_intercept': 'True',\n\n# 'n\\_jobs': 'None',\n\n# 'normalize': 'False'}\n\npprint(metrics)\n\n# {'training\\_score': 1.0,\n\n# 'training\\_mean\\_absolute\\_error': 2.220446049250313e-16,\n\n# 'training\\_mean\\_squared\\_error': 1.9721522630525295e-31,\n\n# 'training\\_r2\\_score': 1.0,\n\n# 'training\\_root\\_mean\\_squared\\_error': 4.440892098500626e-16}\n\npprint(tags)\n\n# {'estimator\\_class': 'sklearn.linear\\_model.\\_base.LinearRegression',\n\n# 'estimator\\_name': 'LinearRegression'}\n\npprint(artifacts)\n# ['model/MLmodel', 'model/conda.yaml', 'model/model.pkl']\n\nParameters\n* **log\\_input\\_examples** – If `True`, input examples from training datasets are collected and\nlogged along with scikit-learn model artifacts during training. If\n`False`, input examples are not logged.\nNote: Input examples are MLflow model attributes\nand are only collected if `log\\_models`\n\n==================\n Document 2 \n----------------\n demonstrating autolog capabilities.\n\nimport fastai.vision as vis\nimport mlflow.fastai\nfrom mlflow import MlflowClient\n\n\ndef main(epochs=5, learning\\_rate=0.01):\n    # Download and untar the MNIST data set\n    path = vis.untar\\_data(vis.URLs.MNIST\\_SAMPLE)\n\n    # Prepare, transform, and normalize the data\n    data = vis.ImageDataBunch.from\\_folder(\n        path, ds\\_tfms=(vis.rand\\_pad(2, 28), []), bs=64\n    )\n    data.normalize(vis.imagenet\\_stats)\n\n    # Create CNN the Learner model\n    model = vis.cnn\\_learner(data, vis.models.resnet18, metrics=vis.accuracy)\n\n    # Enable auto logging\n    mlflow.fastai.autolog()\n\n    # Start MLflow session\n    with mlflow.start\\_run() as run:\n        model.fit(epochs, learning\\_rate)\n\n    # fetch the auto logged parameters, metrics, and artifacts\n    print\\_auto\\_logged\\_info(mlflow.get\\_run(run\\_id=run.info.run\\_id))\n\n\nmain()\n\n\noutput \n\n```\nrun_id: 5a23dcbcaa334637814dbce7a00b2f6a\nartifacts: ['model/MLmodel', 'model/conda.yaml', 'model/model.fastai']\nparams: {'wd': 'None',\n         'bn_wd': 'True',\n         'opt_func': 'Adam',\n         'epochs': '5', '\n         train_bn': 'True',\n         'num_layers': '60',\n         'lr': '0.01',\n         'true_wd': 'True'}\nmetrics: {'train_loss': 0.024,\n          'accuracy': 0.99214,\n          'valid_loss': 0.021}\n# Tags model summary omitted too long\ntags: {...}\n\n\n\nFastai autologged MLflow entities \n\n\n`mlflow.fastai.``get\\_default\\_conda\\_env`(*include\\_cloudpickle=False*)[source] \n\n`mlflow.fastai.``get\\_default\\_pip\\_requirements`(*include\\_cloudpickle=False*)[source] \n\n`mlflow.fastai.``load\\_model`(*model\\_uri*, *dst\\_path=None*)[source] \nLoad a fastai model from a local file or a run.\n\nReturns\nA fastai model (an instance of fastai.Learner).\n\n```\nimport mlflow.fastai\n\n\n# Define the Learner model\nmodel = ...\n\n\n# log the fastai Leaner model\nwith mlflow.start\\_run() as run:\n    model.fit(epochs, learning\\_rate)\n    mlflow.fastai.log\\_model(model, \"model\")\n\n# Load the model for scoring\nmodel\\_uri = f\"runs:/{run.info.run\\_id}/model\"\nloaded\\_model = mlflow.fastai.load\\_model(model\\_uri)\nresults = loaded\\_model.predict(predict\\_data)\n\n\n`mlflow.fastai.``log\\_model`(*fastai\\_learner*, *artifact\\_path*, *conda\\_env=None*, *code\\_paths=None*, *registered\\_model\\_name=None*, *signature: mlflow.models.signature.ModelSignature = None*, *input\\_example: Union[pandas.core.frame.DataFrame, numpy.ndarray, dict, list, csr\\_matrix, csc\\_matrix, str, bytes] = None*, *await\\_registration\\_for=300*, *pip\\_requirements=None*, *extra\\_pip\\_requirements=None*, *metadata=None*, *\\*\\*kwargs*)[source] \nLog a fastai model\n\n==================\n Document 3 \n----------------\n# Creating custom Pyfunc models\n\n\nMLflow’s persistence modules provide convenience functions for creating models with the\n`pyfunc` flavor in a variety of machine learning frameworks (scikit-learn, Keras, Pytorch, and\nmore); however, they do not cover every use case. For example, you may want to create an MLflow\nmodel with the `pyfunc` flavor using a framework that MLflow does not natively support.\nAlternatively, you may want to build an MLflow model that executes custom logic when evaluating\nqueries, such as preprocessing and postprocessing routines. Therefore, `mlflow.pyfunc`\nprovides utilities for creating `pyfunc` models from arbitrary code and model data.\n\n\nThe `save\\_model()` and `log\\_model()` methods are designed to support multiple workflows\nfor creating custom `pyfunc` models that incorporate custom inference logic and artifacts\nthat the logic may require.\n\n\nAn artifact is a file or directory, such as a serialized model or a CSV. For example, a\nserialized TensorFlow graph is an artifact. An MLflow model directory is also an artifact.\n### Workflows\n\n\n`save\\_model()` and `log\\_model()` support the following workflows:\n\n\n1. Programmatically defining a new MLflow model, including its attributes and artifacts.\n\n\nGiven a set of artifact URIs, `save\\_model()` and `log\\_model()` can\nautomatically download artifacts from their URIs and create an MLflow model directory.\n\n\nIn this\n\n==================\n Document 4 \n----------------\n## MLModel configuration\n\n\nA Python model contains an `MLmodel` file in **python\\_function** format in its root with the\nfollowing parameters:\n\n\n* loader\\_module [required]:Python module that can load the model. Expected as module identifier\ne.g. `mlflow.sklearn`, it will be imported using `importlib.import\\_module`.\nThe imported module must contain a function with the following signature:\n\n```\n\\_load\\_pyfunc(path: string) -> <pyfunc model implementation>\n\n\nThe path argument is specified by the `data` parameter and may refer to a file or\ndirectory. The model implementation is expected to be an object with a\n`predict` method with the following signature:\n\n```\npredict(\n  model\\_input: [pandas.DataFrame, numpy.ndarray,\n  scipy.sparse.(csc.csc\\_matrix | csr.csr\\_matrix), List[Any], Dict[str, Any]]\n) -> [numpy.ndarray | pandas.(Series | DataFrame) | List]\n\n```\n* code [optional]:Relative path to a directory containing the code packaged with this model.\nAll files and directories inside this directory are added to the Python path\nprior to importing the model loader.\n* data [optional]:Relative path to a file or directory containing model data.\nThe path is passed to the model loader.\n* env [optional]:Relative path to an exported Conda environment. If present this environment\nshould be activated prior to running the model.\n* Optionally, any additional parameters necessary for interpreting the serialized model in\n`pyfunc` format.\n\n\nExample\n\n```\ntree example/sklearn\\_iris/mlruns/run1/outputs/linear-lr\n\n```\n├── MLmodel\n├── code\n│   ├── sklearn_iris.py\n│\n├── data\n│   └── model.pkl\n└── mlflow_env.yml\n\n```\ncat example/sklearn\\_iris/mlruns/run1/outputs/linear-lr/MLmodel\n\n```\npython\\_function:\n  code: code\n  data: data/model.pkl\n  loader\\_module: mlflow.sklearn\n  env: mlflow\\_env.yml\n  main: sklearn\\_iris\n## Creating custom Pyfunc models\n\n\nMLflow’s persistence modules provide convenience functions for creating models with the\n`pyfunc` flavor in a variety of machine learning frameworks (scikit-learn, Keras, Pytorch, and\nmore); however, they do not cover every use case. For example, you may want"
    },
    {
        "instruction": "Develop a program that uses the MLflow API to train a logistic regression classifier on a synthetic binary classification dataset. Log the model and its hyperparameters.",
        "api": "mlflow",
        "output": "#!pip install mlflow\nimport mlflow\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n# Generate a synthetic binary classification dataset\nX = np.random.rand(100, 10)\ny = np.random.randint(2, size=100)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Start an MLflow run\nwith mlflow.start_run() as run:\n    # Train a logistic regression classifier\n    logreg = LogisticRegression(random_state=42)\n    logreg.fit(X_train, y_train)\n    \n    # Log hyperparameters\n    mlflow.log_params(logreg.get_params())\n    \n    # Log the trained model\n    mlflow.sklearn.log_model(logreg, \"logistic_regression_model\")\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n## Workflows\n\n\n`save\\_model()` and `log\\_model()` support the following workflows:\n\n\n1. Programmatically defining a new MLflow model, including its attributes and artifacts.\n\n\nGiven a set of artifact URIs, `save\\_model()` and `log\\_model()` can\nautomatically download artifacts from their URIs and create an MLflow model directory.\n\n\nIn this case, you must define a Python class which inherits from `PythonModel`,\ndefining `predict()` and, optionally, `load\\_context()`. An instance of this class is\nspecified via the `python\\_model` parameter; it is automatically serialized and deserialized\nas a Python class, including all of its attributes.\n2. Interpreting pre-existing data as an MLflow model.\n\n\nIf you already have a directory containing model data, `save\\_model()` and\n`log\\_model()` can import the data as an MLflow model. The `data\\_path` parameter\nspecifies the local filesystem path to the directory containing model data.\n\n\nIn this case, you must provide a Python module, called a loader module. The\nloader module defines a `\\_load\\_pyfunc()` method that performs the following tasks:\n\n\n\t* Load data from the specified `data\\_path`. For example, this process may include\n\tdeserializing pickled Python objects or models or parsing CSV files.\n\t* Construct and return a pyfunc-compatible model wrapper. As in the first\n\tuse case, this wrapper must define a `predict()` method that is used to evaluate\n\tqueries. `predict()` must adhere to the Inference API.The `loader\\_module` parameter specifies the name of your loader module.\n\n\nFor an example loader module implementation, refer to the loader module\nimplementation in mlflow.sklearn.\n\n### Which workflow is right for my use case?\n\n\nWe consider the first workflow to be more user-friendly and generally recommend it for the\nfollowing reasons:\n\n\n* It automatically resolves and collects specified model artifacts.\n* It automatically serializes and deserializes the `python\\_model` instance\n\n==================\n Document 1 \n----------------\n mlflow.sklearn\n\n\nThe `mlflow.sklearn` module provides an API for logging and loading scikit-learn models. This\nmodule exports scikit-learn models with the following flavors:\n\nPython (native) pickle formatThis is the main flavor that can be loaded back into scikit-learn.\n\n`mlflow.pyfunc`Produced for use by generic pyfunc-based deployment tools and batch inference.\nNOTE: The mlflow.pyfunc flavor is only added for scikit-learn models that define predict(),\nsince predict() is required for pyfunc model inference.\n\n\n`mlflow.sklearn.``autolog`(*log\\_input\\_examples=False*, *log\\_model\\_signatures=True*, *log\\_models=True*, *log\\_datasets=True*, *disable=False*, *exclusive=False*, *disable\\_for\\_unsupported\\_versions=False*, *silent=False*, *max\\_tuning\\_runs=5*, *log\\_post\\_training\\_metrics=True*, *serialization\\_format='cloudpickle'*, *registered\\_model\\_name=None*, *pos\\_label=None*, *extra\\_tags=None*)[source] \n\n\nAutologging is known to be compatible with the following package versions: `0.22.1` <= `scikit-learn` <= `1.3.0`. Autologging may not succeed when used with package versions outside of this range.\n\nEnables (or disables) and configures autologging for scikit-learn estimators.\n\n**When is autologging performed?**Autologging is performed when you call:\n\n\n* `estimator.fit()`\n* `estimator.fit\\_predict()`\n* `estimator.fit\\_transform()`\n\n**Logged information**\n**Parameters*** Parameters obtained by `estimator.get\\_params(deep=True)`. Note that `get\\_params`\nis called with `deep=True`. This means when you fit a meta estimator that chains\na series of estimators, the parameters of these child estimators are also logged.\n\n**Training metrics*** A training score obtained by `estimator.score`. Note that the training score is\ncomputed using parameters given to `fit()`.\n* Common metrics for classifier:\n\n\n\t+ precision score\n\t+ recall score\n\t+ f1 score\n\t+ accuracy scoreIf the classifier has method `predict\\_proba`, we additionally log:\n\n\n\t+ log loss\n\t+ roc auc score\n* Common metrics for regressor:\n\n\n\t+ mean squared error\n\t+ root mean squared error\n\t+ mean absolute error\n\t+ r2 score\n\n**Post training metrics**When users call metric APIs after model training, MLflow tries to capture the metric API\nresults and log them as MLflow metrics to the Run associated with the model. The following\ntypes of scikit-learn metric APIs are supported:\n\n\n* model.score\n* metric APIs defined in the sklearn.metrics module\n\n\n* If the metric function is from sklearn.metrics, the MLflow “metric\\_name” is the\nmetric function name. If the metric function is model.score, then “metric\\_name” is\n“{model\\_class\\_name}\\_score”.\n* If multiple calls are made to the same scikit-learn metric API, each subsequent call\nadds a “call\\_index” (starting from 2) to the metric key.\n* MLflow uses the prediction input dataset variable name as the “dataset\\_name” in the\nmetric key. The “prediction input dataset variable” refers to the variable which was\nused as the first argument of the associated model.predict or model.score call.\nNote: MLflow captures the “prediction input dataset” instance in the outermost call\nframe and fetches the variable name in the outermost call frame. If the “prediction\ninput dataset” instance is an intermediate expression without a defined variable\nname, the dataset name is set to “unknown\\_dataset”. If multiple “prediction input\ndataset” instances have the same variable name, then subsequent ones will append an\nindex (starting from 2) to the inspected dataset name.\n\n**Limitations*** MLflow can only map the original prediction result object returned by a model\nprediction API (including predict / predict\\_proba / predict\\_log\\_proba / transform,\nbut excluding fit\\_predict / fit\\_transform.) to an MLflow run.\nMLflow cannot find run information\nfor other objects derived from a given prediction result (e.g. by copying or selecting\na subset of the prediction result). scikit-learn metric APIs invoked on derived objects\ndo not log metrics to MLflow.\n* Autologging must be enabled before scikit-learn metric APIs are imported from\nsklearn.metrics. Metric APIs imported before autologging is enabled do not log\nmetrics to MLflow runs.\n* If user define a scorer which is not based on metric APIs in sklearn.metrics, then\nthen post training metric autologging for the scorer is invalid.\n\n**Tags*** An estimator class name (e.g. “LinearRegression”).\n* A fully qualified estimator class name\n(e.g. “sklearn.linear\\_model.\\_base.LinearRegression”).\n\n**Artifacts*** An MLflow Model with the `mlflow.sklearn` flavor containing a fitted estimator\n(logged by `mlflow.sklearn.log\\_model()`). The Model also contains the\n`mlflow.pyfunc` flavor when the scikit-learn estimator defines predict().\n* For post training metrics API calls, a “metric\\_info.json” artifact is logged. This is a\nJSON object whose keys are MLflow post training metric names\n(see “Post training metrics” section for the key format) and whose values are the\ncorresponding metric call commands that produced the metrics, e.g.\n`accuracy\\_score(y\\_true=test\\_iris\\_y, y\\_pred=pred\\_iris\\_y, normalize=False)`.\n\n**How does autologging work for meta estimators?**When a meta estimator (e.g. Pipeline, GridSearchCV) calls `fit()`, it internally calls\n`fit()` on its child estimators. Autologging does NOT perform logging on these constituent\n`fit()` calls.\n\n**Parameter search**In addition to recording the information discussed above, autologging for parameter\nsearch meta estimators (GridSearchCV and RandomizedSearchCV) records child runs\nwith metrics for each set of explored parameters, as well as artifacts and parameters\nfor the best model (if available).\n\n**Supported estimators*** All estimators obtained by sklearn.utils.all\\_estimators (including meta estimators).\n* Pipeline\n* Parameter search estimators (GridSearchCV and RandomizedSearchCV)\n\n\n**Example**\n\n\nSee more examples\n\n```\nfrom pprint import pprint\nimport numpy as np\nfrom sklearn.linear\\_model import LinearRegression\nimport mlflow\nfrom mlflow import MlflowClient\n\n\ndef fetch\\_logged\\_data(run\\_id):\n    client = MlflowClient()\n    data = client.get\\_run(run\\_id).data\n    tags = {k: v for k, v in data.tags.items() if not k.startswith(\"mlflow.\")}\n    artifacts = [f.path for f in client.list\\_artifacts(run\\_id, \"model\")]\n    return data.params, data.metrics, tags, artifacts\n\n\n# enable autologging\nmlflow.sklearn.autolog()\n\n\n# prepare training data\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\ny = np.dot(X, np.array([1, 2])) + 3\n\n\n# train a model\nmodel = LinearRegression()\nwith mlflow.start\\_run() as run:\n    model.fit(X, y)\n\n\n# fetch logged data\nparams, metrics, tags, artifacts = fetch\\_logged\\_data(run.info.run\\_id)\n\npprint(params)\n\n# {'copy\\_X': 'True',\n\n# 'fit\\_intercept': 'True',\n\n# 'n\\_jobs': 'None',\n\n# 'normalize': 'False'}\n\npprint(metrics)\n\n# {'training\\_score': 1.0,\n\n# 'training\\_mean\\_absolute\\_error': 2.220446049250313e-16,\n\n# 'training\\_mean\\_squared\\_error': 1.9721522630525295e-31,\n\n# 'training\\_r2\\_score': 1.0,\n\n# 'training\\_root\\_mean\\_squared\\_error': 4.440892098500626e-16}\n\npprint(tags)\n\n# {'estimator\\_class': 'sklearn.linear\\_model.\\_base.LinearRegression',\n\n# 'estimator\\_name': 'LinearRegression'}\n\npprint(artifacts)\n# ['model/MLmodel', 'model/conda.yaml', 'model/model.pkl']\n\nParameters\n* **log\\_input\\_examples** – If `True`, input examples from training datasets are collected and\nlogged along with scikit-learn model artifacts during training. If\n`False`, input examples are not logged.\nNote: Input examples are MLflow model attributes\nand are only collected if `log\\_models`\n\n==================\n Document 2 \n----------------\n mlflow.lightgbm\n\n\nThe `mlflow.lightgbm` module provides an API for logging and loading LightGBM models.\nThis module exports LightGBM models with the following flavors:\n\nLightGBM (native) formatThis is the main flavor that can be loaded back into LightGBM.\n\n\n`mlflow.lightgbm.``autolog`(*log\\_input\\_examples=False*, *log\\_model\\_signatures=True*, *log\\_models=True*, *log\\_datasets=True*, *disable=False*, *exclusive=False*, *disable\\_for\\_unsupported\\_versions=False*, *silent=False*, *registered\\_model\\_name=None*, *extra\\_tags=None*)[source] \n\n\nAutologging is known to be compatible with the following package versions: `3.1.1` <= `lightgbm` <= `4.1.0`. Autologging may not succeed when used with package versions outside of this range.\n\nEnables (or disables) and configures autologging from LightGBM to MLflow. Logs the following:\n\n\n* parameters specified in lightgbm.train.\n* metrics on each iteration (if `valid\\_sets` specified).\n* metrics at the best iteration (if `early\\_stopping\\_rounds` specified or `early\\_stopping`callback is set).\n* feature importance (both “split” and “gain”) as JSON files and plots.\n* trained model, including:\n\t+ an example of valid input.\n\t+ inferred signature of the inputs and outputs of the model.\n\n\nNote that the scikit-learn API is now supported.\n\nParameters\n* **log\\_input\\_examples** – If `True`, input examples from training datasets are collected and\nlogged along with LightGBM model artifacts during training. If\n`False`, input examples are not logged.\nNote: Input examples are MLflow model attributes\nand are only collected if `log\\_models` is also `True`.\n* **log\\_model\\_signatures** – If `True`,\n`ModelSignatures`\ndescribing model inputs and outputs are collected and logged along\nwith LightGBM model artifacts during training. If `False`,\nsignatures are not logged.\nNote: Model signatures are MLflow model attributes\nand are only collected if `log\\_models` is also `True`.\n* **log\\_models** – If `True`, trained models are logged as MLflow model artifacts.\nIf `False`, trained models are not logged.\nInput examples and model signatures, which are attributes of MLflow models,\nare also omitted when `log\\_models` is `False`.\n* **log\\_datasets** – If `True`, train and validation dataset information is logged to MLflow\nTracking if applicable. If `False`, dataset information is not logged.\n* **disable** – If `True`, disables the LightGBM autologging integration. If `False`,\nenables the LightGBM autologging integration.\n* **exclusive** – If `True`, autologged content is not logged to user-created fluent runs.\nIf `False`, autologged content is logged to the active fluent run,\nwhich may be user-created.\n* **disable\\_for\\_unsupported\\_versions** – If `True`, disable autologging for versions of\nlightgbm that have not been tested against this version of the MLflow client\nor are incompatible.\n* **silent** – If `True`, suppress all event logs and warnings from MLflow during LightGBM\nautologging. If `False`, show all events and warnings during LightGBM\nautologging.\n* **registered\\_model\\_name** – If given, each time a model is trained, it is registered as a\nnew model version of the registered model with this name.\nThe registered model is created if it does not already exist.\n* **extra\\_tags** – A dictionary of extra tags to set on each managed run created by autologging.\n\n```\nimport mlflow\nfrom lightgbm import LGBMClassifier\nfrom sklearn import datasets\n\n\ndef print\\_auto\\_logged\\_info(run):\n    tags = {k: v for k, v in run.data.tags.items() if not k.startswith(\"mlflow.\")}\n    artifacts = [\n        f.path for f in mlflow.MlflowClient().list\\_artifacts(run.info.run\\_id, \"model\")\n    ]\n    feature\\_importances = [\n        f.path\n        for f in mlflow.MlflowClient().list\\_artifacts(run.info.run\\_id)\n        if f.path != \"model\"\n    ]\n    print(f\"run\\_id: {run.info.run\\_id}\")\n    print(f\"artifacts: {artifacts}\")\n    print(f\"feature\\_importances: {feature\\_importances}\")\n    print(f\"params: {run.data.params}\")\n    print(f\"metrics: {run.data.metrics}\")\n    print(f\"tags: {tags}\")\n\n\n# Load iris dataset\nX, y = datasets.load\\_iris(return\\_X\\_y=True, as\\_frame=True)\n\n\n# Initialize our model\nmodel = LGBMClassifier(objective=\"multiclass\", random\\_state=42)\n\n\n# Auto log all MLflow entities\nmlflow.lightgbm.autolog()\n\n\n# Train the model\nwith mlflow.start\\_run() as run:\n    model.fit(X, y)\n\n\n# fetch the auto logged parameters and metrics\nprint\\_auto\\_logged\\_info(mlflow.get\\_run(run\\_id=run.info.run\\_id))\n\n```\nrun_id: e08dd59d57a74971b68cf78a724dfaf6\nartifacts: ['model/MLmodel',\n            'model/conda.yaml',\n            'model/model.pkl',\n            'model/python_env.yaml',\n            'model/requirements.txt']\nfeature_importances: ['feature_importance_gain.json',\n                      'feature_importance_gain.png',\n                      'feature_importance_split.json',\n                      'feature_importance_split.png']\nparams: {'boosting_type': 'gbdt',\n         'categorical_feature': 'auto',\n         'colsample_bytree': '1.0',\n         ...\n         'verbose_eval': 'warn'}\nmetrics: {}\ntags: {}\n\n\n`mlflow.lightgbm.``get\\_default\\_conda\\_env`(*include\\_cloudpickle=False*)[source] \n\n`mlflow.lightgbm.``get\\_default\\_pip\\_requirements`(*include\\_cloudpickle=False*)[source] \n\n`mlflow.lightgbm.``load\\_model`(*model\\_uri*, *dst\\_path=None*)[source] \nLoad a LightGBM model from a local file or a run.\n\nReturns\nA LightGBM model (an instance of lightgbm.Booster) or a LightGBM scikit-learn\nmodel, depending on the saved model class specification.\n\n```\nfrom lightgbm import LGBMClassifier\nfrom sklearn import datasets\nimport mlflow\n\n\n# Load iris dataset\nX, y = datasets.load\\_iris(return\\_X\\_y=True, as\\_frame=True)\n\n\n# Train the model\nmodel.fit(X, y)\n\n# Load model for inference\nmodel\\_uri = f\"runs:/{mlflow.last\\_active\\_run().info.run\\_id}/model\"\nloaded\\_model = mlflow.lightgbm.load\\_model(model\\_uri)\nprint(loaded\\_model.predict(X[:5]))\n\n```\n[0 0 0 0 0]\n\n\n`mlflow.lightgbm.``log\\_model`(*lgb\\_model*, *artifact\\_path*, *conda\\_env=None*, *code\\_paths=None*, *registered\\_model\\_name=None*, *signature: mlflow.models.signature.ModelSignature = None*, *input\\_example: Union[pandas.core.frame.DataFrame, numpy.ndarray, dict, list, csr\\_matrix, csc\\_matrix, str, bytes] = None*, *await\\_registration\\_for=300*, *pip\\_requirements=None*, *extra\\_pip\\_requirements=None*, *metadata=None*, *\\*\\*kwargs*)[source] \nLog a LightGBM\n\n==================\n Document 3 \n----------------\n Python API\n\n\nThe MLflow Python API is organized into the following modules. The most common functions are\nexposed in the `mlflow` module, so we recommend starting there.\n\n* mlflow\n* mlflow.artifacts\n* mlflow.catboost\n* mlflow.client\n* mlflow.data\n* mlflow.deployments\n* mlflow.diviner\n* mlflow.entities\n* mlflow.environment\\_variables\n* mlflow.fastai\n* mlflow.gateway\n* mlflow.gluon\n* mlflow.h2o\n* mlflow.johnsnowlabs\n* mlflow.langchain\n* mlflow.lightgbm\n* mlflow.llm\n* mlflow.mleap\n* mlflow.models\n* mlflow.onnx\n* mlflow.paddle\n* mlflow.pmdarima\n* mlflow.projects\n* mlflow.prophet\n* mlflow.pyfunc\n* mlflow.pyspark.ml\n* mlflow.pytorch\n* mlflow.recipes\n* mlflow.sagemaker\n* mlflow.sentence\\_transformers\n* mlflow.server\n* mlflow.shap\n* mlflow.sklearn\n* mlflow.spacy\n* mlflow.spark\n* mlflow.statsmodels\n* mlflow.tensorflow\n* mlflow.transformers\n* mlflow.types\n* mlflow.xgboost\n* mlflow.openai\n\nSee also the index of all functions and classes.\n\n## Log Levels\n\n\nMLflow Python APIs log information during execution using the Python Logging API. You can\nconfigure the log level for MLflow logs using the following code snippet. Learn more about Python\nlog levels at the\nPython language logging guide.\n\n```\nimport logging\n\nlogger = logging.getLogger(\"mlflow\")\n\n\n# Set log level to debugging\nlogger.setLevel(logging.DEBUG)\n\n```\n\n\n# mlflow\n\n\nThe `mlflow` module provides a high-level “fluent” API for starting and managing MLflow runs.\nFor example:\n\n```\nimport mlflow\n\nmlflow.start\\_run()\nmlflow.log\\_param(\"my\", \"param\")\nmlflow.log\\_metric(\"score\", 100)\nmlflow.end\\_run()\n\n\nYou can also use the context manager syntax like this:\n\n```\nwith mlflow.start\\_run() as run:\n    mlflow.log\\_param(\"my\", \"param\")\n    mlflow.log\\_metric(\"score\","
    },
    {
        "instruction": "Create a program that uses the MLflow API to train a simple decision tree classifier on the Iris dataset. Log the model, its hyperparameters, and accuracy as a metric. Save the model as an artifact.",
        "api": "mlflow",
        "output": "#!pip install mlflow\nimport mlflow\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load the Iris dataset\ndata = load_iris()\nX, y = data.data, data.target\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Start an MLflow run\nwith mlflow.start_run() as run:\n    # Train a Decision Tree classifier\n    clf = DecisionTreeClassifier()\n    clf.fit(X_train, y_train)\n    \n    # Log hyperparameters\n    mlflow.log_param(\"criterion\", clf.criterion)\n    mlflow.log_param(\"max_depth\", clf.max_depth)\n    \n    # Make predictions on the test set\n    y_pred = clf.predict(X_test)\n    \n    # Calculate and log accuracy as a metric\n    accuracy = accuracy_score(y_test, y_pred)\n    mlflow.log_metric(\"accuracy\", accuracy)\n    \n    # Save the model as an artifact\n    mlflow.sklearn.log_model(clf, \"decision_tree_model\")",
        "documentation": "\n\n==================\n Document 0 \n----------------\n mlflow.sklearn\n\n\nThe `mlflow.sklearn` module provides an API for logging and loading scikit-learn models. This\nmodule exports scikit-learn models with the following flavors:\n\nPython (native) pickle formatThis is the main flavor that can be loaded back into scikit-learn.\n\n`mlflow.pyfunc`Produced for use by generic pyfunc-based deployment tools and batch inference.\nNOTE: The mlflow.pyfunc flavor is only added for scikit-learn models that define predict(),\nsince predict() is required for pyfunc model inference.\n\n\n`mlflow.sklearn.``autolog`(*log\\_input\\_examples=False*, *log\\_model\\_signatures=True*, *log\\_models=True*, *log\\_datasets=True*, *disable=False*, *exclusive=False*, *disable\\_for\\_unsupported\\_versions=False*, *silent=False*, *max\\_tuning\\_runs=5*, *log\\_post\\_training\\_metrics=True*, *serialization\\_format='cloudpickle'*, *registered\\_model\\_name=None*, *pos\\_label=None*, *extra\\_tags=None*)[source] \n\n\nAutologging is known to be compatible with the following package versions: `0.22.1` <= `scikit-learn` <= `1.3.0`. Autologging may not succeed when used with package versions outside of this range.\n\nEnables (or disables) and configures autologging for scikit-learn estimators.\n\n**When is autologging performed?**Autologging is performed when you call:\n\n\n* `estimator.fit()`\n* `estimator.fit\\_predict()`\n* `estimator.fit\\_transform()`\n\n**Logged information**\n**Parameters*** Parameters obtained by `estimator.get\\_params(deep=True)`. Note that `get\\_params`\nis called with `deep=True`. This means when you fit a meta estimator that chains\na series of estimators, the parameters of these child estimators are also logged.\n\n**Training metrics*** A training score obtained by `estimator.score`. Note that the training score is\ncomputed using parameters given to `fit()`.\n* Common metrics for classifier:\n\n\n\t+ precision score\n\t+ recall score\n\t+ f1 score\n\t+ accuracy scoreIf the classifier has method `predict\\_proba`, we additionally log:\n\n\n\t+ log loss\n\t+ roc auc score\n* Common metrics for regressor:\n\n\n\t+ mean squared error\n\t+ root mean squared error\n\t+ mean absolute error\n\t+ r2 score\n\n**Post training metrics**When users call metric APIs after model training, MLflow tries to capture the metric API\nresults and log them as MLflow metrics to the Run associated with the model. The following\ntypes of scikit-learn metric APIs are supported:\n\n\n* model.score\n* metric APIs defined in the sklearn.metrics module\n\n\n* If the metric function is from sklearn.metrics, the MLflow “metric\\_name” is the\nmetric function name. If the metric function is model.score, then “metric\\_name” is\n“{model\\_class\\_name}\\_score”.\n* If multiple calls are made to the same scikit-learn metric API, each subsequent call\nadds a “call\\_index” (starting from 2) to the metric key.\n* MLflow uses the prediction input dataset variable name as the “dataset\\_name” in the\nmetric key. The “prediction input dataset variable” refers to the variable which was\nused as the first argument of the associated model.predict or model.score call.\nNote: MLflow captures the “prediction input dataset” instance in the outermost call\nframe and fetches the variable name in the outermost call frame. If the “prediction\ninput dataset” instance is an intermediate expression without a defined variable\nname, the dataset name is set to “unknown\\_dataset”. If multiple “prediction input\ndataset” instances have the same variable name, then subsequent ones will append an\nindex (starting from 2) to the inspected dataset name.\n\n**Limitations*** MLflow can only map the original prediction result object returned by a model\nprediction API (including predict / predict\\_proba / predict\\_log\\_proba / transform,\nbut excluding fit\\_predict / fit\\_transform.) to an MLflow run.\nMLflow cannot find run information\nfor other objects derived from a given prediction result (e.g. by copying or selecting\na subset of the prediction result). scikit-learn metric APIs invoked on derived objects\ndo not log metrics to MLflow.\n* Autologging must be enabled before scikit-learn metric APIs are imported from\nsklearn.metrics. Metric APIs imported before autologging is enabled do not log\nmetrics to MLflow runs.\n* If user define a scorer which is not based on metric APIs in sklearn.metrics, then\nthen post training metric autologging for the scorer is invalid.\n\n**Tags*** An estimator class name (e.g. “LinearRegression”).\n* A fully qualified estimator class name\n(e.g. “sklearn.linear\\_model.\\_base.LinearRegression”).\n\n**Artifacts*** An MLflow Model with the `mlflow.sklearn` flavor containing a fitted estimator\n(logged by `mlflow.sklearn.log\\_model()`). The Model also contains the\n`mlflow.pyfunc` flavor when the scikit-learn estimator defines predict().\n* For post training metrics API calls, a “metric\\_info.json” artifact is logged. This is a\nJSON object whose keys are MLflow post training metric names\n(see “Post training metrics” section for the key format) and whose values are the\ncorresponding metric call commands that produced the metrics, e.g.\n`accuracy\\_score(y\\_true=test\\_iris\\_y, y\\_pred=pred\\_iris\\_y, normalize=False)`.\n\n**How does autologging work for meta estimators?**When a meta estimator (e.g. Pipeline, GridSearchCV) calls `fit()`, it internally calls\n`fit()` on its child estimators. Autologging does NOT perform logging on these constituent\n`fit()` calls.\n\n**Parameter search**In addition to recording the information discussed above, autologging for parameter\nsearch meta estimators (GridSearchCV and RandomizedSearchCV) records child runs\nwith metrics for each set of explored parameters, as well as artifacts and parameters\nfor the best model (if available).\n\n**Supported estimators*** All estimators obtained by sklearn.utils.all\\_estimators (including meta estimators).\n* Pipeline\n* Parameter search estimators (GridSearchCV and RandomizedSearchCV)\n\n\n**Example**\n\n\nSee more examples\n\n```\nfrom pprint import pprint\nimport numpy as np\nfrom sklearn.linear\\_model import LinearRegression\nimport mlflow\nfrom mlflow import MlflowClient\n\n\ndef fetch\\_logged\\_data(run\\_id):\n    client = MlflowClient()\n    data = client.get\\_run(run\\_id).data\n    tags = {k: v for k, v in data.tags.items() if not k.startswith(\"mlflow.\")}\n    artifacts = [f.path for f in client.list\\_artifacts(run\\_id, \"model\")]\n    return data.params, data.metrics, tags, artifacts\n\n\n# enable autologging\nmlflow.sklearn.autolog()\n\n\n# prepare training data\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\ny = np.dot(X, np.array([1, 2])) + 3\n\n\n# train a model\nmodel = LinearRegression()\nwith mlflow.start\\_run() as run:\n    model.fit(X, y)\n\n\n# fetch logged data\nparams, metrics, tags, artifacts = fetch\\_logged\\_data(run.info.run\\_id)\n\npprint(params)\n\n# {'copy\\_X': 'True',\n\n# 'fit\\_intercept': 'True',\n\n# 'n\\_jobs': 'None',\n\n# 'normalize': 'False'}\n\npprint(metrics)\n\n# {'training\\_score': 1.0,\n\n# 'training\\_mean\\_absolute\\_error': 2.220446049250313e-16,\n\n# 'training\\_mean\\_squared\\_error': 1.9721522630525295e-31,\n\n# 'training\\_r2\\_score': 1.0,\n\n# 'training\\_root\\_mean\\_squared\\_error': 4.440892098500626e-16}\n\npprint(tags)\n\n# {'estimator\\_class': 'sklearn.linear\\_model.\\_base.LinearRegression',\n\n# 'estimator\\_name': 'LinearRegression'}\n\npprint(artifacts)\n# ['model/MLmodel', 'model/conda.yaml', 'model/model.pkl']\n\nParameters\n* **log\\_input\\_examples** – If `True`, input examples from training datasets are collected and\nlogged along with scikit-learn model artifacts during training. If\n`False`, input examples are not logged.\nNote: Input examples are MLflow model attributes\nand are only collected if `log\\_models`\n\n==================\n Document 1 \n----------------\n mlflow.lightgbm\n\n\nThe `mlflow.lightgbm` module provides an API for logging and loading LightGBM models.\nThis module exports LightGBM models with the following flavors:\n\nLightGBM (native) formatThis is the main flavor that can be loaded back into LightGBM.\n\n\n`mlflow.lightgbm.``autolog`(*log\\_input\\_examples=False*, *log\\_model\\_signatures=True*, *log\\_models=True*, *log\\_datasets=True*, *disable=False*, *exclusive=False*, *disable\\_for\\_unsupported\\_versions=False*, *silent=False*, *registered\\_model\\_name=None*, *extra\\_tags=None*)[source] \n\n\nAutologging is known to be compatible with the following package versions: `3.1.1` <= `lightgbm` <= `4.1.0`. Autologging may not succeed when used with package versions outside of this range.\n\nEnables (or disables) and configures autologging from LightGBM to MLflow. Logs the following:\n\n\n* parameters specified in lightgbm.train.\n* metrics on each iteration (if `valid\\_sets` specified).\n* metrics at the best iteration (if `early\\_stopping\\_rounds` specified or `early\\_stopping`callback is set).\n* feature importance (both “split” and “gain”) as JSON files and plots.\n* trained model, including:\n\t+ an example of valid input.\n\t+ inferred signature of the inputs and outputs of the model.\n\n\nNote that the scikit-learn API is now supported.\n\nParameters\n* **log\\_input\\_examples** – If `True`, input examples from training datasets are collected and\nlogged along with LightGBM model artifacts during training. If\n`False`, input examples are not logged.\nNote: Input examples are MLflow model attributes\nand are only collected if `log\\_models` is also `True`.\n* **log\\_model\\_signatures** – If `True`,\n`ModelSignatures`\ndescribing model inputs and outputs are collected and logged along\nwith LightGBM model artifacts during training. If `False`,\nsignatures are not logged.\nNote: Model signatures are MLflow model attributes\nand are only collected if `log\\_models` is also `True`.\n* **log\\_models** – If `True`, trained models are logged as MLflow model artifacts.\nIf `False`, trained models are not logged.\nInput examples and model signatures, which are attributes of MLflow models,\nare also omitted when `log\\_models` is `False`.\n* **log\\_datasets** – If `True`, train and validation dataset information is logged to MLflow\nTracking if applicable. If `False`, dataset information is not logged.\n* **disable** – If `True`, disables the LightGBM autologging integration. If `False`,\nenables the LightGBM autologging integration.\n* **exclusive** – If `True`, autologged content is not logged to user-created fluent runs.\nIf `False`, autologged content is logged to the active fluent run,\nwhich may be user-created.\n* **disable\\_for\\_unsupported\\_versions** – If `True`, disable autologging for versions of\nlightgbm that have not been tested against this version of the MLflow client\nor are incompatible.\n* **silent** – If `True`, suppress all event logs and warnings from MLflow during LightGBM\nautologging. If `False`, show all events and warnings during LightGBM\nautologging.\n* **registered\\_model\\_name** – If given, each time a model is trained, it is registered as a\nnew model version of the registered model with this name.\nThe registered model is created if it does not already exist.\n* **extra\\_tags** – A dictionary of extra tags to set on each managed run created by autologging.\n\n```\nimport mlflow\nfrom lightgbm import LGBMClassifier\nfrom sklearn import datasets\n\n\ndef print\\_auto\\_logged\\_info(run):\n    tags = {k: v for k, v in run.data.tags.items() if not k.startswith(\"mlflow.\")}\n    artifacts = [\n        f.path for f in mlflow.MlflowClient().list\\_artifacts(run.info.run\\_id, \"model\")\n    ]\n    feature\\_importances = [\n        f.path\n        for f in mlflow.MlflowClient().list\\_artifacts(run.info.run\\_id)\n        if f.path != \"model\"\n    ]\n    print(f\"run\\_id: {run.info.run\\_id}\")\n    print(f\"artifacts: {artifacts}\")\n    print(f\"feature\\_importances: {feature\\_importances}\")\n    print(f\"params: {run.data.params}\")\n    print(f\"metrics: {run.data.metrics}\")\n    print(f\"tags: {tags}\")\n\n\n# Load iris dataset\nX, y = datasets.load\\_iris(return\\_X\\_y=True, as\\_frame=True)\n\n\n# Initialize our model\nmodel = LGBMClassifier(objective=\"multiclass\", random\\_state=42)\n\n\n# Auto log all MLflow entities\nmlflow.lightgbm.autolog()\n\n\n# Train the model\nwith mlflow.start\\_run() as run:\n    model.fit(X, y)\n\n\n# fetch the auto logged parameters and metrics\nprint\\_auto\\_logged\\_info(mlflow.get\\_run(run\\_id=run.info.run\\_id))\n\n```\nrun_id: e08dd59d57a74971b68cf78a724dfaf6\nartifacts: ['model/MLmodel',\n            'model/conda.yaml',\n            'model/model.pkl',\n            'model/python_env.yaml',\n            'model/requirements.txt']\nfeature_importances: ['feature_importance_gain.json',\n                      'feature_importance_gain.png',\n                      'feature_importance_split.json',\n                      'feature_importance_split.png']\nparams: {'boosting_type': 'gbdt',\n         'categorical_feature': 'auto',\n         'colsample_bytree': '1.0',\n         ...\n         'verbose_eval': 'warn'}\nmetrics: {}\ntags: {}\n\n\n`mlflow.lightgbm.``get\\_default\\_conda\\_env`(*include\\_cloudpickle=False*)[source] \n\n`mlflow.lightgbm.``get\\_default\\_pip\\_requirements`(*include\\_cloudpickle=False*)[source] \n\n`mlflow.lightgbm.``load\\_model`(*model\\_uri*, *dst\\_path=None*)[source] \nLoad a LightGBM model from a local file or a run.\n\nReturns\nA LightGBM model (an instance of lightgbm.Booster) or a LightGBM scikit-learn\nmodel, depending on the saved model class specification.\n\n```\nfrom lightgbm import LGBMClassifier\nfrom sklearn import datasets\nimport mlflow\n\n\n# Load iris dataset\nX, y = datasets.load\\_iris(return\\_X\\_y=True, as\\_frame=True)\n\n\n# Train the model\nmodel.fit(X, y)\n\n# Load model for inference\nmodel\\_uri = f\"runs:/{mlflow.last\\_active\\_run().info.run\\_id}/model\"\nloaded\\_model = mlflow.lightgbm.load\\_model(model\\_uri)\nprint(loaded\\_model.predict(X[:5]))\n\n```\n[0 0 0 0 0]\n\n\n`mlflow.lightgbm.``log\\_model`(*lgb\\_model*, *artifact\\_path*, *conda\\_env=None*, *code\\_paths=None*, *registered\\_model\\_name=None*, *signature: mlflow.models.signature.ModelSignature = None*, *input\\_example: Union[pandas.core.frame.DataFrame, numpy.ndarray, dict, list, csr\\_matrix, csc\\_matrix, str, bytes] = None*, *await\\_registration\\_for=300*, *pip\\_requirements=None*, *extra\\_pip\\_requirements=None*, *metadata=None*, *\\*\\*kwargs*)[source] \nLog a LightGBM\n\n==================\n Document 2 \n----------------\n demonstrating autolog capabilities.\n\nimport fastai.vision as vis\nimport mlflow.fastai\nfrom mlflow import MlflowClient\n\n\ndef main(epochs=5, learning\\_rate=0.01):\n    # Download and untar the MNIST data set\n    path = vis.untar\\_data(vis.URLs.MNIST\\_SAMPLE)\n\n    # Prepare, transform, and normalize the data\n    data = vis.ImageDataBunch.from\\_folder(\n        path, ds\\_tfms=(vis.rand\\_pad(2, 28), []), bs=64\n    )\n    data.normalize(vis.imagenet\\_stats)\n\n    # Create CNN the Learner model\n    model = vis.cnn\\_learner(data, vis.models.resnet18, metrics=vis.accuracy)\n\n    # Enable auto logging\n    mlflow.fastai.autolog()\n\n    # Start MLflow session\n    with mlflow.start\\_run() as run:\n        model.fit(epochs, learning\\_rate)\n\n    # fetch the auto logged parameters, metrics, and artifacts\n    print\\_auto\\_logged\\_info(mlflow.get\\_run(run\\_id=run.info.run\\_id))\n\n\nmain()\n\n\noutput \n\n```\nrun_id: 5a23dcbcaa334637814dbce7a00b2f6a\nartifacts: ['model/MLmodel', 'model/conda.yaml', 'model/model.fastai']\nparams: {'wd': 'None',\n         'bn_wd': 'True',\n         'opt_func': 'Adam',\n         'epochs': '5', '\n         train_bn': 'True',\n         'num_layers': '60',\n         'lr': '0.01',\n         'true_wd': 'True'}\nmetrics: {'train_loss': 0.024,\n          'accuracy': 0.99214,\n          'valid_loss': 0.021}\n# Tags model summary omitted too long\ntags: {...}\n\n\n\nFastai autologged MLflow entities \n\n\n`mlflow.fastai.``get\\_default\\_conda\\_env`(*include\\_cloudpickle=False*)[source] \n\n`mlflow.fastai.``get\\_default\\_pip\\_requirements`(*include\\_cloudpickle=False*)[source] \n\n`mlflow.fastai.``load\\_model`(*model\\_uri*, *dst\\_path=None*)[source] \nLoad a fastai model from a local file or a run.\n\nReturns\nA fastai model (an instance of fastai.Learner).\n\n```\nimport mlflow.fastai\n\n\n# Define the Learner model\nmodel = ...\n\n\n# log the fastai Leaner model\nwith mlflow.start\\_run() as run:\n    model.fit(epochs, learning\\_rate)\n    mlflow.fastai.log\\_model(model, \"model\")\n\n# Load the model for scoring\nmodel\\_uri = f\"runs:/{run.info.run\\_id}/model\"\nloaded\\_model = mlflow.fastai.load\\_model(model\\_uri)\nresults = loaded\\_model.predict(predict\\_data)\n\n\n`mlflow.fastai.``log\\_model`(*fastai\\_learner*, *artifact\\_path*, *conda\\_env=None*, *code\\_paths=None*, *registered\\_model\\_name=None*, *signature: mlflow.models.signature.ModelSignature = None*, *input\\_example: Union[pandas.core.frame.DataFrame, numpy.ndarray, dict, list, csr\\_matrix, csc\\_matrix, str, bytes] = None*, *await\\_registration\\_for=300*, *pip\\_requirements=None*, *extra\\_pip\\_requirements=None*, *metadata=None*, *\\*\\*kwargs*)[source] \nLog a fastai model\n\n==================\n Document 3 \n----------------\n## Workflows\n\n\n`save\\_model()` and `log\\_model()` support the following workflows:\n\n\n1. Programmatically defining a new MLflow model, including its attributes and artifacts.\n\n\nGiven a set of artifact URIs, `save\\_model()` and `log\\_model()` can\nautomatically download artifacts from their URIs and create an MLflow model directory.\n\n\nIn this case, you must define a Python class which inherits from `PythonModel`,\ndefining `predict()` and, optionally, `load\\_context()`. An instance of this class is\nspecified via the `python\\_model` parameter; it is automatically serialized and deserialized\nas a Python class, including all of its attributes.\n2. Interpreting pre-existing data as an MLflow model.\n\n\nIf you already have a directory containing model data, `save\\_model()` and\n`log\\_model()` can import the data as an MLflow model. The `data\\_path` parameter\nspecifies the local filesystem path to the directory containing model data.\n\n\nIn this case, you must provide a Python module, called a loader module. The\nloader module defines a `\\_load\\_pyfunc()` method that performs the following tasks:\n\n\n\t* Load data from the specified `data\\_path`. For example, this process may include\n\tdeserializing pickled Python objects or models or parsing CSV files.\n\t* Construct and return a pyfunc-compatible model wrapper. As in the first\n\tuse case, this wrapper must define a `predict()` method that is used to evaluate\n\tqueries. `predict()` must adhere to the Inference API.The `loader\\_module` parameter specifies the name of your loader module.\n\n\nFor an example loader module implementation, refer to the loader module\nimplementation in mlflow.sklearn.\n\n### Which workflow is right for my use case?\n\n\nWe consider the first workflow to be more user-friendly and generally recommend it for the\nfollowing reasons:\n\n\n* It automatically resolves and collects specified model artifacts.\n* It automatically serializes and deserializes the `python\\_model` instance\n\n==================\n Document 4 \n----------------\n# Creating custom Pyfunc models\n\n\nMLflow’s persistence modules provide convenience functions for creating models with the\n`pyfunc` flavor in a variety of machine learning frameworks (scikit-learn, Keras, Pytorch, and\nmore); however, they do not cover every use case. For example, you may want to create an MLflow\nmodel with the `pyfunc` flavor using a framework that MLflow does not natively support.\nAlternatively, you may want to build an MLflow model that executes custom logic when evaluating\nqueries, such as preprocessing and postprocessing routines. Therefore, `mlflow.pyfunc`\nprovides utilities for creating `pyfunc` models from arbitrary code and model data.\n\n\nThe `save\\_model()` and `log\\_model()` methods are designed to support multiple workflows\nfor creating custom `pyfunc` models that incorporate custom inference logic and artifacts\nthat the logic may require.\n\n\nAn artifact is a file or directory, such as a serialized model or a CSV. For example, a\nserialized TensorFlow graph is an artifact. An MLflow model directory is also an artifact.\n### Workflows\n\n\n`save\\_model()` and `log\\_model()` support the following workflows:\n\n\n1. Programmatically defining a new MLflow model, including its attributes and artifacts.\n\n\nGiven a set of artifact URIs, `save\\_model()` and `log\\_model()` can\nautomatically download artifacts from their URIs and create an MLflow model directory.\n\n\nIn this\n\n==================\n Document 5 \n----------------\n use Pandas DataFrame to make predictions\nnp\\_array = ...\npredictions = pd\\_model(np\\_array)\n\n\n`mlflow.paddle.``log\\_model`(*pd\\_model*, *artifact\\_path*, *training=False*, *conda\\_env=None*, *code\\_paths=None*, *registered\\_model\\_name=None*, *signature: mlflow.models.signature.ModelSignature = None*, *input\\_example: Union[pandas.core.frame.DataFrame, numpy.ndarray, dict, list, csr\\_matrix, csc\\_matrix, str, bytes] = None*, *await\\_registration\\_for=300*, *pip\\_requirements=None*, *extra\\_pip\\_requirements=None*, *metadata=None*)[source] \nLog a paddle model as an MLflow artifact for the current run. Produces an MLflow Model\ncontaining the following flavors:\n\n> \n> * `mlflow.paddle`\n> * `mlflow.pyfunc`. NOTE: This flavor is only included for paddle models\n> that define predict(), since predict() is required for pyfunc model inference.\n> \n> \n> \n\nParameters\n* **pd\\_model** – paddle model to be saved.\n* **artifact\\_path** – Run-relative artifact path.\n* **training** – Only valid when saving a model trained using the PaddlePaddle high level API.\nIf set to True, the saved model supports both re-training and\ninference. If set to False, it only supports inference.\n* **conda\\_env** – Either a dictionary representation of a Conda environment or the path to a conda environment yaml\nfile. If provided, this describes the environment this model should be run in. At minimum, it\nshould specify the dependencies contained in `get\\_default\\_conda\\_env()`. If `None`, a conda\nenvironment with pip requirements inferred by `mlflow.models.infer\\_pip\\_requirements()` is added\nto the model. If the requirement inference fails, it falls back to using\n`get\\_default\\_pip\\_requirements()`. pip requirements from `conda\\_env` are written to a pip\n`requirements.txt` file and the full conda environment is written to `conda.yaml`.\nThe following is an *example* dictionary representation of a conda environment:\n\n```\n{\n    \"name\": \"mlflow-env\",\n    \"channels\": [\"conda-forge\"],\n    \"dependencies\": [\n        \"python=3.8.15\",\n        {\n            \"pip\": [\n                \"paddle==x.y.z\"\n            ],\n        },\n    ],\n}\n\n```\n* **input\\_example** – one or several instances of valid model input. The input example is used\nas a hint of what data to feed the model. It will be converted to a Pandas\nDataFrame and then serialized to json using the Pandas split-oriented\nformat, or a numpy array where the example will be serialized to json\nby converting it to a list. Bytes are base64-encoded. When the `signature` parameter is\n`None`, the input example is used to infer a model signature.\n* **await\\_registration\\_for** – Number of seconds to wait for the model version to finish\nbeing created and is in `READY` status. By default, the function\nwaits for five minutes. Specify 0 or None to skip waiting.\n* **pip\\_requirements** – Either an iterable of pip requirement strings\n(e.g. `[\"paddle\", \"-r requirements.txt\", \"-c constraints.txt\"]`) or the string path to\na pip requirements file on the local filesystem (e.g. `\"requirements.txt\"`). If provided, this\ndescribes the environment this model should be run in. If `None`, a default list of requirements\nis inferred by `mlflow.models.infer\\_pip\\_requirements()` from the current software environment.\nIf the requirement inference fails, it falls back to using `get\\_default\\_pip\\_requirements()`.\nBoth requirements and constraints are automatically parsed and written to `requirements.txt` and\n`constraints.txt` files, respectively, and stored as part of the model. Requirements are also\nwritten to the `pip` section of the model’s conda environment (`conda.yaml`) file.\n* **extra\\_pip\\_requirements** – Either an iterable of pip requirement strings\n(e.g. `[\"pandas\", \"-r requirements.txt\", \"-c constraints.txt\"]`) or the string path to\na pip requirements file on the local filesystem (e.g. `\"requirements.txt\"`). If provided, this\ndescribes additional pip requirements that are appended to a default set of pip requirements\ngenerated automatically based on the user’s current software environment. Both requirements and\nconstraints are automatically parsed and written to `requirements.txt` and `constraints.txt`\nfiles, respectively, and stored as part of the model. Requirements are also written to the `pip`\nsection of the model’s conda environment (`conda.yaml`) file.\n\n\ndef load\\_data():\n    ...\n\n\nclass Regressor:\n    ...\n\n\nmodel = Regressor()\nmodel.train()\ntraining\\_data, test\\_data = load\\_data()\nopt = paddle.optimizer.SGD(learning\\_rate=0.01, parameters=model.parameters())\n\nEPOCH\\_NUM = 10\nBATCH\\_SIZE = 10\n\nfor epoch\\_id in range(EPOCH\\_NUM):\n    ...\n\nmlflow.log\\_param(\"learning\\_rate\", 0.01)\nmlflow.paddle.log\\_model(model, \"model\")\nsk\\_path\\_dir = ...\nmlflow.paddle.save\\_model(model, sk\\_path\\_dir)\n\n\n`mlflow.paddle.``save\\_model`(*pd\\_model*, *path*, *training=False*, *conda\\_env=None*, *code\\_paths=None*, *mlflow\\_model=None*, *signature: mlflow.models.signature.ModelSignature = None*, *input\\_example: Union[pandas.core.frame.DataFrame, numpy.ndarray, dict, list, csr\\_matrix, csc\\_matrix, str, bytes] = None*, *pip\\_requirements=None*, *extra\\_pip\\_requirements=None*, *metadata=None*)[source] \nSave a paddle model to a path on the local file system. Produces an MLflow Model\ncontaining the following flavors:\n\nParameters\n* **pd\\_model** – paddle model to be saved.\n* **path** – Local path where the model is to be saved.\n* **training** – Only valid when saving a model trained using the PaddlePaddle high level API.\nIf set to True, the saved model supports both re-training and\ninference. If set to False, it only supports inference.\n* **conda\\_env** – Either a dictionary representation of a Conda environment or the path to a conda environment yaml\nfile. If provided, this describes the environment this model should be run in. At minimum, it\nshould specify the dependencies contained in `get\\_default\\_conda\\_env()`. If `None`, a conda\nenvironment with pip requirements inferred by `mlflow.models.infer\\_pip\\_requirements()` is added\nto the model. If the requirement inference fails, it falls back to using\n`get\\_default\\_pip\\_requirements()`. pip requirements from `conda\\_env` are written to a pip\n`requirements.txt` file and the full conda environment is written to `conda.yaml`.\nThe following is an *example* dictionary representation of a conda environment:\n\n```\n* **input\\_example** – one or several instances of valid model input. The input example is used\nas a hint of what data to feed the model. It will be converted to a Pandas\nDataFrame and then serialized to json using the Pandas split-oriented\nformat, or a numpy array where the example will be serialized to json\nby converting it to a list. Bytes are base64-encoded. When the `signature` parameter is\n`None`, the input example is used to infer a model signature.\n* **pip\\_requirements** – Either an iterable of pip requirement strings\n(e.g. `[\"paddle\", \"-r requirements.txt\", \"-c constraints.txt\"]`) or the string path to\na pip requirements file on the local filesystem (e.g. `\"requirements.txt\"`). If provided, this\ndescribes the environment this model should be run in. If `None`, a default list of requirements\nis inferred by `mlflow.models.infer\\_pip\\_requirements()` from the current software environment.\nIf the requirement inference fails, it falls back to using `get\\_default\\_pip\\_requirements()`.\nBoth requirements and constraints are automatically parsed and written to `requirements.txt` and\n`constraints.txt` files, respectively, and stored as part of the model. Requirements are also\nwritten to the `pip` section of the model’s conda environment (`conda.yaml`) file.\n* **extra\\_pip\\_requirements** – Either an iterable of pip requirement strings\n(e.g. `[\"pandas\", \"-r requirements.txt\", \"-c constraints.txt\"]`) or the string path to\na pip requirements file on the local filesystem (e.g. `\"requirements.txt\"`). If provided, this\ndescribes additional pip requirements that are appended to a default set of pip requirements\ngenerated automatically based on the user’s current software environment. Both requirements and\nconstraints are automatically parsed and written to `requirements.txt` and `constraints.txt`\nfiles, respectively, and stored as part of the model. Requirements are also written to the `pip`\nsection of the model’s conda environment (`conda.yaml`) file.\n\n```\nimport mlflow.paddle\nimport paddle\nfrom paddle.nn import Linear\nimport paddle.nn.functional as F\nimport numpy as np\nimport os\nimport random\nfrom sklearn.datasets import load\\_diabetes\nfrom sklearn.model\\_selection import train\\_test\\_split\nfrom sklearn import preprocessing\n\n\ndef load\\_data():\n    # dataset on boston housing prediction\n    X, y = load\\_diabetes(return\\_X\\_y=True, as\\_frame=True)\n\n    min\\_max\\_scaler = preprocessing.MinMaxScaler()\n    X\\_min\\_max = min\\_max\\_scaler.fit\\_transform(X)\n    X\\_normalized = preprocessing.scale(X\\_min\\_max, with\\_std=False)\n\n    X\\_train, X\\_test, y\\_train, y\\_test = train\\_test\\_split(\n        X\\_normalized, y, test\\_size=0.2, random\\_state=42\n    )\n\n    y\\_train = y\\_train.reshape(-1, 1)\n    y\\_test = y\\_test.reshape(-1, 1)\n    return np.concatenate((X\\_train, y\\_train), axis=1), np.concatenate(\n        (X\\_test, y\\_test), axis=1\n    )\n\n\nclass Regressor(paddle.nn.Layer):\n    def \\_\\_init\\_\\_(self):\n        super().\\_\\_init\\_\\_()\n\n        self.fc = Linear(in\\_features=13, out\\_features=1)\n\n    @paddle.jit.to\\_static\n    def forward(self, inputs):\n        x = self.fc(inputs)\n        return x\n\nfor epoch\\_id in range(EPOCH\\_NUM):\n    np.random.shuffle(training\\_data)\n    mini\\_batches = [\n        training\\_data[k : k + BATCH\\_SIZE]\n        for k in range(0, len(training\\_data), BATCH\\_SIZE)\n    ]\n    for iter\\_id, mini\\_batch in enumerate(mini\\_batches):\n        x = np.array(mini\\_batch[:, :-1]).astype(\"float32\")\n        y = np.array(mini\\_batch[:, -1:]).astype(\"float32\")\n        house\\_features = paddle.to\\_tensor(x)\n        prices = paddle.to\\_tensor(y)\n\n        predicts = model(house\\_features)\n\n        loss = F.square\\_error\\_cost(predicts, label=prices)\n        avg\\_loss = paddle.mean(loss)\n        if iter\\_id % 20 == 0:\n            print(f\"epoch: {epoch\\_id}, iter: {iter\\_id}, loss is: {avg\\_loss.numpy()}\")\n\n        avg\\_loss.backward()\n        opt.step()\n        opt.clear\\_grad()\n\nmlflow.log\\_param(\"learning\\_rate\", 0.01)\nmlflow.paddle.log\\_model(model, \"model\")\nsk\\_path\\_dir = \"./test-out\"\nmlflow.paddle.save\\_model(model, sk\\_path\\_dir)\nprint(\"Model saved in run %s\" % mlflow.active\\_run().info.run\\_uuid)\n\n# mlflow.pmdarima\n\n\nThe `mlflow.pmdarima` module provides an API for logging and loading `pmdarima` models.\nThis module exports univariate `pmdarima` models in the following formats:\n\nPmdarima formatSerialized instance of a `pmdarima` model using pickle.\n\n`mlflow.pyfunc`\n> \n> Produced for use by generic pyfunc-based deployment tools and for batch auditing\n> of historical forecasts.\n> \n> \n> \n\n```\nimport pandas as pd\nimport mlflow\nimport mlflow.pyfunc\nimport pmdarima\nfrom pmdarima import auto\\_arima\n\n\n\n# Define a custom model class\nclass PmdarimaWrapper(mlflow.pyfunc.PythonModel):\n    def load\\_context(self, context):\n        self.model = context.artifacts[\"model\"]\n\n    def predict(self, context, model\\_input):\n        return self.model.predict(n\\_periods=model\\_input.shape[0])\n\n\n\n# Specify locations of source data and the model artifact\nSOURCE\\_DATA = \"https://raw.githubusercontent.com/facebook/prophet/master/examples/example\\_retail\\_sales.csv\"\nARTIFACT\\_PATH = \"model\"\n\n\n# Read data and recode columns\nsales\\_data = pd.read\\_csv(SOURCE\\_DATA)\nsales\\_data.rename(columns={\"y\": \"sales\", \"ds\": \"date\"}, inplace=True)\n\n\n# Split the data into train/test\ntrain\\_size = int(0.8 \\* len(sales\\_data))\ntrain, \\_ = sales\\_data[:train\\_size], sales\\_data[train\\_size:]\n\n\n# Create the model\nmodel = pmdarima.auto\\_arima(train[\"sales\"], seasonal=True, m=12)\n\n# Log the model\nwith mlflow.start\\_run():\n    wrapper = PmdarimaWrapper()\n    mlflow.pyfunc.log\\_model(\n        artifact\\_path=\"model\",\n        python\\_model=wrapper,\n        artifacts={\"model\": mlflow.pyfunc.model\\_to\\_dict(model)},\n\n==================\n Document 6 \n----------------\n## MLModel configuration\n\n\nA Python model contains an `MLmodel` file in **python\\_function** format in its root with the\nfollowing parameters:\n\n\n* loader\\_module [required]:Python module that can load the model. Expected as module identifier\ne.g. `mlflow.sklearn`, it will be imported using `importlib.import\\_module`.\nThe imported module must contain a function with the following signature:\n\n```\n\\_load\\_pyfunc(path: string) -> <pyfunc model implementation>\n\n\nThe path argument is specified by the `data` parameter and may refer to a file or\ndirectory. The model implementation is expected to be an object with a\n`predict` method with the following signature:\n\n```\npredict(\n  model\\_input: [pandas.DataFrame, numpy.ndarray,\n  scipy.sparse.(csc.csc\\_matrix | csr.csr\\_matrix), List[Any], Dict[str, Any]]\n) -> [numpy.ndarray | pandas.(Series | DataFrame) | List]\n\n```\n* code [optional]:Relative path to a directory containing the code packaged with this model.\nAll files and directories inside this directory are added to the Python path\nprior to importing the model loader.\n* data [optional]:Relative path to a file or directory containing model data.\nThe path is passed to the model loader.\n* env [optional]:Relative path to an exported Conda environment. If present this environment\nshould be activated prior to running the model.\n* Optionally, any additional parameters necessary for interpreting the serialized model in\n`pyfunc` format.\n\n\nExample\n\n```\ntree example/sklearn\\_iris/mlruns/run1/outputs/linear-lr\n\n```\n├── MLmodel\n├── code\n│   ├── sklearn_iris.py\n│\n├── data\n│   └── model.pkl\n└── mlflow_env.yml\n\n```\ncat example/sklearn\\_iris/mlruns/run1/outputs/linear-lr/MLmodel\n\n```\npython\\_function:\n  code: code\n  data: data/model.pkl\n  loader\\_module: mlflow.sklearn\n  env: mlflow\\_env.yml\n  main: sklearn\\_iris\n## Creating custom Pyfunc models\n\n\nMLflow’s persistence modules provide convenience functions for creating models with the\n`pyfunc` flavor in a variety of machine learning frameworks (scikit-learn, Keras, Pytorch, and\nmore); however, they do not cover every use case. For example, you may want"
    },
    {
        "instruction": "Create a Python program using the 'more-itertools' API to demonstrate the grouping and filtering of elements in a list based on a condition. The program should take a list of strings, group them into consecutive sublists using the 'consecutive_groups' function from 'more-itertools', and filter out sublists that contain only strings with all numeric characters. Finally, print the filtered sublists and calculate the length of each sublist and print the lengths.",
        "api": "more-itertools",
        "output": "#!pip install more-itertools\nfrom more_itertools import consecutive_groups\n\nstrings = [\"123\", \"456\", \"789\", \"abc\", \"def\", \"ghi\", \"123456789\"]\npartitioned_sublists = list(consecutive_groups(strings))\nfiltered_sublists = [sublist for sublist in partitioned_sublists if all(word.isdigit() for word in sublist)]\nlengths = [len(sublist) for sublist in filtered_sublists]\n\nprint(\"Filtered Sublists:\")\nfor sublist in filtered_sublists:\n    print(list(sublist))\n\nprint(\"Lengths:\")\nprint(lengths)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n# Summarizing¶\n\n\nThese tools return summarized or aggregated data from an iterable.\n\n\n`more_itertools.``ilen`(*iterable*)[source]¶\nReturn the number of items in *iterable*.\n\n```\n>>> ilen(x for x in range(1000000) if x % 3 == 0)\n333334\n\n\nThis consumes the iterable, so handle with care.\n\n`more_itertools.``unique_to_each`(*\\*iterables*)[source]¶\nReturn the elements from each of the input iterables that aren’t in the\nother input iterables.\n\n\nFor example, suppose you have a set of packages, each with a set of\ndependencies:\n\n```\n{'pkg\\_1': {'A', 'B'}, 'pkg\\_2': {'B', 'C'}, 'pkg\\_3': {'B', 'D'}}\n\n\nIf you remove one package, which dependencies can also be removed?\n\n\nIf `pkg\\_1` is removed, then `A` is no longer necessary - it is not\nassociated with `pkg\\_2` or `pkg\\_3`. Similarly, `C` is only needed for\n`pkg\\_2`, and `D` is only needed for `pkg\\_3`:\n\n```\n>>> unique\\_to\\_each({'A', 'B'}, {'B', 'C'}, {'B', 'D'})\n[['A'], ['C'], ['D']]\n\n\nIf there are duplicates in one input iterable that aren’t in the others\nthey will be duplicated in the output. Input order is preserved:\n\n```\n>>> unique\\_to\\_each(\"mississippi\", \"missouri\")\n[['p', 'p'], ['o', 'u', 'r']]\n\n\nIt is assumed that the elements of each iterable are hashable.\n\n`more_itertools.``sample`(*iterable*, *k=1*, *weights=None*)[source]¶\nReturn a *k*-length list of elements chosen (without replacement)\nfrom the *iterable*. Like `random.sample()`, but works on iterables\nof unknown length.\n\n```\n>>> iterable = range(100)\n>>> sample(iterable, 5)  # doctest: +SKIP\n[81, 60, 96, 16, 4]\n\n\nAn iterable with *weights* may also be given:\n\n```\n>>> iterable = range(100)\n>>> weights = (i \\* i + 1 for i in range(100))\n>>> sampled = sample(iterable, 5, weights=weights)  # doctest: +SKIP\n[79, 67, 74, 66, 78]\n\n\nThe algorithm can also be used to generate weighted random permutations.\nThe relative weight of each item determines the probability that it\nappears late in the permutation.\n\n```\n>>> data = \"abcdefgh\"\n>>> weights = range(1, len(data) + 1)\n>>> sample(data, k=len(data), weights=weights)  # doctest: +SKIP\n['c', 'a', 'b', 'e', 'g', 'd', 'h', 'f']\n\n`more_itertools.``consecutive_groups`(*iterable*, *ordering=lambda x: x*)[source]¶\nYield groups of consecutive items using `itertools.groupby()`.\nThe *ordering* function determines whether two items are adjacent by\nreturning their position.\n\n\nBy default, the ordering function is the identity function. This is\nsuitable for finding runs of numbers:\n\n```\n>>> iterable = [1, 10, 11, 12, 20, 30, 31, 32, 33, 40]\n>>> for group in consecutive\\_groups(iterable):\n...     print(list(group))\n[1]\n[10, 11, 12]\n[20]\n[30, 31, 32, 33]\n[40]\n\n\nFor finding runs of adjacent letters, try using the `index()` method\nof a string of letters:\n\n```\n>>> from string import ascii\\_lowercase\n>>> iterable = 'abcdfgilmnop'\n>>> ordering = ascii\\_lowercase.index\n>>> for group in consecutive\\_groups(iterable, ordering):\n...     print(list(group))\n['a', 'b', 'c', 'd']\n['f', 'g']\n['i']\n['l', 'm', 'n', 'o', 'p']\n\n\nEach group of consecutive items is an iterator that shares it source with\n*iterable*. When an an output group is advanced, the previous group is\nno longer available unless its elements are copied (e.g., into a `list`).\n\n```\n>>> iterable = [1, 2, 11, 12, 21, 22]\n>>> saved\\_groups = []\n>>> for group in consecutive\\_groups(iterable):\n...     saved\\_groups.append(list(group))  # Copy group elements\n>>> saved\\_groups\n[[1, 2], [11, 12], [21, 22]]\n\n*class* `more_itertools.``run_length`[source]¶\n`run\\_length.encode()` compresses an iterable with run-length encoding.\nIt yields groups of repeated items with the count of how many times they\nwere repeated:\n\n```\n>>> uncompressed = 'abbcccdddd'\n>>> list(run\\_length.encode(uncompressed))\n[('a', 1), ('b', 2), ('c', 3), ('d', 4)]\n\n\n`run\\_length.decode()` decompresses an iterable that was previously\ncompressed with run-length encoding. It yields the items of the\ndecompressed iterable:\n\n```\n>>> compressed = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]\n>>> list(run\\_length.decode(compressed))\n['a', 'b', 'b', 'c', 'c', 'c', 'd', 'd', 'd', 'd']\n\n`more_itertools.``map_reduce`(*iterable*, *keyfunc*, *valuefunc=None*, *reducefunc=None*)[source]¶\nReturn a dictionary that maps the items in *iterable* to categories\ndefined by *keyfunc*, transforms them with *valuefunc*, and\nthen summarizes them by category with *reducefunc*.\n\n\n*valuefunc* defaults to the identity function if it is unspecified.\nIf *reducefunc* is unspecified, no summarization takes place:\n\n```\n>>> keyfunc = lambda x: x.upper()\n>>> result = map\\_reduce('abbccc', keyfunc)\n>>> sorted(result.items())\n[('A', ['a']), ('B', ['b', 'b']), ('C', ['c', 'c', 'c'])]\n\n\nSpecifying *valuefunc* transforms the categorized items:\n\n```\n>>> keyfunc = lambda x: x.upper()\n>>> valuefunc = lambda x: 1\n>>> result = map\\_reduce('abbccc', keyfunc, valuefunc)\n>>> sorted(result.items())\n[('A', [1]), ('B', [1, 1]), ('C', [1, 1, 1])]\n\n\nSpecifying *reducefunc* summarizes the categorized items:\n\n```\n>>> keyfunc = lambda x: x.upper()\n>>> valuefunc = lambda x: 1\n>>> reducefunc = sum\n>>> result = map\\_reduce('abbccc', keyfunc, valuefunc, reducefunc)\n>>> sorted(result.items())\n[('A', 1), ('B', 2), ('C', 3)]\n\n\nYou may want to filter the input iterable before applying the map/reduce\nprocedure:\n\n```\n>>> all\\_items = range(30)\n>>> items = [x for x in all\\_items if 10 <= x <= 20]  # Filter\n>>> keyfunc = lambda x: x % 2  # Evens map to 0; odds to 1\n>>> categories = map\\_reduce(items, keyfunc=keyfunc)\n>>> sorted(categories.items())\n[(0, [10, 12, 14, 16, 18, 20]), (1, [11, 13, 15, 17, 19])]\n>>> summaries = map\\_reduce(items, keyfunc=keyfunc, reducefunc=sum)\n>>> sorted(summaries.items())\n[(0, 90), (1, 75)]\n\n\nNote that all items in the iterable are gathered into a list before the\nsummarization step, which may require significant storage.\n\n\nThe returned object is a `collections.defaultdict` with the\n`default\\_factory` set to `None`, such that it behaves like a normal\ndictionary.\n\n`more_itertools.``exactly_n`(*iterable*, *n*, *predicate=bool*)[source]¶\nReturn `True` if exactly `n` items in the iterable are `True`\naccording to the *predicate* function.\n\n```\n>>> exactly\\_n([True, True, False], 2)\nTrue\n>>> exactly\\_n([True, True, False], 1)\nFalse\n>>> exactly\\_n([0, 1, 2, 3, 4, 5], 3, lambda x: x < 3)\nTrue\n\n\nThe iterable will be advanced until `n + 1` truthy items are encountered,\nso avoid calling it on infinite iterables.\n\n`more_itertools.``is_sorted`(*iterable*, *key=None*, *reverse=False*, *strict=False*)[source]¶\nReturns `True` if the items of iterable are in sorted order, and\n`False` otherwise. *key* and *reverse* have the same meaning that they do\nin the built-in `sorted()` function.\n\n```\n>>> is\\_sorted(['1', '2', '3', '4', '5'], key=int)\nTrue\n>>> is\\_sorted([5, 4, 3, 1, 2], reverse=True)\nFalse\n\n\nIf *strict*, tests for strict sorting, that is, returns `False` if equal\nelements are found:\n\n```\n>>> is\\_sorted([1, 2, 2])\nTrue\n>>> is\\_sorted([1, 2, 2], strict=True)\nFalse\n\n\nThe function returns `False` after encountering the first out-of-order\nitem. If there are no out-of-order items, the iterable is exhausted.\n\n`more_itertools.``all_unique`(*iterable*, *key=None*)[source]¶\nReturns `True` if all the elements of *iterable* are unique (no two\nelements are equal).\n\n```\n>>> all\\_unique('ABCB')\nFalse\n\n\nIf a *key* function is specified, it will be used to make comparisons.\n\n```\n>>> all\\_unique('ABCb')\nTrue\n>>> all\\_unique('ABCb', str.lower)\nFalse\n\n\nThe function returns as soon as the first non-unique element is\nencountered. Iterables with a mix of hashable and unhashable items can\nbe used, but the function will be slower for unhashable items.\n\n`more_itertools.``minmax`(*iterable*, *\\**[, *key*, *default*])[source]¶\n\n`more_itertools.``minmax`(*arg1*, *arg2*, *\\*args*[, *key*])[source]\nReturns both the smallest and largest items in an iterable\nor the largest of two or more arguments.\n\n```\n>>> minmax([3, 1, 5])\n(1, 5)\n\n```\n>>> minmax(4, 2, 6)\n(2, 6)\n\n\nIf a *key* function is provided, it will be used to transform the input\nitems for comparison.\n\n```\n>>> minmax([5, 30], key=str)  # '30' sorts before '5'\n(30, 5)\n\n\nIf a *default* value is provided, it will be returned if there are no\ninput items.\n\n```\n>>> minmax([], default=(0, 0))\n(0, 0)\n\n\nOtherwise `ValueError` is raised.\n\n\nThis function is based on the\nrecipe by\nRaymond Hettinger and takes care to minimize the number of comparisons\nperformed.\n\n`more_itertools.``iequals`(*\\*iterables*)[source]¶\nReturn `True` if all given *iterables* are equal to each other,\nwhich means that they contain the same elements in the same order.\n\n\nThe function is useful for comparing iterables of different data types\nor iterables that do not support equality checks.\n\n```\n>>> iequals(\"abc\", ['a', 'b', 'c'], ('a', 'b', 'c'), iter(\"abc\"))\nTrue\n\n```\n>>> iequals(\"abc\", \"acb\")\nFalse\n\n\nNot to be confused with `all\\_equals()`, which checks whether all\nelements of iterable are equal to each other.\n\n\n`more_itertools.``all_equal`(*iterable*)[source]¶\nReturns `True` if all the elements are equal to each other.\n\n```\n>>> all\\_equal('aaaa')\nTrue\n>>> all\\_equal('aaab')\nFalse\n\n`more_itertools.``first_true`(*iterable*, *default=None*, *pred=None*)[source]¶\nReturns the first true value in the iterable.\n\n\nIf no true value is found, returns *default*\n\n\nIf *pred* is not None, returns the first item for which\n`pred(item) == True` .\n\n```\n>>> first\\_true(range(10))\n1\n>>> first\\_true(range(10), pred=lambda x: x > 5)\n6\n>>> first\\_true(range(10), default='missing', pred=lambda x: x > 9)\n'missing'\n\n`more_itertools.``quantify`(*iterable*, *pred=bool*)[source]¶\nReturn the how many times the predicate is true.\n\n```\n>>> quantify([True, False, True])\n2\n## Selecting¶\n\n\nThese tools yield certain items from an iterable.\n\n\n*class* `more_itertools.``islice_extended`(*iterable*, *stop*)[source]¶\n\n*class* `more_itertools.``islice_extended`(*iterable*, *start*, *stop*[, *step*])[source]\nAn extension of `itertools.islice()` that supports negative values\nfor *stop*, *start*, and *step*.\n\n```\n>>> iterable = iter('abcdefgh')\n>>> list(islice\\_extended(iterable, -4, -1))\n['e', 'f', 'g']\n\n\nSlices with negative values require some caching\n\n==================\n Document 1 \n----------------\n\n\n\n# API Reference¶\n\n\nMore routines for operating on iterables, beyond itertools\n\n## Grouping¶\n\n\nThese tools yield groups of items from a source iterable.\n\n\n---\n\n\n**New itertools**\n\n\n`more_itertools.``chunked`(*iterable*, *n*, *strict=False*)[source]¶\nBreak *iterable* into lists of length *n*:\n\n```\n>>> list(chunked([1, 2, 3, 4, 5, 6], 3))\n[[1, 2, 3], [4, 5, 6]]\n\n```\n\n\nBy the default, the last yielded list will have\n\n==================\n Document 2 \n----------------\n# Selecting¶\n\n\nThese tools yield certain items from an iterable.\n\n\n*class* `more_itertools.``islice_extended`(*iterable*, *stop*)[source]¶\n\n*class* `more_itertools.``islice_extended`(*iterable*, *start*, *stop*[, *step*])[source]\nAn extension of `itertools.islice()` that supports negative values\nfor *stop*, *start*, and *step*.\n\n```\n>>> iterable = iter('abcdefgh')\n>>> list(islice\\_extended(iterable, -4, -1))\n['e', 'f', 'g']\n\n\nSlices with negative values require some caching of *iterable*, but this\nfunction takes care to minimize the amount of memory required.\n\n\nFor example, you can use a negative step with an infinite iterator:\n\n```\n>>> from itertools import count\n>>> list(islice\\_extended(count(), 110, 99, -2))\n[110, 108, 106, 104, 102, 100]\n\n\nYou can also use slice notation directly:\n\n```\n>>> iterable = map(str, count())\n>>> it = islice\\_extended(iterable)[10:20:2]\n>>> list(it)\n['10', '12', '14', '16', '18']\n\n`more_itertools.``first`(*iterable*[, *default*])[source]¶\nReturn the first item of *iterable*, or *default* if *iterable* is\nempty.\n\n```\n>>> first([0, 1, 2, 3])\n0\n>>> first([], 'some default')\n'some default'\n\n\nIf *default* is not provided and there are no items in the iterable,\nraise `ValueError`.\n\n\n`first()` is useful when you have a generator of expensive-to-retrieve\nvalues and want any arbitrary one. It is marginally shorter than\n`next(iter(iterable), default)`.\n\n`more_itertools.``last`(*iterable*[, *default*])[source]¶\nReturn the last item of *iterable*, or *default* if *iterable* is\nempty.\n\n```\n>>> last([0, 1, 2, 3])\n3\n>>> last([], 'some default')\n'some default'\n\n`more_itertools.``one`(*iterable*, *too\\_short=ValueError*, *too\\_long=ValueError*)[source]¶\nReturn the first item from *iterable*, which is expected to contain only\nthat item. Raise an exception if *iterable* is empty or has more than one\nitem.\n\n\n`one()` is useful for ensuring that an iterable contains only one item.\nFor example, it can be used to retrieve the result of a database query\nthat is expected to return a single row.\n\n\nIf *iterable* is empty, `ValueError` will be raised. You may specify a\ndifferent exception with the *too\\_short* keyword:\n\n```\n>>> it = []\n>>> one(it)  # doctest: +IGNORE\\_EXCEPTION\\_DETAIL\nTraceback (most recent call last):\n...\nValueError: too many items in iterable (expected 1)'\n>>> too\\_short = IndexError('too few items')\n>>> one(it, too\\_short=too\\_short)  # doctest: +IGNORE\\_EXCEPTION\\_DETAIL\nTraceback (most recent call last):\n...\nIndexError: too few items\n\n\nSimilarly, if *iterable* contains more than one item, `ValueError` will\nbe raised. You may specify a different exception with the *too\\_long*\nkeyword:\n\n```\n>>> it = ['too', 'many']\n>>> one(it)  # doctest: +IGNORE\\_EXCEPTION\\_DETAIL\nTraceback (most recent call last):\n...\nValueError: Expected exactly one item in iterable, but got 'too',\n'many', and perhaps more.\n>>> too\\_long = RuntimeError\n>>> one(it, too\\_long=too\\_long)  # doctest: +IGNORE\\_EXCEPTION\\_DETAIL\nTraceback (most recent call last):\n...\nRuntimeError\n\n\nNote that `one()` attempts to advance *iterable* twice to ensure there\nis only one item. See `spy()` or `peekable()` to check iterable\ncontents less destructively.\n\n`more_itertools.``only`(*iterable*, *default=None*, *too\\_long=ValueError*)[source]¶\nIf *iterable* has only one item, return it.\nIf it has zero items, return *default*.\nIf it has more than one item, raise the exception given by *too\\_long*,\nwhich is `ValueError` by default.\n\n```\n>>> only([], default='missing')\n'missing'\n>>> only([1])\n1\n>>> only([1, 2])  # doctest: +IGNORE\\_EXCEPTION\\_DETAIL\nTraceback (most recent call last):\n...\nValueError: Expected exactly one item in iterable, but got 1, 2,\n and perhaps more.'\n>>> only([1, 2], too\\_long=TypeError)  # doctest: +IGNORE\\_EXCEPTION\\_DETAIL\nTraceback (most recent call last):\n...\nTypeError\n\n\nNote that `only()` attempts to advance *iterable* twice to ensure there\nis only one item. See `spy()` or `peekable()` to check\niterable contents less destructively.\n\n`more_itertools.``strictly_n`(*iterable*, *too\\_short=None*, *too\\_long=None*)[source]¶\nValidate that *iterable* has exactly *n* items and return them if\nit does. If it has fewer than *n* items, call function *too\\_short*\nwith those items. If it has more than *n* items, call function\n*too\\_long* with the first `n + 1` items.\n\n```\n>>> iterable = ['a', 'b', 'c', 'd']\n>>> n = 4\n>>> list(strictly\\_n(iterable, n))\n['a', 'b', 'c', 'd']\n\n\nBy default, *too\\_short* and *too\\_long* are functions that raise\n`ValueError`.\n\n```\n>>> list(strictly\\_n('ab', 3))  # doctest: +IGNORE\\_EXCEPTION\\_DETAIL\nTraceback (most recent call last):\n...\nValueError: too few items in iterable (got 2)\n\n```\n>>> list(strictly\\_n('abc', 2))  # doctest: +IGNORE\\_EXCEPTION\\_DETAIL\nTraceback (most recent call last):\n...\nValueError: too many items in iterable (got at least 3)\n\n\nYou can instead supply functions that do something else.\n*too\\_short* will be called with the number of items in *iterable*.\n*too\\_long* will be called with n + 1.\n\n```\n>>> def too\\_short(item\\_count):\n...     raise RuntimeError\n>>> it = strictly\\_n('abcd', 6, too\\_short=too\\_short)\n>>> list(it)  # doctest: +IGNORE\\_EXCEPTION\\_DETAIL\nTraceback (most recent call last):\n...\nRuntimeError\n\n```\n>>> def too\\_long(item\\_count):\n...     print('The boss is going to hear about this')\n>>> it = strictly\\_n('abcdef', 4, too\\_long=too\\_long)\n>>> list(it)\nThe boss is going to hear about this\n['a', 'b', 'c', 'd']\n\n`more_itertools.``strip`(*iterable*, *pred*)[source]¶\nYield the items from *iterable*, but strip any from the\nbeginning and end for which *pred* returns `True`.\n\n\nFor example, to remove a set of items from both ends of an iterable:\n\n```\n>>> iterable = (None, False, None, 1, 2, None, 3, False, None)\n>>> pred = lambda x: x in {None, False, ''}\n>>> list(strip(iterable, pred))\n[1, 2, None, 3]\n\n\nThis function is analogous to `str.strip()`.\n\n`more_itertools.``lstrip`(*iterable*, *pred*)[source]¶\nYield the items from *iterable*, but strip any from the beginning\nfor which *pred* returns `True`.\n\n\nFor example, to remove a set of items from the start of an iterable:\n\n```\n>>> iterable = (None, False, None, 1, 2, None, 3, False, None)\n>>> pred = lambda x: x in {None, False, ''}\n>>> list(lstrip(iterable, pred))\n[1, 2, None, 3, False, None]\n\n\nThis function is analogous to to `str.lstrip()`, and is essentially\nan wrapper for `itertools.dropwhile()`.\n\n`more_itertools.``rstrip`(*iterable*, *pred*)[source]¶\nYield the items from *iterable*, but strip any from the end\nfor which *pred* returns `True`.\n\n\nFor example, to remove a set of items from the end of an iterable:\n\n```\n>>> iterable = (None, False, None, 1, 2, None, 3, False, None)\n>>> pred = lambda x: x in {None, False, ''}\n>>> list(rstrip(iterable, pred))\n[None, False, None, 1, 2, None, 3]\n\n\nThis function is analogous to `str.rstrip()`.\n\n`more_itertools.``filter_except`(*validator*, *iterable*, *\\*exceptions*)[source]¶\nYield the items from *iterable* for which the *validator* function does\nnot raise one of the specified *exceptions*.\n\n\n*validator* is called for each item in *iterable*.\nIt should be a function that accepts one argument and raises an exception\nif that item is not valid.\n\n```\n>>> iterable = ['1', '2', 'three', '4', None]\n>>> list(filter\\_except(int, iterable, ValueError, TypeError))\n['1', '2', '4']\n\n\nIf an exception other than one given by *exceptions* is raised by\n*validator*, it is raised like normal.\n\n`more_itertools.``map_except`(*function*, *iterable*, *\\*exceptions*)[source]¶\nTransform each item from *iterable* with *function* and yield the\nresult, unless *function* raises one of the specified *exceptions*.\n\n\n*function* is called to transform each item in *iterable*.\nIt should accept one argument.\n\n```\n>>> iterable = ['1', '2', 'three', '4', None]\n>>> list(map\\_except(int, iterable, ValueError, TypeError))\n[1, 2, 4]\n\n\nIf an exception other than one given by *exceptions* is raised by\n*function*, it is raised like normal.\n\n`more_itertools.``nth_or_last`(*iterable*, *n*[, *default*])[source]¶\nReturn the nth or the last item of *iterable*,\nor *default* if *iterable* is empty.\n\n```\n>>> nth\\_or\\_last([0, 1, 2, 3], 2)\n2\n>>> nth\\_or\\_last([0, 1], 2)\n1\n>>> nth\\_or\\_last([], 0, 'some default')\n'some default'\n\n`more_itertools.``unique_in_window`(*iterable*, *n*, *key=None*)[source]¶\nYield the items from *iterable* that haven’t been seen recently.\n*n* is the size of the lookback window.\n\n```\n>>> iterable = [0, 1, 0, 2, 3, 0]\n>>> n = 3\n>>> list(unique\\_in\\_window(iterable, n))\n[0, 1, 2, 3, 0]\n\n\nThe *key* function, if provided, will be used to determine uniqueness:\n\n```\n>>> list(unique\\_in\\_window('abAcda', 3, key=lambda x: x.lower()))\n['a', 'b', 'c', 'd', 'a']\n\n\nThe items in *iterable* must be hashable.\n\n`more_itertools.``duplicates_everseen`(*iterable*, *key=None*)[source]¶\nYield duplicate elements after their first appearance.\n\n```\n>>> list(duplicates\\_everseen('mississippi'))\n['s', 'i', 's', 's', 'i', 'p', 'i']\n>>> list(duplicates\\_everseen('AaaBbbCccAaa', str.lower))\n['a', 'a', 'b', 'b', 'c', 'c', 'A', 'a', 'a']\n\n\nThis function is analagous to `unique\\_everseen()` and is subject to\nthe same performance considerations.\n\n`more_itertools.``duplicates_justseen`(*iterable*, *key=None*)[source]¶\nYields serially-duplicate elements after their first appearance.\n\n```\n>>> list(duplicates\\_justseen('mississippi'))\n['s', 's', 'p']\n>>> list(duplicates\\_justseen('AaaBbbCccAaa', str.lower))\n['a', 'a', 'b', 'b', 'c', 'c', 'a', 'a']\n\n\nThis function is analagous to `unique\\_justseen()`.\n\n`more_itertools.``longest_common_prefix`(*iterables*)[source]¶\nYield elements of the longest common prefix amongst given *iterables*.\n\n```\n>>> ''.join(longest\\_common\\_prefix(['abcd', 'abc', 'abf']))\n'ab'\n\n`more_itertools.``takewhile_inclusive`(*predicate*, *iterable*)[source]¶\nA variant of `takewhile()` that yields one additional element.\n\n```\n>>> list(takewhile\\_inclusive(lambda x: x < 5, [1, 4, 6, 4, 1]))\n[1, 4, 6]\n\n\n`takewhile()` would return `[1, 4]`.\n\n\n`more_itertools.``nth`(*iterable*, *n*, *default=None*)[source]¶\nReturns the nth item or a default value.\n\n```\n>>> l = range(10)\n>>> nth(l, 3)\n3\n>>> nth(l, 20, \"zebra\")\n'zebra'\n\n`more_itertools.``before_and_after`(*predicate*, *it*)[source]¶\nA variant of `takewhile()` that allows complete access to the\nremainder of the iterator.\n\n```\n>>> it = iter('ABCdEfGhI')\n>>> all\\_upper, remainder = before\\_and\\_after(str.isupper, it)\n>>> ''.join(all\\_upper)\n'ABC'\n>>> ''.join(remainder) # takewhile() would lose the 'd'\n'dEfGhI'\n\n\nNote that the first iterator must be fully consumed before the second\niterator can generate valid results.\n\n`more_itertools.``take`(*n*, *iterable*)[source]¶\nReturn first *n* items of the iterable as a list.\n\n```\n>>> take(3, range(10))\n[0, 1, 2]\n\n\nIf there are fewer than *n* items in the iterable, all of them are\nreturned.\n\n```\n>>> take(10, range(3))\n[0, 1, 2]\n\n`more_itertools.``tail`(*n*, *iterable*)[source]¶\nReturn an iterator over the last *n* items of *iterable*.\n\n```\n>>> t = tail(3, 'ABCDEFG')\n>>> list(t)\n['E', 'F', 'G']\n\n`more_itertools.``unique_everseen`(*iterable*, *key=None*)[source]¶\nYield unique elements, preserving order.\n\n```\n>>> list(unique\\_everseen('AAAABBBCCDAABBB'))\n['A', 'B', 'C', 'D']\n>>> list(unique\\_everseen('ABBCcAD', str.lower))\n['A', 'B', 'C', 'D']\n\n\nSequences with a mix of hashable and unhashable items can be used.\nThe function will be slower (i.e., O(n^2)) for unhashable items.\n\n\nRemember that `list` objects are unhashable - you can use the *key*\nparameter to transform the list to a tuple (which is hashable) to\navoid a slowdown.\n\n```\n>>> iterable = ([1, 2], [2, 3], [1, 2])\n>>> list(unique\\_everseen(iterable))  # Slow\n[[1, 2], [2, 3]]\n>>> list(unique\\_everseen(iterable, key=tuple))  # Faster\n[[1, 2], [2, 3]]\n\n\nSimilary, you may want to convert unhashable `set` objects with\n`key=frozenset`. For `dict` objects,\n`key=lambda x: frozenset(x.items())` can be used.\n\n`more_itertools.``unique_justseen`(*iterable*, *key=None*)[source]¶\nYields elements in order, ignoring serial duplicates\n\n```\n>>> list(unique\\_justseen('AAAABBBCCDAABBB'))\n['A', 'B', 'C', 'D', 'A', 'B']\n>>> list(unique\\_justseen('ABBCcAD', str.lower))\n['A', 'B', 'C', 'A', 'D']\n## Combinatorics¶\n\n\nThese tools yield combinatorial arrangements of items from iterables.\n\n\n`more_itertools.``distinct_permutations`(*iterable*, *r=None*)[source]¶\nYield successive distinct permutations of the elements in *iterable*.\n\n```\n>>> sorted(distinct\\_permutations([1, 0, 1]))\n[(0, 1, 1), (1, 0, 1), (1, 1, 0)]\n\n\nEquivalent to `set(permutations(iterable))`, except duplicates are not\ngenerated and thrown away. For\n\n==================\n Document 3 \n----------------\n# Grouping¶\n\n\nThese tools yield groups of items from a source iterable.\n\n\n---\n\n\n**New itertools**\n\n\n`more_itertools.``chunked`(*iterable*, *n*, *strict=False*)[source]¶\nBreak *iterable* into lists of length *n*:\n\n```\n>>> list(chunked([1, 2, 3, 4, 5, 6], 3))\n[[1, 2, 3], [4, 5, 6]]\n\n```\n\n\nBy the default, the last yielded list will have fewer than *n* elements\nif the length of *iterable* is not divisible by *n*:\n\n```\n>>> list(chunked([1, 2, 3, 4, 5, 6, 7, 8], 3))\n[[1, 2, 3], [4, 5, 6], [7, 8]]\n\n\nTo use a fill-in value instead, see the `grouper()` recipe.\n\n\nIf the length of *iterable* is not divisible by *n* and *strict* is\n`True`, then `ValueError` will be raised before the last\nlist is yielded.\n\n`more_itertools.``ichunked`(*iterable*, *n*)[source]¶\nBreak *iterable* into sub-iterables with *n* elements each.\n`ichunked()` is like `chunked()`, but it yields iterables\ninstead of lists.\n\n\nIf the sub-iterables are read in order, the elements of *iterable*\nwon’t be stored in memory.\nIf they are read out of order, `itertools.tee()` is used to cache\nelements as necessary.\n\n```\n>>> from itertools import count\n>>> all\\_chunks = ichunked(count(), 4)\n>>> c\\_1, c\\_2, c\\_3 = next(all\\_chunks), next(all\\_chunks), next(all\\_chunks)\n>>> list(c\\_2)  # c\\_1's elements have been cached; c\\_3's haven't been\n[4, 5, 6, 7]\n>>> list(c\\_1)\n[0, 1, 2, 3]\n>>> list(c\\_3)\n[8, 9, 10, 11]\n\n`more_itertools.``chunked_even`(*iterable*, *n*)[source]¶\nBreak *iterable* into lists of approximately length *n*.\nItems are distributed such the lengths of the lists differ by at most\n1 item.\n\n```\n>>> iterable = [1, 2, 3, 4, 5, 6, 7]\n>>> n = 3\n>>> list(chunked\\_even(iterable, n))  # List lengths: 3, 2, 2\n[[1, 2, 3], [4, 5], [6, 7]]\n>>> list(chunked(iterable, n))  # List lengths: 3, 3, 1\n[[1, 2, 3], [4, 5, 6], [7]]\n\n`more_itertools.``sliced`(*seq*, *n*, *strict=False*)[source]¶\nYield slices of length *n* from the sequence *seq*.\n\n```\n>>> list(sliced((1, 2, 3, 4, 5, 6), 3))\n[(1, 2, 3), (4, 5, 6)]\n\n\nBy the default, the last yielded slice will have fewer than *n* elements\nif the length of *seq* is not divisible by *n*:\n\n```\n>>> list(sliced((1, 2, 3, 4, 5, 6, 7, 8), 3))\n[(1, 2, 3), (4, 5, 6), (7, 8)]\n\n\nIf the length of *seq* is not divisible by *n* and *strict* is\n`True`, then `ValueError` will be raised before the last\nslice is yielded.\n\n\nThis function will only work for iterables that support slicing.\nFor non-sliceable iterables, see `chunked()`.\n\n`more_itertools.``constrained_batches`(*iterable*, *max\\_size*, *max\\_count=None*, *get\\_len=len*, *strict=True*)[source]¶\nYield batches of items from *iterable* with a combined size limited by\n*max\\_size*.\n\n```\n>>> iterable = [b'12345', b'123', b'12345678', b'1', b'1', b'12', b'1']\n>>> list(constrained\\_batches(iterable, 10))\n[(b'12345', b'123'), (b'12345678', b'1', b'1'), (b'12', b'1')]\n\n\nIf a *max\\_count* is supplied, the number of items per batch is also\nlimited:\n\n```\n>>> iterable = [b'12345', b'123', b'12345678', b'1', b'1', b'12', b'1']\n>>> list(constrained\\_batches(iterable, 10, max\\_count = 2))\n[(b'12345', b'123'), (b'12345678', b'1'), (b'1', b'12'), (b'1',)]\n\n\nIf a *get\\_len* function is supplied, use that instead of `len()` to\ndetermine item size.\n\n\nIf *strict* is `True`, raise `ValueError` if any single item is bigger\nthan *max\\_size*. Otherwise, allow single items to exceed *max\\_size*.\n\n`more_itertools.``distribute`(*n*, *iterable*)[source]¶\nDistribute the items from *iterable* among *n* smaller iterables.\n\n```\n>>> group\\_1, group\\_2 = distribute(2, [1, 2, 3, 4, 5, 6])\n>>> list(group\\_1)\n[1, 3, 5]\n>>> list(group\\_2)\n[2, 4, 6]\n\n\nIf the length of *iterable* is not evenly divisible by *n*, then the\nlength of the returned iterables will not be identical:\n\n```\n>>> children = distribute(3, [1, 2, 3, 4, 5, 6, 7])\n>>> [list(c) for c in children]\n[[1, 4, 7], [2, 5], [3, 6]]\n\n\nIf the length of *iterable* is smaller than *n*, then the last returned\niterables will be empty:\n\n```\n>>> children = distribute(5, [1, 2, 3])\n>>> [list(c) for c in children]\n[[1], [2], [3], [], []]\n\n\nThis function uses `itertools.tee()` and may require significant\nstorage. If you need the order items in the smaller iterables to match the\noriginal iterable, see `divide()`.\n\n`more_itertools.``divide`(*n*, *iterable*)[source]¶\nDivide the elements from *iterable* into *n* parts, maintaining\norder.\n\n```\n>>> group\\_1, group\\_2 = divide(2, [1, 2, 3, 4, 5, 6])\n>>> list(group\\_1)\n[1, 2, 3]\n>>> list(group\\_2)\n[4, 5, 6]\n\n```\n>>> children = divide(3, [1, 2, 3, 4, 5, 6, 7])\n>>> [list(c) for c in children]\n[[1, 2, 3], [4, 5], [6, 7]]\n\n\nIf the length of the iterable is smaller than n, then the last returned\niterables will be empty:\n\n```\n>>> children = divide(5, [1, 2, 3])\n>>> [list(c) for c in children]\n[[1], [2], [3], [], []]\n\n\nThis function will exhaust the iterable before returning and may require\nsignificant storage. If order is not important, see `distribute()`,\nwhich does not first pull the iterable into memory.\n\n`more_itertools.``split_at`(*iterable*, *pred*, *maxsplit=-1*, *keep\\_separator=False*)[source]¶\nYield lists of items from *iterable*, where each list is delimited by\nan item where callable *pred* returns `True`.\n\n```\n>>> list(split\\_at('abcdcba', lambda x: x == 'b'))\n[['a'], ['c', 'd', 'c'], ['a']]\n\n```\n>>> list(split\\_at(range(10), lambda n: n % 2 == 1))\n[[0], [2], [4], [6], [8], []]\n\n\nAt most *maxsplit* splits are done. If *maxsplit* is not specified or -1,\nthen there is no limit on the number of splits:\n\n```\n>>> list(split\\_at(range(10), lambda n: n % 2 == 1, maxsplit=2))\n[[0], [2], [4, 5, 6, 7, 8, 9]]\n\n\nBy default, the delimiting items are not included in the output.\nTo include them, set *keep\\_separator* to `True`.\n\n```\n>>> list(split\\_at('abcdcba', lambda x: x == 'b', keep\\_separator=True))\n[['a'], ['b'], ['c', 'd', 'c'], ['b'], ['a']]\n\n`more_itertools.``split_before`(*iterable*, *pred*, *maxsplit=-1*)[source]¶\nYield lists of items from *iterable*, where each list ends just before\nan item for which callable *pred* returns `True`:\n\n```\n>>> list(split\\_before('OneTwo', lambda s: s.isupper()))\n[['O', 'n', 'e'], ['T', 'w', 'o']]\n\n```\n>>> list(split\\_before(range(10), lambda n: n % 3 == 0))\n[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n\n```\n>>> list(split\\_before(range(10), lambda n: n % 3 == 0, maxsplit=2))\n[[0, 1, 2], [3, 4, 5], [6, 7, 8, 9]]\n\n`more_itertools.``split_after`(*iterable*, *pred*, *maxsplit=-1*)[source]¶\nYield lists of items from *iterable*, where each list ends with an\nitem where callable *pred* returns `True`:\n\n```\n>>> list(split\\_after('one1two2', lambda s: s.isdigit()))\n[['o', 'n', 'e', '1'], ['t', 'w', 'o', '2']]\n\n```\n>>> list(split\\_after(range(10), lambda n: n % 3 == 0))\n[[0], [1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\n```\n>>> list(split\\_after(range(10), lambda n: n % 3 == 0, maxsplit=2))\n[[0], [1, 2, 3], [4, 5, 6, 7, 8, 9]]\n\n`more_itertools.``split_into`(*iterable*, *sizes*)[source]¶\nYield a list of sequential items from *iterable* of length ‘n’ for each\ninteger ‘n’ in *sizes*.\n\n```\n>>> list(split\\_into([1,2,3,4,5,6], [1,2,3]))\n[[1], [2, 3], [4, 5, 6]]\n\n\nIf the sum of *sizes* is smaller than the length of *iterable*, then the\nremaining items of *iterable* will not be returned.\n\n```\n>>> list(split\\_into([1,2,3,4,5,6], [2,3]))\n[[1, 2], [3, 4, 5]]\n\n\nIf the sum of *sizes* is larger than the length of *iterable*, fewer items\nwill be returned in the iteration that overruns *iterable* and further\nlists will be empty:\n\n```\n>>> list(split\\_into([1,2,3,4], [1,2,3,4]))\n[[1], [2, 3], [4], []]\n\n\nWhen a `None` object is encountered in *sizes*, the returned list will\ncontain items up to the end of *iterable* the same way that itertools.slice\ndoes:\n\n```\n>>> list(split\\_into([1,2,3,4,5,6,7,8,9,0], [2,3,None]))\n[[1, 2], [3, 4, 5], [6, 7, 8, 9, 0]]\n\n\n`split\\_into()` can be useful for grouping a series of items where the\nsizes of the groups are not uniform. An example would be where in a row\nfrom a table, multiple columns represent elements of the same feature\n(e.g. a point represented by x,y,z) but, the format is not the same for\nall columns.\n\n`more_itertools.``split_when`(*iterable*, *pred*, *maxsplit=-1*)[source]¶\nSplit *iterable* into pieces based on the output of *pred*.\n*pred* should be a function that takes successive pairs of items and\nreturns `True` if the iterable should be split in between them.\n\n\nFor example, to find runs of increasing numbers, split the iterable when\nelement `i` is larger than element `i + 1`:\n\n```\n>>> list(split\\_when([1, 2, 3, 3, 2, 5, 2, 4, 2], lambda x, y: x > y))\n[[1, 2, 3, 3], [2, 5], [2, 4], [2]]\n\n```\n>>> list(split\\_when([1, 2, 3, 3, 2, 5, 2, 4, 2],\n...                 lambda x, y: x > y, maxsplit=2))\n[[1, 2, 3, 3], [2, 5], [2, 4, 2]]\n\n`more_itertools.``bucket`(*iterable*, *key*, *validator=None*)[source]¶\nWrap *iterable* and return an object that buckets it iterable into\nchild iterables based on a *key* function.\n\n```\n>>> iterable = ['a1', 'b1', 'c1', 'a2', 'b2', 'c2', 'b3']\n>>> s = bucket(iterable, key=lambda x: x[0])  # Bucket by 1st character\n>>> sorted(list(s))  # Get the keys\n['a', 'b', 'c']\n>>> a\\_iterable = s['a']\n>>> next(a\\_iterable)\n'a1'\n>>> next(a\\_iterable)\n'a2'\n>>> list(s['b'])\n['b1', 'b2', 'b3']\n\n\nThe original iterable will be advanced and its items will be cached until\nthey are used by the child iterables. This may require significant storage.\n\n\nBy default, attempting to select a bucket to which no items belong will\nexhaust the iterable and cache all values.\nIf you specify a *validator* function, selected buckets will instead be\nchecked against it.\n\n```\n>>> from itertools import count\n>>> it = count(1, 2)  # Infinite sequence of odd numbers\n>>> key = lambda x: x % 10  # Bucket by last digit\n>>> validator = lambda x: x in {1, 3, 5, 7, 9}  # Odd digits only\n>>> s = bucket(it, key=key, validator=validator)\n>>> 2 in s\nFalse\n>>> list(s[2])\n[]\n\n`more_itertools.``unzip`(*iterable*)[source]¶\nThe inverse of `zip()`, this function disaggregates the elements\nof the zipped *iterable*.\n\n\nThe `i`-th iterable contains the `i`-th element from each element\nof the zipped iterable. The first element is used to determine the\nlength of the remaining elements.\n\n```\n>>> iterable = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]\n>>> letters, numbers = unzip(iterable)\n>>> list(letters)\n['a', 'b', 'c', 'd']\n>>> list(numbers)\n[1, 2, 3, 4]\n\n\nThis is similar to using `zip(\\*iterable)`, but it avoids reading\n*iterable* into memory. Note, however, that this function uses\n`itertools.tee()` and thus may require significant storage.\n\n---\n\n\n**Itertools recipes**\n\n\n`more_itertools.``batched`(*iterable*, *n*)[source]¶\nBatch data into lists of length *n*. The last batch may be shorter.\n\n```\n>>> list(batched('ABCDEFG', 3))\n[('A', 'B', 'C'), ('D', 'E', 'F'), ('G',)]\n\n\nOn Python 3.12 and above, this is an alias for `itertools.batched()`.\n\n`more_itertools.``grouper`(*iterable*, *n*, *incomplete='fill'*, *fillvalue=None*)[source]¶\nGroup elements from *iterable* into fixed-length groups of length *n*.\n\n```\n>>> list(grouper('ABCDEF', 3))\n[('A', 'B', 'C'), ('D', 'E', 'F')]\n\n\nThe keyword arguments *incomplete* and *fillvalue* control what happens for\niterables whose length is not a multiple of *n*.\n\n\nWhen *incomplete* is ‘fill’, the last group will contain instances of\n*fillvalue*.\n\n```\n>>> list(grouper('ABCDEFG', 3, incomplete='fill', fillvalue='x'))\n[('A', 'B', 'C'), ('D', 'E', 'F'), ('G', 'x', 'x')]\n\n\nWhen *incomplete* is ‘ignore’, the last group will not be emitted.\n\n```\n>>> list(grouper('ABCDEFG', 3, incomplete='ignore', fillvalue='x'))\n[('A', 'B', 'C'), ('D', 'E', 'F')]\n\n\nWhen *incomplete* is ‘strict’, a subclass of ValueError will be raised.\n\n```\n>>> it = grouper('ABCDEFG', 3, incomplete='strict')\n>>> list(it)  # doctest: +IGNORE\\_EXCEPTION\\_DETAIL\nTraceback (most recent call last):\n...\nUnequalIterablesError\n\n`more_itertools.``partition`(*pred*, *iterable*)[source]¶\nReturns a 2-tuple of iterables derived from the input iterable.\nThe first yields the items that have `pred(item) == False`.\nThe second yields the items that have `pred(item) == True`.\n\n```\n>>> is\\_odd = lambda x: x % 2 != 0\n>>> iterable = range(10)\n>>> even\\_items, odd\\_items = partition(is\\_odd, iterable)\n>>> list(even\\_items), list(odd\\_items)\n([0, 2, 4, 6, 8], [1, 3, 5, 7, 9])\n\n\nIf *pred* is None, `bool()` is used.\n\n```\n>>> iterable = [0, 1, False, True, '', ' ']\n>>> false\\_items, true\\_items = partition(None, iterable)\n>>> list(false\\_items), list(true\\_items)\n([0, False, ''], [1, True, ' '])\n\n`more_itertools.``transpose`(*it*)[source]¶\nSwap the rows and columns of the input.\n\n```\n>>> list(transpose([(1, 2, 3), (11, 22, 33)]))\n[(1, 11), (2, 22), (3, 33)]\n\n\nThe caller should ensure that the dimensions of the input are compatible.\nIf the input is empty, no output will be produced.\n## Lookahead and lookback¶\n\n\nThese tools peek at an iterable’s values without advancing it.\n\n\n`more_itertools.``spy`(*iterable*, *n=1*)[source]¶\nReturn a 2-tuple with a list containing the first *n* elements of\n*iterable*, and an iterator with the same items as *iterable*.\nThis allows you to “look ahead” at\n\n==================\n Document 4 \n----------------\n# Augmenting¶\n\n\nThese tools yield items from an iterable, plus additional data.\n\n\n`more_itertools.``count_cycle`(*iterable*, *n=None*)[source]¶\nCycle through the items from *iterable* up to *n* times, yielding\nthe number of completed cycles along with each item. If *n* is omitted the\nprocess repeats indefinitely.\n\n```\n>>> list(count\\_cycle('AB', 3))\n[(0, 'A'), (0, 'B'), (1, 'A'), (1, 'B'), (2, 'A'), (2, 'B')]\n\n`more_itertools.``intersperse`(*e*, *iterable*, *n=1*)[source]¶\nIntersperse filler element *e* among the items in *iterable*, leaving\n*n* items between each filler element.\n\n```\n>>> list(intersperse('!', [1, 2, 3, 4, 5]))\n[1, '!', 2, '!', 3, '!', 4, '!', 5]\n\n```\n>>> list(intersperse(None, [1, 2, 3, 4, 5], n=2))\n[1, 2, None, 3, 4, None, 5]\n\n`more_itertools.``padded`(*iterable*, *fillvalue=None*, *n=None*, *next\\_multiple=False*)[source]¶\nYield the elements from *iterable*, followed by *fillvalue*, such that\nat least *n* items are emitted.\n\n```\n>>> list(padded([1, 2, 3], '?', 5))\n[1, 2, 3, '?', '?']\n\n\nIf *next\\_multiple* is `True`, *fillvalue* will be emitted until the\nnumber of items emitted is a multiple of *n*:\n\n```\n>>> list(padded([1, 2, 3, 4], n=3, next\\_multiple=True))\n[1, 2, 3, 4, None, None]\n\n\nIf *n* is `None`, *fillvalue* will be emitted indefinitely.\n\n`more_itertools.``mark_ends`(*iterable*)[source]¶\nYield 3-tuples of the form `(is\\_first, is\\_last, item)`.\n\n```\n>>> list(mark\\_ends('ABC'))\n[(True, False, 'A'), (False, False, 'B'), (False, True, 'C')]\n\n\nUse this when looping over an iterable to take special action on its first\nand/or last items:\n\n```\n>>> iterable = ['Header', 100, 200, 'Footer']\n>>> total = 0\n>>> for is\\_first, is\\_last, item in mark\\_ends(iterable):\n...     if is\\_first:\n...         continue  # Skip the header\n...     if is\\_last:\n...         continue  # Skip the footer\n...     total += item\n>>> print(total)\n300\n\n`more_itertools.``repeat_each`(*iterable*, *n=2*)[source]¶\nRepeat each element in *iterable* *n* times.\n\n```\n>>> list(repeat\\_each('ABC', 3))\n['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C']\n\n`more_itertools.``repeat_last`(*iterable*, *default=None*)[source]¶\nAfter the *iterable* is exhausted, keep yielding its last element.\n\n```\n>>> list(islice(repeat\\_last(range(3)), 5))\n[0, 1, 2, 2, 2]\n\n\nIf the iterable is empty, yield *default* forever:\n\n```\n>>> list(islice(repeat\\_last(range(0), 42), 5))\n[42, 42, 42, 42, 42]\n\n`more_itertools.``adjacent`(*predicate*, *iterable*, *distance=1*)[source]¶\nReturn an iterable over (bool, item) tuples where the item is\ndrawn from *iterable* and the bool indicates whether\nthat item satisfies the *predicate* or is adjacent to an item that does.\n\n\nFor example, to find whether items are adjacent to a `3`:\n\n```\n>>> list(adjacent(lambda x: x == 3, range(6)))\n[(False, 0), (False, 1), (True, 2), (True, 3), (True, 4), (False, 5)]\n\n\nSet *distance* to change what counts as adjacent. For example, to find\nwhether items are two places away from a `3`:\n\n```\n>>> list(adjacent(lambda x: x == 3, range(6), distance=2))\n[(False, 0), (True, 1), (True, 2), (True, 3), (True, 4), (True, 5)]\n\n\nThis is useful for contextualizing the results of a search function.\nFor example, a code comparison tool might want to identify lines that\nhave changed, but also surrounding lines to give the viewer of the diff\ncontext.\n\n\nThe predicate function will only be called once for each item in the\niterable.\n\n\nSee also `groupby\\_transform()`, which can be used with this function\nto group ranges of items with the same bool value.\n\n`more_itertools.``groupby_transform`(*iterable*, *keyfunc=None*, *valuefunc=None*, *reducefunc=None*)[source]¶\nAn extension of `itertools.groupby()` that can apply transformations\nto the grouped data.\n\n\n* *keyfunc* is a function computing a key value for each item in *iterable*\n* *valuefunc* is a function that transforms the individual items from\n*iterable* after grouping\n* *reducefunc* is a function that transforms each group of items\n\n```\n>>> iterable = 'aAAbBBcCC'\n>>> keyfunc = lambda k: k.upper()\n>>> valuefunc = lambda v: v.lower()\n>>> reducefunc = lambda g: ''.join(g)\n>>> list(groupby\\_transform(iterable, keyfunc, valuefunc, reducefunc))\n[('A', 'aaa'), ('B', 'bbb'), ('C', 'ccc')]\n\n\nEach optional argument defaults to an identity function if not specified.\n\n\n`groupby\\_transform()` is useful when grouping elements of an iterable\nusing a separate iterable as the key. To do this, `zip()` the iterables\nand pass a *keyfunc* that extracts the first element and a *valuefunc*\nthat extracts the second element:\n\n```\n>>> from operator import itemgetter\n>>> keys = [0, 0, 1, 1, 1, 2, 2, 2, 3]\n>>> values = 'abcdefghi'\n>>> iterable = zip(keys, values)\n>>> grouper = groupby\\_transform(iterable, itemgetter(0), itemgetter(1))\n>>> [(k, ''.join(g)) for k, g in grouper]\n[(0, 'ab'), (1, 'cde'), (2, 'fgh'), (3, 'i')]\n\n\nNote that the order of items in the iterable is significant.\nOnly adjacent items are grouped together, so if you don’t want any\nduplicate groups, you should sort the iterable by the key function.\n\n\n`more_itertools.``padnone`()\n\n`more_itertools.``pad_none`(*iterable*)[source]¶\nReturns the sequence of elements and then returns `None` indefinitely.\n\n```\n>>> take(5, pad\\_none(range(3)))\n[0, 1, 2, None, None]\n\n\nUseful for emulating the behavior of the built-in `map()` function.\n\n\nSee also `padded()`.\n\n`more_itertools.``ncycles`(*iterable*, *n*)[source]¶\nReturns the sequence elements *n* times\n\n```\n>>> list(ncycles([\"a\", \"b\"], 3))\n['a', 'b', 'a', 'b', 'a', 'b']\n## Combining¶\n\n\nThese tools combine multiple iterables.\n\n\n`more_itertools.``collapse`(*iterable*, *base\\_type=None*, *levels=None*)[source]¶\nFlatten an iterable with multiple levels of nesting (e.g., a list of\nlists of tuples) into non-iterable types.\n\n```\n>>> iterable = [(1, 2), ([3, 4], [[5], [6]])]\n>>> list(collapse(iterable))\n[1, 2, 3, 4, 5, 6]\n\n\nBinary and text"
    },
    {
        "instruction": "Create a Python program using the 'more-itertools' API to demonstrate the grouping and filtering of elements in a list based on a condition. The program should take a list of strings, group them into consecutive sublists using the 'consecutive_groups' function from 'more-itertools', and filter out sublists that contain only strings with all uppercase letters. Finally, print the filtered sublists and calculate the length of each sublist and print the lengths.",
        "api": "more-itertools",
        "output": "#!pip install more-itertools\nfrom more_itertools import consecutive_groups\n\nstrings = [\"APPLE\", \"BANANA\", \"CHERRY\", \"DATE\", \"ELDERBERRY\", \"FIG\", \"GRAPE\"]\npartitioned_sublists = list(consecutive_groups(strings))\nfiltered_sublists = [sublist for sublist in partitioned_sublists if all(word.isupper() for word in sublist)]\nlengths = [len(sublist) for sublist in filtered_sublists]\n\nprint(\"Filtered Sublists:\")\nfor sublist in filtered_sublists:\n    print(list(sublist))\n\nprint(\"Lengths:\")\nprint(lengths)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n# Summarizing¶\n\n\nThese tools return summarized or aggregated data from an iterable.\n\n\n`more_itertools.``ilen`(*iterable*)[source]¶\nReturn the number of items in *iterable*.\n\n```\n>>> ilen(x for x in range(1000000) if x % 3 == 0)\n333334\n\n\nThis consumes the iterable, so handle with care.\n\n`more_itertools.``unique_to_each`(*\\*iterables*)[source]¶\nReturn the elements from each of the input iterables that aren’t in the\nother input iterables.\n\n\nFor example, suppose you have a set of packages, each with a set of\ndependencies:\n\n```\n{'pkg\\_1': {'A', 'B'}, 'pkg\\_2': {'B', 'C'}, 'pkg\\_3': {'B', 'D'}}\n\n\nIf you remove one package, which dependencies can also be removed?\n\n\nIf `pkg\\_1` is removed, then `A` is no longer necessary - it is not\nassociated with `pkg\\_2` or `pkg\\_3`. Similarly, `C` is only needed for\n`pkg\\_2`, and `D` is only needed for `pkg\\_3`:\n\n```\n>>> unique\\_to\\_each({'A', 'B'}, {'B', 'C'}, {'B', 'D'})\n[['A'], ['C'], ['D']]\n\n\nIf there are duplicates in one input iterable that aren’t in the others\nthey will be duplicated in the output. Input order is preserved:\n\n```\n>>> unique\\_to\\_each(\"mississippi\", \"missouri\")\n[['p', 'p'], ['o', 'u', 'r']]\n\n\nIt is assumed that the elements of each iterable are hashable.\n\n`more_itertools.``sample`(*iterable*, *k=1*, *weights=None*)[source]¶\nReturn a *k*-length list of elements chosen (without replacement)\nfrom the *iterable*. Like `random.sample()`, but works on iterables\nof unknown length.\n\n```\n>>> iterable = range(100)\n>>> sample(iterable, 5)  # doctest: +SKIP\n[81, 60, 96, 16, 4]\n\n\nAn iterable with *weights* may also be given:\n\n```\n>>> iterable = range(100)\n>>> weights = (i \\* i + 1 for i in range(100))\n>>> sampled = sample(iterable, 5, weights=weights)  # doctest: +SKIP\n[79, 67, 74, 66, 78]\n\n\nThe algorithm can also be used to generate weighted random permutations.\nThe relative weight of each item determines the probability that it\nappears late in the permutation.\n\n```\n>>> data = \"abcdefgh\"\n>>> weights = range(1, len(data) + 1)\n>>> sample(data, k=len(data), weights=weights)  # doctest: +SKIP\n['c', 'a', 'b', 'e', 'g', 'd', 'h', 'f']\n\n`more_itertools.``consecutive_groups`(*iterable*, *ordering=lambda x: x*)[source]¶\nYield groups of consecutive items using `itertools.groupby()`.\nThe *ordering* function determines whether two items are adjacent by\nreturning their position.\n\n\nBy default, the ordering function is the identity function. This is\nsuitable for finding runs of numbers:\n\n```\n>>> iterable = [1, 10, 11, 12, 20, 30, 31, 32, 33, 40]\n>>> for group in consecutive\\_groups(iterable):\n...     print(list(group))\n[1]\n[10, 11, 12]\n[20]\n[30, 31, 32, 33]\n[40]\n\n\nFor finding runs of adjacent letters, try using the `index()` method\nof a string of letters:\n\n```\n>>> from string import ascii\\_lowercase\n>>> iterable = 'abcdfgilmnop'\n>>> ordering = ascii\\_lowercase.index\n>>> for group in consecutive\\_groups(iterable, ordering):\n...     print(list(group))\n['a', 'b', 'c', 'd']\n['f', 'g']\n['i']\n['l', 'm', 'n', 'o', 'p']\n\n\nEach group of consecutive items is an iterator that shares it source with\n*iterable*. When an an output group is advanced, the previous group is\nno longer available unless its elements are copied (e.g., into a `list`).\n\n```\n>>> iterable = [1, 2, 11, 12, 21, 22]\n>>> saved\\_groups = []\n>>> for group in consecutive\\_groups(iterable):\n...     saved\\_groups.append(list(group))  # Copy group elements\n>>> saved\\_groups\n[[1, 2], [11, 12], [21, 22]]\n\n*class* `more_itertools.``run_length`[source]¶\n`run\\_length.encode()` compresses an iterable with run-length encoding.\nIt yields groups of repeated items with the count of how many times they\nwere repeated:\n\n```\n>>> uncompressed = 'abbcccdddd'\n>>> list(run\\_length.encode(uncompressed))\n[('a', 1), ('b', 2), ('c', 3), ('d', 4)]\n\n\n`run\\_length.decode()` decompresses an iterable that was previously\ncompressed with run-length encoding. It yields the items of the\ndecompressed iterable:\n\n```\n>>> compressed = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]\n>>> list(run\\_length.decode(compressed))\n['a', 'b', 'b', 'c', 'c', 'c', 'd', 'd', 'd', 'd']\n\n`more_itertools.``map_reduce`(*iterable*, *keyfunc*, *valuefunc=None*, *reducefunc=None*)[source]¶\nReturn a dictionary that maps the items in *iterable* to categories\ndefined by *keyfunc*, transforms them with *valuefunc*, and\nthen summarizes them by category with *reducefunc*.\n\n\n*valuefunc* defaults to the identity function if it is unspecified.\nIf *reducefunc* is unspecified, no summarization takes place:\n\n```\n>>> keyfunc = lambda x: x.upper()\n>>> result = map\\_reduce('abbccc', keyfunc)\n>>> sorted(result.items())\n[('A', ['a']), ('B', ['b', 'b']), ('C', ['c', 'c', 'c'])]\n\n\nSpecifying *valuefunc* transforms the categorized items:\n\n```\n>>> keyfunc = lambda x: x.upper()\n>>> valuefunc = lambda x: 1\n>>> result = map\\_reduce('abbccc', keyfunc, valuefunc)\n>>> sorted(result.items())\n[('A', [1]), ('B', [1, 1]), ('C', [1, 1, 1])]\n\n\nSpecifying *reducefunc* summarizes the categorized items:\n\n```\n>>> keyfunc = lambda x: x.upper()\n>>> valuefunc = lambda x: 1\n>>> reducefunc = sum\n>>> result = map\\_reduce('abbccc', keyfunc, valuefunc, reducefunc)\n>>> sorted(result.items())\n[('A', 1), ('B', 2), ('C', 3)]\n\n\nYou may want to filter the input iterable before applying the map/reduce\nprocedure:\n\n```\n>>> all\\_items = range(30)\n>>> items = [x for x in all\\_items if 10 <= x <= 20]  # Filter\n>>> keyfunc = lambda x: x % 2  # Evens map to 0; odds to 1\n>>> categories = map\\_reduce(items, keyfunc=keyfunc)\n>>> sorted(categories.items())\n[(0, [10, 12, 14, 16, 18, 20]), (1, [11, 13, 15, 17, 19])]\n>>> summaries = map\\_reduce(items, keyfunc=keyfunc, reducefunc=sum)\n>>> sorted(summaries.items())\n[(0, 90), (1, 75)]\n\n\nNote that all items in the iterable are gathered into a list before the\nsummarization step, which may require significant storage.\n\n\nThe returned object is a `collections.defaultdict` with the\n`default\\_factory` set to `None`, such that it behaves like a normal\ndictionary.\n\n`more_itertools.``exactly_n`(*iterable*, *n*, *predicate=bool*)[source]¶\nReturn `True` if exactly `n` items in the iterable are `True`\naccording to the *predicate* function.\n\n```\n>>> exactly\\_n([True, True, False], 2)\nTrue\n>>> exactly\\_n([True, True, False], 1)\nFalse\n>>> exactly\\_n([0, 1, 2, 3, 4, 5], 3, lambda x: x < 3)\nTrue\n\n\nThe iterable will be advanced until `n + 1` truthy items are encountered,\nso avoid calling it on infinite iterables.\n\n`more_itertools.``is_sorted`(*iterable*, *key=None*, *reverse=False*, *strict=False*)[source]¶\nReturns `True` if the items of iterable are in sorted order, and\n`False` otherwise. *key* and *reverse* have the same meaning that they do\nin the built-in `sorted()` function.\n\n```\n>>> is\\_sorted(['1', '2', '3', '4', '5'], key=int)\nTrue\n>>> is\\_sorted([5, 4, 3, 1, 2], reverse=True)\nFalse\n\n\nIf *strict*, tests for strict sorting, that is, returns `False` if equal\nelements are found:\n\n```\n>>> is\\_sorted([1, 2, 2])\nTrue\n>>> is\\_sorted([1, 2, 2], strict=True)\nFalse\n\n\nThe function returns `False` after encountering the first out-of-order\nitem. If there are no out-of-order items, the iterable is exhausted.\n\n`more_itertools.``all_unique`(*iterable*, *key=None*)[source]¶\nReturns `True` if all the elements of *iterable* are unique (no two\nelements are equal).\n\n```\n>>> all\\_unique('ABCB')\nFalse\n\n\nIf a *key* function is specified, it will be used to make comparisons.\n\n```\n>>> all\\_unique('ABCb')\nTrue\n>>> all\\_unique('ABCb', str.lower)\nFalse\n\n\nThe function returns as soon as the first non-unique element is\nencountered. Iterables with a mix of hashable and unhashable items can\nbe used, but the function will be slower for unhashable items.\n\n`more_itertools.``minmax`(*iterable*, *\\**[, *key*, *default*])[source]¶\n\n`more_itertools.``minmax`(*arg1*, *arg2*, *\\*args*[, *key*])[source]\nReturns both the smallest and largest items in an iterable\nor the largest of two or more arguments.\n\n```\n>>> minmax([3, 1, 5])\n(1, 5)\n\n```\n>>> minmax(4, 2, 6)\n(2, 6)\n\n\nIf a *key* function is provided, it will be used to transform the input\nitems for comparison.\n\n```\n>>> minmax([5, 30], key=str)  # '30' sorts before '5'\n(30, 5)\n\n\nIf a *default* value is provided, it will be returned if there are no\ninput items.\n\n```\n>>> minmax([], default=(0, 0))\n(0, 0)\n\n\nOtherwise `ValueError` is raised.\n\n\nThis function is based on the\nrecipe by\nRaymond Hettinger and takes care to minimize the number of comparisons\nperformed.\n\n`more_itertools.``iequals`(*\\*iterables*)[source]¶\nReturn `True` if all given *iterables* are equal to each other,\nwhich means that they contain the same elements in the same order.\n\n\nThe function is useful for comparing iterables of different data types\nor iterables that do not support equality checks.\n\n```\n>>> iequals(\"abc\", ['a', 'b', 'c'], ('a', 'b', 'c'), iter(\"abc\"))\nTrue\n\n```\n>>> iequals(\"abc\", \"acb\")\nFalse\n\n\nNot to be confused with `all\\_equals()`, which checks whether all\nelements of iterable are equal to each other.\n\n\n`more_itertools.``all_equal`(*iterable*)[source]¶\nReturns `True` if all the elements are equal to each other.\n\n```\n>>> all\\_equal('aaaa')\nTrue\n>>> all\\_equal('aaab')\nFalse\n\n`more_itertools.``first_true`(*iterable*, *default=None*, *pred=None*)[source]¶\nReturns the first true value in the iterable.\n\n\nIf no true value is found, returns *default*\n\n\nIf *pred* is not None, returns the first item for which\n`pred(item) == True` .\n\n```\n>>> first\\_true(range(10))\n1\n>>> first\\_true(range(10), pred=lambda x: x > 5)\n6\n>>> first\\_true(range(10), default='missing', pred=lambda x: x > 9)\n'missing'\n\n`more_itertools.``quantify`(*iterable*, *pred=bool*)[source]¶\nReturn the how many times the predicate is true.\n\n```\n>>> quantify([True, False, True])\n2\n## Selecting¶\n\n\nThese tools yield certain items from an iterable.\n\n\n*class* `more_itertools.``islice_extended`(*iterable*, *stop*)[source]¶\n\n*class* `more_itertools.``islice_extended`(*iterable*, *start*, *stop*[, *step*])[source]\nAn extension of `itertools.islice()` that supports negative values\nfor *stop*, *start*, and *step*.\n\n```\n>>> iterable = iter('abcdefgh')\n>>> list(islice\\_extended(iterable, -4, -1))\n['e', 'f', 'g']\n\n\nSlices with negative values require some caching\n\n==================\n Document 1 \n----------------\n\n\n\n# API Reference¶\n\n\nMore routines for operating on iterables, beyond itertools\n\n## Grouping¶\n\n\nThese tools yield groups of items from a source iterable.\n\n\n---\n\n\n**New itertools**\n\n\n`more_itertools.``chunked`(*iterable*, *n*, *strict=False*)[source]¶\nBreak *iterable* into lists of length *n*:\n\n```\n>>> list(chunked([1, 2, 3, 4, 5, 6], 3))\n[[1, 2, 3], [4, 5, 6]]\n\n```\n\n\nBy the default, the last yielded list will have\n\n==================\n Document 2 \n----------------\n# Selecting¶\n\n\nThese tools yield certain items from an iterable.\n\n\n*class* `more_itertools.``islice_extended`(*iterable*, *stop*)[source]¶\n\n*class* `more_itertools.``islice_extended`(*iterable*, *start*, *stop*[, *step*])[source]\nAn extension of `itertools.islice()` that supports negative values\nfor *stop*, *start*, and *step*.\n\n```\n>>> iterable = iter('abcdefgh')\n>>> list(islice\\_extended(iterable, -4, -1))\n['e', 'f', 'g']\n\n\nSlices with negative values require some caching of *iterable*, but this\nfunction takes care to minimize the amount of memory required.\n\n\nFor example, you can use a negative step with an infinite iterator:\n\n```\n>>> from itertools import count\n>>> list(islice\\_extended(count(), 110, 99, -2))\n[110, 108, 106, 104, 102, 100]\n\n\nYou can also use slice notation directly:\n\n```\n>>> iterable = map(str, count())\n>>> it = islice\\_extended(iterable)[10:20:2]\n>>> list(it)\n['10', '12', '14', '16', '18']\n\n`more_itertools.``first`(*iterable*[, *default*])[source]¶\nReturn the first item of *iterable*, or *default* if *iterable* is\nempty.\n\n```\n>>> first([0, 1, 2, 3])\n0\n>>> first([], 'some default')\n'some default'\n\n\nIf *default* is not provided and there are no items in the iterable,\nraise `ValueError`.\n\n\n`first()` is useful when you have a generator of expensive-to-retrieve\nvalues and want any arbitrary one. It is marginally shorter than\n`next(iter(iterable), default)`.\n\n`more_itertools.``last`(*iterable*[, *default*])[source]¶\nReturn the last item of *iterable*, or *default* if *iterable* is\nempty.\n\n```\n>>> last([0, 1, 2, 3])\n3\n>>> last([], 'some default')\n'some default'\n\n`more_itertools.``one`(*iterable*, *too\\_short=ValueError*, *too\\_long=ValueError*)[source]¶\nReturn the first item from *iterable*, which is expected to contain only\nthat item. Raise an exception if *iterable* is empty or has more than one\nitem.\n\n\n`one()` is useful for ensuring that an iterable contains only one item.\nFor example, it can be used to retrieve the result of a database query\nthat is expected to return a single row.\n\n\nIf *iterable* is empty, `ValueError` will be raised. You may specify a\ndifferent exception with the *too\\_short* keyword:\n\n```\n>>> it = []\n>>> one(it)  # doctest: +IGNORE\\_EXCEPTION\\_DETAIL\nTraceback (most recent call last):\n...\nValueError: too many items in iterable (expected 1)'\n>>> too\\_short = IndexError('too few items')\n>>> one(it, too\\_short=too\\_short)  # doctest: +IGNORE\\_EXCEPTION\\_DETAIL\nTraceback (most recent call last):\n...\nIndexError: too few items\n\n\nSimilarly, if *iterable* contains more than one item, `ValueError` will\nbe raised. You may specify a different exception with the *too\\_long*\nkeyword:\n\n```\n>>> it = ['too', 'many']\n>>> one(it)  # doctest: +IGNORE\\_EXCEPTION\\_DETAIL\nTraceback (most recent call last):\n...\nValueError: Expected exactly one item in iterable, but got 'too',\n'many', and perhaps more.\n>>> too\\_long = RuntimeError\n>>> one(it, too\\_long=too\\_long)  # doctest: +IGNORE\\_EXCEPTION\\_DETAIL\nTraceback (most recent call last):\n...\nRuntimeError\n\n\nNote that `one()` attempts to advance *iterable* twice to ensure there\nis only one item. See `spy()` or `peekable()` to check iterable\ncontents less destructively.\n\n`more_itertools.``only`(*iterable*, *default=None*, *too\\_long=ValueError*)[source]¶\nIf *iterable* has only one item, return it.\nIf it has zero items, return *default*.\nIf it has more than one item, raise the exception given by *too\\_long*,\nwhich is `ValueError` by default.\n\n```\n>>> only([], default='missing')\n'missing'\n>>> only([1])\n1\n>>> only([1, 2])  # doctest: +IGNORE\\_EXCEPTION\\_DETAIL\nTraceback (most recent call last):\n...\nValueError: Expected exactly one item in iterable, but got 1, 2,\n and perhaps more.'\n>>> only([1, 2], too\\_long=TypeError)  # doctest: +IGNORE\\_EXCEPTION\\_DETAIL\nTraceback (most recent call last):\n...\nTypeError\n\n\nNote that `only()` attempts to advance *iterable* twice to ensure there\nis only one item. See `spy()` or `peekable()` to check\niterable contents less destructively.\n\n`more_itertools.``strictly_n`(*iterable*, *too\\_short=None*, *too\\_long=None*)[source]¶\nValidate that *iterable* has exactly *n* items and return them if\nit does. If it has fewer than *n* items, call function *too\\_short*\nwith those items. If it has more than *n* items, call function\n*too\\_long* with the first `n + 1` items.\n\n```\n>>> iterable = ['a', 'b', 'c', 'd']\n>>> n = 4\n>>> list(strictly\\_n(iterable, n))\n['a', 'b', 'c', 'd']\n\n\nBy default, *too\\_short* and *too\\_long* are functions that raise\n`ValueError`.\n\n```\n>>> list(strictly\\_n('ab', 3))  # doctest: +IGNORE\\_EXCEPTION\\_DETAIL\nTraceback (most recent call last):\n...\nValueError: too few items in iterable (got 2)\n\n```\n>>> list(strictly\\_n('abc', 2))  # doctest: +IGNORE\\_EXCEPTION\\_DETAIL\nTraceback (most recent call last):\n...\nValueError: too many items in iterable (got at least 3)\n\n\nYou can instead supply functions that do something else.\n*too\\_short* will be called with the number of items in *iterable*.\n*too\\_long* will be called with n + 1.\n\n```\n>>> def too\\_short(item\\_count):\n...     raise RuntimeError\n>>> it = strictly\\_n('abcd', 6, too\\_short=too\\_short)\n>>> list(it)  # doctest: +IGNORE\\_EXCEPTION\\_DETAIL\nTraceback (most recent call last):\n...\nRuntimeError\n\n```\n>>> def too\\_long(item\\_count):\n...     print('The boss is going to hear about this')\n>>> it = strictly\\_n('abcdef', 4, too\\_long=too\\_long)\n>>> list(it)\nThe boss is going to hear about this\n['a', 'b', 'c', 'd']\n\n`more_itertools.``strip`(*iterable*, *pred*)[source]¶\nYield the items from *iterable*, but strip any from the\nbeginning and end for which *pred* returns `True`.\n\n\nFor example, to remove a set of items from both ends of an iterable:\n\n```\n>>> iterable = (None, False, None, 1, 2, None, 3, False, None)\n>>> pred = lambda x: x in {None, False, ''}\n>>> list(strip(iterable, pred))\n[1, 2, None, 3]\n\n\nThis function is analogous to `str.strip()`.\n\n`more_itertools.``lstrip`(*iterable*, *pred*)[source]¶\nYield the items from *iterable*, but strip any from the beginning\nfor which *pred* returns `True`.\n\n\nFor example, to remove a set of items from the start of an iterable:\n\n```\n>>> iterable = (None, False, None, 1, 2, None, 3, False, None)\n>>> pred = lambda x: x in {None, False, ''}\n>>> list(lstrip(iterable, pred))\n[1, 2, None, 3, False, None]\n\n\nThis function is analogous to to `str.lstrip()`, and is essentially\nan wrapper for `itertools.dropwhile()`.\n\n`more_itertools.``rstrip`(*iterable*, *pred*)[source]¶\nYield the items from *iterable*, but strip any from the end\nfor which *pred* returns `True`.\n\n\nFor example, to remove a set of items from the end of an iterable:\n\n```\n>>> iterable = (None, False, None, 1, 2, None, 3, False, None)\n>>> pred = lambda x: x in {None, False, ''}\n>>> list(rstrip(iterable, pred))\n[None, False, None, 1, 2, None, 3]\n\n\nThis function is analogous to `str.rstrip()`.\n\n`more_itertools.``filter_except`(*validator*, *iterable*, *\\*exceptions*)[source]¶\nYield the items from *iterable* for which the *validator* function does\nnot raise one of the specified *exceptions*.\n\n\n*validator* is called for each item in *iterable*.\nIt should be a function that accepts one argument and raises an exception\nif that item is not valid.\n\n```\n>>> iterable = ['1', '2', 'three', '4', None]\n>>> list(filter\\_except(int, iterable, ValueError, TypeError))\n['1', '2', '4']\n\n\nIf an exception other than one given by *exceptions* is raised by\n*validator*, it is raised like normal.\n\n`more_itertools.``map_except`(*function*, *iterable*, *\\*exceptions*)[source]¶\nTransform each item from *iterable* with *function* and yield the\nresult, unless *function* raises one of the specified *exceptions*.\n\n\n*function* is called to transform each item in *iterable*.\nIt should accept one argument.\n\n```\n>>> iterable = ['1', '2', 'three', '4', None]\n>>> list(map\\_except(int, iterable, ValueError, TypeError))\n[1, 2, 4]\n\n\nIf an exception other than one given by *exceptions* is raised by\n*function*, it is raised like normal.\n\n`more_itertools.``nth_or_last`(*iterable*, *n*[, *default*])[source]¶\nReturn the nth or the last item of *iterable*,\nor *default* if *iterable* is empty.\n\n```\n>>> nth\\_or\\_last([0, 1, 2, 3], 2)\n2\n>>> nth\\_or\\_last([0, 1], 2)\n1\n>>> nth\\_or\\_last([], 0, 'some default')\n'some default'\n\n`more_itertools.``unique_in_window`(*iterable*, *n*, *key=None*)[source]¶\nYield the items from *iterable* that haven’t been seen recently.\n*n* is the size of the lookback window.\n\n```\n>>> iterable = [0, 1, 0, 2, 3, 0]\n>>> n = 3\n>>> list(unique\\_in\\_window(iterable, n))\n[0, 1, 2, 3, 0]\n\n\nThe *key* function, if provided, will be used to determine uniqueness:\n\n```\n>>> list(unique\\_in\\_window('abAcda', 3, key=lambda x: x.lower()))\n['a', 'b', 'c', 'd', 'a']\n\n\nThe items in *iterable* must be hashable.\n\n`more_itertools.``duplicates_everseen`(*iterable*, *key=None*)[source]¶\nYield duplicate elements after their first appearance.\n\n```\n>>> list(duplicates\\_everseen('mississippi'))\n['s', 'i', 's', 's', 'i', 'p', 'i']\n>>> list(duplicates\\_everseen('AaaBbbCccAaa', str.lower))\n['a', 'a', 'b', 'b', 'c', 'c', 'A', 'a', 'a']\n\n\nThis function is analagous to `unique\\_everseen()` and is subject to\nthe same performance considerations.\n\n`more_itertools.``duplicates_justseen`(*iterable*, *key=None*)[source]¶\nYields serially-duplicate elements after their first appearance.\n\n```\n>>> list(duplicates\\_justseen('mississippi'))\n['s', 's', 'p']\n>>> list(duplicates\\_justseen('AaaBbbCccAaa', str.lower))\n['a', 'a', 'b', 'b', 'c', 'c', 'a', 'a']\n\n\nThis function is analagous to `unique\\_justseen()`.\n\n`more_itertools.``longest_common_prefix`(*iterables*)[source]¶\nYield elements of the longest common prefix amongst given *iterables*.\n\n```\n>>> ''.join(longest\\_common\\_prefix(['abcd', 'abc', 'abf']))\n'ab'\n\n`more_itertools.``takewhile_inclusive`(*predicate*, *iterable*)[source]¶\nA variant of `takewhile()` that yields one additional element.\n\n```\n>>> list(takewhile\\_inclusive(lambda x: x < 5, [1, 4, 6, 4, 1]))\n[1, 4, 6]\n\n\n`takewhile()` would return `[1, 4]`.\n\n\n`more_itertools.``nth`(*iterable*, *n*, *default=None*)[source]¶\nReturns the nth item or a default value.\n\n```\n>>> l = range(10)\n>>> nth(l, 3)\n3\n>>> nth(l, 20, \"zebra\")\n'zebra'\n\n`more_itertools.``before_and_after`(*predicate*, *it*)[source]¶\nA variant of `takewhile()` that allows complete access to the\nremainder of the iterator.\n\n```\n>>> it = iter('ABCdEfGhI')\n>>> all\\_upper, remainder = before\\_and\\_after(str.isupper, it)\n>>> ''.join(all\\_upper)\n'ABC'\n>>> ''.join(remainder) # takewhile() would lose the 'd'\n'dEfGhI'\n\n\nNote that the first iterator must be fully consumed before the second\niterator can generate valid results.\n\n`more_itertools.``take`(*n*, *iterable*)[source]¶\nReturn first *n* items of the iterable as a list.\n\n```\n>>> take(3, range(10))\n[0, 1, 2]\n\n\nIf there are fewer than *n* items in the iterable, all of them are\nreturned.\n\n```\n>>> take(10, range(3))\n[0, 1, 2]\n\n`more_itertools.``tail`(*n*, *iterable*)[source]¶\nReturn an iterator over the last *n* items of *iterable*.\n\n```\n>>> t = tail(3, 'ABCDEFG')\n>>> list(t)\n['E', 'F', 'G']\n\n`more_itertools.``unique_everseen`(*iterable*, *key=None*)[source]¶\nYield unique elements, preserving order.\n\n```\n>>> list(unique\\_everseen('AAAABBBCCDAABBB'))\n['A', 'B', 'C', 'D']\n>>> list(unique\\_everseen('ABBCcAD', str.lower))\n['A', 'B', 'C', 'D']\n\n\nSequences with a mix of hashable and unhashable items can be used.\nThe function will be slower (i.e., O(n^2)) for unhashable items.\n\n\nRemember that `list` objects are unhashable - you can use the *key*\nparameter to transform the list to a tuple (which is hashable) to\navoid a slowdown.\n\n```\n>>> iterable = ([1, 2], [2, 3], [1, 2])\n>>> list(unique\\_everseen(iterable))  # Slow\n[[1, 2], [2, 3]]\n>>> list(unique\\_everseen(iterable, key=tuple))  # Faster\n[[1, 2], [2, 3]]\n\n\nSimilary, you may want to convert unhashable `set` objects with\n`key=frozenset`. For `dict` objects,\n`key=lambda x: frozenset(x.items())` can be used.\n\n`more_itertools.``unique_justseen`(*iterable*, *key=None*)[source]¶\nYields elements in order, ignoring serial duplicates\n\n```\n>>> list(unique\\_justseen('AAAABBBCCDAABBB'))\n['A', 'B', 'C', 'D', 'A', 'B']\n>>> list(unique\\_justseen('ABBCcAD', str.lower))\n['A', 'B', 'C', 'A', 'D']\n## Combinatorics¶\n\n\nThese tools yield combinatorial arrangements of items from iterables.\n\n\n`more_itertools.``distinct_permutations`(*iterable*, *r=None*)[source]¶\nYield successive distinct permutations of the elements in *iterable*.\n\n```\n>>> sorted(distinct\\_permutations([1, 0, 1]))\n[(0, 1, 1), (1, 0, 1), (1, 1, 0)]\n\n\nEquivalent to `set(permutations(iterable))`, except duplicates are not\ngenerated and thrown away. For\n\n==================\n Document 3 \n----------------\n# Grouping¶\n\n\nThese tools yield groups of items from a source iterable.\n\n\n---\n\n\n**New itertools**\n\n\n`more_itertools.``chunked`(*iterable*, *n*, *strict=False*)[source]¶\nBreak *iterable* into lists of length *n*:\n\n```\n>>> list(chunked([1, 2, 3, 4, 5, 6], 3))\n[[1, 2, 3], [4, 5, 6]]\n\n```\n\n\nBy the default, the last yielded list will have fewer than *n* elements\nif the length of *iterable* is not divisible by *n*:\n\n```\n>>> list(chunked([1, 2, 3, 4, 5, 6, 7, 8], 3))\n[[1, 2, 3], [4, 5, 6], [7, 8]]\n\n\nTo use a fill-in value instead, see the `grouper()` recipe.\n\n\nIf the length of *iterable* is not divisible by *n* and *strict* is\n`True`, then `ValueError` will be raised before the last\nlist is yielded.\n\n`more_itertools.``ichunked`(*iterable*, *n*)[source]¶\nBreak *iterable* into sub-iterables with *n* elements each.\n`ichunked()` is like `chunked()`, but it yields iterables\ninstead of lists.\n\n\nIf the sub-iterables are read in order, the elements of *iterable*\nwon’t be stored in memory.\nIf they are read out of order, `itertools.tee()` is used to cache\nelements as necessary.\n\n```\n>>> from itertools import count\n>>> all\\_chunks = ichunked(count(), 4)\n>>> c\\_1, c\\_2, c\\_3 = next(all\\_chunks), next(all\\_chunks), next(all\\_chunks)\n>>> list(c\\_2)  # c\\_1's elements have been cached; c\\_3's haven't been\n[4, 5, 6, 7]\n>>> list(c\\_1)\n[0, 1, 2, 3]\n>>> list(c\\_3)\n[8, 9, 10, 11]\n\n`more_itertools.``chunked_even`(*iterable*, *n*)[source]¶\nBreak *iterable* into lists of approximately length *n*.\nItems are distributed such the lengths of the lists differ by at most\n1 item.\n\n```\n>>> iterable = [1, 2, 3, 4, 5, 6, 7]\n>>> n = 3\n>>> list(chunked\\_even(iterable, n))  # List lengths: 3, 2, 2\n[[1, 2, 3], [4, 5], [6, 7]]\n>>> list(chunked(iterable, n))  # List lengths: 3, 3, 1\n[[1, 2, 3], [4, 5, 6], [7]]\n\n`more_itertools.``sliced`(*seq*, *n*, *strict=False*)[source]¶\nYield slices of length *n* from the sequence *seq*.\n\n```\n>>> list(sliced((1, 2, 3, 4, 5, 6), 3))\n[(1, 2, 3), (4, 5, 6)]\n\n\nBy the default, the last yielded slice will have fewer than *n* elements\nif the length of *seq* is not divisible by *n*:\n\n```\n>>> list(sliced((1, 2, 3, 4, 5, 6, 7, 8), 3))\n[(1, 2, 3), (4, 5, 6), (7, 8)]\n\n\nIf the length of *seq* is not divisible by *n* and *strict* is\n`True`, then `ValueError` will be raised before the last\nslice is yielded.\n\n\nThis function will only work for iterables that support slicing.\nFor non-sliceable iterables, see `chunked()`.\n\n`more_itertools.``constrained_batches`(*iterable*, *max\\_size*, *max\\_count=None*, *get\\_len=len*, *strict=True*)[source]¶\nYield batches of items from *iterable* with a combined size limited by\n*max\\_size*.\n\n```\n>>> iterable = [b'12345', b'123', b'12345678', b'1', b'1', b'12', b'1']\n>>> list(constrained\\_batches(iterable, 10))\n[(b'12345', b'123'), (b'12345678', b'1', b'1'), (b'12', b'1')]\n\n\nIf a *max\\_count* is supplied, the number of items per batch is also\nlimited:\n\n```\n>>> iterable = [b'12345', b'123', b'12345678', b'1', b'1', b'12', b'1']\n>>> list(constrained\\_batches(iterable, 10, max\\_count = 2))\n[(b'12345', b'123'), (b'12345678', b'1'), (b'1', b'12'), (b'1',)]\n\n\nIf a *get\\_len* function is supplied, use that instead of `len()` to\ndetermine item size.\n\n\nIf *strict* is `True`, raise `ValueError` if any single item is bigger\nthan *max\\_size*. Otherwise, allow single items to exceed *max\\_size*.\n\n`more_itertools.``distribute`(*n*, *iterable*)[source]¶\nDistribute the items from *iterable* among *n* smaller iterables.\n\n```\n>>> group\\_1, group\\_2 = distribute(2, [1, 2, 3, 4, 5, 6])\n>>> list(group\\_1)\n[1, 3, 5]\n>>> list(group\\_2)\n[2, 4, 6]\n\n\nIf the length of *iterable* is not evenly divisible by *n*, then the\nlength of the returned iterables will not be identical:\n\n```\n>>> children = distribute(3, [1, 2, 3, 4, 5, 6, 7])\n>>> [list(c) for c in children]\n[[1, 4, 7], [2, 5], [3, 6]]\n\n\nIf the length of *iterable* is smaller than *n*, then the last returned\niterables will be empty:\n\n```\n>>> children = distribute(5, [1, 2, 3])\n>>> [list(c) for c in children]\n[[1], [2], [3], [], []]\n\n\nThis function uses `itertools.tee()` and may require significant\nstorage. If you need the order items in the smaller iterables to match the\noriginal iterable, see `divide()`.\n\n`more_itertools.``divide`(*n*, *iterable*)[source]¶\nDivide the elements from *iterable* into *n* parts, maintaining\norder.\n\n```\n>>> group\\_1, group\\_2 = divide(2, [1, 2, 3, 4, 5, 6])\n>>> list(group\\_1)\n[1, 2, 3]\n>>> list(group\\_2)\n[4, 5, 6]\n\n```\n>>> children = divide(3, [1, 2, 3, 4, 5, 6, 7])\n>>> [list(c) for c in children]\n[[1, 2, 3], [4, 5], [6, 7]]\n\n\nIf the length of the iterable is smaller than n, then the last returned\niterables will be empty:\n\n```\n>>> children = divide(5, [1, 2, 3])\n>>> [list(c) for c in children]\n[[1], [2], [3], [], []]\n\n\nThis function will exhaust the iterable before returning and may require\nsignificant storage. If order is not important, see `distribute()`,\nwhich does not first pull the iterable into memory.\n\n`more_itertools.``split_at`(*iterable*, *pred*, *maxsplit=-1*, *keep\\_separator=False*)[source]¶\nYield lists of items from *iterable*, where each list is delimited by\nan item where callable *pred* returns `True`.\n\n```\n>>> list(split\\_at('abcdcba', lambda x: x == 'b'))\n[['a'], ['c', 'd', 'c'], ['a']]\n\n```\n>>> list(split\\_at(range(10), lambda n: n % 2 == 1))\n[[0], [2], [4], [6], [8], []]\n\n\nAt most *maxsplit* splits are done. If *maxsplit* is not specified or -1,\nthen there is no limit on the number of splits:\n\n```\n>>> list(split\\_at(range(10), lambda n: n % 2 == 1, maxsplit=2))\n[[0], [2], [4, 5, 6, 7, 8, 9]]\n\n\nBy default, the delimiting items are not included in the output.\nTo include them, set *keep\\_separator* to `True`.\n\n```\n>>> list(split\\_at('abcdcba', lambda x: x == 'b', keep\\_separator=True))\n[['a'], ['b'], ['c', 'd', 'c'], ['b'], ['a']]\n\n`more_itertools.``split_before`(*iterable*, *pred*, *maxsplit=-1*)[source]¶\nYield lists of items from *iterable*, where each list ends just before\nan item for which callable *pred* returns `True`:\n\n```\n>>> list(split\\_before('OneTwo', lambda s: s.isupper()))\n[['O', 'n', 'e'], ['T', 'w', 'o']]\n\n```\n>>> list(split\\_before(range(10), lambda n: n % 3 == 0))\n[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n\n```\n>>> list(split\\_before(range(10), lambda n: n % 3 == 0, maxsplit=2))\n[[0, 1, 2], [3, 4, 5], [6, 7, 8, 9]]\n\n`more_itertools.``split_after`(*iterable*, *pred*, *maxsplit=-1*)[source]¶\nYield lists of items from *iterable*, where each list ends with an\nitem where callable *pred* returns `True`:\n\n```\n>>> list(split\\_after('one1two2', lambda s: s.isdigit()))\n[['o', 'n', 'e', '1'], ['t', 'w', 'o', '2']]\n\n```\n>>> list(split\\_after(range(10), lambda n: n % 3 == 0))\n[[0], [1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\n```\n>>> list(split\\_after(range(10), lambda n: n % 3 == 0, maxsplit=2))\n[[0], [1, 2, 3], [4, 5, 6, 7, 8, 9]]\n\n`more_itertools.``split_into`(*iterable*, *sizes*)[source]¶\nYield a list of sequential items from *iterable* of length ‘n’ for each\ninteger ‘n’ in *sizes*.\n\n```\n>>> list(split\\_into([1,2,3,4,5,6], [1,2,3]))\n[[1], [2, 3], [4, 5, 6]]\n\n\nIf the sum of *sizes* is smaller than the length of *iterable*, then the\nremaining items of *iterable* will not be returned.\n\n```\n>>> list(split\\_into([1,2,3,4,5,6], [2,3]))\n[[1, 2], [3, 4, 5]]\n\n\nIf the sum of *sizes* is larger than the length of *iterable*, fewer items\nwill be returned in the iteration that overruns *iterable* and further\nlists will be empty:\n\n```\n>>> list(split\\_into([1,2,3,4], [1,2,3,4]))\n[[1], [2, 3], [4], []]\n\n\nWhen a `None` object is encountered in *sizes*, the returned list will\ncontain items up to the end of *iterable* the same way that itertools.slice\ndoes:\n\n```\n>>> list(split\\_into([1,2,3,4,5,6,7,8,9,0], [2,3,None]))\n[[1, 2], [3, 4, 5], [6, 7, 8, 9, 0]]\n\n\n`split\\_into()` can be useful for grouping a series of items where the\nsizes of the groups are not uniform. An example would be where in a row\nfrom a table, multiple columns represent elements of the same feature\n(e.g. a point represented by x,y,z) but, the format is not the same for\nall columns.\n\n`more_itertools.``split_when`(*iterable*, *pred*, *maxsplit=-1*)[source]¶\nSplit *iterable* into pieces based on the output of *pred*.\n*pred* should be a function that takes successive pairs of items and\nreturns `True` if the iterable should be split in between them.\n\n\nFor example, to find runs of increasing numbers, split the iterable when\nelement `i` is larger than element `i + 1`:\n\n```\n>>> list(split\\_when([1, 2, 3, 3, 2, 5, 2, 4, 2], lambda x, y: x > y))\n[[1, 2, 3, 3], [2, 5], [2, 4], [2]]\n\n```\n>>> list(split\\_when([1, 2, 3, 3, 2, 5, 2, 4, 2],\n...                 lambda x, y: x > y, maxsplit=2))\n[[1, 2, 3, 3], [2, 5], [2, 4, 2]]\n\n`more_itertools.``bucket`(*iterable*, *key*, *validator=None*)[source]¶\nWrap *iterable* and return an object that buckets it iterable into\nchild iterables based on a *key* function.\n\n```\n>>> iterable = ['a1', 'b1', 'c1', 'a2', 'b2', 'c2', 'b3']\n>>> s = bucket(iterable, key=lambda x: x[0])  # Bucket by 1st character\n>>> sorted(list(s))  # Get the keys\n['a', 'b', 'c']\n>>> a\\_iterable = s['a']\n>>> next(a\\_iterable)\n'a1'\n>>> next(a\\_iterable)\n'a2'\n>>> list(s['b'])\n['b1', 'b2', 'b3']\n\n\nThe original iterable will be advanced and its items will be cached until\nthey are used by the child iterables. This may require significant storage.\n\n\nBy default, attempting to select a bucket to which no items belong will\nexhaust the iterable and cache all values.\nIf you specify a *validator* function, selected buckets will instead be\nchecked against it.\n\n```\n>>> from itertools import count\n>>> it = count(1, 2)  # Infinite sequence of odd numbers\n>>> key = lambda x: x % 10  # Bucket by last digit\n>>> validator = lambda x: x in {1, 3, 5, 7, 9}  # Odd digits only\n>>> s = bucket(it, key=key, validator=validator)\n>>> 2 in s\nFalse\n>>> list(s[2])\n[]\n\n`more_itertools.``unzip`(*iterable*)[source]¶\nThe inverse of `zip()`, this function disaggregates the elements\nof the zipped *iterable*.\n\n\nThe `i`-th iterable contains the `i`-th element from each element\nof the zipped iterable. The first element is used to determine the\nlength of the remaining elements.\n\n```\n>>> iterable = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]\n>>> letters, numbers = unzip(iterable)\n>>> list(letters)\n['a', 'b', 'c', 'd']\n>>> list(numbers)\n[1, 2, 3, 4]\n\n\nThis is similar to using `zip(\\*iterable)`, but it avoids reading\n*iterable* into memory. Note, however, that this function uses\n`itertools.tee()` and thus may require significant storage.\n\n---\n\n\n**Itertools recipes**\n\n\n`more_itertools.``batched`(*iterable*, *n*)[source]¶\nBatch data into lists of length *n*. The last batch may be shorter.\n\n```\n>>> list(batched('ABCDEFG', 3))\n[('A', 'B', 'C'), ('D', 'E', 'F'), ('G',)]\n\n\nOn Python 3.12 and above, this is an alias for `itertools.batched()`.\n\n`more_itertools.``grouper`(*iterable*, *n*, *incomplete='fill'*, *fillvalue=None*)[source]¶\nGroup elements from *iterable* into fixed-length groups of length *n*.\n\n```\n>>> list(grouper('ABCDEF', 3))\n[('A', 'B', 'C'), ('D', 'E', 'F')]\n\n\nThe keyword arguments *incomplete* and *fillvalue* control what happens for\niterables whose length is not a multiple of *n*.\n\n\nWhen *incomplete* is ‘fill’, the last group will contain instances of\n*fillvalue*.\n\n```\n>>> list(grouper('ABCDEFG', 3, incomplete='fill', fillvalue='x'))\n[('A', 'B', 'C'), ('D', 'E', 'F'), ('G', 'x', 'x')]\n\n\nWhen *incomplete* is ‘ignore’, the last group will not be emitted.\n\n```\n>>> list(grouper('ABCDEFG', 3, incomplete='ignore', fillvalue='x'))\n[('A', 'B', 'C'), ('D', 'E', 'F')]\n\n\nWhen *incomplete* is ‘strict’, a subclass of ValueError will be raised.\n\n```\n>>> it = grouper('ABCDEFG', 3, incomplete='strict')\n>>> list(it)  # doctest: +IGNORE\\_EXCEPTION\\_DETAIL\nTraceback (most recent call last):\n...\nUnequalIterablesError\n\n`more_itertools.``partition`(*pred*, *iterable*)[source]¶\nReturns a 2-tuple of iterables derived from the input iterable.\nThe first yields the items that have `pred(item) == False`.\nThe second yields the items that have `pred(item) == True`.\n\n```\n>>> is\\_odd = lambda x: x % 2 != 0\n>>> iterable = range(10)\n>>> even\\_items, odd\\_items = partition(is\\_odd, iterable)\n>>> list(even\\_items), list(odd\\_items)\n([0, 2, 4, 6, 8], [1, 3, 5, 7, 9])\n\n\nIf *pred* is None, `bool()` is used.\n\n```\n>>> iterable = [0, 1, False, True, '', ' ']\n>>> false\\_items, true\\_items = partition(None, iterable)\n>>> list(false\\_items), list(true\\_items)\n([0, False, ''], [1, True, ' '])\n\n`more_itertools.``transpose`(*it*)[source]¶\nSwap the rows and columns of the input.\n\n```\n>>> list(transpose([(1, 2, 3), (11, 22, 33)]))\n[(1, 11), (2, 22), (3, 33)]\n\n\nThe caller should ensure that the dimensions of the input are compatible.\nIf the input is empty, no output will be produced.\n## Lookahead and lookback¶\n\n\nThese tools peek at an iterable’s values without advancing it.\n\n\n`more_itertools.``spy`(*iterable*, *n=1*)[source]¶\nReturn a 2-tuple with a list containing the first *n* elements of\n*iterable*, and an iterator with the same items as *iterable*.\nThis allows you to “look ahead” at\n\n==================\n Document 4 \n----------------\n# Combining¶\n\n\nThese tools combine multiple iterables.\n\n\n`more_itertools.``collapse`(*iterable*, *base\\_type=None*, *levels=None*)[source]¶\nFlatten an iterable with multiple levels of nesting (e.g., a list of\nlists of tuples) into non-iterable types.\n\n```\n>>> iterable = [(1, 2), ([3, 4], [[5], [6]])]\n>>> list(collapse(iterable))\n[1, 2, 3, 4, 5, 6]\n\n\nBinary and text strings are not considered iterable and\nwill not be collapsed.\n\n\nTo avoid collapsing other types, specify *base\\_type*:\n\n```\n>>> iterable = ['ab', ('cd', 'ef'), ['gh', 'ij']]\n>>> list(collapse(iterable, base\\_type=tuple))\n['ab', ('cd', 'ef'), 'gh', 'ij']\n\n\nSpecify *levels* to stop flattening after a certain level:\n\n```\n>>> iterable = [('a', ['b']), ('c', ['d'])]\n>>> list(collapse(iterable))  # Fully flattened\n['a', 'b', 'c', 'd']\n>>> list(collapse(iterable, levels=1))  # Only one level flattened\n['a', ['b'], 'c', ['d']]\n\n`more_itertools.``interleave`(*\\*iterables*)[source]¶\nReturn a new iterable yielding from each iterable in turn,\nuntil the shortest is exhausted.\n\n```\n>>> list(interleave([1, 2, 3], [4, 5], [6, 7, 8]))\n[1, 4, 6, 2, 5, 7]\n\n\nFor a version that doesn’t terminate after the shortest iterable is\nexhausted, see `interleave\\_longest()`.\n\n`more_itertools.``interleave_longest`(*\\*iterables*)[source]¶\nReturn a new iterable yielding from each iterable in turn,\nskipping any that are exhausted.\n\n```\n>>> list(interleave\\_longest([1, 2, 3], [4, 5], [6, 7, 8]))\n[1, 4, 6, 2, 5, 7, 3, 8]\n\n\nThis function produces the same output as `roundrobin()`, but may\nperform better for some inputs (in particular when the number of iterables\nis large).\n\n`more_itertools.``interleave_evenly`(*iterables*, *lengths=None*)[source]¶\nInterleave multiple iterables so that their elements are evenly distributed\nthroughout the output sequence.\n\n```\n>>> iterables = [1, 2, 3, 4, 5], ['a', 'b']\n>>> list(interleave\\_evenly(iterables))\n[1, 2, 'a', 3, 4, 'b', 5]\n\n```\n>>> iterables = [[1, 2, 3], [4, 5], [6, 7, 8]]\n>>> list(interleave\\_evenly(iterables))\n[1, 6, 4, 2, 7, 3, 8, 5]\n\n\nThis function requires iterables of known length. Iterables without\n`\\_\\_len\\_\\_()` can be used by manually specifying lengths with *lengths*:\n\n```\n>>> from itertools import combinations, repeat\n>>> iterables = [combinations(range(4), 2), ['a', 'b', 'c']]\n>>> lengths = [4 \\* (4 - 1) // 2, 3]\n>>> list(interleave\\_evenly(iterables, lengths=lengths))\n[(0, 1), (0, 2), 'a', (0, 3), (1, 2), 'b', (1, 3), (2, 3), 'c']\n\n\nBased on Bresenham’s algorithm.\n\n`more_itertools.``partial_product`(*\\*iterables*)[source]¶\nYields tuples containing one item from each iterator, with subsequent\ntuples changing a single item at a time by advancing each iterator until it\nis exhausted. This sequence guarantees every value in each iterable is\noutput at least once without generating all possible combinations.\n\n\nThis may be useful, for example, when testing an expensive function.\n\n```\n>>> list(partial\\_product('AB', 'C', 'DEF'))\n[('A', 'C', 'D'), ('B', 'C', 'D'), ('B', 'C', 'E'), ('B', 'C', 'F')]\n\n`more_itertools.``sort_together`(*iterables*, *key\\_list=(0*, *)*, *key=None*, *reverse=False*)[source]¶\nReturn the input iterables sorted together, with *key\\_list* as the\npriority for sorting. All iterables are trimmed to the length of the\nshortest one.\n\n\nThis can be used like the sorting function in a spreadsheet. If each\niterable represents a column of data, the key list determines which\ncolumns are used for sorting.\n\n\nBy default, all iterables are sorted using the `0`-th iterable:\n\n```\n>>> iterables = [(4, 3, 2, 1), ('a', 'b', 'c', 'd')]\n>>> sort\\_together(iterables)\n[(1, 2, 3, 4), ('d', 'c', 'b', 'a')]\n\n\nSet a different key list to sort according to another iterable.\nSpecifying multiple keys dictates how ties are broken:\n\n```\n>>> iterables = [(3, 1, 2), (0, 1, 0), ('c', 'b', 'a')]\n>>> sort\\_together(iterables, key\\_list=(1, 2))\n[(2, 3, 1), (0, 0, 1), ('a', 'c', 'b')]\n\n\nTo sort by a function of the elements of the iterable, pass a *key*\nfunction. Its arguments are the elements of the iterables corresponding to\nthe key list:\n\n```\n>>> names = ('a', 'b', 'c')\n>>> lengths = (1, 2, 3)\n>>> widths = (5, 2, 1)\n>>> def area(length, width):\n...     return length \\* width\n>>> sort\\_together([names, lengths, widths], key\\_list=(1, 2), key=area)\n[('c', 'b', 'a'), (3, 2, 1), (1, 2, 5)]\n\n\nSet *reverse* to `True` to sort in descending order.\n\n```\n>>> sort\\_together([(1, 2, 3), ('c', 'b', 'a')], reverse=True)\n[(3, 2, 1), ('a', 'b', 'c')]\n\n`more_itertools.``value_chain`(*\\*args*)[source]¶\nYield all arguments passed to the function in the same order in which\nthey were passed. If an argument itself is iterable then iterate over its\nvalues.\n\n```\n>>> list(value\\_chain(1, 2, 3, [4, 5, 6]))\n[1, 2, 3, 4, 5, 6]\n\n\nBinary and text strings are not considered iterable and are emitted\nas-is:\n\n```\n>>> list(value\\_chain('12', '34', ['56', '78']))\n['12', '34', '56', '78']\n\n\nMultiple levels of nesting are not flattened.\n\n`more_itertools.``zip_offset`(*\\*iterables*, *offsets*, *longest=False*, *fillvalue=None*)[source]¶\n`zip` the input *iterables* together, but offset the i-th iterable\nby the i-th item in *offsets*.\n\n```\n>>> list(zip\\_offset('0123', 'abcdef', offsets=(0, 1)))\n[('0', 'b'), ('1', 'c'), ('2', 'd'), ('3', 'e')]\n\n\nThis can be used as a lightweight alternative to SciPy or pandas to analyze\ndata sets in which some series have a lead or lag relationship.\n\n\nBy default, the sequence will end when the shortest iterable is exhausted.\nTo continue until the longest iterable is exhausted, set *longest* to\n`True`.\n\n```\n>>> list(zip\\_offset('0123', 'abcdef', offsets=(0, 1), longest=True))\n[('0', 'b'), ('1', 'c'), ('2', 'd'), ('3', 'e'), (None, 'f')]\n\n`more_itertools.``zip_equal`(*\\*iterables*)[source]¶\n`zip` the input *iterables* together, but raise\n`UnequalIterablesError` if they aren’t all the same length.\n\n```\n>>> it\\_1 = range(3)\n>>> it\\_2 = iter('abc')\n>>> list(zip\\_equal(it\\_1, it\\_2))\n[(0, 'a'), (1, 'b'), (2, 'c')]\n\n```\n>>> it\\_1 = range(3)\n>>> it\\_2 = iter('abcd')\n>>> list(zip\\_equal(it\\_1, it\\_2)) # doctest: +IGNORE\\_EXCEPTION\\_DETAIL\nTraceback (most recent call last):\n...\nmore\\_itertools.more.UnequalIterablesError: Iterables have different\nlengths\n\n`more_itertools.``zip_broadcast`(*\\*objects*, *scalar\\_types=(str*, *bytes)*, *strict=False*)[source]¶\nA version of `zip()` that “broadcasts” any scalar\n(i.e., non-iterable) items into output tuples.\n\n```\n>>> iterable\\_1 = [1, 2, 3]\n>>> iterable\\_2 = ['a', 'b', 'c']\n>>> scalar = '\\_'\n>>> list(zip\\_broadcast(iterable\\_1, iterable\\_2, scalar))\n[(1, 'a', '\\_'), (2, 'b', '\\_'), (3, 'c', '\\_')]\n\n\nThe *scalar\\_types* keyword argument determines what types are considered\nscalar. It is set to `(str, bytes)` by default. Set it to `None` to\ntreat strings and byte strings as iterable:\n\n```\n>>> list(zip\\_broadcast('abc', 0, 'xyz', scalar\\_types=None))\n[('a', 0, 'x'), ('b', 0, 'y'), ('c', 0, 'z')]\n\n\nIf the *strict* keyword argument is `True`, then\n`UnequalIterablesError` will be raised if any of the iterables have\ndifferent lengths.\n\n\n`more_itertools.``dotproduct`(*vec1*, *vec2*)[source]¶\nReturns the dot product of the two iterables.\n\n```\n>>> dotproduct([10, 10], [20, 20])\n400\n\n`more_itertools.``convolve`(*signal*, *kernel*)[source]¶\nConvolve the iterable *signal* with the iterable *kernel*.\n\n```\n>>> signal = (1, 2, 3, 4, 5)\n>>> kernel = [3, 2, 1]\n>>> list(convolve(signal, kernel))\n[3, 8, 14, 20, 26, 14, 5]\n\n\nNote: the input arguments are not interchangeable, as the *kernel*\nis immediately consumed and stored.\n\n`more_itertools.``flatten`(*listOfLists*)[source]¶\nReturn an iterator flattening one level of nesting in a list of lists.\n\n```\n>>> list(flatten([[0, 1], [2, 3]]))\n[0, 1, 2, 3]\n\n\nSee also `collapse()`, which can flatten multiple levels of nesting.\n\n`more_itertools.``roundrobin`(*\\*iterables*)[source]¶\nYields an item from each iterable, alternating between them.\n\n```\n>>> list(roundrobin('ABC', 'D', 'EF'))\n['A', 'D', 'E', 'B', 'F', 'C']\n\n\nThis function produces the same output as `interleave\\_longest()`, but\nmay perform better for some inputs (in particular when the number of\niterables is small).\n\n`more_itertools.``prepend`(*value*, *iterator*)[source]¶\nYield *value*, followed by the elements in *iterator*.\n\n```\n>>> value = '0'\n>>> iterator = ['1', '2', '3']\n>>> list(prepend(value, iterator))\n['0', '1', '2', '3']\n\n\nTo prepend multiple values, see `itertools.chain()`\nor `value\\_chain()`.\n## Summarizing¶\n\n\nThese tools return summarized or aggregated data from an iterable.\n\n\n`more_itertools.``ilen`(*iterable*)[source]¶\nReturn the number of items in *iterable*.\n\n```\n>>> ilen(x for x in range(1000000) if x % 3 == 0)\n333334\n\n\nThis consumes the iterable, so handle with care.\n\n`more_itertools.``unique_to_each`(*\\*iterables*)[source]¶\nReturn the elements from each of"
    },
    {
        "instruction": "Create a Python program that uses the 'numpy' API to perform matrix operations, including generating a random matrix, computing its transpose, and calculating the matrix product.",
        "api": "numpy",
        "output": "#!pip install numpy\nimport numpy as np\n\n# Generate a random matrix of size (3x3)\noriginal_matrix = np.random.rand(3, 3)\nprint(\"Original Matrix: \", original_matrix)\n\n# Compute the matrix's transpose\ntransposed_matrix = np.transpose(original_matrix)\nprint(\"Transposed Matrix: \", transposed_matrix)\n\n# Calculate the matrix product\nmatrix_product = np.dot(original_matrix, transposed_matrix)\nprint(\"Matrix Product: \", matrix_product)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n Linear algebra (`numpy.linalg`)#\n\n\nThe NumPy linear algebra functions rely on BLAS and LAPACK to provide efficient\nlow level implementations of standard linear algebra algorithms. Those\nlibraries may be provided by NumPy itself using C versions of a subset of their\nreference implementations but, when possible, highly optimized libraries that\ntake advantage of specialized processor functionality are preferred. Examples\nof such libraries are OpenBLAS, MKL (TM), and ATLAS. Because those libraries\nare multithreaded and processor dependent, environmental variables and external\npackages such as threadpoolctl may be needed to control the number of threads\nor specify the processor architecture.\n\n\nThe SciPy library also contains a `linalg` submodule, and there is\noverlap in the functionality provided by the SciPy and NumPy submodules. SciPy\ncontains functions not found in `numpy.linalg`, such as functions related to\nLU decomposition and the Schur decomposition, multiple ways of calculating the\npseudoinverse, and matrix transcendentals such as the matrix logarithm. Some\nfunctions that exist in both have augmented functionality in `scipy.linalg`.\nFor example, `scipy.linalg.eig` can take a second matrix argument for solving\ngeneralized eigenvalue problems. Some functions in NumPy, however, have more\nflexible broadcasting options. For example, `numpy.linalg.solve` can handle\n“stacked” arrays, while `scipy.linalg.solve` accepts only a single square\narray as its first argument.\n\n\nThe term *matrix* as it is used on this page indicates a 2d `numpy.array`\nobject, and *not* a `numpy.matrix` object. The latter is no longer\nrecommended, even for linear algebra. See\nthe matrix object documentation for\nmore information.\n\n\n## The `@` operator#\n\n\nIntroduced in NumPy 1.10.0, the `@` operator is preferable to\nother methods when computing the matrix product between 2d arrays. The\n`numpy.matmul` function implements the `@` operator.\n\n\n## Matrix and vector products#\n\n\n|  |  |\n| --- | --- |\n| `dot`(a, b[, out]) | Dot product of two arrays. |\n| `linalg.multi\\_dot`(arrays, \\*[, out]) | Compute the dot product of two or more arrays in a single function call, while automatically selecting\n\n==================\n Document 1 \n----------------\n# Numerical ranges#\n\n\n|  |  |\n| --- | --- |\n| `arange`([start,] stop[, step,][, dtype, like]) | Return evenly spaced values within a given interval. |\n| `linspace`(start, stop[, num, endpoint, ...]) | Return evenly spaced numbers over a specified interval. |\n| `logspace`(start, stop[, num, endpoint, base, ...]) | Return numbers spaced evenly on a log scale. |\n| `geomspace`(start, stop[, num, endpoint, ...]) | Return numbers spaced evenly on a log scale (a geometric progression). |\n| `meshgrid`(\\*xi[, copy, sparse, indexing]) | Return a list of coordinate matrices from coordinate vectors. |\n| `mgrid` | An instance which returns a dense multi-dimensional \"meshgrid\". |\n| `ogrid` | An instance which returns an open multi-dimensional \"meshgrid\". |\n\n\n## Building matrices#\n\n\n|  |  |\n| --- | --- |\n| `diag`(v[, k]) | Extract a diagonal or construct a diagonal array. |\n| `diagflat`(v[, k]) | Create a two-dimensional array with the flattened input as a diagonal. |\n| `tri`(N[, M, k, dtype, like]) | An array with ones at and below the given diagonal and zeros elsewhere. |\n| `tril`(m[, k]) | Lower triangle of an array. |\n| `triu`(m[, k]) | Upper triangle of an array. |\n| `vander`(x[, N, increasing]) | Generate a Vandermonde matrix. |\n\n\n\n## The Matrix class#\n\n\n|  |  |\n| --- | --- |\n| `mat`(data[, dtype]) | Interpret the input as a matrix. |\n| `bmat`(obj[, ldict, gdict]) | Build a matrix object from a string, nested sequence, or array. |\n\n\n\n# Array manipulation routines#\n\n\n## Basic operations#\n\n\n|  |  |\n| --- | --- |\n| `copyto`(dst, src[, casting, where]) | Copies values from one array to another, broadcasting as necessary. |\n| `shape`(a) | Return the shape of an array. |\n\n\n\n## Changing array shape#\n\n\n|  |  |\n| --- | --- |\n| `reshape`(a, newshape[, order]) | Gives a new shape to an array without changing its data. |\n| `ravel`(a[, order]) | Return a contiguous flattened array. |\n| `ndarray.flat` | A 1-D iterator over the array. |\n| `ndarray.flatten`([order]) | Return a copy of the array collapsed into one dimension. |\n\n\n\n## Transpose-like operations#\n\n\n|  |  |\n| --- | --- |\n| `moveaxis`(a, source, destination) | Move axes of an array to new positions. |\n| `rollaxis`(a, axis[, start]) | Roll the specified axis backwards, until it lies in a given position. |\n| `swapaxes`(a, axis1, axis2) | Interchange two axes of an array. |\n| `ndarray.T` | View of the transposed array. |\n| `transpose`(a[, axes]) | Returns an array with axes transposed. |\n\n\n## Changing number of dimensions#\n\n\n|  |  |\n| --- | --- |\n| `atleast\\_1d`(\\*arys) | Convert inputs to arrays with at least one dimension. |\n| `atleast\\_2d`(\\*arys) | View inputs as arrays with at least two dimensions. |\n| `atleast\\_3d`(\\*arys) | View\n\n==================\n Document 2 \n----------------\n# Rearranging elements#\n\n\n|  |  |\n| --- | --- |\n| `flip`(m[, axis]) | Reverse the order of elements in an array along the given axis. |\n| `fliplr`(m) | Reverse the order of elements along axis 1 (left/right). |\n| `flipud`(m) | Reverse the order of elements along axis 0 (up/down). |\n| `reshape`(a, newshape[, order]) | Gives a new shape to an array without changing its data. |\n| `roll`(a, shift[, axis]) | Roll array elements along a given axis. |\n| `rot90`(m[, k, axes]) | Rotate an array by 90 degrees in the plane specified by axes. |\n\n\n# Binary operations#\n\n\n## Elementwise bit operations#\n\n\n\n## Bit packing#\n\n\n|  |  |\n| --- | --- |\n| `packbits`(a, /[, axis, bitorder]) | Packs the elements of a binary-valued array into bits in a uint8 array. |\n| `unpackbits`(a, /[, axis, count, bitorder]) | Unpacks elements of a uint8 array into a binary-valued output array. |\n\n\n\n## Output formatting#\n\n\n|  |  |\n| --- | --- |\n| `binary\\_repr`(num[, width]) | Return the binary representation of the input number as a string. |\n\n\n\n# String operations#\n\n\nThe `numpy.char` module provides a set of vectorized string\noperations for arrays of type `numpy.str\\_` or `numpy.bytes\\_`. For example\n\n```\n>>> np.char.capitalize([\"python\", \"numpy\"])\narray(['Python', 'Numpy'], dtype='<U6')\n>>> np.char.add([\"num\", \"doc\"], [\"py\", \"umentation\"])\narray(['numpy', 'documentation'], dtype='<U13')\n\n\nThe methods in this module are based on the methods in `String`\n\n## String operations#\n\n\n|  |  |\n| --- | --- |\n| `add`(x1, x2) | Return element-wise string concatenation for two arrays of str or unicode. |\n| `multiply`(a, i) | Return (a \\* i), that is string multiple concatenation, element-wise. |\n| `mod`(a, values) |\n\n==================\n Document 3 \n----------------\n# Background#\n\n\nThe API exposed by NumPy for third-party extensions has grown over\nyears of releases, and has allowed programmers to directly access\nNumPy functionality from C. This API can be best described as\n“organic”. It has emerged from multiple competing desires and from\nmultiple points of view over the years, strongly influenced by the\ndesire to make it easy for users to move to NumPy from Numeric and\nNumarray. The core API originated with Numeric in 1995 and there are\npatterns such as the heavy use of macros written to mimic Python’s\nC-API as well as account for compiler technology of the late 90’s.\nThere is also only a small group of volunteers who have had very little\ntime to spend on improving this API.\n\n\nThere is an ongoing effort to improve the API.\nIt is important in this effort\nto ensure that code that compiles for NumPy 1.X continues to\ncompile for NumPy 1.X. At the same time, certain API’s will be marked\nas deprecated so that future-looking code can avoid these API’s and\nfollow better practices.\n\n\nAnother important role played by deprecation markings in the C API is to move\ntowards hiding internal details of the NumPy implementation. For those\nneeding direct, easy, access to the data of ndarrays, this will not\nremove this ability. Rather, there are many potential performance\noptimizations which require changing the implementation details, and\nNumPy developers have been unable to try them because of the high\nvalue of preserving ABI compatibility. By deprecating this direct\naccess, we will in the future be able to improve NumPy’s performance\nin ways we cannot presently.\n\n## Deprecation Mechanism NPY\\_NO\\_DEPRECATED\\_API#\n\n\nIn C, there is no equivalent to the deprecation warnings that Python\nsupports. One way to do deprecations is to flag them in the\ndocumentation and release notes, then remove or change the deprecated\nfeatures in a future major version"
    },
    {
        "instruction": "Create a Python program that uses the 'numpy' API to perform statistical operations on a given dataset. The program should read a CSV file containing numerical data, calculate the mean, median, and standard deviation of each column, and display the results.",
        "api": "numpy",
        "output": "#!pip install numpy\nimport numpy as np\nimport csv\n\n# Read the CSV file\ndata = []\nwith open('data.csv', 'r') as file:\n    reader = csv.reader(file)\n    for row in reader:\n        data.append(row)\n\n# Convert the data to a numpy array\ndata_array = np.array(data, dtype=float)\n\n# Calculate the mean of each column\nmean = np.mean(data_array, axis=0)\nprint(\"Mean: \", mean)\n\n# Calculate the median of each column\nmedian = np.median(data_array, axis=0)\nprint(\"Median: \", median)\n\n# Calculate the standard deviation of each column\nstd_dev = np.std(data_array, axis=0)\nprint(\"Standard Deviation: \", std_dev)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n# Background#\n\n\nThe API exposed by NumPy for third-party extensions has grown over\nyears of releases, and has allowed programmers to directly access\nNumPy functionality from C. This API can be best described as\n“organic”. It has emerged from multiple competing desires and from\nmultiple points of view over the years, strongly influenced by the\ndesire to make it easy for users to move to NumPy from Numeric and\nNumarray. The core API originated with Numeric in 1995 and there are\npatterns such as the heavy use of macros written to mimic Python’s\nC-API as well as account for compiler technology of the late 90’s.\nThere is also only a small group of volunteers who have had very little\ntime to spend on improving this API.\n\n\nThere is an ongoing effort to improve the API.\nIt is important in this effort\nto ensure that code that compiles for NumPy 1.X continues to\ncompile for NumPy 1.X. At the same time, certain API’s will be marked\nas deprecated so that future-looking code can avoid these API’s and\nfollow better practices.\n\n\nAnother important role played by deprecation markings in the C API is to move\ntowards hiding internal details of the NumPy implementation. For those\nneeding direct, easy, access to the data of ndarrays, this will not\nremove this ability. Rather, there are many potential performance\noptimizations which require changing the implementation details, and\nNumPy developers have been unable to try them because of the high\nvalue of preserving ABI compatibility. By deprecating this direct\naccess, we will in the future be able to improve NumPy’s performance\nin ways we cannot presently.\n\n## Deprecation Mechanism NPY\\_NO\\_DEPRECATED\\_API#\n\n\nIn C, there is no equivalent to the deprecation warnings that Python\nsupports. One way to do deprecations is to flag them in the\ndocumentation and release notes, then remove or change the deprecated\nfeatures in a future major version\n\n==================\n Document 1 \n----------------\n# Advice for using NumPy on untrusted data#\n\n\nA user who can freely execute NumPy (or Python) functions must be considered\nto have the same privilege as the process/Python interpreter.\n\n\nThat said, NumPy should be generally safe to use on *data* provided by\nunprivileged users and read through safe API functions (e.g. loaded from a\ntext file or `.npy` file without pickle support).\nMalicious *values* or *data sizes* should never lead to privilege escalation.\n\n\nThe following points may be useful or should be noted when working with\nuntrusted data:\n\n\n* Exhausting memory can result in an out-of-memory kill, which is a possible\ndenial of service attack. Possible causes could be:\n\n\n\t+ Functions reading text files, which may require much more memory than\n\tthe original input file size.\n\t+ If users can create arbitrarily shaped arrays, NumPy’s broadcasting means\n\tthat intermediate or result arrays can be much larger than the inputs.\n* NumPy structured dtypes allow for a large amount of complexity. Fortunately,\nmost code fails gracefully when a structured dtype is provided unexpectedly.\nHowever, code should either disallow untrusted users to provide these\n(e.g. via `.npy` files) or carefully check the fields included for\nnested structured/subarray dtypes.\n* Passing on user input should generally be considered unsafe\n(except for the data being read).\nAn example would be `np.dtype(user\\_string)` or `dtype=user\\_string`.\n* The speed of operations can depend on values and memory order can lead to\nlarger temporary memory use and slower execution.\nThis means that operations may be significantly slower or use more memory\ncompared to simple test cases.\n* When reading data, consider enforcing a specific shape (e.g. one dimensional)\nor dtype such as `float64`, `float32`, or `int64` to reduce complexity.\n\n\nWhen working with non-trivial untrusted data, it is advisable to sandbox the\nanalysis to guard against potential privilege escalation.\nThis is especially advisable if further libraries based on NumPy are used since\nthese add additional complexity and potential security issues.\n\n\n# NumPy and SWIG#\n\n* numpy.i: a SWIG Interface File for NumPy\n\t+ Introduction\n\t+ Using numpy.i\n\t+ Available Typemaps\n\t+ NumPy Array Scalars and SWIG\n\t+ Helper Functions\n\t+ Beyond the Provided Typemaps\n\t+ Summary\n* Testing the numpy.i Typemaps\n\t+ Introduction\n\t+ Testing Organization\n\t+ Testing Header Files\n\t+ Testing Source Files\n\t+ Testing SWIG Interface Files\n\t+ Testing Python Scripts\n\n\n# The N-dimensional array (`ndarray`)#\n\n\nAn `ndarray` is a (usually fixed-size) multidimensional\ncontainer of items of the same type and size. The number of dimensions\nand items in an array is defined by its `shape`,\nwhich is a `tuple` of *N* non-negative integers that\n\n==================\n Document 2 \n----------------\n# NumPy Array Scalars and SWIG#\n\n\nSWIG has sophisticated type checking for numerical types. For\nexample, if your C/C++ routine expects an integer as input, the code\ngenerated by SWIG will check for both Python integers and\nPython long integers, and raise an overflow error if the provided\nPython integer is too big to cast down to a C integer. With the\nintroduction of NumPy scalar arrays into your Python code, you\nmight conceivably extract an integer from a NumPy array and attempt\nto pass this to a SWIG-wrapped C/C++ function that expects an\n`int`, but the SWIG type checking will not recognize the NumPy\narray scalar as an integer. (Often, this does in fact work – it\ndepends on whether NumPy recognizes the integer type you are using\nas inheriting from the Python integer type on the platform you are\nusing. Sometimes, this means that code that works on a 32-bit machine\nwill fail on a 64-bit machine.)\n\n\nIf you get a Python error that looks like the following:\n\n```\nTypeError: in method 'MyClass\\_MyMethod', argument 2 of type 'int'\n\n\nand the argument you are passing is an integer extracted from a\nNumPy array, then you have stumbled upon this problem. The\nsolution is to modify the SWIG type conversion system to accept\nNumPy array scalars in addition to the standard integer types.\nFortunately, this capability has been provided for you. Simply copy\nthe file:\n\n```\npyfragments.swg\n\n\nto the working build directory for you project, and this problem will\nbe fixed. It is suggested that you do this anyway, as it only\nincreases the capabilities of your Python interface.\n### Why is There a Second File?#\n\n\nThe SWIG type checking and conversion system is a complicated\ncombination of C macros, SWIG macros, SWIG typemaps and SWIG\nfragments. Fragments are a way to conditionally insert code into your\nwrapper file if it is needed,\n\n==================\n Document 3 \n----------------\n# Migration advice#\n\n\nThere are several build systems which are good options to migrate to. Assuming\nyou have compiled code in your package (if not, you have several good options,\ne.g. the build backends offered by Poetry, Hatch or PDM) and you want to be\nusing a well-designed, modern and reliable build system, we recommend:\n\n\n1. Meson, and the meson-python build backend\n2. CMake, and the scikit-build-core build backend\n\n\nIf you have modest needs (only simple Cython/C extensions; no need for Fortran,\nBLAS/LAPACK, nested `setup.py` files, or other features of\n`numpy.distutils`) and have been happy with `numpy.distutils` so far, you\ncan also consider switching to `setuptools`. Note that most functionality of\n`numpy.distutils` is unlikely to be ported to `setuptools`.\n### Moving to Meson#\n\n\nSciPy has moved to Meson and meson-python for its 1.9.0 release. During\nthis process, remaining issues with Meson’s Python support and\nfeature parity with `numpy.distutils` were resolved. *Note: parity means a\nlarge superset (because Meson is a good general-purpose build\n\n==================\n Document 4 \n----------------\n NumPy security#\n\n\nSecurity issues can be reported privately as described in the project README\nand when opening a new issue on the issue tracker.\nThe Python security reporting guidelines\nare a good resource and its notes apply also to NumPy.\n\n\nNumPy’s maintainers are not security experts. However, we are conscientious\nabout security and experts of both the NumPy codebase and how it’s used.\nPlease do notify us before creating security advisories against NumPy as\nwe are happy to prioritize issues or help with assessing the severity of a bug.\nA security advisory we are not aware of beforehand can lead to a lot of work\nfor all involved parties.\n## Advice for using NumPy on untrusted data#\n\n\nA user who can freely execute NumPy (or Python) functions must be considered\nto have the same privilege as the process/Python interpreter.\n\n\nThat said, NumPy should be generally safe to use on *data* provided by\nunprivileged\n\n==================\n Document 5 \n----------------\n# Averages and variances#\n\n\n|  |  |\n| --- | --- |\n| `median`(a[, axis, out, overwrite\\_input, keepdims]) | Compute the median along the specified axis. |\n| `average`(a[, axis, weights, returned, keepdims]) | Compute the weighted average along the specified axis. |\n| `mean`(a[, axis, dtype, out, keepdims, where]) | Compute the arithmetic mean along the specified axis. |\n| `std`(a[, axis, dtype, out, ddof, keepdims, where]) | Compute the standard deviation along the specified axis. |\n| `var`(a[, axis, dtype, out, ddof, keepdims, where]) | Compute the variance along the specified axis. |\n| `nanmedian`(a[, axis, out, overwrite\\_input, ...]) | Compute the median along the specified axis, while ignoring NaNs. |\n| `nanmean`(a[, axis, dtype, out, keepdims, where]) | Compute the arithmetic mean along the specified axis, ignoring NaNs. |\n| `nanstd`(a[, axis, dtype, out, ddof, ...]) | Compute the standard deviation along the specified axis, while ignoring NaNs. |\n| `nanvar`(a[, axis, dtype, out, ddof, ...]) | Compute the variance along the specified axis, while ignoring NaNs. |\n\n\n## Correlating#\n\n\n|  |  |\n| --- | --- |\n| `corrcoef`(x[, y, rowvar, bias, ddof, dtype]) | Return Pearson product-moment correlation coefficients. |\n| `correlate`(a, v[, mode]) | Cross-correlation of two 1-dimensional sequences. |\n| `cov`(m[, y, rowvar, bias, ddof, fweights, ...]) | Estimate a covariance matrix, given data and weights. |\n\n\n## Histograms#\n\n\n|  |  |\n| --- | --- |\n| `histogram`(a[, bins, range, density, weights]) | Compute the histogram of a dataset. |\n| `histogram2d`(x, y[, bins, range, density, ...]) | Compute the bi-dimensional histogram of two data samples. |\n| `histogramdd`(sample[, bins, range, density, ...]) | Compute the multidimensional histogram of some data. |\n|\n\n==================\n Document 6 \n----------------\n# Order statistics#\n\n\n|  |  |\n| --- | --- |\n| `ptp`(a[, axis, out, keepdims]) | Range of values (maximum - minimum) along an axis. |\n| `percentile`(a, q[, axis, out, ...]) | Compute the q-th percentile of the data along the specified axis. |\n| `nanpercentile`(a, q[, axis, out, ...]) | Compute the qth percentile of the data along the specified axis, while ignoring nan values. |\n| `quantile`(a, q[, axis, out, overwrite\\_input, ...]) | Compute the q-th quantile of the data along the specified axis. |\n| `nanquantile`(a, q[, axis, out, ...]) | Compute the qth quantile of the data along the specified axis, while ignoring nan values. |\n\n## Averages and variances#\n\n\n|  |  |\n| --- | --- |\n| `median`(a[, axis, out, overwrite\\_input, keepdims]) | Compute the median along the specified axis. |\n| `average`(a[, axis, weights, returned, keepdims]) | Compute the weighted average along the specified axis. |\n| `mean`(a[, axis, dtype, out, keepdims, where]) | Compute the arithmetic mean along"
    },
    {
        "instruction": "Create a Python program using the 'optuna' API to optimize the hyperparameters of a random forest classifier. The program should search for the best combination of 'n_estimators' and 'max_depth' hyperparameters for a random forest classifier on a custom dataset and print the cross-validated F1 score.",
        "api": "optuna",
        "output": "#!pip install optuna scikit-learn\nimport optuna\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import f1_score\n\n# Generate a custom dataset\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n\n# Define the machine learning task: Random Forest Classifier\ndef objective(trial):\n    # Define hyperparameters to search\n    n_estimators = trial.suggest_int(\"n_estimators\", 50, 200)\n    max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n\n    # Create and evaluate the random forest classifier with the chosen hyperparameters\n    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n    f1_scores = cross_val_score(clf, X, y, cv=5, scoring='f1')\n\n    return np.mean(f1_scores)\n\n# Create a study and optimize hyperparameters\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=50)\n\n    # Get the best hyperparameters\n    best_params = study.best_params\n    n_estimators = best_params[\"n_estimators\"]\n    max_depth = best_params[\"max_depth\"]\n\n    print(\"Best Hyperparameters:\", best_params)\n\n    # Create and evaluate the random forest classifier with the best hyperparameters\n    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n    f1_scores = cross_val_score(clf, X, y, cv=5, scoring='f1')\n    mean_f1_score = np.mean(f1_scores)\n\n    print(\"Cross-Validated F1 Score with Best Hyperparameters:\", mean_f1_score)\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n\n# API Reference\n\n* optuna\n* optuna.artifacts\n* optuna.cli\n* optuna.distributions\n* optuna.exceptions\n* optuna.importance\n* optuna.integration\n* optuna.logging\n* optuna.pruners\n* optuna.samplers\n* optuna.search\\_space\n* optuna.storages\n* optuna.study\n* optuna.terminator\n* optuna.trial\n* optuna.visualization\n\n\n# optuna\n\n\nThe `optuna` module is primarily used as an alias for basic Optuna functionality coded in other modules. Currently, two modules are aliased: (1) from `optuna.study`, functions regarding the Study lifecycle, and (2) from `optuna.exceptions`, the TrialPruned Exception raised when\n\n==================\n Document 1 \n----------------\n optuna.integration\n\n\nThe `integration` module contains classes used to integrate Optuna with external machine learning frameworks.\n\nNote\n\n\nOptuna’s integration modules for third-party libraries have started migrating from Optuna itself to a package called\noptuna-integration. Please check the repository and\nthe documentation.\n\nFor most of the ML frameworks supported by Optuna, the corresponding Optuna integration class serves only to implement a callback object and functions, compliant with the framework’s specific callback API, to be called with each intermediate step in the model training. The functionality implemented in these callbacks across the different ML frameworks includes:\n\n\n1. Reporting intermediate model scores back to the Optuna trial using `optuna.trial.Trial.report()`,\n2. According to the results of `optuna.trial.Trial.should\\_prune()`, pruning the current model by raising `optuna.TrialPruned()`, and\n3. Reporting intermediate Optuna data such as the current trial number back to the framework, as done in `MLflowCallback`.\n\n\nFor scikit-learn, an integrated `OptunaSearchCV` estimator is available that combines scikit-learn BaseEstimator functionality with access to a class-level `Study` object.\n\n## BoTorch\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.BoTorchSampler` | A sampler that uses BoTorch, a Bayesian optimization library built on top of PyTorch. |\n| `optuna.integration.botorch.logei\\_candidates\\_func` | Log Expected Improvement (LogEI). |\n| `optuna.integration.botorch.qei\\_candidates\\_func` | Quasi MC-based batch Expected Improvement (qEI). |\n| `optuna.integration.botorch.qnei\\_candidates\\_func` | Quasi MC-based batch Noisy Expected Improvement (qNEI). |\n| `optuna.integration.botorch.qehvi\\_candidates\\_func` | Quasi MC-based batch Expected Hypervolume Improvement (qEHVI). |\n| `optuna.integration.botorch.qnehvi\\_candidates\\_func` | Quasi MC-based batch Noisy Expected Hypervolume Improvement (qNEHVI). |\n| `optuna.integration.botorch.qparego\\_candidates\\_func` | Quasi MC-based extended ParEGO (qParEGO) for constrained multi-objective optimization. |\n\n\n\n## CatBoost\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.CatBoostPruningCallback` | Callback for catboost to prune unpromising trials. |\n\n\n\n## Dask\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.DaskStorage` | Dask-compatible storage class. |\n\n\n\n## fast.ai\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.FastAIV1PruningCallback` | FastAI callback to prune unpromising trials for fastai. |\n| `optuna.integration.FastAIV2PruningCallback` | FastAI callback to prune unpromising trials for fastai. |\n| `optuna.integration.FastAIPruningCallback` | alias of `FastAIV2PruningCallback` |\n\n\n\n## LightGBM\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.LightGBMPruningCallback` | Callback for LightGBM to prune unpromising trials. |\n| `optuna.integration.lightgbm.train` | Wrapper of LightGBM Training API to tune hyperparameters. |\n| `optuna.integration.lightgbm.LightGBMTuner` | Hyperparameter tuner for LightGBM. |\n| `optuna.integration.lightgbm.LightGBMTunerCV` | Hyperparameter tuner for LightGBM with cross-validation. |\n\n\n\n## MLflow\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.MLflowCallback` | Callback to track Optuna trials with MLflow. |\n\n\n\n## Weights & Biases\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.WeightsAndBiasesCallback` | Callback to track Optuna trials with Weights & Biases. |\n\n\n\n## pycma\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.PyCmaSampler` | A Sampler using cma library as the backend. |\n| `optuna.integration.CmaEsSampler` | Wrapper class of PyCmaSampler for backward compatibility. |\n\n\n\n## PyTorch\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.PyTorchIgnitePruningHandler` | PyTorch Ignite handler to prune unpromising trials. |\n| `optuna.integration.PyTorchLightningPruningCallback` | PyTorch Lightning callback to prune unpromising trials. |\n| `optuna.integration.TorchDistributedTrial` | A wrapper of `Trial` to incorporate Optuna with PyTorch distributed. |\n\n\n\n## scikit-learn\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.OptunaSearchCV` | Hyperparameter search with cross-validation. |\n\n\n\n## scikit-optimize\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.SkoptSampler` | Sampler using Scikit-Optimize as the backend. |\n\n\n\n## SHAP\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.ShapleyImportanceEvaluator` | Shapley (SHAP) parameter importance evaluator. |\n\n\n\n## TensorFlow\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.TensorBoardCallback` | Callback to track Optuna trials with TensorBoard. |\n\n\n\n## XGBoost\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.XGBoostPruningCallback` | Callback for XGBoost to prune unpromising trials. |\n\n\n# optuna.logging\n\n\nThe `logging` module implements logging using the Python `logging` package. Library users may be especially interested in setting verbosity levels using `set\\_verbosity()` to one of `optuna.logging.CRITICAL` (aka `optuna.logging.FATAL`), `optuna.logging.ERROR`, `optuna.logging.WARNING` (aka `optuna.logging.WARN`), `optuna.logging.INFO`, or `optuna.logging.DEBUG`.\n\n\n|  |  |\n|\n\n==================\n Document 2 \n----------------\noptuna.visualization.matplotlib\n\n\nThe following functions use Matplotlib as a backend.\n\n|  |  |\n| --- | --- |\n| `optuna.visualization.matplotlib.plot\\_contour` | Plot the parameter relationship as contour plot in a study with Matplotlib. |\n| `optuna.visualization.matplotlib.plot\\_edf` | Plot the objective value EDF (empirical distribution function) of a study with Matplotlib. |\n| `optuna.visualization.matplotlib.plot\\_hypervolume\\_history` | Plot hypervolume history of all trials in a study with Matplotlib. |\n| `optuna.visualization.matplotlib.plot\\_intermediate\\_values` | Plot intermediate values of all trials in a study with Matplotlib. |\n| `optuna.visualization.matplotlib.plot\\_optimization\\_history` | Plot optimization history of all trials in a study with Matplotlib. |\n| `optuna.visualization.matplotlib.plot\\_parallel\\_coordinate` | Plot the high-dimensional parameter relationships in a study with Matplotlib. |\n| `optuna.visualization.matplotlib.plot\\_param\\_importances` | Plot hyperparameter importances with Matplotlib. |\n| `optuna.visualization.matplotlib.plot\\_pareto\\_front` | Plot the Pareto front of a study. |\n| `optuna.visualization.matplotlib.plot\\_rank` | Plot parameter relations as scatter plots with colors indicating ranks of target value. |\n| `optuna.visualization.matplotlib.plot\\_slice` | Plot the parameter relationship as slice plot in a study with Matplotlib. |\n| `optuna.visualization.matplotlib.plot\\_terminator\\_improvement` | Plot the potentials for future objective improvement. |\n| `optuna.visualization.matplotlib.plot\\_timeline` | Plot the timeline of a study. |\n| `optuna.visualization.matplotlib.is\\_available` | Returns whether visualization with Matplotlib is available or not. |\n\n==================\n Document 3 \n----------------\n optuna.importance.FanovaImportanceEvaluator\n\n\n*class* optuna.importance.FanovaImportanceEvaluator(*\\**, *n\\_trees=64*, *max\\_depth=64*, *seed=None*)[source]\nfANOVA importance evaluator.\n\n\nImplements the fANOVA hyperparameter importance evaluation algorithm in\nAn Efficient Approach for Assessing Hyperparameter Importance.\n\n\nfANOVA fits a random forest regression model that predicts the objective values\nof `COMPLETE` trials given their parameter configurations.\nThe more accurate this model is, the more reliable the importances assessed\nby this class are.\n\n\nThis class takes over 1 minute when given a study that contains 1000+ trials.\nWe published optuna-fast-fanova library,\nthat is a Cython accelerated fANOVA implementation. By using it, you can get hyperparameter\nimportances within a few seconds.\n\n\nRequires the sklearn Python package.\n\n\nThe performance of fANOVA depends on the prediction performance of the underlying\nrandom forest model. In order to obtain high prediction performance, it is necessary to\ncover a wide range of the hyperparameter search space. It is recommended to use an\nexploration-oriented sampler such as `RandomSampler`.\n\n\nFor how to cite the original work, please refer to\nhttps://automl.github.io/fanova/cite.html.\n\n\nParameters:\n* **n\\_trees** (*int*) – The number of trees in the forest.\n* **max\\_depth** (*int*) – The maximum depth of the trees in the forest.\n* **seed** (*int* *|* *None*) – Controls the randomness of the forest. For deterministic behavior, specify a value\nother than `None`.\n\n\n|  |  |\n| --- | --- |\n| `evaluate`(study[, params, target]) | Evaluate parameter importances based on completed trials in the given study. |\n\n\nevaluate(*study*, *params=None*, *\\**, *target=None*)[source]\nEvaluate parameter importances based on completed trials in the given study.\n\n\nThis method is not meant to be called by library users.\n\n\nPlease refer to `get\\_param\\_importances()` for how a concrete\nevaluator should implement this method.\n\n\nParameters:\n* **study** (*Study*) – An optimized study.\n* **params** (*List**[**str**]* *|* *None*) – A list of names of parameters to assess.\nIf `None`, all parameters that are present in all of the completed trials are\nassessed.\n* **target** (*Callable**[**[**FrozenTrial**]**,* *float**]* *|* *None*) – A function to specify the value to evaluate importances.\nIf it is `None` and `study` is being used for single-objective optimization,\nthe objective values are used. Can also be used for other trial attributes, such as\nthe duration, like `target=lambda t: t.duration.total\\_seconds()`.\n\n\nSpecify this argument if `study` is being used for multi-objective\noptimization. For example, to get the hyperparameter importance of the first\nobjective, use `target=lambda t: t.values[0]` for the target parameter.\n\nReturns:\nA `dict` where the keys are parameter names and the values are assessed\nimportances.\n\n# optuna.importance.MeanDecreaseImpurityImportanceEvaluator\n\n\n*class* optuna.importance.MeanDecreaseImpurityImportanceEvaluator(*\\**, *n\\_trees=64*, *max\\_depth=64*, *seed=None*)[source]\nMean Decrease Impurity (MDI) parameter importance evaluator.\n\n\nThis evaluator fits fits a random forest regression model that predicts the objective values\nof `COMPLETE` trials given their parameter configurations.\nFeature importances are then computed using MDI.\n\n\nThis evaluator requires the sklearn Python package\nand is based on sklearn.ensemble.RandomForestClassifier.feature\\_importances\\_.\n\n\nParameters:\n* **n\\_trees** (*int*) – Number of trees in the random forest.\n* **max\\_depth** (*int*) – The maximum depth of each tree in the random forest.\n* **seed** (*int* *|* *None*) – Seed for the random forest.\n\n# optuna.integration.BoTorchSampler\n\n\n*class* optuna.integration.BoTorchSampler(*\\**, *candidates\\_func=None*, *constraints\\_func=None*, *n\\_startup\\_trials=10*, *consider\\_running\\_trials=False*, *independent\\_sampler=None*, *seed=None*, *device=None*)[source]\nA sampler that uses BoTorch, a Bayesian optimization library built on top of PyTorch.\n\n\nThis sampler allows using BoTorch’s optimization algorithms from Optuna to suggest parameter\nconfigurations. Parameters are transformed to continuous space\n\n==================\n Document 4 \n----------------\n optuna.integration.lightgbm.LightGBMTuner\n\n\n*class* optuna.integration.lightgbm.LightGBMTuner(*params*, *train\\_set*, *num\\_boost\\_round=1000*, *valid\\_sets=None*, *valid\\_names=None*, *fobj=None*, *feval=None*, *feature\\_name='auto'*, *categorical\\_feature='auto'*, *early\\_stopping\\_rounds=None*, *evals\\_result=None*, *verbose\\_eval='warn'*, *learning\\_rates=None*, *keep\\_training\\_booster=False*, *callbacks=None*, *time\\_budget=None*, *sample\\_size=None*, *study=None*, *optuna\\_callbacks=None*, *model\\_dir=None*, *verbosity=None*, *show\\_progress\\_bar=True*, *\\**, *optuna\\_seed=None*)[source]\nHyperparameter tuner for LightGBM.\n\n\nIt optimizes the following hyperparameters in a stepwise manner:\n`lambda\\_l1`, `lambda\\_l2`, `num\\_leaves`, `feature\\_fraction`, `bagging\\_fraction`,\n`bagging\\_freq` and `min\\_child\\_samples`.\n\n\nYou can find the details of the algorithm and benchmark results in this blog article by Kohei Ozaki, a Kaggle Grandmaster.\n\n\nArguments and keyword arguments for lightgbm.train() can be passed.\nThe arguments that only `LightGBMTuner` has are\nlisted below:\n\nParameters:\n* **time\\_budget** (*int* *|* *None*) – A time budget for parameter tuning in seconds.\n* **study** (*optuna.study.Study* *|* *None*) – A `Study` instance to store optimization results. The\n`Trial` instances in it has the following user attributes:\n`elapsed\\_secs` is the elapsed time since the optimization starts.\n`average\\_iteration\\_time` is the average time of iteration to train the booster\nmodel in the trial. `lgbm\\_params` is a JSON-serialized dictionary of LightGBM\nparameters used in the trial.\n* **optuna\\_callbacks** (*list**[**Callable**[**[**Study**,* *FrozenTrial**]**,* *None**]**]* *|* *None*) – List of Optuna callback functions that are invoked at the end of each trial.\nEach function must accept two parameters with the following types in this order:\n`Study` and `FrozenTrial`.\nPlease note that this is not a `callbacks` argument of lightgbm.train() .\n* **model\\_dir** (*str* *|* *None*) – A directory to save boosters. By default, it is set to `None` and no boosters are\nsaved. Please set shared directory (e.g., directories on NFS) if you want to access\n`get\\_best\\_booster()` in distributed\nenvironments. Otherwise, it may raise `ValueError`. If the directory does not\nexist, it will be created. The filenames of the boosters will be\n`{model\\_dir}/{trial\\_number}.pkl` (e.g., `./boosters/0.pkl`).\n* **verbosity** (*int* *|* *None*) – A verbosity level to change Optuna’s logging level. The level is aligned to\nLightGBM’s verbosity .\n\n\nDeprecated in v2.0.0. `verbosity` argument will be removed in the future.\nThe removal of this feature is currently scheduled for v4.0.0,\nbut this schedule is subject to change.\n\n\nPlease use `set\\_verbosity()` instead.\n* **show\\_progress\\_bar** (*bool*) – Flag to show progress bars or not. To disable progress bar, set this `False`.\n\n\nProgress bars will be fragmented by logging messages of LightGBM and Optuna.\nPlease suppress such messages to show the progress bars properly.\n* **optuna\\_seed** (*int* *|* *None*) – `seed` of `TPESampler` for random number generator\nthat affects sampling for `num\\_leaves`, `bagging\\_fraction`, `bagging\\_freq`,\n`lambda\\_l1`, and `lambda\\_l2`.\n\n\nThe deterministic parameter of LightGBM makes training reproducible.\nPlease enable it when you use this argument.\n* **params** (*dict**[**str**,* *Any**]*) –\n* **train\\_set** (*lgb.Dataset*) –\n* **num\\_boost\\_round** (*int*) –\n* **valid\\_sets** (*'VALID\\_SET\\_TYPE'* *|* *None*) –\n* **valid\\_names** (*Any* *|* *None*) –\n* **fobj** (*Callable**[**...**,* *Any**]* *|* *None*) –\n* **feval** (*Callable**[**...**,* *Any**]* *|* *None*) –\n* **feature\\_name** (*str*) –\n* **categorical\\_feature** (*str*) –\n* **early\\_stopping\\_rounds** (*int* *|* *None*) –\n* **evals\\_result** (*dict**[**Any**,* *Any**]* *|* *None*) –\n* **verbose\\_eval** (*bool* *|* *int* *|* *str* *|* *None*) –\n* **learning\\_rates** (*list**[**float**]* *|* *None*) –\n* **keep\\_training\\_booster** (*bool*) –\n* **callbacks** (*list**[**Callable**[**...**,* *Any**]**]* *|* *None*) –\n* **sample\\_size** (*int* *|* *None*) –\n\n\n|  |  |\n| --- | --- |\n| `compare\\_validation\\_metrics`(val\\_score, best\\_score) |  |\n| `get\\_best\\_booster`() | Return the best booster. |\n| `higher\\_is\\_better`() |  |\n| `run`() | Perform the hyperparameter-tuning with given parameters. |\n| `sample\\_train\\_set`() | Make subset of self.train\\_set Dataset object. |\n| `tune\\_bagging`([n\\_trials]) |  |\n| `tune\\_feature\\_fraction`([n\\_trials]) |  |\n| `tune\\_feature\\_fraction\\_stage2`([n\\_trials]) |  |\n| `tune\\_min\\_data\\_in\\_leaf`() |  |\n| `tune\\_num\\_leaves`([n\\_trials]) |  |\n| `tune\\_regularization\\_factors`([n\\_trials]) |  |\n\n\n|  |  |\n| --- | --- |\n| `best\\_params` | Return parameters of the best booster. |\n| `best\\_score` | Return the score of the best booster. |\n\n\n*property* best\\_params*: dict[str, Any]*\nReturn parameters of the best booster.\n\n*property* best\\_score*: float*\nReturn the score of the best booster.\n\nget\\_best\\_booster()[source]\nReturn the best booster.\n\n\nIf the best booster cannot be found, `ValueError` will be raised. To prevent the\nerrors, please save boosters by specifying the `model\\_dir` argument of\n`\\_\\_init\\_\\_()`,\nwhen you resume tuning or you run tuning in parallel.\n\nReturn type:\n*Booster*\n\nrun()\nPerform the hyperparameter-tuning with given parameters.\n\nsample\\_train\\_set()\nMake subset of self.train\\_set Dataset object.\n# optuna.integration.lightgbm.LightGBMTunerCV\n\n\n*class* optuna.integration.lightgbm.LightGBMTunerCV(*params*, *train\\_set*, *num\\_boost\\_round=1000*, *folds=None*, *nfold=5*, *stratified=True*, *shuffle=True*, *fobj=None*, *feval=None*, *feature\\_name='auto'*, *categorical\\_feature='auto'*, *early\\_stopping\\_rounds=None*, *fpreproc=None*, *verbose\\_eval=None*, *show\\_stdv=True*, *seed=0*, *callbacks=None*, *time\\_budget=None*, *sample\\_size=None*, *study=None*, *optuna\\_callbacks=None*, *verbosity=None*, *show\\_progress\\_bar=True*, *model\\_dir=None*, *return\\_cvbooster=False*, *\\**, *optuna\\_seed=None*)[source]\nHyperparameter tuner for LightGBM with cross-validation.\n\n\nIt employs the same stepwise approach as\n`LightGBMTuner`.\n`LightGBMTunerCV`\n\n==================\n Document 5 \n----------------\n optuna\n\n\nThe `optuna` module is primarily used as an alias for basic Optuna functionality coded in other modules. Currently, two modules are aliased: (1) from `optuna.study`, functions regarding the Study lifecycle, and (2) from `optuna.exceptions`, the TrialPruned Exception raised when a trial is pruned.\n\n\n|  |  |\n| --- | --- |\n| `optuna.create\\_study` | Create a new `Study`. |\n| `optuna.load\\_study` | Load the existing `Study` that has the specified name. |\n| `optuna.delete\\_study` | Delete a `Study` object. |\n| `optuna.copy\\_study` | Copy study from one storage to another. |\n| `optuna.get\\_all\\_study\\_summaries` | Get all history of studies stored in a specified storage. |\n| `optuna.TrialPruned` | Exception for pruned trials. |\n\n# optuna.artifacts\n\n\nThe `artifacts` module provides the way to manage artifacts (output files) in Optuna.\n\n\n|  |  |\n| --- | --- |\n| `optuna.artifacts.FileSystemArtifactStore` | An artifact backend for file systems. |\n| `optuna.artifacts.Boto3ArtifactStore` | An artifact backend for Boto3. |\n| `optuna.artifacts.upload\\_artifact` | Upload an artifact to the artifact store. |\n\n\n# optuna.cli\n\n\nThe `cli` module implements Optuna’s command-line functionality.\n\n\nFor detail, please see the result of\n\n```\n$ optuna --help\n\n```\n\nSee also\n\n\nThe Command-Line Interface tutorial provides use-cases with examples.\n\n\n# optuna.distributions\n\n\nThe `distributions` module defines various classes representing probability distributions, mainly used to suggest initial hyperparameter values for an optimization trial. Distribution classes inherit from a library-internal `BaseDistribution`, and is initialized with specific parameters, such as the `low` and `high`"
    },
    {
        "instruction": "Create a Python program using the 'optuna' API to optimize the hyperparameters of a linear discriminant analysis (LDA) model. The program should search for the best combination of 'n_components' and 'solver' hyperparameters for an LDA model on a custom dataset and print the explained variance ratio.",
        "api": "optuna",
        "output": "#!pip install optuna scikit-learn\nimport optuna\nimport numpy as np\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.datasets import make_classification\n\n# Generate a custom dataset\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n\n# Define the machine learning task: Linear Discriminant Analysis (LDA)\ndef objective(trial):\n    # Define hyperparameters to search\n    n_components = trial.suggest_int(\"n_components\", 1, 20)\n    solver = trial.suggest_categorical(\"solver\", [\"svd\", \"lsqr\", \"eigen\"])\n\n    # Create and evaluate the LDA model with the chosen hyperparameters\n    lda = LinearDiscriminantAnalysis(n_components=n_components, solver=solver)\n    lda.fit(X, y)\n    explained_variance = np.sum(lda.explained_variance_ratio_)\n\n    return explained_variance\n\n# Create a study and optimize hyperparameters\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=50)\n\n    # Get the best hyperparameters\n    best_params = study.best_params\n    n_components = best_params[\"n_components\"]\n    solver = best_params[\"solver\"]\n\n    print(\"Best Hyperparameters:\", best_params)\n\n    # Create and evaluate the LDA model with the best hyperparameters\n    lda = LinearDiscriminantAnalysis(n_components=n_components, solver=solver)\n    lda.fit(X, y)\n    explained_variance = np.sum(lda.explained_variance_ratio_\n\n)\n\n    print(\"Explained Variance Ratio with Best Hyperparameters:\", explained_variance)\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n\n# API Reference\n\n* optuna\n* optuna.artifacts\n* optuna.cli\n* optuna.distributions\n* optuna.exceptions\n* optuna.importance\n* optuna.integration\n* optuna.logging\n* optuna.pruners\n* optuna.samplers\n* optuna.search\\_space\n* optuna.storages\n* optuna.study\n* optuna.terminator\n* optuna.trial\n* optuna.visualization\n\n\n# optuna\n\n\nThe `optuna` module is primarily used as an alias for basic Optuna functionality coded in other modules. Currently, two modules are aliased: (1) from `optuna.study`, functions regarding the Study lifecycle, and (2) from `optuna.exceptions`, the TrialPruned Exception raised when\n\n==================\n Document 1 \n----------------\n optuna\n\n\nThe `optuna` module is primarily used as an alias for basic Optuna functionality coded in other modules. Currently, two modules are aliased: (1) from `optuna.study`, functions regarding the Study lifecycle, and (2) from `optuna.exceptions`, the TrialPruned Exception raised when a trial is pruned.\n\n\n|  |  |\n| --- | --- |\n| `optuna.create\\_study` | Create a new `Study`. |\n| `optuna.load\\_study` | Load the existing `Study` that has the specified name. |\n| `optuna.delete\\_study` | Delete a `Study` object. |\n| `optuna.copy\\_study` | Copy study from one storage to another. |\n| `optuna.get\\_all\\_study\\_summaries` | Get all history of studies stored in a specified storage. |\n| `optuna.TrialPruned` | Exception for pruned trials. |\n\n# optuna.artifacts\n\n\nThe `artifacts` module provides the way to manage artifacts (output files) in Optuna.\n\n\n|  |  |\n| --- | --- |\n| `optuna.artifacts.FileSystemArtifactStore` | An artifact backend for file systems. |\n| `optuna.artifacts.Boto3ArtifactStore` | An artifact backend for Boto3. |\n| `optuna.artifacts.upload\\_artifact` | Upload an artifact to the artifact store. |\n\n\n# optuna.cli\n\n\nThe `cli` module implements Optuna’s command-line functionality.\n\n\nFor detail, please see the result of\n\n```\n$ optuna --help\n\n```\n\nSee also\n\n\nThe Command-Line Interface tutorial provides use-cases with examples.\n\n\n# optuna.distributions\n\n\nThe `distributions` module defines various classes representing probability distributions, mainly used to suggest initial hyperparameter values for an optimization trial. Distribution classes inherit from a library-internal `BaseDistribution`, and is initialized with specific parameters, such as the `low` and `high`\n\n==================\n Document 2 \n----------------\noptuna.visualization.matplotlib\n\n\nThe following functions use Matplotlib as a backend.\n\n|  |  |\n| --- | --- |\n| `optuna.visualization.matplotlib.plot\\_contour` | Plot the parameter relationship as contour plot in a study with Matplotlib. |\n| `optuna.visualization.matplotlib.plot\\_edf` | Plot the objective value EDF (empirical distribution function) of a study with Matplotlib. |\n| `optuna.visualization.matplotlib.plot\\_hypervolume\\_history` | Plot hypervolume history of all trials in a study with Matplotlib. |\n| `optuna.visualization.matplotlib.plot\\_intermediate\\_values` | Plot intermediate values of all trials in a study with Matplotlib. |\n| `optuna.visualization.matplotlib.plot\\_optimization\\_history` | Plot optimization history of all trials in a study with Matplotlib. |\n| `optuna.visualization.matplotlib.plot\\_parallel\\_coordinate` | Plot the high-dimensional parameter relationships in a study with Matplotlib. |\n| `optuna.visualization.matplotlib.plot\\_param\\_importances` | Plot hyperparameter importances with Matplotlib. |\n| `optuna.visualization.matplotlib.plot\\_pareto\\_front` | Plot the Pareto front of a study. |\n| `optuna.visualization.matplotlib.plot\\_rank` | Plot parameter relations as scatter plots with colors indicating ranks of target value. |\n| `optuna.visualization.matplotlib.plot\\_slice` | Plot the parameter relationship as slice plot in a study with Matplotlib. |\n| `optuna.visualization.matplotlib.plot\\_terminator\\_improvement` | Plot the potentials for future objective improvement. |\n| `optuna.visualization.matplotlib.plot\\_timeline` | Plot the timeline of a study. |\n| `optuna.visualization.matplotlib.is\\_available` | Returns whether visualization with Matplotlib is available or not. |\n\n==================\n Document 3 \n----------------\n optuna.integration\n\n\nThe `integration` module contains classes used to integrate Optuna with external machine learning frameworks.\n\nNote\n\n\nOptuna’s integration modules for third-party libraries have started migrating from Optuna itself to a package called\noptuna-integration. Please check the repository and\nthe documentation.\n\nFor most of the ML frameworks supported by Optuna, the corresponding Optuna integration class serves only to implement a callback object and functions, compliant with the framework’s specific callback API, to be called with each intermediate step in the model training. The functionality implemented in these callbacks across the different ML frameworks includes:\n\n\n1. Reporting intermediate model scores back to the Optuna trial using `optuna.trial.Trial.report()`,\n2. According to the results of `optuna.trial.Trial.should\\_prune()`, pruning the current model by raising `optuna.TrialPruned()`, and\n3. Reporting intermediate Optuna data such as the current trial number back to the framework, as done in `MLflowCallback`.\n\n\nFor scikit-learn, an integrated `OptunaSearchCV` estimator is available that combines scikit-learn BaseEstimator functionality with access to a class-level `Study` object.\n\n## BoTorch\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.BoTorchSampler` | A sampler that uses BoTorch, a Bayesian optimization library built on top of PyTorch. |\n| `optuna.integration.botorch.logei\\_candidates\\_func` | Log Expected Improvement (LogEI). |\n| `optuna.integration.botorch.qei\\_candidates\\_func` | Quasi MC-based batch Expected Improvement (qEI). |\n| `optuna.integration.botorch.qnei\\_candidates\\_func` | Quasi MC-based batch Noisy Expected Improvement (qNEI). |\n| `optuna.integration.botorch.qehvi\\_candidates\\_func` | Quasi MC-based batch Expected Hypervolume Improvement (qEHVI). |\n| `optuna.integration.botorch.qnehvi\\_candidates\\_func` | Quasi MC-based batch Noisy Expected Hypervolume Improvement (qNEHVI). |\n| `optuna.integration.botorch.qparego\\_candidates\\_func` | Quasi MC-based extended ParEGO (qParEGO) for constrained multi-objective optimization. |\n\n\n\n## CatBoost\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.CatBoostPruningCallback` | Callback for catboost to prune unpromising trials. |\n\n\n\n## Dask\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.DaskStorage` | Dask-compatible storage class. |\n\n\n\n## fast.ai\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.FastAIV1PruningCallback` | FastAI callback to prune unpromising trials for fastai. |\n| `optuna.integration.FastAIV2PruningCallback` | FastAI callback to prune unpromising trials for fastai. |\n| `optuna.integration.FastAIPruningCallback` | alias of `FastAIV2PruningCallback` |\n\n\n\n## LightGBM\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.LightGBMPruningCallback` | Callback for LightGBM to prune unpromising trials. |\n| `optuna.integration.lightgbm.train` | Wrapper of LightGBM Training API to tune hyperparameters. |\n| `optuna.integration.lightgbm.LightGBMTuner` | Hyperparameter tuner for LightGBM. |\n| `optuna.integration.lightgbm.LightGBMTunerCV` | Hyperparameter tuner for LightGBM with cross-validation. |\n\n\n\n## MLflow\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.MLflowCallback` | Callback to track Optuna trials with MLflow. |\n\n\n\n## Weights & Biases\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.WeightsAndBiasesCallback` | Callback to track Optuna trials with Weights & Biases. |\n\n\n\n## pycma\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.PyCmaSampler` | A Sampler using cma library as the backend. |\n| `optuna.integration.CmaEsSampler` | Wrapper class of PyCmaSampler for backward compatibility. |\n\n\n\n## PyTorch\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.PyTorchIgnitePruningHandler` | PyTorch Ignite handler to prune unpromising trials. |\n| `optuna.integration.PyTorchLightningPruningCallback` | PyTorch Lightning callback to prune unpromising trials. |\n| `optuna.integration.TorchDistributedTrial` | A wrapper of `Trial` to incorporate Optuna with PyTorch distributed. |\n\n\n\n## scikit-learn\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.OptunaSearchCV` | Hyperparameter search with cross-validation. |\n\n\n\n## scikit-optimize\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.SkoptSampler` | Sampler using Scikit-Optimize as the backend. |\n\n\n\n## SHAP\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.ShapleyImportanceEvaluator` | Shapley (SHAP) parameter importance evaluator. |\n\n\n\n## TensorFlow\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.TensorBoardCallback` | Callback to track Optuna trials with TensorBoard. |\n\n\n\n## XGBoost\n\n\n|  |  |\n| --- | --- |\n| `optuna.integration.XGBoostPruningCallback` | Callback for XGBoost to prune unpromising trials. |\n\n\n# optuna.logging\n\n\nThe `logging` module implements logging using the Python `logging` package. Library users may be especially interested in setting verbosity levels using `set\\_verbosity()` to one of `optuna.logging.CRITICAL` (aka `optuna.logging.FATAL`), `optuna.logging.ERROR`, `optuna.logging.WARNING` (aka `optuna.logging.WARN`), `optuna.logging.INFO`, or `optuna.logging.DEBUG`.\n\n\n|  |  |\n|"
    },
    {
        "instruction": "Create a Python program that uses the 'pandas' API to read a JSON file containing sales data, calculate total sales revenue for each product, identify the product with the highest revenue, and create a summary DataFrame.",
        "api": "pandas",
        "output": "#!pip install pandas\nimport pandas as pd\n\n# Read the JSON file into a DataFrame\ndf = pd.read_json('sales_data.json')\n\n# Calculate total sales revenue for each product\nproduct_sales = df.groupby('Product')['Quantity'].sum() * df.groupby('Product')['Price'].sum()\nproduct_sales = product_sales.reset_index()\nproduct_sales.columns = ['Product', 'Total Revenue']\n\n# Identify the product with the highest revenue\nhighest_revenue_product = product_sales[product_sales['Total Revenue'] == product_sales['Total Revenue'].max()]\n\n# Create a summary DataFrame\nsummary_df = pd.concat([highest_revenue_product, product_sales], axis=1)\n\nprint(\"Summary Dataframe: \")\nprint(summary_df)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n# Excel#\n\n\n|  |  |\n| --- | --- |\n| `read\\_excel`(io[,Â sheet\\_name,Â header,Â names,Â ...]) | Read an Excel file into a pandas DataFrame. |\n| `DataFrame.to\\_excel`(excel\\_writer[,Â ...]) | Write object to an Excel sheet. |\n| `ExcelFile`(path\\_or\\_buffer[,Â engine,Â ...]) | Class for parsing tabular Excel sheets into DataFrame objects. |\n| `ExcelFile.book` |  |\n| `ExcelFile.sheet\\_names` |  |\n| `ExcelFile.parse`([sheet\\_name,Â header,Â names,Â ...]) | Parse specified sheet(s) into a DataFrame. |\n\n\n|  |  |\n| --- | --- |\n| `Styler.to\\_excel`(excel\\_writer[,Â sheet\\_name,Â ...]) | Write Styler to an Excel sheet. |\n\n\n|  |  |\n| --- | --- |\n| `ExcelWriter`(path[,Â engine,Â date\\_format,Â ...]) | Class for writing DataFrame objects into excel sheets. |\n\n\n## JSON#\n\n\n|  |  |\n| --- | --- |\n| `read\\_json`(path\\_or\\_buf,Â \\*[,Â orient,Â typ,Â ...]) | Convert a JSON string to pandas object. |\n| `json\\_normalize`(data[,Â record\\_path,Â meta,Â ...]) | Normalize semi-structured JSON data into a flat table. |\n| `DataFrame.to\\_json`([path\\_or\\_buf,Â orient,Â ...]) | Convert the object to a JSON string. |\n\n\n|  |  |\n| --- | --- |\n| `build\\_table\\_schema`(data[,Â index,Â ...]) | Create a Table schema from `data`. |\n\n\n\n## HTML#\n\n\n|  |  |\n| --- | --- |\n| `read\\_html`(io,Â \\*[,Â match,Â flavor,Â header,Â ...]) | Read HTML tables into a `list` of `DataFrame` objects. |\n| `DataFrame.to\\_html`([buf,Â columns,Â col\\_space,Â ...]) | Render a DataFrame as an HTML table. |\n\n\n|  |  |\n| --- | --- |\n| `Styler.to\\_html`([buf,Â table\\_uuid,Â ...]) | Write Styler to a file, buffer or string in HTML-CSS format. |\n\n\n\n## XML#\n\n\n|  |  |\n| --- | --- |\n| `read\\_xml`(path\\_or\\_buffer,Â \\*[,Â xpath,Â ...]) | Read XML document into a `DataFrame` object. |\n| `DataFrame.to\\_xml`([path\\_or\\_buffer,Â index,Â ...]) | Render a DataFrame to an XML document. |\n\n\n\n## Latex#\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.to\\_latex`([buf,Â columns,Â header,Â ...]) | Render object to a LaTeX tabular, longtable, or nested table. |\n\n\n|  |  |\n| --- | --- |\n| `Styler.to\\_latex`([buf,Â column\\_format,Â ...]) | Write Styler to a file, buffer or string in LaTeX format. |\n\n\n## HDFStore: PyTables (HDF5)#\n\n\n|  |  |\n| --- | --- |\n| `read\\_hdf`(path\\_or\\_buf[,Â key,Â mode,Â errors,Â ...]) | Read from the store, close it if we opened it. |\n| `HDFStore.put`(key,Â value[,Â format,Â index,Â ...]) | Store object in HDFStore. |\n| `HDFStore.append`(key,Â value[,Â format,Â axes,Â ...]) | Append to Table in file. |\n|\n\n==================\n Document 1 \n----------------\n API reference#\n\n\nThis page gives an overview of all public pandas objects, functions and\nmethods. All classes and functions exposed in `pandas.\\*` namespace are public.\n\n\nThe following subpackages are public.\n\n\n* `pandas.errors`: Custom exception and warnings classes that are raised by pandas.\n* `pandas.plotting`: Plotting public API.\n* `pandas.testing`: Functions that are useful for writing tests involving pandas objects.\n* `pandas.api.extensions`: Functions and classes for extending pandas objects.\n* `pandas.api.indexers`: Functions and classes for rolling window indexers.\n* `pandas.api.interchange`: DataFrame interchange protocol.\n* `pandas.api.types`: Datatype classes and functions.\n* `pandas.api.typing`: Classes that may be necessary for type-hinting.\nThese are classes that are encountered as intermediate results but should not be instantiated\ndirectly by users. These classes are not to be confused with classes from the\npandas-stubs package\nwhich has classes in addition to those that occur in pandas for type-hinting.\n\n\nIn addition, public functions in `pandas.io` and `pandas.tseries` submodules\nare mentioned in the documentation.\n\nWarning\n\n\nThe `pandas.core`, `pandas.compat`, and `pandas.util` top-level modules are PRIVATE. Stable functionality in such modules is not guaranteed.\n\n\n* Input/output\n\t+ Pickling\n\t+ Flat file\n\t+ Clipboard\n\t+ Excel\n\t+ JSON\n\t+ HTML\n\t+ XML\n\t+ Latex\n\t+ HDFStore: PyTables (HDF5)\n\t+ Feather\n\t+ Parquet\n\t+ ORC\n\t+ SAS\n\t+ SPSS\n\t+ SQL\n\t+ Google BigQuery\n\t+ STATA\n* General functions\n\t+ Data manipulations\n\t+ Top-level missing data\n\t+ Top-level dealing with numeric data\n\t+ Top-level dealing with datetimelike data\n\t+ Top-level dealing with Interval data\n\t+ Top-level evaluation\n\t+ Hashing\n\t+ Importing from other DataFrame libraries\n* Series\n\t+ Constructor\n\t+ Attributes\n\t+ Conversion\n\t+ Indexing, iteration\n\t+ Binary operator functions\n\t+ Function application, GroupBy & window\n\t+ Computations / descriptive stats\n\t+ Reindexing / selection / label manipulation\n\t+ Missing data handling\n\t+ Reshaping, sorting\n\t+ Combining / comparing / joining / merging\n\t+ Time Series-related\n\t+ Accessors\n\t+ Plotting\n\t+ Serialization / IO / conversion\n* DataFrame\n\t+ Constructor\n\t+ Attributes and underlying data\n\t+ Conversion\n\t+ Indexing, iteration\n\t+ Binary operator functions\n\t+ Function application, GroupBy & window\n\t+ Computations / descriptive stats\n\t+ Reindexing / selection / label manipulation\n\t+ Missing data handling\n\t+ Reshaping, sorting, transposing\n\t+ Combining / comparing / joining / merging\n\t+ Time Series-related\n\t+ Flags\n\t+ Metadata\n\t+ Plotting\n\t+ Sparse accessor\n\t+ Serialization / IO / conversion\n* pandas arrays, scalars, and data types\n\t+ Objects\n\t+ Utilities\n* Index objects\n\t+ Index\n\t+ Numeric Index\n\t+ CategoricalIndex\n\t+ IntervalIndex\n\t+ MultiIndex\n\t+ DatetimeIndex\n\t+ TimedeltaIndex\n\t+ PeriodIndex\n* Date offsets\n\t+ DateOffset\n\t+ BusinessDay\n\t+ BusinessHour\n\t+ CustomBusinessDay\n\t+ CustomBusinessHour\n\t+ MonthEnd\n\t+ MonthBegin\n\t+ BusinessMonthEnd\n\t+ BusinessMonthBegin\n\t+ CustomBusinessMonthEnd\n\t+ CustomBusinessMonthBegin\n\t+ SemiMonthEnd\n\t+ SemiMonthBegin\n\t+ Week\n\t+ WeekOfMonth\n\t+ LastWeekOfMonth\n\t+ BQuarterEnd\n\t+ BQuarterBegin\n\t+ QuarterEnd\n\t+ QuarterBegin\n\t+ BYearEnd\n\t+ BYearBegin\n\t+ YearEnd\n\t+ YearBegin\n\t+ FY5253\n\t+ FY5253Quarter\n\t+ Easter\n\t+ Tick\n\t+ Day\n\t+ Hour\n\t+ Minute\n\t+ Second\n\t+ Milli\n\t+ Micro\n\t+ Nano\n* Frequencies\n\t+ pandas.tseries.frequencies.to\\_offset\n* Window\n\t+ Rolling window functions\n\t+ Weighted window functions\n\t+ Expanding window functions\n\t+ Exponentially-weighted window functions\n\t+ Window indexer\n* GroupBy\n\t+ Indexing, iteration\n\t+ Function application helper\n\t+ Function application\n\t+ `DataFrameGroupBy` computations / descriptive stats\n\t+ `SeriesGroupBy` computations / descriptive stats\n\t+ Plotting and visualization\n* Resampling\n\t+ Indexing, iteration\n\t+ Function application\n\t+ Upsampling\n\t+ Computations / descriptive stats\n* Style\n\t+ Styler constructor\n\t+ Styler properties\n\t+ Style application\n\t+ Builtin styles\n\t+ Style export and import\n* Plotting\n\t+ pandas.plotting.andrews\\_curves\n\t+ pandas.plotting.autocorrelation\\_plot\n\t+ pandas.plotting.bootstrap\\_plot\n\t+ pandas.plotting.boxplot\n\t+ pandas.plotting.deregister\\_matplotlib\\_converters\n\t+ pandas.plotting.lag\\_plot\n\t+ pandas.plotting.parallel\\_coordinates\n\t+ pandas.plotting.plot\\_params\n\t+ pandas.plotting.radviz\n\t+ pandas.plotting.register\\_matplotlib\\_converters\n\t+ pandas.plotting.scatter\\_matrix\n\t+ pandas.plotting.table\n* Options and settings\n\t+ Working with options\n\t+ Numeric formatting\n* Extensions\n\t+ pandas.api.extensions.register\\_extension\\_dtype\n\t+ pandas.api.extensions.register\\_dataframe\\_accessor\n\t+ pandas.api.extensions.register\\_series\\_accessor\n\t+ pandas.api.extensions.register\\_index\\_accessor\n\t+ pandas.api.extensions.ExtensionDtype\n\t+ pandas.api.extensions.ExtensionArray\n\t+ pandas.arrays.NumpyExtensionArray\n\t+ pandas.api.indexers.check\\_array\\_indexer\n* Testing\n\t+ Assertion functions\n\t+ Exceptions and warnings\n\t+ Bug report function\n\t+ Test suite runner\n\n\n# Input/output#\n\n\n## Pickling#\n\n\n|  |  |\n| --- | --- |\n| `read\\_pickle`(filepath\\_or\\_buffer[,Â ...]) | Load pickled pandas object (or any object) from file. |\n| `DataFrame.to\\_pickle`(path[,Â compression,Â ...]) | Pickle (serialize) object to file. |\n\n\n\n## Flat file#\n\n\n|  |  |\n| --- | --- |\n| `read\\_table`(filepath\\_or\\_buffer,Â \\*[,Â sep,Â ...]) | Read general delimited file into DataFrame. |\n| `read\\_csv`(filepath\\_or\\_buffer,Â \\*[,Â sep,Â ...]) | Read a comma-separated values (csv) file into DataFrame. |\n| `DataFrame.to\\_csv`([path\\_or\\_buf,Â sep,Â na\\_rep,Â ...]) | Write object to a comma-separated values (csv) file. |\n| `read\\_fwf`(filepath\\_or\\_buffer,Â \\*[,Â colspecs,Â ...]) | Read a table of fixed-width formatted lines into DataFrame. |\n\n\n\n## Clipboard#\n\n\n|  |  |\n| --- | --- |\n| `read\\_clipboard`([sep,Â dtype\\_backend]) | Read text from clipboard and pass to `read\\_csv()`. |\n| `DataFrame.to\\_clipboard`([excel,Â sep]) | Copy object to the system clipboard. |\n\n\n## Excel#\n\n\n|  |  |\n| --- | --- |\n| `read\\_excel`(io[,Â sheet\\_name,Â header,Â names,Â ...]) | Read an Excel file into a pandas DataFrame. |\n| `DataFrame.to\\_excel`(excel\\_writer[,Â ...]) | Write object to an Excel sheet. |\n| `ExcelFile`(path\\_or\\_buffer[,Â engine,Â ...]) | Class for parsing tabular Excel sheets into DataFrame\n\n==================\n Document 2 \n----------------\n# Attributes and underlying data#\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.index` | The index (row labels) of the DataFrame. |\n| `DataFrame.columns` | The column labels of the DataFrame. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.dtypes` | Return the dtypes in the DataFrame. |\n| `DataFrame.info`([verbose,Â buf,Â max\\_cols,Â ...]) | Print a concise summary of a DataFrame. |\n| `DataFrame.select\\_dtypes`([include,Â exclude]) | Return a subset of the DataFrame's columns based on the column dtypes. |\n| `DataFrame.values` | Return a Numpy representation of the DataFrame. |\n| `DataFrame.axes` | Return a list representing the axes of the DataFrame. |\n| `DataFrame.ndim` | Return an int representing the number of axes / array dimensions. |\n| `DataFrame.size` | Return an int representing the number of elements in this object. |\n| `DataFrame.shape` | Return a tuple representing the dimensionality of the DataFrame. |\n| `DataFrame.memory\\_usage`([index,Â deep]) | Return the memory usage of each column in bytes. |\n| `DataFrame.empty` | Indicator whether Series/DataFrame is empty. |\n| `DataFrame.set\\_flags`(\\*[,Â copy,Â ...]) | Return a new object with updated flags. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.astype`(dtype[,Â copy,Â errors]) | Cast a pandas object to a specified dtype `dtype`. |\n| `DataFrame.convert\\_dtypes`([infer\\_objects,Â ...]) | Convert columns to the best possible dtypes using dtypes supporting `pd.NA`. |\n| `DataFrame.infer\\_objects`([copy]) | Attempt to infer better dtypes for object columns. |\n| `DataFrame.copy`([deep]) | Make a copy of this object's indices and data. |\n| `DataFrame.bool`() | (DEPRECATED) Return the bool of a single element Series or DataFrame. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.head`([n]) | Return the first n rows. |\n| `DataFrame.at` | Access a single value for a row/column label pair. |\n| `DataFrame.iat` | Access a single value for a row/column pair by integer position. |\n| `DataFrame.loc` | Access a group of rows and columns by label(s) or a boolean array. |\n| `DataFrame.iloc` | Purely integer-location based indexing for selection by position. |\n| `DataFrame.insert`(loc,Â column,Â value[,Â ...]) | Insert column into DataFrame at specified location. |\n| `DataFrame.\\_\\_iter\\_\\_`() | Iterate over info axis. |\n| `DataFrame.items`() | Iterate over (column name, Series) pairs. |\n| `DataFrame.keys`() | Get the 'info axis' (see Indexing for more). |\n| `DataFrame.iterrows`() | Iterate over DataFrame rows as (index, Series) pairs. |\n| `DataFrame.itertuples`([index,Â name]) | Iterate over DataFrame rows as namedtuples. |\n| `DataFrame.pop`(item) | Return item and drop from frame. |\n| `DataFrame.tail`([n]) | Return the last n rows. |\n| `DataFrame.xs`(key[,Â axis,Â level,Â drop\\_level]) | Return cross-section from the Series/DataFrame. |\n| `DataFrame.get`(key[,Â default]) | Get item from object for given key (ex: DataFrame column). |\n| `DataFrame.isin`(values) | Whether each element in the DataFrame is contained in values. |\n| `DataFrame.where`(cond[,Â other,Â inplace,Â ...]) | Replace values where the condition is False. |\n| `DataFrame.mask`(cond[,Â other,Â inplace,Â axis,Â ...]) | Replace values where the condition is True. |\n| `DataFrame.query`(expr,Â \\*[,Â inplace]) | Query the columns of a DataFrame with a boolean expression. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.\\_\\_add\\_\\_`(other) | Get Addition of DataFrame and other, column-wise. |\n| `DataFrame.add`(other[,Â axis,Â level,Â fill\\_value]) | Get Addition of dataframe and other, element-wise (binary operator add). |\n| `DataFrame.sub`(other[,Â axis,Â level,Â fill\\_value]) | Get Subtraction of dataframe and other, element-wise (binary operator sub). |\n| `DataFrame.mul`(other[,Â axis,Â level,Â fill\\_value]) | Get Multiplication of dataframe and other, element-wise (binary operator mul). |\n| `DataFrame.div`(other[,Â axis,Â level,Â fill\\_value]) | Get Floating division of dataframe and other, element-wise (binary operator truediv). |\n| `DataFrame.truediv`(other[,Â axis,Â level,Â ...]) | Get Floating division of dataframe and other, element-wise (binary operator truediv). |\n| `DataFrame.floordiv`(other[,Â axis,Â level,Â ...]) | Get Integer division of dataframe and other, element-wise (binary operator floordiv). |\n| `DataFrame.mod`(other[,Â axis,Â level,Â fill\\_value]) | Get Modulo of dataframe and other, element-wise (binary operator mod). |\n| `DataFrame.pow`(other[,Â axis,Â level,Â fill\\_value]) | Get Exponential power of dataframe and other, element-wise (binary operator pow). |\n| `DataFrame.dot`(other) | Compute the matrix multiplication between the DataFrame and other. |\n| `DataFrame.radd`(other[,Â axis,Â level,Â fill\\_value]) | Get Addition of dataframe and other, element-wise (binary operator radd). |\n| `DataFrame.rsub`(other[,Â axis,Â level,Â fill\\_value]) | Get Subtraction of dataframe and other, element-wise (binary operator rsub). |\n| `DataFrame.rmul`(other[,Â axis,Â level,Â fill\\_value]) | Get Multiplication of dataframe and other, element-wise (binary operator rmul). |\n| `DataFrame.rdiv`(other[,Â axis,Â level,Â fill\\_value]) | Get Floating division of dataframe and other, element-wise (binary operator rtruediv). |\n| `DataFrame.rtruediv`(other[,Â axis,Â level,Â ...]) | Get Floating division of dataframe and other, element-wise (binary operator rtruediv). |\n| `DataFrame.rfloordiv`(other[,Â axis,Â level,Â ...]) | Get Integer division of dataframe and other, element-wise (binary operator rfloordiv). |\n| `DataFrame.rmod`(other[,Â axis,Â level,Â fill\\_value]) | Get Modulo of dataframe and other, element-wise (binary operator rmod). |\n| `DataFrame.rpow`(other[,Â axis,Â level,Â fill\\_value]) | Get Exponential power of dataframe and other, element-wise (binary operator rpow). |\n| `DataFrame.lt`(other[,Â axis,Â level]) | Get Less than of dataframe and other, element-wise (binary operator lt). |\n| `DataFrame.gt`(other[,Â axis,Â level]) | Get Greater than of dataframe and other, element-wise (binary operator gt). |\n| `DataFrame.le`(other[,Â axis,Â level]) | Get Less than or equal to of dataframe and other, element-wise (binary operator le). |\n| `DataFrame.ge`(other[,Â axis,Â level]) | Get Greater than or equal to of dataframe and other, element-wise (binary operator ge). |\n| `DataFrame.ne`(other[,Â axis,Â level]) | Get Not equal to of dataframe and other, element-wise (binary operator ne). |\n| `DataFrame.eq`(other[,Â axis,Â level]) | Get Equal to of dataframe and other, element-wise (binary operator eq). |\n| `DataFrame.combine`(other,Â func[,Â fill\\_value,Â ...]) | Perform column-wise combine with another DataFrame. |\n| `DataFrame.combine\\_first`(other) | Update null elements with value in the same location in other. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.apply`(func[,Â axis,Â raw,Â ...]) | Apply a function along an axis of the DataFrame. |\n| `DataFrame.map`(func[,Â na\\_action]) | Apply a function to a Dataframe elementwise. |\n| `DataFrame.applymap`(func[,Â na\\_action]) | (DEPRECATED) Apply a function to a Dataframe elementwise. |\n| `DataFrame.pipe`(func,Â \\*args,Â \\*\\*kwargs) | Apply chainable functions that expect Series or DataFrames. |\n| `DataFrame.agg`([func,Â axis]) | Aggregate using one or more operations over the specified axis. |\n| `DataFrame.aggregate`([func,Â axis]) | Aggregate using one or more operations over the specified axis. |\n| `DataFrame.transform`(func[,Â axis]) | Call `func` on self producing a DataFrame with the same axis shape as self. |\n| `DataFrame.groupby`([by,Â axis,Â level,Â ...]) | Group DataFrame using a mapper or by a Series of columns. |\n| `DataFrame.rolling`(window[,Â min\\_periods,Â ...]) | Provide rolling window calculations. |\n| `DataFrame.expanding`([min\\_periods,Â axis,Â method]) | Provide expanding window calculations. |\n| `DataFrame.ewm`([com,Â span,Â halflife,Â alpha,Â ...]) | Provide exponentially weighted (EW) calculations. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.abs`() | Return a Series/DataFrame with absolute numeric value of each element. |\n| `DataFrame.all`([axis,Â bool\\_only,Â skipna]) | Return whether all elements are True, potentially over an axis. |\n| `DataFrame.any`(\\*[,Â axis,Â bool\\_only,Â skipna]) | Return whether any element is True, potentially over an axis. |\n| `DataFrame.clip`([lower,Â upper,Â axis,Â inplace]) | Trim values at input threshold(s). |\n| `DataFrame.corr`([method,Â min\\_periods,Â ...]) | Compute pairwise correlation of columns, excluding NA/null values. |\n| `DataFrame.corrwith`(other[,Â axis,Â drop,Â ...]) | Compute pairwise correlation. |\n| `DataFrame.count`([axis,Â numeric\\_only]) | Count non-NA cells for each column or row. |\n| `DataFrame.cov`([min\\_periods,Â ddof,Â numeric\\_only]) | Compute pairwise covariance of columns, excluding NA/null values. |\n| `DataFrame.cummax`([axis,Â skipna]) | Return cumulative maximum over a DataFrame or Series axis. |\n| `DataFrame.cummin`([axis,Â skipna]) | Return cumulative minimum over a DataFrame or Series axis. |\n| `DataFrame.cumprod`([axis,Â skipna]) | Return cumulative product over a DataFrame or Series axis. |\n| `DataFrame.cumsum`([axis,Â skipna]) | Return cumulative sum over a DataFrame or Series axis. |\n| `DataFrame.describe`([percentiles,Â include,Â ...]) | Generate descriptive statistics. |\n| `DataFrame.diff`([periods,Â axis]) | First discrete difference of element. |\n| `DataFrame.eval`(expr,Â \\*[,Â inplace]) | Evaluate a string describing operations on DataFrame columns. |\n| `DataFrame.kurt`([axis,Â skipna,Â numeric\\_only]) | Return unbiased kurtosis over requested axis. |\n| `DataFrame.kurtosis`([axis,Â skipna,Â numeric\\_only]) | Return unbiased kurtosis over requested axis. |\n| `DataFrame.max`([axis,Â skipna,Â numeric\\_only]) | Return the maximum of the values over the requested axis. |\n| `DataFrame.mean`([axis,Â skipna,Â numeric\\_only]) | Return the mean of the values over the requested axis. |\n| `DataFrame.median`([axis,Â skipna,Â numeric\\_only]) | Return the median of the values over the requested axis. |\n| `DataFrame.min`([axis,Â skipna,Â numeric\\_only]) | Return the minimum of the values over the requested axis. |\n| `DataFrame.mode`([axis,Â numeric\\_only,Â dropna]) | Get the mode(s) of each element along the selected axis. |\n| `DataFrame.pct\\_change`([periods,Â fill\\_method,Â ...]) | Fractional change between the current and a prior element. |\n| `DataFrame.prod`([axis,Â skipna,Â numeric\\_only,Â ...]) | Return the product of the values over the requested axis. |\n| `DataFrame.product`([axis,Â skipna,Â ...]) | Return the product of the values over the requested axis. |\n| `DataFrame.quantile`([q,Â axis,Â numeric\\_only,Â ...]) | Return values at the given quantile over requested axis. |\n| `DataFrame.rank`([axis,Â method,Â numeric\\_only,Â ...]) | Compute numerical data ranks (1 through n) along axis. |\n| `DataFrame.round`([decimals]) | Round a DataFrame to a variable number of decimal places. |\n| `DataFrame.sem`([axis,Â skipna,Â ddof,Â numeric\\_only]) | Return unbiased standard error of the mean over requested axis. |\n| `DataFrame.skew`([axis,Â skipna,Â numeric\\_only]) | Return unbiased skew over requested axis. |\n| `DataFrame.sum`([axis,Â skipna,Â numeric\\_only,Â ...]) | Return the sum of the values over the requested axis. |\n| `DataFrame.std`([axis,Â skipna,Â ddof,Â numeric\\_only]) | Return sample standard deviation over requested axis. |\n| `DataFrame.var`([axis,Â skipna,Â ddof,Â numeric\\_only]) | Return unbiased variance over requested axis. |\n| `DataFrame.nunique`([axis,Â dropna]) | Count number of distinct elements in specified axis. |\n| `DataFrame.value\\_counts`([subset,Â normalize,Â ...]) | Return a Series containing the frequency of each distinct row in the Dataframe. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.add\\_prefix`(prefix[,Â axis]) | Prefix labels with string prefix. |\n| `DataFrame.add\\_suffix`(suffix[,Â axis]) | Suffix labels with string suffix. |\n| `DataFrame.align`(other[,Â join,Â axis,Â level,Â ...]) | Align two objects on their axes with the specified join method. |\n| `DataFrame.at\\_time`(time[,Â asof,Â axis]) | Select values at particular time of day (e.g., 9:30AM). |\n| `DataFrame.between\\_time`(start\\_time,Â end\\_time) | Select values between particular times of the day (e.g., 9:00-9:30 AM). |\n| `DataFrame.drop`([labels,Â axis,Â index,Â ...]) | Drop specified labels from rows or columns. |\n| `DataFrame.drop\\_duplicates`([subset,Â keep,Â ...]) | Return DataFrame with duplicate rows removed. |\n| `DataFrame.duplicated`([subset,Â keep]) | Return boolean Series denoting duplicate rows. |\n| `DataFrame.equals`(other) | Test whether two objects contain the same elements. |\n| `DataFrame.filter`([items,Â like,Â regex,Â axis]) | Subset the dataframe rows or columns according to the specified index labels. |\n| `DataFrame.first`(offset) | Select initial periods of time series data based on a date offset. |\n| `DataFrame.head`([n]) | Return the first n rows. |\n| `DataFrame.idxmax`([axis,Â skipna,Â numeric\\_only]) | Return index of first occurrence of maximum over requested axis. |\n| `DataFrame.idxmin`([axis,Â skipna,Â numeric\\_only]) | Return index of first occurrence of minimum over requested axis. |\n| `DataFrame.last`(offset) | Select final periods of time series data based on a date offset. |\n| `DataFrame.reindex`([labels,Â index,Â columns,Â ...]) | Conform DataFrame to new index with optional filling logic. |\n| `DataFrame.reindex\\_like`(other[,Â method,Â ...]) | Return an object with matching indices as other object. |\n| `DataFrame.rename`([mapper,Â index,Â columns,Â ...]) | Rename columns or index labels. |\n| `DataFrame.rename\\_axis`([mapper,Â index,Â ...]) | Set the name of the axis for the index or columns. |\n| `DataFrame.reset\\_index`([level,Â drop,Â ...]) | Reset the index, or a level of it. |\n| `DataFrame.sample`([n,Â frac,Â replace,Â ...]) | Return a random sample of items from an axis of object. |\n| `DataFrame.set\\_axis`(labels,Â \\*[,Â axis,Â copy]) | Assign desired index to given axis. |\n| `DataFrame.set\\_index`(keys,Â \\*[,Â drop,Â append,Â ...]) | Set the DataFrame index using existing columns. |\n| `DataFrame.tail`([n]) | Return the last n rows. |\n| `DataFrame.take`(indices[,Â axis]) | Return the elements in the given *positional* indices along an axis. |\n| `DataFrame.truncate`([before,Â after,Â axis,Â copy]) | Truncate a Series or DataFrame before and after some index value. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.backfill`(\\*[,Â axis,Â inplace,Â ...]) | (DEPRECATED) Fill NA/NaN values by using the next valid observation to fill the gap. |\n| `DataFrame.bfill`(\\*[,Â axis,Â inplace,Â limit,Â ...]) | Fill NA/NaN values by using the next valid observation to fill the gap. |\n| `DataFrame.dropna`(\\*[,Â axis,Â how,Â thresh,Â ...]) | Remove missing values. |\n| `DataFrame.ffill`(\\*[,Â axis,Â inplace,Â limit,Â ...]) | Fill NA/NaN values by propagating the last valid observation to next valid. |\n| `DataFrame.fillna`([value,Â method,Â axis,Â ...]) | Fill NA/NaN values using the specified method. |\n| `DataFrame.interpolate`([method,Â axis,Â limit,Â ...]) | Fill NaN values using an interpolation method. |\n| `DataFrame.isna`() | Detect missing values. |\n| `DataFrame.isnull`() | DataFrame.isnull is an alias for DataFrame.isna. |\n| `DataFrame.notna`() | Detect existing (non-missing) values. |\n| `DataFrame.notnull`() | DataFrame.notnull is an alias for DataFrame.notna. |\n| `DataFrame.pad`(\\*[,Â axis,Â inplace,Â limit,Â ...]) | (DEPRECATED) Fill NA/NaN values by propagating the last valid observation to next valid. |\n| `DataFrame.replace`([to\\_replace,Â value,Â ...]) | Replace values given in to\\_replace with value. |\n\n## Reshaping, sorting, transposing#\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.droplevel`(level[,Â axis]) | Return Series/DataFrame with requested index / column level(s) removed. |\n| `DataFrame.pivot`(\\*,Â columns[,Â index,Â values]) | Return reshaped DataFrame organized by given index / column values. |\n| `DataFrame.pivot\\_table`([values,Â index,Â ...]) | Create\n\n==================\n Document 3 \n----------------\n\n# API reference#\n\n\nThis page gives an overview of all public pandas objects, functions and\nmethods. All classes and functions exposed in `pandas.\\*` namespace are public.\n\n\nThe following subpackages are public.\n\n\n* `pandas.errors`: Custom exception and warnings classes that are raised by pandas.\n* `pandas.plotting`:"
    },
    {
        "instruction": "Create a Python program that uses the 'pandas' API to read a CSV file containing sales data, calculate the total sales revenue for each customer, identify the customer with the highest sales revenue, and create a summary DataFrame.",
        "api": "pandas",
        "output": "#!pip install pandas\nimport pandas as pd\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv('sales_data.csv')\n\n# Calculate the total sales revenue for each customer\ncustomer_total_revenue = df.groupby('Customer')['Revenue'].sum()\ncustomer_total_revenue = customer_total_revenue.reset_index()\ncustomer_total_revenue.columns = ['Customer', 'Total Revenue']\n\n# Identify the customer with the highest sales revenue\nhighest_total_revenue_customer = customer_total_revenue[customer_total_revenue['Total Revenue'] == customer_total_revenue['Total Revenue'].max()]\n\n# Create a summary DataFrame\nsummary_df = pd.concat([highest_total_revenue_customer, customer_total_revenue], axis=1)\n\nprint(\"Summary Dataframe: \")\nprint(summary_df)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n# Excel#\n\n\n|  |  |\n| --- | --- |\n| `read\\_excel`(io[,Â sheet\\_name,Â header,Â names,Â ...]) | Read an Excel file into a pandas DataFrame. |\n| `DataFrame.to\\_excel`(excel\\_writer[,Â ...]) | Write object to an Excel sheet. |\n| `ExcelFile`(path\\_or\\_buffer[,Â engine,Â ...]) | Class for parsing tabular Excel sheets into DataFrame objects. |\n| `ExcelFile.book` |  |\n| `ExcelFile.sheet\\_names` |  |\n| `ExcelFile.parse`([sheet\\_name,Â header,Â names,Â ...]) | Parse specified sheet(s) into a DataFrame. |\n\n\n|  |  |\n| --- | --- |\n| `Styler.to\\_excel`(excel\\_writer[,Â sheet\\_name,Â ...]) | Write Styler to an Excel sheet. |\n\n\n|  |  |\n| --- | --- |\n| `ExcelWriter`(path[,Â engine,Â date\\_format,Â ...]) | Class for writing DataFrame objects into excel sheets. |\n\n\n## JSON#\n\n\n|  |  |\n| --- | --- |\n| `read\\_json`(path\\_or\\_buf,Â \\*[,Â orient,Â typ,Â ...]) | Convert a JSON string to pandas object. |\n| `json\\_normalize`(data[,Â record\\_path,Â meta,Â ...]) | Normalize semi-structured JSON data into a flat table. |\n| `DataFrame.to\\_json`([path\\_or\\_buf,Â orient,Â ...]) | Convert the object to a JSON string. |\n\n\n|  |  |\n| --- | --- |\n| `build\\_table\\_schema`(data[,Â index,Â ...]) | Create a Table schema from `data`. |\n\n\n\n## HTML#\n\n\n|  |  |\n| --- | --- |\n| `read\\_html`(io,Â \\*[,Â match,Â flavor,Â header,Â ...]) | Read HTML tables into a `list` of `DataFrame` objects. |\n| `DataFrame.to\\_html`([buf,Â columns,Â col\\_space,Â ...]) | Render a DataFrame as an HTML table. |\n\n\n|  |  |\n| --- | --- |\n| `Styler.to\\_html`([buf,Â table\\_uuid,Â ...]) | Write Styler to a file, buffer or string in HTML-CSS format. |\n\n\n\n## XML#\n\n\n|  |  |\n| --- | --- |\n| `read\\_xml`(path\\_or\\_buffer,Â \\*[,Â xpath,Â ...]) | Read XML document into a `DataFrame` object. |\n| `DataFrame.to\\_xml`([path\\_or\\_buffer,Â index,Â ...]) | Render a DataFrame to an XML document. |\n\n\n\n## Latex#\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.to\\_latex`([buf,Â columns,Â header,Â ...]) | Render object to a LaTeX tabular, longtable, or nested table. |\n\n\n|  |  |\n| --- | --- |\n| `Styler.to\\_latex`([buf,Â column\\_format,Â ...]) | Write Styler to a file, buffer or string in LaTeX format. |\n\n\n## HDFStore: PyTables (HDF5)#\n\n\n|  |  |\n| --- | --- |\n| `read\\_hdf`(path\\_or\\_buf[,Â key,Â mode,Â errors,Â ...]) | Read from the store, close it if we opened it. |\n| `HDFStore.put`(key,Â value[,Â format,Â index,Â ...]) | Store object in HDFStore. |\n| `HDFStore.append`(key,Â value[,Â format,Â axes,Â ...]) | Append to Table in file. |\n|\n\n==================\n Document 1 \n----------------\n API reference#\n\n\nThis page gives an overview of all public pandas objects, functions and\nmethods. All classes and functions exposed in `pandas.\\*` namespace are public.\n\n\nThe following subpackages are public.\n\n\n* `pandas.errors`: Custom exception and warnings classes that are raised by pandas.\n* `pandas.plotting`: Plotting public API.\n* `pandas.testing`: Functions that are useful for writing tests involving pandas objects.\n* `pandas.api.extensions`: Functions and classes for extending pandas objects.\n* `pandas.api.indexers`: Functions and classes for rolling window indexers.\n* `pandas.api.interchange`: DataFrame interchange protocol.\n* `pandas.api.types`: Datatype classes and functions.\n* `pandas.api.typing`: Classes that may be necessary for type-hinting.\nThese are classes that are encountered as intermediate results but should not be instantiated\ndirectly by users. These classes are not to be confused with classes from the\npandas-stubs package\nwhich has classes in addition to those that occur in pandas for type-hinting.\n\n\nIn addition, public functions in `pandas.io` and `pandas.tseries` submodules\nare mentioned in the documentation.\n\nWarning\n\n\nThe `pandas.core`, `pandas.compat`, and `pandas.util` top-level modules are PRIVATE. Stable functionality in such modules is not guaranteed.\n\n\n* Input/output\n\t+ Pickling\n\t+ Flat file\n\t+ Clipboard\n\t+ Excel\n\t+ JSON\n\t+ HTML\n\t+ XML\n\t+ Latex\n\t+ HDFStore: PyTables (HDF5)\n\t+ Feather\n\t+ Parquet\n\t+ ORC\n\t+ SAS\n\t+ SPSS\n\t+ SQL\n\t+ Google BigQuery\n\t+ STATA\n* General functions\n\t+ Data manipulations\n\t+ Top-level missing data\n\t+ Top-level dealing with numeric data\n\t+ Top-level dealing with datetimelike data\n\t+ Top-level dealing with Interval data\n\t+ Top-level evaluation\n\t+ Hashing\n\t+ Importing from other DataFrame libraries\n* Series\n\t+ Constructor\n\t+ Attributes\n\t+ Conversion\n\t+ Indexing, iteration\n\t+ Binary operator functions\n\t+ Function application, GroupBy & window\n\t+ Computations / descriptive stats\n\t+ Reindexing / selection / label manipulation\n\t+ Missing data handling\n\t+ Reshaping, sorting\n\t+ Combining / comparing / joining / merging\n\t+ Time Series-related\n\t+ Accessors\n\t+ Plotting\n\t+ Serialization / IO / conversion\n* DataFrame\n\t+ Constructor\n\t+ Attributes and underlying data\n\t+ Conversion\n\t+ Indexing, iteration\n\t+ Binary operator functions\n\t+ Function application, GroupBy & window\n\t+ Computations / descriptive stats\n\t+ Reindexing / selection / label manipulation\n\t+ Missing data handling\n\t+ Reshaping, sorting, transposing\n\t+ Combining / comparing / joining / merging\n\t+ Time Series-related\n\t+ Flags\n\t+ Metadata\n\t+ Plotting\n\t+ Sparse accessor\n\t+ Serialization / IO / conversion\n* pandas arrays, scalars, and data types\n\t+ Objects\n\t+ Utilities\n* Index objects\n\t+ Index\n\t+ Numeric Index\n\t+ CategoricalIndex\n\t+ IntervalIndex\n\t+ MultiIndex\n\t+ DatetimeIndex\n\t+ TimedeltaIndex\n\t+ PeriodIndex\n* Date offsets\n\t+ DateOffset\n\t+ BusinessDay\n\t+ BusinessHour\n\t+ CustomBusinessDay\n\t+ CustomBusinessHour\n\t+ MonthEnd\n\t+ MonthBegin\n\t+ BusinessMonthEnd\n\t+ BusinessMonthBegin\n\t+ CustomBusinessMonthEnd\n\t+ CustomBusinessMonthBegin\n\t+ SemiMonthEnd\n\t+ SemiMonthBegin\n\t+ Week\n\t+ WeekOfMonth\n\t+ LastWeekOfMonth\n\t+ BQuarterEnd\n\t+ BQuarterBegin\n\t+ QuarterEnd\n\t+ QuarterBegin\n\t+ BYearEnd\n\t+ BYearBegin\n\t+ YearEnd\n\t+ YearBegin\n\t+ FY5253\n\t+ FY5253Quarter\n\t+ Easter\n\t+ Tick\n\t+ Day\n\t+ Hour\n\t+ Minute\n\t+ Second\n\t+ Milli\n\t+ Micro\n\t+ Nano\n* Frequencies\n\t+ pandas.tseries.frequencies.to\\_offset\n* Window\n\t+ Rolling window functions\n\t+ Weighted window functions\n\t+ Expanding window functions\n\t+ Exponentially-weighted window functions\n\t+ Window indexer\n* GroupBy\n\t+ Indexing, iteration\n\t+ Function application helper\n\t+ Function application\n\t+ `DataFrameGroupBy` computations / descriptive stats\n\t+ `SeriesGroupBy` computations / descriptive stats\n\t+ Plotting and visualization\n* Resampling\n\t+ Indexing, iteration\n\t+ Function application\n\t+ Upsampling\n\t+ Computations / descriptive stats\n* Style\n\t+ Styler constructor\n\t+ Styler properties\n\t+ Style application\n\t+ Builtin styles\n\t+ Style export and import\n* Plotting\n\t+ pandas.plotting.andrews\\_curves\n\t+ pandas.plotting.autocorrelation\\_plot\n\t+ pandas.plotting.bootstrap\\_plot\n\t+ pandas.plotting.boxplot\n\t+ pandas.plotting.deregister\\_matplotlib\\_converters\n\t+ pandas.plotting.lag\\_plot\n\t+ pandas.plotting.parallel\\_coordinates\n\t+ pandas.plotting.plot\\_params\n\t+ pandas.plotting.radviz\n\t+ pandas.plotting.register\\_matplotlib\\_converters\n\t+ pandas.plotting.scatter\\_matrix\n\t+ pandas.plotting.table\n* Options and settings\n\t+ Working with options\n\t+ Numeric formatting\n* Extensions\n\t+ pandas.api.extensions.register\\_extension\\_dtype\n\t+ pandas.api.extensions.register\\_dataframe\\_accessor\n\t+ pandas.api.extensions.register\\_series\\_accessor\n\t+ pandas.api.extensions.register\\_index\\_accessor\n\t+ pandas.api.extensions.ExtensionDtype\n\t+ pandas.api.extensions.ExtensionArray\n\t+ pandas.arrays.NumpyExtensionArray\n\t+ pandas.api.indexers.check\\_array\\_indexer\n* Testing\n\t+ Assertion functions\n\t+ Exceptions and warnings\n\t+ Bug report function\n\t+ Test suite runner\n\n\n# Input/output#\n\n\n## Pickling#\n\n\n|  |  |\n| --- | --- |\n| `read\\_pickle`(filepath\\_or\\_buffer[,Â ...]) | Load pickled pandas object (or any object) from file. |\n| `DataFrame.to\\_pickle`(path[,Â compression,Â ...]) | Pickle (serialize) object to file. |\n\n\n\n## Flat file#\n\n\n|  |  |\n| --- | --- |\n| `read\\_table`(filepath\\_or\\_buffer,Â \\*[,Â sep,Â ...]) | Read general delimited file into DataFrame. |\n| `read\\_csv`(filepath\\_or\\_buffer,Â \\*[,Â sep,Â ...]) | Read a comma-separated values (csv) file into DataFrame. |\n| `DataFrame.to\\_csv`([path\\_or\\_buf,Â sep,Â na\\_rep,Â ...]) | Write object to a comma-separated values (csv) file. |\n| `read\\_fwf`(filepath\\_or\\_buffer,Â \\*[,Â colspecs,Â ...]) | Read a table of fixed-width formatted lines into DataFrame. |\n\n\n\n## Clipboard#\n\n\n|  |  |\n| --- | --- |\n| `read\\_clipboard`([sep,Â dtype\\_backend]) | Read text from clipboard and pass to `read\\_csv()`. |\n| `DataFrame.to\\_clipboard`([excel,Â sep]) | Copy object to the system clipboard. |\n\n\n## Excel#\n\n\n|  |  |\n| --- | --- |\n| `read\\_excel`(io[,Â sheet\\_name,Â header,Â names,Â ...]) | Read an Excel file into a pandas DataFrame. |\n| `DataFrame.to\\_excel`(excel\\_writer[,Â ...]) | Write object to an Excel sheet. |\n| `ExcelFile`(path\\_or\\_buffer[,Â engine,Â ...]) | Class for parsing tabular Excel sheets into DataFrame\n\n==================\n Document 2 \n----------------\n# Attributes and underlying data#\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.index` | The index (row labels) of the DataFrame. |\n| `DataFrame.columns` | The column labels of the DataFrame. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.dtypes` | Return the dtypes in the DataFrame. |\n| `DataFrame.info`([verbose,Â buf,Â max\\_cols,Â ...]) | Print a concise summary of a DataFrame. |\n| `DataFrame.select\\_dtypes`([include,Â exclude]) | Return a subset of the DataFrame's columns based on the column dtypes. |\n| `DataFrame.values` | Return a Numpy representation of the DataFrame. |\n| `DataFrame.axes` | Return a list representing the axes of the DataFrame. |\n| `DataFrame.ndim` | Return an int representing the number of axes / array dimensions. |\n| `DataFrame.size` | Return an int representing the number of elements in this object. |\n| `DataFrame.shape` | Return a tuple representing the dimensionality of the DataFrame. |\n| `DataFrame.memory\\_usage`([index,Â deep]) | Return the memory usage of each column in bytes. |\n| `DataFrame.empty` | Indicator whether Series/DataFrame is empty. |\n| `DataFrame.set\\_flags`(\\*[,Â copy,Â ...]) | Return a new object with updated flags. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.astype`(dtype[,Â copy,Â errors]) | Cast a pandas object to a specified dtype `dtype`. |\n| `DataFrame.convert\\_dtypes`([infer\\_objects,Â ...]) | Convert columns to the best possible dtypes using dtypes supporting `pd.NA`. |\n| `DataFrame.infer\\_objects`([copy]) | Attempt to infer better dtypes for object columns. |\n| `DataFrame.copy`([deep]) | Make a copy of this object's indices and data. |\n| `DataFrame.bool`() | (DEPRECATED) Return the bool of a single element Series or DataFrame. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.head`([n]) | Return the first n rows. |\n| `DataFrame.at` | Access a single value for a row/column label pair. |\n| `DataFrame.iat` | Access a single value for a row/column pair by integer position. |\n| `DataFrame.loc` | Access a group of rows and columns by label(s) or a boolean array. |\n| `DataFrame.iloc` | Purely integer-location based indexing for selection by position. |\n| `DataFrame.insert`(loc,Â column,Â value[,Â ...]) | Insert column into DataFrame at specified location. |\n| `DataFrame.\\_\\_iter\\_\\_`() | Iterate over info axis. |\n| `DataFrame.items`() | Iterate over (column name, Series) pairs. |\n| `DataFrame.keys`() | Get the 'info axis' (see Indexing for more). |\n| `DataFrame.iterrows`() | Iterate over DataFrame rows as (index, Series) pairs. |\n| `DataFrame.itertuples`([index,Â name]) | Iterate over DataFrame rows as namedtuples. |\n| `DataFrame.pop`(item) | Return item and drop from frame. |\n| `DataFrame.tail`([n]) | Return the last n rows. |\n| `DataFrame.xs`(key[,Â axis,Â level,Â drop\\_level]) | Return cross-section from the Series/DataFrame. |\n| `DataFrame.get`(key[,Â default]) | Get item from object for given key (ex: DataFrame column). |\n| `DataFrame.isin`(values) | Whether each element in the DataFrame is contained in values. |\n| `DataFrame.where`(cond[,Â other,Â inplace,Â ...]) | Replace values where the condition is False. |\n| `DataFrame.mask`(cond[,Â other,Â inplace,Â axis,Â ...]) | Replace values where the condition is True. |\n| `DataFrame.query`(expr,Â \\*[,Â inplace]) | Query the columns of a DataFrame with a boolean expression. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.\\_\\_add\\_\\_`(other) | Get Addition of DataFrame and other, column-wise. |\n| `DataFrame.add`(other[,Â axis,Â level,Â fill\\_value]) | Get Addition of dataframe and other, element-wise (binary operator add). |\n| `DataFrame.sub`(other[,Â axis,Â level,Â fill\\_value]) | Get Subtraction of dataframe and other, element-wise (binary operator sub). |\n| `DataFrame.mul`(other[,Â axis,Â level,Â fill\\_value]) | Get Multiplication of dataframe and other, element-wise (binary operator mul). |\n| `DataFrame.div`(other[,Â axis,Â level,Â fill\\_value]) | Get Floating division of dataframe and other, element-wise (binary operator truediv). |\n| `DataFrame.truediv`(other[,Â axis,Â level,Â ...]) | Get Floating division of dataframe and other, element-wise (binary operator truediv). |\n| `DataFrame.floordiv`(other[,Â axis,Â level,Â ...]) | Get Integer division of dataframe and other, element-wise (binary operator floordiv). |\n| `DataFrame.mod`(other[,Â axis,Â level,Â fill\\_value]) | Get Modulo of dataframe and other, element-wise (binary operator mod). |\n| `DataFrame.pow`(other[,Â axis,Â level,Â fill\\_value]) | Get Exponential power of dataframe and other, element-wise (binary operator pow). |\n| `DataFrame.dot`(other) | Compute the matrix multiplication between the DataFrame and other. |\n| `DataFrame.radd`(other[,Â axis,Â level,Â fill\\_value]) | Get Addition of dataframe and other, element-wise (binary operator radd). |\n| `DataFrame.rsub`(other[,Â axis,Â level,Â fill\\_value]) | Get Subtraction of dataframe and other, element-wise (binary operator rsub). |\n| `DataFrame.rmul`(other[,Â axis,Â level,Â fill\\_value]) | Get Multiplication of dataframe and other, element-wise (binary operator rmul). |\n| `DataFrame.rdiv`(other[,Â axis,Â level,Â fill\\_value]) | Get Floating division of dataframe and other, element-wise (binary operator rtruediv). |\n| `DataFrame.rtruediv`(other[,Â axis,Â level,Â ...]) | Get Floating division of dataframe and other, element-wise (binary operator rtruediv). |\n| `DataFrame.rfloordiv`(other[,Â axis,Â level,Â ...]) | Get Integer division of dataframe and other, element-wise (binary operator rfloordiv). |\n| `DataFrame.rmod`(other[,Â axis,Â level,Â fill\\_value]) | Get Modulo of dataframe and other, element-wise (binary operator rmod). |\n| `DataFrame.rpow`(other[,Â axis,Â level,Â fill\\_value]) | Get Exponential power of dataframe and other, element-wise (binary operator rpow). |\n| `DataFrame.lt`(other[,Â axis,Â level]) | Get Less than of dataframe and other, element-wise (binary operator lt). |\n| `DataFrame.gt`(other[,Â axis,Â level]) | Get Greater than of dataframe and other, element-wise (binary operator gt). |\n| `DataFrame.le`(other[,Â axis,Â level]) | Get Less than or equal to of dataframe and other, element-wise (binary operator le). |\n| `DataFrame.ge`(other[,Â axis,Â level]) | Get Greater than or equal to of dataframe and other, element-wise (binary operator ge). |\n| `DataFrame.ne`(other[,Â axis,Â level]) | Get Not equal to of dataframe and other, element-wise (binary operator ne). |\n| `DataFrame.eq`(other[,Â axis,Â level]) | Get Equal to of dataframe and other, element-wise (binary operator eq). |\n| `DataFrame.combine`(other,Â func[,Â fill\\_value,Â ...]) | Perform column-wise combine with another DataFrame. |\n| `DataFrame.combine\\_first`(other) | Update null elements with value in the same location in other. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.apply`(func[,Â axis,Â raw,Â ...]) | Apply a function along an axis of the DataFrame. |\n| `DataFrame.map`(func[,Â na\\_action]) | Apply a function to a Dataframe elementwise. |\n| `DataFrame.applymap`(func[,Â na\\_action]) | (DEPRECATED) Apply a function to a Dataframe elementwise. |\n| `DataFrame.pipe`(func,Â \\*args,Â \\*\\*kwargs) | Apply chainable functions that expect Series or DataFrames. |\n| `DataFrame.agg`([func,Â axis]) | Aggregate using one or more operations over the specified axis. |\n| `DataFrame.aggregate`([func,Â axis]) | Aggregate using one or more operations over the specified axis. |\n| `DataFrame.transform`(func[,Â axis]) | Call `func` on self producing a DataFrame with the same axis shape as self. |\n| `DataFrame.groupby`([by,Â axis,Â level,Â ...]) | Group DataFrame using a mapper or by a Series of columns. |\n| `DataFrame.rolling`(window[,Â min\\_periods,Â ...]) | Provide rolling window calculations. |\n| `DataFrame.expanding`([min\\_periods,Â axis,Â method]) | Provide expanding window calculations. |\n| `DataFrame.ewm`([com,Â span,Â halflife,Â alpha,Â ...]) | Provide exponentially weighted (EW) calculations. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.abs`() | Return a Series/DataFrame with absolute numeric value of each element. |\n| `DataFrame.all`([axis,Â bool\\_only,Â skipna]) | Return whether all elements are True, potentially over an axis. |\n| `DataFrame.any`(\\*[,Â axis,Â bool\\_only,Â skipna]) | Return whether any element is True, potentially over an axis. |\n| `DataFrame.clip`([lower,Â upper,Â axis,Â inplace]) | Trim values at input threshold(s). |\n| `DataFrame.corr`([method,Â min\\_periods,Â ...]) | Compute pairwise correlation of columns, excluding NA/null values. |\n| `DataFrame.corrwith`(other[,Â axis,Â drop,Â ...]) | Compute pairwise correlation. |\n| `DataFrame.count`([axis,Â numeric\\_only]) | Count non-NA cells for each column or row. |\n| `DataFrame.cov`([min\\_periods,Â ddof,Â numeric\\_only]) | Compute pairwise covariance of columns, excluding NA/null values. |\n| `DataFrame.cummax`([axis,Â skipna]) | Return cumulative maximum over a DataFrame or Series axis. |\n| `DataFrame.cummin`([axis,Â skipna]) | Return cumulative minimum over a DataFrame or Series axis. |\n| `DataFrame.cumprod`([axis,Â skipna]) | Return cumulative product over a DataFrame or Series axis. |\n| `DataFrame.cumsum`([axis,Â skipna]) | Return cumulative sum over a DataFrame or Series axis. |\n| `DataFrame.describe`([percentiles,Â include,Â ...]) | Generate descriptive statistics. |\n| `DataFrame.diff`([periods,Â axis]) | First discrete difference of element. |\n| `DataFrame.eval`(expr,Â \\*[,Â inplace]) | Evaluate a string describing operations on DataFrame columns. |\n| `DataFrame.kurt`([axis,Â skipna,Â numeric\\_only]) | Return unbiased kurtosis over requested axis. |\n| `DataFrame.kurtosis`([axis,Â skipna,Â numeric\\_only]) | Return unbiased kurtosis over requested axis. |\n| `DataFrame.max`([axis,Â skipna,Â numeric\\_only]) | Return the maximum of the values over the requested axis. |\n| `DataFrame.mean`([axis,Â skipna,Â numeric\\_only]) | Return the mean of the values over the requested axis. |\n| `DataFrame.median`([axis,Â skipna,Â numeric\\_only]) | Return the median of the values over the requested axis. |\n| `DataFrame.min`([axis,Â skipna,Â numeric\\_only]) | Return the minimum of the values over the requested axis. |\n| `DataFrame.mode`([axis,Â numeric\\_only,Â dropna]) | Get the mode(s) of each element along the selected axis. |\n| `DataFrame.pct\\_change`([periods,Â fill\\_method,Â ...]) | Fractional change between the current and a prior element. |\n| `DataFrame.prod`([axis,Â skipna,Â numeric\\_only,Â ...]) | Return the product of the values over the requested axis. |\n| `DataFrame.product`([axis,Â skipna,Â ...]) | Return the product of the values over the requested axis. |\n| `DataFrame.quantile`([q,Â axis,Â numeric\\_only,Â ...]) | Return values at the given quantile over requested axis. |\n| `DataFrame.rank`([axis,Â method,Â numeric\\_only,Â ...]) | Compute numerical data ranks (1 through n) along axis. |\n| `DataFrame.round`([decimals]) | Round a DataFrame to a variable number of decimal places. |\n| `DataFrame.sem`([axis,Â skipna,Â ddof,Â numeric\\_only]) | Return unbiased standard error of the mean over requested axis. |\n| `DataFrame.skew`([axis,Â skipna,Â numeric\\_only]) | Return unbiased skew over requested axis. |\n| `DataFrame.sum`([axis,Â skipna,Â numeric\\_only,Â ...]) | Return the sum of the values over the requested axis. |\n| `DataFrame.std`([axis,Â skipna,Â ddof,Â numeric\\_only]) | Return sample standard deviation over requested axis. |\n| `DataFrame.var`([axis,Â skipna,Â ddof,Â numeric\\_only]) | Return unbiased variance over requested axis. |\n| `DataFrame.nunique`([axis,Â dropna]) | Count number of distinct elements in specified axis. |\n| `DataFrame.value\\_counts`([subset,Â normalize,Â ...]) | Return a Series containing the frequency of each distinct row in the Dataframe. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.add\\_prefix`(prefix[,Â axis]) | Prefix labels with string prefix. |\n| `DataFrame.add\\_suffix`(suffix[,Â axis]) | Suffix labels with string suffix. |\n| `DataFrame.align`(other[,Â join,Â axis,Â level,Â ...]) | Align two objects on their axes with the specified join method. |\n| `DataFrame.at\\_time`(time[,Â asof,Â axis]) | Select values at particular time of day (e.g., 9:30AM). |\n| `DataFrame.between\\_time`(start\\_time,Â end\\_time) | Select values between particular times of the day (e.g., 9:00-9:30 AM). |\n| `DataFrame.drop`([labels,Â axis,Â index,Â ...]) | Drop specified labels from rows or columns. |\n| `DataFrame.drop\\_duplicates`([subset,Â keep,Â ...]) | Return DataFrame with duplicate rows removed. |\n| `DataFrame.duplicated`([subset,Â keep]) | Return boolean Series denoting duplicate rows. |\n| `DataFrame.equals`(other) | Test whether two objects contain the same elements. |\n| `DataFrame.filter`([items,Â like,Â regex,Â axis]) | Subset the dataframe rows or columns according to the specified index labels. |\n| `DataFrame.first`(offset) | Select initial periods of time series data based on a date offset. |\n| `DataFrame.head`([n]) | Return the first n rows. |\n| `DataFrame.idxmax`([axis,Â skipna,Â numeric\\_only]) | Return index of first occurrence of maximum over requested axis. |\n| `DataFrame.idxmin`([axis,Â skipna,Â numeric\\_only]) | Return index of first occurrence of minimum over requested axis. |\n| `DataFrame.last`(offset) | Select final periods of time series data based on a date offset. |\n| `DataFrame.reindex`([labels,Â index,Â columns,Â ...]) | Conform DataFrame to new index with optional filling logic. |\n| `DataFrame.reindex\\_like`(other[,Â method,Â ...]) | Return an object with matching indices as other object. |\n| `DataFrame.rename`([mapper,Â index,Â columns,Â ...]) | Rename columns or index labels. |\n| `DataFrame.rename\\_axis`([mapper,Â index,Â ...]) | Set the name of the axis for the index or columns. |\n| `DataFrame.reset\\_index`([level,Â drop,Â ...]) | Reset the index, or a level of it. |\n| `DataFrame.sample`([n,Â frac,Â replace,Â ...]) | Return a random sample of items from an axis of object. |\n| `DataFrame.set\\_axis`(labels,Â \\*[,Â axis,Â copy]) | Assign desired index to given axis. |\n| `DataFrame.set\\_index`(keys,Â \\*[,Â drop,Â append,Â ...]) | Set the DataFrame index using existing columns. |\n| `DataFrame.tail`([n]) | Return the last n rows. |\n| `DataFrame.take`(indices[,Â axis]) | Return the elements in the given *positional* indices along an axis. |\n| `DataFrame.truncate`([before,Â after,Â axis,Â copy]) | Truncate a Series or DataFrame before and after some index value. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.backfill`(\\*[,Â axis,Â inplace,Â ...]) | (DEPRECATED) Fill NA/NaN values by using the next valid observation to fill the gap. |\n| `DataFrame.bfill`(\\*[,Â axis,Â inplace,Â limit,Â ...]) | Fill NA/NaN values by using the next valid observation to fill the gap. |\n| `DataFrame.dropna`(\\*[,Â axis,Â how,Â thresh,Â ...]) | Remove missing values. |\n| `DataFrame.ffill`(\\*[,Â axis,Â inplace,Â limit,Â ...]) | Fill NA/NaN values by propagating the last valid observation to next valid. |\n| `DataFrame.fillna`([value,Â method,Â axis,Â ...]) | Fill NA/NaN values using the specified method. |\n| `DataFrame.interpolate`([method,Â axis,Â limit,Â ...]) | Fill NaN values using an interpolation method. |\n| `DataFrame.isna`() | Detect missing values. |\n| `DataFrame.isnull`() | DataFrame.isnull is an alias for DataFrame.isna. |\n| `DataFrame.notna`() | Detect existing (non-missing) values. |\n| `DataFrame.notnull`() | DataFrame.notnull is an alias for DataFrame.notna. |\n| `DataFrame.pad`(\\*[,Â axis,Â inplace,Â limit,Â ...]) | (DEPRECATED) Fill NA/NaN values by propagating the last valid observation to next valid. |\n| `DataFrame.replace`([to\\_replace,Â value,Â ...]) | Replace values given in to\\_replace with value. |\n\n## Reshaping, sorting, transposing#\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.droplevel`(level[,Â axis]) | Return Series/DataFrame with requested index / column level(s) removed. |\n| `DataFrame.pivot`(\\*,Â columns[,Â index,Â values]) | Return reshaped DataFrame organized by given index / column values. |\n| `DataFrame.pivot\\_table`([values,Â index,Â ...]) | Create\n\n==================\n Document 3 \n----------------\n Plotting#\n\n\nThe following functions are contained in the `pandas.plotting` module.\n\n\n|  |  |\n| --- | --- |\n| `andrews\\_curves`(frame,Â class\\_column[,Â ax,Â ...]) | Generate a matplotlib plot for visualising clusters of multivariate data. |\n| `autocorrelation\\_plot`(series[,Â ax]) | Autocorrelation plot for time series. |\n| `bootstrap\\_plot`(series[,Â fig,Â size,Â samples]) | Bootstrap plot on mean, median and mid-range statistics. |\n| `boxplot`(data[,Â column,Â by,Â ax,Â fontsize,Â ...]) | Make a box plot from DataFrame columns. |\n| `deregister\\_matplotlib\\_converters`() | Remove pandas formatters and converters. |\n| `lag\\_plot`(series[,Â lag,Â ax]) | Lag plot for time series. |\n| `parallel\\_coordinates`(frame,Â class\\_column[,Â ...]) | Parallel coordinates plotting. |\n| `plot\\_params` | Stores pandas plotting options. |\n| `radviz`(frame,Â class\\_column[,Â ax,Â color,Â ...]) | Plot a multidimensional dataset in 2D. |\n| `register\\_matplotlib\\_converters`() | Register pandas formatters and converters with matplotlib. |\n| `scatter\\_matrix`(frame[,Â alpha,Â figsize,Â ax,Â ...]) | Draw a matrix of scatter plots. |\n| `table`(ax,Â data,Â \\*\\*kwargs) | Helper function to convert DataFrame and Series to matplotlib.table. |\n\n# Options and settings#\n\n\nAPI for configuring global behavior. See the User Guide for more.\n\n\n## Working with options#\n\n\n|  |  |\n| --- | --- |\n| `describe\\_option`(pat[,Â \\_print\\_desc]) | Prints the description for one or more registered options. |\n| `reset\\_option`(pat) | Reset one or more options to their default value. |\n| `get\\_option`(pat) | Retrieves the value of the specified option. |\n| `set\\_option`(pat,Â value) | Sets the value of the specified option. |\n| `option\\_context`(\\*args) | Context manager to temporarily set options in the with statement context. |\n\n\n\n## Numeric formatting#\n\n\n|  |  |\n| --- | --- |\n| `set\\_eng\\_float\\_format`([accuracy,Â use\\_eng\\_prefix]) | Format float representation in DataFrame with SI notation. |\n\n\n# Extensions#\n\n\nThese are primarily intended for library authors looking to extend pandas\nobjects.\n\n\n|  |  |\n| --- | --- |\n| `api.extensions.register\\_extension\\_dtype`(cls) | Register an ExtensionType with pandas as class decorator. |\n| `api.extensions.register\\_dataframe\\_accessor`(name) | Register a custom accessor on DataFrame objects.\n\n==================\n Document 4 \n----------------\n pandas.plotting.parallel\\_coordinates#\n\n\npandas.plotting.parallel\\_coordinates(*frame*, *class\\_column*, *cols=None*, *ax=None*, *color=None*, *use\\_columns=False*, *xticks=None*, *colormap=None*, *axvlines=True*, *axvlines\\_kwds=None*, *sort\\_labels=False*, *\\*\\*kwargs*)[source]#\nParallel coordinates plotting.\n\n**frame**DataFrame\n**class\\_column**strColumn name containing class names.\n\n**cols**list, optionalA list of column names to use.\n\n**ax**matplotlib.axis, optionalMatplotlib axis object.\n\n**color**list or tuple, optionalColors to use for the different classes.\n\n**use\\_columns**bool, optionalIf true, columns will be used as xticks.\n\n**xticks**list or tuple, optionalA list of values to use for xticks.\n\n**colormap**str or matplotlib colormap, default NoneColormap to use for line colors.\n\n**axvlines**bool, optionalIf true, vertical lines will be added at each xtick.\n\n**axvlines\\_kwds**keywords, optionalOptions to be passed to axvline method for vertical lines.\n\n**sort\\_labels**bool, default FalseSort class\\_column labels, useful when assigning colors.\n\n```\n>>> df = pd.read\\_csv(\n...     'https://raw.githubusercontent.com/pandas-dev/'\n...     'pandas/main/pandas/tests/io/data/csv/iris.csv'\n... )\n>>> pd.plotting.parallel\\_coordinates(\n...     df, 'Name', color=('#556270', '#4ECDC4', '#C7F464')\n... )  \n\n\n\n# pandas.plotting.plot\\_params#\n\n\npandas.plotting.plot\\_params *= {'xaxis.compat': False}*#\nStores pandas plotting options.\n\n\nAllows for parameter aliasing so you can just use parameter names that are\nthe same as the plot function parameters, but is stored in a canonical\nformat that makes it easy to breakdown into groups later.\n\n```\n>>> np.random.seed(42)\n>>> df = pd.DataFrame({'A': np.random.randn(10),\n...                   'B': np.random.randn(10)},\n...                   index=pd.date\\_range(\"1/1/2000\",\n...                   freq='4MS', periods=10))\n>>> with pd.plotting.plot\\_params.use(\"x\\_compat\", True):\n...     \\_ = df[\"A\"].plot(color=\"r\")\n...     \\_ = df[\"B\"].plot(color=\"g\")\n\n\n\n# pandas.plotting.radviz#\n\n\npandas.plotting.radviz(*frame*, *class\\_column*, *ax=None*, *color=None*, *colormap=None*, *\\*\\*kwds*)[source]#\nPlot a multidimensional dataset in 2D.\n\n\nEach Series in the DataFrame is represented as a evenly distributed\nslice on a circle. Each data point is rendered in the circle according to\nthe value on each Series. Highly"
    },
    {
        "instruction": "Create a Python program that uses the 'pandas' API to read a CSV file containing customer data, calculate the average age for each occupation, identify the occupation with the highest average age, and create a summary DataFrame.",
        "api": "pandas",
        "output": "#!pip install pandas\nimport pandas as pd\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv('customer_data.csv')\n\n# Calculate the average age for each occupation\noccupation_avg_age = df.groupby('Occupation')['Age'].mean()\noccupation_avg_age = occupation_avg_age.reset_index()\noccupation_avg_age.columns = ['Occupation', 'Average Age']\n\n# Identify the occupation with the highest average age\nhighest_avg_age_occupation = occupation_avg_age[occupation_avg_age['Average Age'] == occupation_avg_age['Average Age'].max()]\n\n# Create a summary DataFrame\nsummary_df = pd.concat([highest_avg_age_occupation, occupation_avg_age], axis=1)\n\nprint(\"Summary Dataframe: \")\nprint(summary_df)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n# Excel#\n\n\n|  |  |\n| --- | --- |\n| `read\\_excel`(io[,Â sheet\\_name,Â header,Â names,Â ...]) | Read an Excel file into a pandas DataFrame. |\n| `DataFrame.to\\_excel`(excel\\_writer[,Â ...]) | Write object to an Excel sheet. |\n| `ExcelFile`(path\\_or\\_buffer[,Â engine,Â ...]) | Class for parsing tabular Excel sheets into DataFrame objects. |\n| `ExcelFile.book` |  |\n| `ExcelFile.sheet\\_names` |  |\n| `ExcelFile.parse`([sheet\\_name,Â header,Â names,Â ...]) | Parse specified sheet(s) into a DataFrame. |\n\n\n|  |  |\n| --- | --- |\n| `Styler.to\\_excel`(excel\\_writer[,Â sheet\\_name,Â ...]) | Write Styler to an Excel sheet. |\n\n\n|  |  |\n| --- | --- |\n| `ExcelWriter`(path[,Â engine,Â date\\_format,Â ...]) | Class for writing DataFrame objects into excel sheets. |\n\n\n## JSON#\n\n\n|  |  |\n| --- | --- |\n| `read\\_json`(path\\_or\\_buf,Â \\*[,Â orient,Â typ,Â ...]) | Convert a JSON string to pandas object. |\n| `json\\_normalize`(data[,Â record\\_path,Â meta,Â ...]) | Normalize semi-structured JSON data into a flat table. |\n| `DataFrame.to\\_json`([path\\_or\\_buf,Â orient,Â ...]) | Convert the object to a JSON string. |\n\n\n|  |  |\n| --- | --- |\n| `build\\_table\\_schema`(data[,Â index,Â ...]) | Create a Table schema from `data`. |\n\n\n\n## HTML#\n\n\n|  |  |\n| --- | --- |\n| `read\\_html`(io,Â \\*[,Â match,Â flavor,Â header,Â ...]) | Read HTML tables into a `list` of `DataFrame` objects. |\n| `DataFrame.to\\_html`([buf,Â columns,Â col\\_space,Â ...]) | Render a DataFrame as an HTML table. |\n\n\n|  |  |\n| --- | --- |\n| `Styler.to\\_html`([buf,Â table\\_uuid,Â ...]) | Write Styler to a file, buffer or string in HTML-CSS format. |\n\n\n\n## XML#\n\n\n|  |  |\n| --- | --- |\n| `read\\_xml`(path\\_or\\_buffer,Â \\*[,Â xpath,Â ...]) | Read XML document into a `DataFrame` object. |\n| `DataFrame.to\\_xml`([path\\_or\\_buffer,Â index,Â ...]) | Render a DataFrame to an XML document. |\n\n\n\n## Latex#\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.to\\_latex`([buf,Â columns,Â header,Â ...]) | Render object to a LaTeX tabular, longtable, or nested table. |\n\n\n|  |  |\n| --- | --- |\n| `Styler.to\\_latex`([buf,Â column\\_format,Â ...]) | Write Styler to a file, buffer or string in LaTeX format. |\n\n\n## HDFStore: PyTables (HDF5)#\n\n\n|  |  |\n| --- | --- |\n| `read\\_hdf`(path\\_or\\_buf[,Â key,Â mode,Â errors,Â ...]) | Read from the store, close it if we opened it. |\n| `HDFStore.put`(key,Â value[,Â format,Â index,Â ...]) | Store object in HDFStore. |\n| `HDFStore.append`(key,Â value[,Â format,Â axes,Â ...]) | Append to Table in file. |\n|\n\n==================\n Document 1 \n----------------\n# Attributes and underlying data#\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.index` | The index (row labels) of the DataFrame. |\n| `DataFrame.columns` | The column labels of the DataFrame. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.dtypes` | Return the dtypes in the DataFrame. |\n| `DataFrame.info`([verbose,Â buf,Â max\\_cols,Â ...]) | Print a concise summary of a DataFrame. |\n| `DataFrame.select\\_dtypes`([include,Â exclude]) | Return a subset of the DataFrame's columns based on the column dtypes. |\n| `DataFrame.values` | Return a Numpy representation of the DataFrame. |\n| `DataFrame.axes` | Return a list representing the axes of the DataFrame. |\n| `DataFrame.ndim` | Return an int representing the number of axes / array dimensions. |\n| `DataFrame.size` | Return an int representing the number of elements in this object. |\n| `DataFrame.shape` | Return a tuple representing the dimensionality of the DataFrame. |\n| `DataFrame.memory\\_usage`([index,Â deep]) | Return the memory usage of each column in bytes. |\n| `DataFrame.empty` | Indicator whether Series/DataFrame is empty. |\n| `DataFrame.set\\_flags`(\\*[,Â copy,Â ...]) | Return a new object with updated flags. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.astype`(dtype[,Â copy,Â errors]) | Cast a pandas object to a specified dtype `dtype`. |\n| `DataFrame.convert\\_dtypes`([infer\\_objects,Â ...]) | Convert columns to the best possible dtypes using dtypes supporting `pd.NA`. |\n| `DataFrame.infer\\_objects`([copy]) | Attempt to infer better dtypes for object columns. |\n| `DataFrame.copy`([deep]) | Make a copy of this object's indices and data. |\n| `DataFrame.bool`() | (DEPRECATED) Return the bool of a single element Series or DataFrame. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.head`([n]) | Return the first n rows. |\n| `DataFrame.at` | Access a single value for a row/column label pair. |\n| `DataFrame.iat` | Access a single value for a row/column pair by integer position. |\n| `DataFrame.loc` | Access a group of rows and columns by label(s) or a boolean array. |\n| `DataFrame.iloc` | Purely integer-location based indexing for selection by position. |\n| `DataFrame.insert`(loc,Â column,Â value[,Â ...]) | Insert column into DataFrame at specified location. |\n| `DataFrame.\\_\\_iter\\_\\_`() | Iterate over info axis. |\n| `DataFrame.items`() | Iterate over (column name, Series) pairs. |\n| `DataFrame.keys`() | Get the 'info axis' (see Indexing for more). |\n| `DataFrame.iterrows`() | Iterate over DataFrame rows as (index, Series) pairs. |\n| `DataFrame.itertuples`([index,Â name]) | Iterate over DataFrame rows as namedtuples. |\n| `DataFrame.pop`(item) | Return item and drop from frame. |\n| `DataFrame.tail`([n]) | Return the last n rows. |\n| `DataFrame.xs`(key[,Â axis,Â level,Â drop\\_level]) | Return cross-section from the Series/DataFrame. |\n| `DataFrame.get`(key[,Â default]) | Get item from object for given key (ex: DataFrame column). |\n| `DataFrame.isin`(values) | Whether each element in the DataFrame is contained in values. |\n| `DataFrame.where`(cond[,Â other,Â inplace,Â ...]) | Replace values where the condition is False. |\n| `DataFrame.mask`(cond[,Â other,Â inplace,Â axis,Â ...]) | Replace values where the condition is True. |\n| `DataFrame.query`(expr,Â \\*[,Â inplace]) | Query the columns of a DataFrame with a boolean expression. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.\\_\\_add\\_\\_`(other) | Get Addition of DataFrame and other, column-wise. |\n| `DataFrame.add`(other[,Â axis,Â level,Â fill\\_value]) | Get Addition of dataframe and other, element-wise (binary operator add). |\n| `DataFrame.sub`(other[,Â axis,Â level,Â fill\\_value]) | Get Subtraction of dataframe and other, element-wise (binary operator sub). |\n| `DataFrame.mul`(other[,Â axis,Â level,Â fill\\_value]) | Get Multiplication of dataframe and other, element-wise (binary operator mul). |\n| `DataFrame.div`(other[,Â axis,Â level,Â fill\\_value]) | Get Floating division of dataframe and other, element-wise (binary operator truediv). |\n| `DataFrame.truediv`(other[,Â axis,Â level,Â ...]) | Get Floating division of dataframe and other, element-wise (binary operator truediv). |\n| `DataFrame.floordiv`(other[,Â axis,Â level,Â ...]) | Get Integer division of dataframe and other, element-wise (binary operator floordiv). |\n| `DataFrame.mod`(other[,Â axis,Â level,Â fill\\_value]) | Get Modulo of dataframe and other, element-wise (binary operator mod). |\n| `DataFrame.pow`(other[,Â axis,Â level,Â fill\\_value]) | Get Exponential power of dataframe and other, element-wise (binary operator pow). |\n| `DataFrame.dot`(other) | Compute the matrix multiplication between the DataFrame and other. |\n| `DataFrame.radd`(other[,Â axis,Â level,Â fill\\_value]) | Get Addition of dataframe and other, element-wise (binary operator radd). |\n| `DataFrame.rsub`(other[,Â axis,Â level,Â fill\\_value]) | Get Subtraction of dataframe and other, element-wise (binary operator rsub). |\n| `DataFrame.rmul`(other[,Â axis,Â level,Â fill\\_value]) | Get Multiplication of dataframe and other, element-wise (binary operator rmul). |\n| `DataFrame.rdiv`(other[,Â axis,Â level,Â fill\\_value]) | Get Floating division of dataframe and other, element-wise (binary operator rtruediv). |\n| `DataFrame.rtruediv`(other[,Â axis,Â level,Â ...]) | Get Floating division of dataframe and other, element-wise (binary operator rtruediv). |\n| `DataFrame.rfloordiv`(other[,Â axis,Â level,Â ...]) | Get Integer division of dataframe and other, element-wise (binary operator rfloordiv). |\n| `DataFrame.rmod`(other[,Â axis,Â level,Â fill\\_value]) | Get Modulo of dataframe and other, element-wise (binary operator rmod). |\n| `DataFrame.rpow`(other[,Â axis,Â level,Â fill\\_value]) | Get Exponential power of dataframe and other, element-wise (binary operator rpow). |\n| `DataFrame.lt`(other[,Â axis,Â level]) | Get Less than of dataframe and other, element-wise (binary operator lt). |\n| `DataFrame.gt`(other[,Â axis,Â level]) | Get Greater than of dataframe and other, element-wise (binary operator gt). |\n| `DataFrame.le`(other[,Â axis,Â level]) | Get Less than or equal to of dataframe and other, element-wise (binary operator le). |\n| `DataFrame.ge`(other[,Â axis,Â level]) | Get Greater than or equal to of dataframe and other, element-wise (binary operator ge). |\n| `DataFrame.ne`(other[,Â axis,Â level]) | Get Not equal to of dataframe and other, element-wise (binary operator ne). |\n| `DataFrame.eq`(other[,Â axis,Â level]) | Get Equal to of dataframe and other, element-wise (binary operator eq). |\n| `DataFrame.combine`(other,Â func[,Â fill\\_value,Â ...]) | Perform column-wise combine with another DataFrame. |\n| `DataFrame.combine\\_first`(other) | Update null elements with value in the same location in other. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.apply`(func[,Â axis,Â raw,Â ...]) | Apply a function along an axis of the DataFrame. |\n| `DataFrame.map`(func[,Â na\\_action]) | Apply a function to a Dataframe elementwise. |\n| `DataFrame.applymap`(func[,Â na\\_action]) | (DEPRECATED) Apply a function to a Dataframe elementwise. |\n| `DataFrame.pipe`(func,Â \\*args,Â \\*\\*kwargs) | Apply chainable functions that expect Series or DataFrames. |\n| `DataFrame.agg`([func,Â axis]) | Aggregate using one or more operations over the specified axis. |\n| `DataFrame.aggregate`([func,Â axis]) | Aggregate using one or more operations over the specified axis. |\n| `DataFrame.transform`(func[,Â axis]) | Call `func` on self producing a DataFrame with the same axis shape as self. |\n| `DataFrame.groupby`([by,Â axis,Â level,Â ...]) | Group DataFrame using a mapper or by a Series of columns. |\n| `DataFrame.rolling`(window[,Â min\\_periods,Â ...]) | Provide rolling window calculations. |\n| `DataFrame.expanding`([min\\_periods,Â axis,Â method]) | Provide expanding window calculations. |\n| `DataFrame.ewm`([com,Â span,Â halflife,Â alpha,Â ...]) | Provide exponentially weighted (EW) calculations. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.abs`() | Return a Series/DataFrame with absolute numeric value of each element. |\n| `DataFrame.all`([axis,Â bool\\_only,Â skipna]) | Return whether all elements are True, potentially over an axis. |\n| `DataFrame.any`(\\*[,Â axis,Â bool\\_only,Â skipna]) | Return whether any element is True, potentially over an axis. |\n| `DataFrame.clip`([lower,Â upper,Â axis,Â inplace]) | Trim values at input threshold(s). |\n| `DataFrame.corr`([method,Â min\\_periods,Â ...]) | Compute pairwise correlation of columns, excluding NA/null values. |\n| `DataFrame.corrwith`(other[,Â axis,Â drop,Â ...]) | Compute pairwise correlation. |\n| `DataFrame.count`([axis,Â numeric\\_only]) | Count non-NA cells for each column or row. |\n| `DataFrame.cov`([min\\_periods,Â ddof,Â numeric\\_only]) | Compute pairwise covariance of columns, excluding NA/null values. |\n| `DataFrame.cummax`([axis,Â skipna]) | Return cumulative maximum over a DataFrame or Series axis. |\n| `DataFrame.cummin`([axis,Â skipna]) | Return cumulative minimum over a DataFrame or Series axis. |\n| `DataFrame.cumprod`([axis,Â skipna]) | Return cumulative product over a DataFrame or Series axis. |\n| `DataFrame.cumsum`([axis,Â skipna]) | Return cumulative sum over a DataFrame or Series axis. |\n| `DataFrame.describe`([percentiles,Â include,Â ...]) | Generate descriptive statistics. |\n| `DataFrame.diff`([periods,Â axis]) | First discrete difference of element. |\n| `DataFrame.eval`(expr,Â \\*[,Â inplace]) | Evaluate a string describing operations on DataFrame columns. |\n| `DataFrame.kurt`([axis,Â skipna,Â numeric\\_only]) | Return unbiased kurtosis over requested axis. |\n| `DataFrame.kurtosis`([axis,Â skipna,Â numeric\\_only]) | Return unbiased kurtosis over requested axis. |\n| `DataFrame.max`([axis,Â skipna,Â numeric\\_only]) | Return the maximum of the values over the requested axis. |\n| `DataFrame.mean`([axis,Â skipna,Â numeric\\_only]) | Return the mean of the values over the requested axis. |\n| `DataFrame.median`([axis,Â skipna,Â numeric\\_only]) | Return the median of the values over the requested axis. |\n| `DataFrame.min`([axis,Â skipna,Â numeric\\_only]) | Return the minimum of the values over the requested axis. |\n| `DataFrame.mode`([axis,Â numeric\\_only,Â dropna]) | Get the mode(s) of each element along the selected axis. |\n| `DataFrame.pct\\_change`([periods,Â fill\\_method,Â ...]) | Fractional change between the current and a prior element. |\n| `DataFrame.prod`([axis,Â skipna,Â numeric\\_only,Â ...]) | Return the product of the values over the requested axis. |\n| `DataFrame.product`([axis,Â skipna,Â ...]) | Return the product of the values over the requested axis. |\n| `DataFrame.quantile`([q,Â axis,Â numeric\\_only,Â ...]) | Return values at the given quantile over requested axis. |\n| `DataFrame.rank`([axis,Â method,Â numeric\\_only,Â ...]) | Compute numerical data ranks (1 through n) along axis. |\n| `DataFrame.round`([decimals]) | Round a DataFrame to a variable number of decimal places. |\n| `DataFrame.sem`([axis,Â skipna,Â ddof,Â numeric\\_only]) | Return unbiased standard error of the mean over requested axis. |\n| `DataFrame.skew`([axis,Â skipna,Â numeric\\_only]) | Return unbiased skew over requested axis. |\n| `DataFrame.sum`([axis,Â skipna,Â numeric\\_only,Â ...]) | Return the sum of the values over the requested axis. |\n| `DataFrame.std`([axis,Â skipna,Â ddof,Â numeric\\_only]) | Return sample standard deviation over requested axis. |\n| `DataFrame.var`([axis,Â skipna,Â ddof,Â numeric\\_only]) | Return unbiased variance over requested axis. |\n| `DataFrame.nunique`([axis,Â dropna]) | Count number of distinct elements in specified axis. |\n| `DataFrame.value\\_counts`([subset,Â normalize,Â ...]) | Return a Series containing the frequency of each distinct row in the Dataframe. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.add\\_prefix`(prefix[,Â axis]) | Prefix labels with string prefix. |\n| `DataFrame.add\\_suffix`(suffix[,Â axis]) | Suffix labels with string suffix. |\n| `DataFrame.align`(other[,Â join,Â axis,Â level,Â ...]) | Align two objects on their axes with the specified join method. |\n| `DataFrame.at\\_time`(time[,Â asof,Â axis]) | Select values at particular time of day (e.g., 9:30AM). |\n| `DataFrame.between\\_time`(start\\_time,Â end\\_time) | Select values between particular times of the day (e.g., 9:00-9:30 AM). |\n| `DataFrame.drop`([labels,Â axis,Â index,Â ...]) | Drop specified labels from rows or columns. |\n| `DataFrame.drop\\_duplicates`([subset,Â keep,Â ...]) | Return DataFrame with duplicate rows removed. |\n| `DataFrame.duplicated`([subset,Â keep]) | Return boolean Series denoting duplicate rows. |\n| `DataFrame.equals`(other) | Test whether two objects contain the same elements. |\n| `DataFrame.filter`([items,Â like,Â regex,Â axis]) | Subset the dataframe rows or columns according to the specified index labels. |\n| `DataFrame.first`(offset) | Select initial periods of time series data based on a date offset. |\n| `DataFrame.head`([n]) | Return the first n rows. |\n| `DataFrame.idxmax`([axis,Â skipna,Â numeric\\_only]) | Return index of first occurrence of maximum over requested axis. |\n| `DataFrame.idxmin`([axis,Â skipna,Â numeric\\_only]) | Return index of first occurrence of minimum over requested axis. |\n| `DataFrame.last`(offset) | Select final periods of time series data based on a date offset. |\n| `DataFrame.reindex`([labels,Â index,Â columns,Â ...]) | Conform DataFrame to new index with optional filling logic. |\n| `DataFrame.reindex\\_like`(other[,Â method,Â ...]) | Return an object with matching indices as other object. |\n| `DataFrame.rename`([mapper,Â index,Â columns,Â ...]) | Rename columns or index labels. |\n| `DataFrame.rename\\_axis`([mapper,Â index,Â ...]) | Set the name of the axis for the index or columns. |\n| `DataFrame.reset\\_index`([level,Â drop,Â ...]) | Reset the index, or a level of it. |\n| `DataFrame.sample`([n,Â frac,Â replace,Â ...]) | Return a random sample of items from an axis of object. |\n| `DataFrame.set\\_axis`(labels,Â \\*[,Â axis,Â copy]) | Assign desired index to given axis. |\n| `DataFrame.set\\_index`(keys,Â \\*[,Â drop,Â append,Â ...]) | Set the DataFrame index using existing columns. |\n| `DataFrame.tail`([n]) | Return the last n rows. |\n| `DataFrame.take`(indices[,Â axis]) | Return the elements in the given *positional* indices along an axis. |\n| `DataFrame.truncate`([before,Â after,Â axis,Â copy]) | Truncate a Series or DataFrame before and after some index value. |\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.backfill`(\\*[,Â axis,Â inplace,Â ...]) | (DEPRECATED) Fill NA/NaN values by using the next valid observation to fill the gap. |\n| `DataFrame.bfill`(\\*[,Â axis,Â inplace,Â limit,Â ...]) | Fill NA/NaN values by using the next valid observation to fill the gap. |\n| `DataFrame.dropna`(\\*[,Â axis,Â how,Â thresh,Â ...]) | Remove missing values. |\n| `DataFrame.ffill`(\\*[,Â axis,Â inplace,Â limit,Â ...]) | Fill NA/NaN values by propagating the last valid observation to next valid. |\n| `DataFrame.fillna`([value,Â method,Â axis,Â ...]) | Fill NA/NaN values using the specified method. |\n| `DataFrame.interpolate`([method,Â axis,Â limit,Â ...]) | Fill NaN values using an interpolation method. |\n| `DataFrame.isna`() | Detect missing values. |\n| `DataFrame.isnull`() | DataFrame.isnull is an alias for DataFrame.isna. |\n| `DataFrame.notna`() | Detect existing (non-missing) values. |\n| `DataFrame.notnull`() | DataFrame.notnull is an alias for DataFrame.notna. |\n| `DataFrame.pad`(\\*[,Â axis,Â inplace,Â limit,Â ...]) | (DEPRECATED) Fill NA/NaN values by propagating the last valid observation to next valid. |\n| `DataFrame.replace`([to\\_replace,Â value,Â ...]) | Replace values given in to\\_replace with value. |\n\n## Reshaping, sorting, transposing#\n\n\n|  |  |\n| --- | --- |\n| `DataFrame.droplevel`(level[,Â axis]) | Return Series/DataFrame with requested index / column level(s) removed. |\n| `DataFrame.pivot`(\\*,Â columns[,Â index,Â values]) | Return reshaped DataFrame organized by given index / column values. |\n| `DataFrame.pivot\\_table`([values,Â index,Â ...]) | Create\n\n==================\n Document 2 \n----------------\n API reference#\n\n\nThis page gives an overview of all public pandas objects, functions and\nmethods. All classes and functions exposed in `pandas.\\*` namespace are public.\n\n\nThe following subpackages are public.\n\n\n* `pandas.errors`: Custom exception and warnings classes that are raised by pandas.\n* `pandas.plotting`: Plotting public API.\n* `pandas.testing`: Functions that are useful for writing tests involving pandas objects.\n* `pandas.api.extensions`: Functions and classes for extending pandas objects.\n* `pandas.api.indexers`: Functions and classes for rolling window indexers.\n* `pandas.api.interchange`: DataFrame interchange protocol.\n* `pandas.api.types`: Datatype classes and functions.\n* `pandas.api.typing`: Classes that may be necessary for type-hinting.\nThese are classes that are encountered as intermediate results but should not be instantiated\ndirectly by users. These classes are not to be confused with classes from the\npandas-stubs package\nwhich has classes in addition to those that occur in pandas for type-hinting.\n\n\nIn addition, public functions in `pandas.io` and `pandas.tseries` submodules\nare mentioned in the documentation.\n\nWarning\n\n\nThe `pandas.core`, `pandas.compat`, and `pandas.util` top-level modules are PRIVATE. Stable functionality in such modules is not guaranteed.\n\n\n* Input/output\n\t+ Pickling\n\t+ Flat file\n\t+ Clipboard\n\t+ Excel\n\t+ JSON\n\t+ HTML\n\t+ XML\n\t+ Latex\n\t+ HDFStore: PyTables (HDF5)\n\t+ Feather\n\t+ Parquet\n\t+ ORC\n\t+ SAS\n\t+ SPSS\n\t+ SQL\n\t+ Google BigQuery\n\t+ STATA\n* General functions\n\t+ Data manipulations\n\t+ Top-level missing data\n\t+ Top-level dealing with numeric data\n\t+ Top-level dealing with datetimelike data\n\t+ Top-level dealing with Interval data\n\t+ Top-level evaluation\n\t+ Hashing\n\t+ Importing from other DataFrame libraries\n* Series\n\t+ Constructor\n\t+ Attributes\n\t+ Conversion\n\t+ Indexing, iteration\n\t+ Binary operator functions\n\t+ Function application, GroupBy & window\n\t+ Computations / descriptive stats\n\t+ Reindexing / selection / label manipulation\n\t+ Missing data handling\n\t+ Reshaping, sorting\n\t+ Combining / comparing / joining / merging\n\t+ Time Series-related\n\t+ Accessors\n\t+ Plotting\n\t+ Serialization / IO / conversion\n* DataFrame\n\t+ Constructor\n\t+ Attributes and underlying data\n\t+ Conversion\n\t+ Indexing, iteration\n\t+ Binary operator functions\n\t+ Function application, GroupBy & window\n\t+ Computations / descriptive stats\n\t+ Reindexing / selection / label manipulation\n\t+ Missing data handling\n\t+ Reshaping, sorting, transposing\n\t+ Combining / comparing / joining / merging\n\t+ Time Series-related\n\t+ Flags\n\t+ Metadata\n\t+ Plotting\n\t+ Sparse accessor\n\t+ Serialization / IO / conversion\n* pandas arrays, scalars, and data types\n\t+ Objects\n\t+ Utilities\n* Index objects\n\t+ Index\n\t+ Numeric Index\n\t+ CategoricalIndex\n\t+ IntervalIndex\n\t+ MultiIndex\n\t+ DatetimeIndex\n\t+ TimedeltaIndex\n\t+ PeriodIndex\n* Date offsets\n\t+ DateOffset\n\t+ BusinessDay\n\t+ BusinessHour\n\t+ CustomBusinessDay\n\t+ CustomBusinessHour\n\t+ MonthEnd\n\t+ MonthBegin\n\t+ BusinessMonthEnd\n\t+ BusinessMonthBegin\n\t+ CustomBusinessMonthEnd\n\t+ CustomBusinessMonthBegin\n\t+ SemiMonthEnd\n\t+ SemiMonthBegin\n\t+ Week\n\t+ WeekOfMonth\n\t+ LastWeekOfMonth\n\t+ BQuarterEnd\n\t+ BQuarterBegin\n\t+ QuarterEnd\n\t+ QuarterBegin\n\t+ BYearEnd\n\t+ BYearBegin\n\t+ YearEnd\n\t+ YearBegin\n\t+ FY5253\n\t+ FY5253Quarter\n\t+ Easter\n\t+ Tick\n\t+ Day\n\t+ Hour\n\t+ Minute\n\t+ Second\n\t+ Milli\n\t+ Micro\n\t+ Nano\n* Frequencies\n\t+ pandas.tseries.frequencies.to\\_offset\n* Window\n\t+ Rolling window functions\n\t+ Weighted window functions\n\t+ Expanding window functions\n\t+ Exponentially-weighted window functions\n\t+ Window indexer\n* GroupBy\n\t+ Indexing, iteration\n\t+ Function application helper\n\t+ Function application\n\t+ `DataFrameGroupBy` computations / descriptive stats\n\t+ `SeriesGroupBy` computations / descriptive stats\n\t+ Plotting and visualization\n* Resampling\n\t+ Indexing, iteration\n\t+ Function application\n\t+ Upsampling\n\t+ Computations / descriptive stats\n* Style\n\t+ Styler constructor\n\t+ Styler properties\n\t+ Style application\n\t+ Builtin styles\n\t+ Style export and import\n* Plotting\n\t+ pandas.plotting.andrews\\_curves\n\t+ pandas.plotting.autocorrelation\\_plot\n\t+ pandas.plotting.bootstrap\\_plot\n\t+ pandas.plotting.boxplot\n\t+ pandas.plotting.deregister\\_matplotlib\\_converters\n\t+ pandas.plotting.lag\\_plot\n\t+ pandas.plotting.parallel\\_coordinates\n\t+ pandas.plotting.plot\\_params\n\t+ pandas.plotting.radviz\n\t+ pandas.plotting.register\\_matplotlib\\_converters\n\t+ pandas.plotting.scatter\\_matrix\n\t+ pandas.plotting.table\n* Options and settings\n\t+ Working with options\n\t+ Numeric formatting\n* Extensions\n\t+ pandas.api.extensions.register\\_extension\\_dtype\n\t+ pandas.api.extensions.register\\_dataframe\\_accessor\n\t+ pandas.api.extensions.register\\_series\\_accessor\n\t+ pandas.api.extensions.register\\_index\\_accessor\n\t+ pandas.api.extensions.ExtensionDtype\n\t+ pandas.api.extensions.ExtensionArray\n\t+ pandas.arrays.NumpyExtensionArray\n\t+ pandas.api.indexers.check\\_array\\_indexer\n* Testing\n\t+ Assertion functions\n\t+ Exceptions and warnings\n\t+ Bug report function\n\t+ Test suite runner\n\n\n# Input/output#\n\n\n## Pickling#\n\n\n|  |  |\n| --- | --- |\n| `read\\_pickle`(filepath\\_or\\_buffer[,Â ...]) | Load pickled pandas object (or any object) from file. |\n| `DataFrame.to\\_pickle`(path[,Â compression,Â ...]) | Pickle (serialize) object to file. |\n\n\n\n## Flat file#\n\n\n|  |  |\n| --- | --- |\n| `read\\_table`(filepath\\_or\\_buffer,Â \\*[,Â sep,Â ...]) | Read general delimited file into DataFrame. |\n| `read\\_csv`(filepath\\_or\\_buffer,Â \\*[,Â sep,Â ...]) | Read a comma-separated values (csv) file into DataFrame. |\n| `DataFrame.to\\_csv`([path\\_or\\_buf,Â sep,Â na\\_rep,Â ...]) | Write object to a comma-separated values (csv) file. |\n| `read\\_fwf`(filepath\\_or\\_buffer,Â \\*[,Â colspecs,Â ...]) | Read a table of fixed-width formatted lines into DataFrame. |\n\n\n\n## Clipboard#\n\n\n|  |  |\n| --- | --- |\n| `read\\_clipboard`([sep,Â dtype\\_backend]) | Read text from clipboard and pass to `read\\_csv()`. |\n| `DataFrame.to\\_clipboard`([excel,Â sep]) | Copy object to the system clipboard. |\n\n\n## Excel#\n\n\n|  |  |\n| --- | --- |\n| `read\\_excel`(io[,Â sheet\\_name,Â header,Â names,Â ...]) | Read an Excel file into a pandas DataFrame. |\n| `DataFrame.to\\_excel`(excel\\_writer[,Â ...]) | Write object to an Excel sheet. |\n| `ExcelFile`(path\\_or\\_buffer[,Â engine,Â ...]) | Class for parsing tabular Excel sheets into DataFrame\n\n==================\n Document 3 \n----------------\n pandas.plotting.andrews\\_curves#\n\n\npandas.plotting.andrews\\_curves(*frame*, *class\\_column*, *ax=None*, *samples=200*, *color=None*, *colormap=None*, *\\*\\*kwargs*)[source]#\nGenerate a matplotlib plot for visualising clusters of multivariate data.\n\n\nAndrews curves have the functional form:\n\n\\[f(t) = \\frac{x\\_1}{\\sqrt{2}} + x\\_2 \\sin(t) + x\\_3 \\cos(t) +\nx\\_4 \\sin(2t) + x\\_5 \\cos(2t) + \\cdots\\]\nWhere \\(x\\) coefficients correspond to the values of each dimension\nand \\(t\\) is linearly spaced between \\(-\\pi\\) and \\(+\\pi\\).\nEach row of frame then corresponds to a single curve.\n\n**frame**DataFrameData to be plotted, preferably normalized to (0.0, 1.0).\n\n**class\\_column**labelName of the column containing class names.\n\n**ax**axes object, default NoneAxes to use.\n\n**samples**intNumber of points to plot in each curve.\n\n**color**str, list[str] or tuple[str], optionalColors to use for the different classes. Colors can be strings\nor 3-element floating point RGB values.\n\n**colormap**str or matplotlib colormap object, default NoneColormap to select colors from. If a string, load colormap with that\nname from matplotlib.\n\n**\\*\\*kwargs**Options to pass to matplotlib plotting method.\n\nReturns:\n\n`matplotlib.axes.Axes`\n\n```\n>>> df = pd.read\\_csv(\n...     'https://raw.githubusercontent.com/pandas-dev/'\n...     'pandas/main/pandas/tests/io/data/csv/iris.csv'\n... )\n>>> pd.plotting.andrews\\_curves(df, 'Name')  \n\n\n\n# pandas.plotting.autocorrelation\\_plot#\n\n\npandas.plotting.autocorrelation\\_plot(*series*, *ax=None*, *\\*\\*kwargs*)[source]#\nAutocorrelation plot for time series.\n\n**series**SeriesThe time series to visualize.\n\n**ax**Matplotlib axis object, optionalThe matplotlib axis object to use.\n\nmatplotlib.axes.Axes\n\n\nThe horizontal lines in the plot correspond to 95% and 99% confidence bands.\n\n\nThe dashed line is 99% confidence band.\n\n```\n>>> spacing = np.linspace(-9 \\* np.pi, 9 \\* np.pi, num=1000)\n>>> s = pd.Series(0.7 \\* np.random.rand(1000) + 0.3 \\* np.sin(spacing))\n>>> pd.plotting.autocorrelation\\_plot(s)  \n\n\n\n# pandas.plotting.bootstrap\\_plot#\n\n\npandas.plotting.bootstrap\\_plot(*series*, *fig=None*, *size=50*, *samples=500*, *\\*\\*kwds*)[source]#\nBootstrap plot on mean, median and mid-range statistics.\n\n\nThe bootstrap plot is used to estimate the uncertainty of a statistic\nby relying on random sampling with replacement [1]. This function will\ngenerate bootstrapping plots for mean, median and"
    },
    {
        "instruction": "Create a Python program that utilizes the 'peft' API to generate code for a basic language translation application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a language translation application that can translate text between different languages.",
        "api": "peft",
        "output": "# Install necessary libraries\n#!pip install transformers\n#!pip install peft\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Load Models\nbase_model = \"gpt-3.5-turbo\"\npeft_model = \"gpt-3.5-turbo\"\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(base_model)\n\n# Load peft\nmodel = PeftModel.from_pretrained(model, peft_model)\nmodel = model.eval()\n\n# Generate code for a language translation application\ntranslation_app_code = model.generate_translation_app_code(max_length=500)\n\n# Print generated language translation application code\nprint(translation_app_code)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n \n\n# \nModels\n\n\nPeftModel is the base model class for specifying the base Transformer model and configuration to apply a PEFT method to. The base `PeftModel` contains methods for loading and saving models from the Hub, and supports the PromptEncoder for prompt learning.\n\n\n\n## \nPeftModel\n\n### class peft.PeftModel\n\n<\nsource\n>\n(\nmodel: PreTrainedModel\npeft\\_config: PeftConfig\nadapter\\_name: str = 'default'\n\n)\n\n\nParameters \n\n\n* **model** (PreTrainedModel) — The base transformer model used for Peft.\n* **peft\\_config** (PeftConfig) — The configuration of the Peft model.\n* **adapter\\_name** (`str`) — The name of the adapter, defaults to `\"default\"`.\n\nBase model\n\n==================\n Document 1 \n----------------\n### from\\_json\\_file\n\n<\nsource\n>\n(\npath\\_json\\_file\n\\*\\*kwargs\n\n\n* **path\\_json\\_file** (`str`) —\nThe path to the json file.\n\nLoads a configuration file from a json file.\n\n<\nsource\n>\n(\npretrained\\_model\\_name\\_or\\_path\nsubfolder = None\n\\*\\*kwargs\n\n\n* **pretrained\\_model\\_name\\_or\\_path** (`str`) —\nThe directory or the Hub repository id where the configuration is saved.\n* **kwargs** (additional keyword arguments, *optional*) —\nAdditional keyword arguments passed along to the child class initialization.\n\nThis method loads the configuration of your adapter model from a directory.\n\n<\nsource\n>\n(\nsave\\_directory\n\\*\\*kwargs\n\n\n* **save\\_directory** (`str`) —\nThe directory where the configuration will be saved.\n* **kwargs** (additional keyword arguments, *optional*) —\nAdditional keyword arguments passed along to the push\\_to\\_hub\nmethod.\n\nThis method saves the configuration of your adapter model in a directory.\n\n\n## \nPeftConfig\n\n\n### class peft.PeftConfig\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\n\n\n* **peft\\_type** (Union[`~peft.utils.config.PeftType`, `str`]) — The type of Peft method to use.\n* **task\\_type** (Union[`~peft.utils.config.TaskType`, `str`]) — The type of task to perform.\n* **inference\\_mode** (`bool`, defaults to `False`) — Whether to use the Peft model in inference mode.\n\nThis is the base configuration class to store the configuration of a PeftModel.\n\n\n\n## \nPromptLearningConfig\n\n### class peft.PromptLearningConfig\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\nnum\\_virtual\\_tokens: int = None\ntoken\\_dim: int = None\nnum\\_transformer\\_submodules: typing.Optional[int] = None\nnum\\_attention\\_heads: typing.Optional[int] = None\nnum\\_layers: typing.Optional[int] = None\n\n\n* **num\\_virtual\\_tokens** (`int`)\n\n==================\n Document 2 \n----------------\n## class peft.PeftModelForSequenceClassification\n\n<\nsource\n>\n(\nmodel\npeft\\_config: PeftConfig\nadapter\\_name = 'default'\n\n\n* **model** (PreTrainedModel) — Base transformer model.\n* **peft\\_config** (PeftConfig) — Peft config.\n\nPeft model for sequence classification tasks.\n\n\n* **config** (PretrainedConfig) — The configuration object of the base model.\n* **cls\\_layer\\_name** (`str`) — The name of the classification layer.\n\nExample:\n\n\n Copied\n\n```\n>>> from transformers import AutoModelForSequenceClassification\n>>> from peft import PeftModelForSequenceClassification, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"PREFIX\\_TUNING\",\n...     \"task\\_type\": \"SEQ\\_CLS\",\n...     \"inference\\_mode\": False,\n...     \"num\\_virtual\\_tokens\": 20,\n...     \"token\\_dim\": 768,\n...     \"num\\_transformer\\_submodules\": 1,\n...     \"num\\_attention\\_heads\": 12,\n...     \"num\\_layers\": 12,\n...     \"encoder\\_hidden\\_size\": 768,\n...     \"prefix\\_projection\": False,\n...     \"postprocess\\_past\\_key\\_value\\_function\": None,\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n>>> peft_model = PeftModelForSequenceClassification(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 370178 || all params: 108680450 || trainable%: 0.3406113979101117\n```\n\n## \nPeftModelForTokenClassification\n\n\nA `PeftModel` for token classification tasks.\n\n\n### class peft.PeftModelForTokenClassification\n\n<\nsource\n>\n(\nmodel\npeft\\_config: PeftConfig = None\nadapter\\_name = 'default'\n\nPeft model for token classification tasks.\n\n```\n>>> from transformers import AutoModelForSequenceClassification\n>>> from peft import PeftModelForTokenClassification, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"PREFIX\\_TUNING\",\n...     \"task\\_type\": \"TOKEN\\_CLS\",\n...     \"inference\\_mode\": False,\n...     \"num\\_virtual\\_tokens\": 20,\n...     \"token\\_dim\": 768,\n...     \"num\\_transformer\\_submodules\": 1,\n...     \"num\\_attention\\_heads\": 12,\n...     \"num\\_layers\": 12,\n...     \"encoder\\_hidden\\_size\": 768,\n...     \"prefix\\_projection\": False,\n...     \"postprocess\\_past\\_key\\_value\\_function\": None,\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\")\n>>> peft_model = PeftModelForTokenClassification(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 370178 || all params: 108680450 || trainable%: 0.3406113979101117\n```\n\n\n## \nPeftModelForCausalLM\n\n\nA `PeftModel` for causal language modeling.\n\n\n### class peft.PeftModelForCausalLM\n\nPeft model for causal language modeling.\n\n```\n>>> from transformers import AutoModelForCausalLM\n>>> from peft import PeftModelForCausalLM, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"PREFIX\\_TUNING\",\n...     \"task\\_type\": \"CAUSAL\\_LM\",\n...     \"inference\\_mode\": False,\n...     \"num\\_virtual\\_tokens\": 20,\n...     \"token\\_dim\": 1280,\n...     \"num\\_transformer\\_submodules\": 1,\n...     \"num\\_attention\\_heads\": 20,\n...     \"num\\_layers\": 36,\n...     \"encoder\\_hidden\\_size\": 1280,\n...     \"prefix\\_projection\": False,\n...     \"postprocess\\_past\\_key\\_value\\_function\": None,\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n>>> peft_model = PeftModelForCausalLM(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 1843200 || all params: 775873280 || trainable%: 0.23756456724479544\n```\n\n\n## \nPeftModelForSeq2SeqLM\n\n\nA `PeftModel` for sequence-to-sequence language modeling.\n\n\n### class peft.PeftModelForSeq2SeqLM\n\nPeft model for sequence-to-sequence language modeling.\n\n```\n>>> from transformers import AutoModelForSeq2SeqLM\n>>> from peft import PeftModelForSeq2SeqLM, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"LORA\",\n...     \"task\\_type\": \"SEQ\\_2\\_SEQ\\_LM\",\n...     \"inference\\_mode\": False,\n...     \"r\": 8,\n...     \"target\\_modules\": [\"q\", \"v\"],\n...     \"lora\\_alpha\": 32,\n...     \"lora\\_dropout\": 0.1,\n...     \"fan\\_in\\_fan\\_out\": False,\n...     \"enable\\_lora\": None,\n...     \"bias\": \"none\",\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n>>> peft_model = PeftModelForSeq2SeqLM(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 884736 || all params: 223843584 || trainable%: 0.3952474242013566\n```\n\n\n## \nPeftModelForQuestionAnswering\n\n\nA `PeftModel` for question answering.\n\n\n### class peft.PeftModelForQuestionAnswering\n\nPeft model for extractive question answering.\n\n```\n>>> from transformers import AutoModelForQuestionAnswering\n>>> from peft import PeftModelForQuestionAnswering, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"LORA\",\n...     \"task\\_type\": \"QUESTION\\_ANS\",\n...     \"inference\\_mode\": False,\n...     \"r\": 16,\n...     \"target\\_modules\": [\"query\", \"value\"],\n...     \"lora\\_alpha\": 32,\n...     \"lora\\_dropout\": 0.05,\n...     \"fan\\_in\\_fan\\_out\": False,\n...     \"bias\": \"none\",\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-cased\")\n>>> peft_model = PeftModelForQuestionAnswering(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 592900 || all params: 108312580 || trainable%: 0.5473971721475013\n```\n\n\n## \nPeftModelForFeatureExtraction\n\n\nA `PeftModel` for getting extracting features/embeddings from transformer models.\n\n\n### class peft.PeftModelForFeatureExtraction\n\nPeft model for extracting features/embeddings from transformer models\n\n\n* **config** (PretrainedConfig) — The configuration object of the base model.\n\n```\n>>> from transformers import AutoModel\n>>> from peft import PeftModelForFeatureExtraction, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"LORA\",\n...     \"task\\_type\": \"FEATURE\\_EXTRACTION\",\n...     \"inference\\_mode\": False,\n...     \"r\": 16,\n...     \"target\\_modules\": [\"query\", \"value\"],\n...     \"lora\\_alpha\": 32,\n...     \"lora\\_dropout\": 0.05,\n...     \"fan\\_in\\_fan\\_out\": False,\n...     \"bias\": \"none\",\n... }\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModel.from_pretrained(\"bert-base-cased\")\n>>> peft_model = PeftModelForFeatureExtraction(model, peft_config)\n>>> peft_model.print_trainable_parameters()\n```\n\n\n# \nConfiguration\n\n\nThe configuration classes stores the configuration of a PeftModel, PEFT adapter models, and the configurations of `PrefixTuning`, `PromptTuning`, and PromptEncoder. They contain methods for saving and loading model configurations from the Hub, specifying the PEFT method to use, type of task to perform, and model configurations like number of layers and number of attention heads.\n\n\n\n## \nPeftConfigMixin\n\n\n### class peft.config.PeftConfigMixin\n\n<\nsource\n>\n(\npeft\\_type: typing.Optional[peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\n\n\n* **peft\\_type** (Union[`~peft.utils.config.PeftType`, `str`]) — The type of Peft method to use.\n\nThis is the base configuration class for PEFT adapter models. It contains all the methods that are common to all\nPEFT adapter models. This class inherits from PushToHubMixin which contains the methods to\npush your model to the Hub. The method `save_pretrained` will save the configuration of your adapter model in a\ndirectory. The method `from_pretrained` will load the configuration of your adapter model from a directory.\n\n#### from\\_json\\_file\n\n<\nsource\n>\n(\npath\\_json\\_file\n\\*\\*kwargs\n\n\n* **path\\_json\\_file** (`str`) —\nThe path to the json file.\n\nLoads a configuration file from a json file.\n\n<\nsource\n>\n(\npretrained\\_model\\_name\\_or\\_path\nsubfolder = None\n\\*\\*kwargs\n\n\n* **pretrained\\_model\\_name\\_or\\_path** (`str`) —\nThe directory or the Hub repository id where the configuration is saved.\n* **kwargs** (additional keyword arguments, *optional*) —\nAdditional keyword\n\n==================\n Document 3 \n----------------\n## class peft.PromptEmbedding\n\n<\nsource\n>\n(\nconfig\nword\\_embeddings\n\n\n* **config** (PromptTuningConfig) — The configuration of the prompt embedding.\n* **word\\_embeddings** (`torch.nn.Module`) — The word embeddings of the base transformer model.\n\nThe model to encode virtual tokens into prompt embeddings.\n\n\n* **embedding** (`torch.nn.Embedding`) — The embedding layer of the prompt embedding.\n\n```\n>>> from peft import PromptEmbedding, PromptTuningConfig\n\n>>> config = PromptTuningConfig(\n...     peft_type=\"PROMPT\\_TUNING\",\n...     task_type=\"SEQ\\_2\\_SEQ\\_LM\",\n...     num_virtual_tokens=20,\n...     token_dim=768,\n...     num_transformer_submodules=1,\n...     num_attention_heads=12,\n...     num_layers=12,\n...     prompt_tuning_init=\"TEXT\",\n...     prompt_tuning_init_text=\"Predict if sentiment of this review is positive, negative or neutral\",\n...     tokenizer_name_or_path=\"t5-base\",\n... )\n\n>>> # t5\\_model.shared is the word embeddings of the base model\n>>> prompt_embedding = PromptEmbedding(config, t5_model.shared)\n```\n\nInput Shape: (`batch_size`, `total_virtual_tokens`)\n\n\nOutput Shape: (`batch_size`, `total_virtual_tokens`, `token_dim`)\n\n\n## \nIA3\n\n### class peft.IA3Config\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\ntarget\\_modules: typing.Union[typing.List[str], str, NoneType] = None\nfeedforward\\_modules: typing.Union[typing.List[str], str, NoneType] = None\nfan\\_in\\_fan\\_out: bool = False\nmodules\\_to\\_save: typing.Optional[typing.List[str]] = None\ninit\\_ia3\\_weights: bool\n\n==================\n Document 4 \n----------------\n## class peft.PromptLearningConfig\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\nnum\\_virtual\\_tokens: int = None\ntoken\\_dim: int = None\nnum\\_transformer\\_submodules: typing.Optional[int] = None\nnum\\_attention\\_heads: typing.Optional[int] = None\nnum\\_layers: typing.Optional[int] = None\n\n\n* **num\\_virtual\\_tokens** (`int`) — The number of virtual tokens to use.\n* **token\\_dim** (`int`) — The hidden embedding dimension of the base transformer model.\n* **num\\_transformer\\_submodules** (`int`) — The number of transformer submodules in the base transformer model.\n* **num\\_attention\\_heads** (`int`) — The number of attention heads in the base transformer model.\n* **num\\_layers** (`int`) — The number of layers in the base transformer model.\n\nThis is the base configuration class to store the configuration of `PrefixTuning`, PromptEncoder, or\n`PromptTuning`.\n\n\n \n\n# \nTuners\n\n\nEach tuner (or PEFT method) has a configuration and model.\n\n\n\n## \nLoRA\n\n\nFor finetuning a model with LoRA.\n\n### class peft.LoraConfig\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\nr: int = 8\ntarget\\_modules: typing.Union[typing.List[str], str, NoneType] = None\nlora\\_alpha: int = 8\nlora\\_dropout: float = 0.0\nfan\\_in\\_fan\\_out: bool = False\nbias:\n\n==================\n Document 5 \n----------------\n## class peft.PeftModel\n\n<\nsource\n>\n(\nmodel: PreTrainedModel\npeft\\_config: PeftConfig\nadapter\\_name: str = 'default'\n\n)\n\n\nParameters \n\n\n* **model** (PreTrainedModel) — The base transformer model used for Peft.\n* **peft\\_config** (PeftConfig) — The configuration of the Peft model.\n* **adapter\\_name** (`str`) — The name of the adapter, defaults to `\"default\"`.\n\nBase model encompassing various Peft methods.\n\n\n**Attributes**:\n\n\n* **base\\_model** (PreTrainedModel) — The base transformer model used for Peft.\n* **peft\\_config** (PeftConfig) — The configuration of the Peft model.\n* **modules\\_to\\_save** (`list` of `str`) — The list of sub-module names to save when\nsaving the model.\n* **prompt\\_encoder** (PromptEncoder) — The prompt encoder used for Peft if\nusing PromptLearningConfig.\n* **prompt\\_tokens** (`torch.Tensor`) — The virtual prompt tokens used for Peft if\nusing PromptLearningConfig.\n* **transformer\\_backbone\\_name** (`str`) — The name of the transformer\nbackbone in the base model if using PromptLearningConfig.\n* **word\\_embeddings** (`torch.nn.Embedding`) — The word embeddings of the transformer backbone\nin the base model if using PromptLearningConfig.\n\n#### create\\_or\\_update\\_model\\_card\n\n<\nsource\n>\n(\noutput\\_dir: str\n\nUpdates or create model card to include information about peft:\n\n\n1. Adds `peft` library tag\n2. Adds peft version\n3. Adds quantization information if it was used\n\n\n\n#### disable\\_adapter\n\n<\nsource\n>\n(\n)\n\nDisables the adapter module.\n\n\n#### forward\n\n<\nsource\n>\n(\n\\*args: Any\n\\*\\*kwargs: Any\n\nForward pass of the model.\n\n#### from\\_pretrained\n\n<\nsource\n>\n(\nmodel: PreTrainedModel\nmodel\\_id: Union[str, os.PathLike]\nadapter\\_name: str = 'default'\nis\\_trainable: bool = False\nconfig: Optional[PeftConfig] = None\n\\*\\*kwargs: Any\n\n\n* **model** (PreTrainedModel) —\nThe model to be adapted. The model should be initialized with the\nfrom\\_pretrained method from the 🤗 Transformers library.\n* **model\\_id** (`str` or `os.PathLike`) —\nThe"
    },
    {
        "instruction": "Develop a Python program using the 'peft' API to generate code for a basic e-commerce website. Utilize the 'gpt-3.5-turbo' model for website code generation. The program should generate code for a simple e-commerce website with product listings, a shopping cart, and checkout functionality.",
        "api": "peft",
        "output": "# Install necessary libraries\n#!pip install transformers\n#!pip install peft\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Load Models\nbase_model = \"gpt-3.5-turbo\"\npeft_model = \"gpt-3.5-turbo\"\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(base_model)\n\n# Load peft\nmodel = PeftModel.from_pretrained(model, peft_model)\nmodel = model.eval()\n\n# Generate code for a basic e-commerce website\necommerce_website_code = model.generate_ecommerce_website_code(max_length=500)\n\n# Print generated e-commerce website code\nprint(ecommerce_website_code)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n### from\\_json\\_file\n\n<\nsource\n>\n(\npath\\_json\\_file\n\\*\\*kwargs\n\n\n* **path\\_json\\_file** (`str`) —\nThe path to the json file.\n\nLoads a configuration file from a json file.\n\n<\nsource\n>\n(\npretrained\\_model\\_name\\_or\\_path\nsubfolder = None\n\\*\\*kwargs\n\n\n* **pretrained\\_model\\_name\\_or\\_path** (`str`) —\nThe directory or the Hub repository id where the configuration is saved.\n* **kwargs** (additional keyword arguments, *optional*) —\nAdditional keyword arguments passed along to the child class initialization.\n\nThis method loads the configuration of your adapter model from a directory.\n\n<\nsource\n>\n(\nsave\\_directory\n\\*\\*kwargs\n\n\n* **save\\_directory** (`str`) —\nThe directory where the configuration will be saved.\n* **kwargs** (additional keyword arguments, *optional*) —\nAdditional keyword arguments passed along to the push\\_to\\_hub\nmethod.\n\nThis method saves the configuration of your adapter model in a directory.\n\n\n## \nPeftConfig\n\n\n### class peft.PeftConfig\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\n\n\n* **peft\\_type** (Union[`~peft.utils.config.PeftType`, `str`]) — The type of Peft method to use.\n* **task\\_type** (Union[`~peft.utils.config.TaskType`, `str`]) — The type of task to perform.\n* **inference\\_mode** (`bool`, defaults to `False`) — Whether to use the Peft model in inference mode.\n\nThis is the base configuration class to store the configuration of a PeftModel.\n\n\n\n## \nPromptLearningConfig\n\n### class peft.PromptLearningConfig\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\nnum\\_virtual\\_tokens: int = None\ntoken\\_dim: int = None\nnum\\_transformer\\_submodules: typing.Optional[int] = None\nnum\\_attention\\_heads: typing.Optional[int] = None\nnum\\_layers: typing.Optional[int] = None\n\n\n* **num\\_virtual\\_tokens** (`int`)\n\n==================\n Document 1 \n----------------\n\n\n \n\n# \nModels\n\n\nPeftModel is the base model class for specifying the base Transformer model and configuration to apply a PEFT method to. The base `PeftModel` contains methods for loading and saving models from the Hub, and supports the PromptEncoder for prompt learning.\n\n\n\n## \nPeftModel\n\n### class peft.PeftModel\n\n<\nsource\n>\n(\nmodel: PreTrainedModel\npeft\\_config: PeftConfig\nadapter\\_name: str = 'default'\n\n)\n\n\nParameters \n\n\n* **model** (PreTrainedModel) — The base transformer model used for Peft.\n* **peft\\_config** (PeftConfig) — The configuration of the Peft model.\n* **adapter\\_name** (`str`) — The name of the adapter, defaults to `\"default\"`.\n\nBase model\n\n==================\n Document 2 \n----------------\n## class peft.PeftModel\n\n<\nsource\n>\n(\nmodel: PreTrainedModel\npeft\\_config: PeftConfig\nadapter\\_name: str = 'default'\n\n)\n\n\nParameters \n\n\n* **model** (PreTrainedModel) — The base transformer model used for Peft.\n* **peft\\_config** (PeftConfig) — The configuration of the Peft model.\n* **adapter\\_name** (`str`) — The name of the adapter, defaults to `\"default\"`.\n\nBase model encompassing various Peft methods.\n\n\n**Attributes**:\n\n\n* **base\\_model** (PreTrainedModel) — The base transformer model used for Peft.\n* **peft\\_config** (PeftConfig) — The configuration of the Peft model.\n* **modules\\_to\\_save** (`list` of `str`) — The list of sub-module names to save when\nsaving the model.\n* **prompt\\_encoder** (PromptEncoder) — The prompt encoder used for Peft if\nusing PromptLearningConfig.\n* **prompt\\_tokens** (`torch.Tensor`) — The virtual prompt tokens used for Peft if\nusing PromptLearningConfig.\n* **transformer\\_backbone\\_name** (`str`) — The name of the transformer\nbackbone in the base model if using PromptLearningConfig.\n* **word\\_embeddings** (`torch.nn.Embedding`) — The word embeddings of the transformer backbone\nin the base model if using PromptLearningConfig.\n\n#### create\\_or\\_update\\_model\\_card\n\n<\nsource\n>\n(\noutput\\_dir: str\n\nUpdates or create model card to include information about peft:\n\n\n1. Adds `peft` library tag\n2. Adds peft version\n3. Adds quantization information if it was used\n\n\n\n#### disable\\_adapter\n\n<\nsource\n>\n(\n)\n\nDisables the adapter module.\n\n\n#### forward\n\n<\nsource\n>\n(\n\\*args: Any\n\\*\\*kwargs: Any\n\nForward pass of the model.\n\n#### from\\_pretrained\n\n<\nsource\n>\n(\nmodel: PreTrainedModel\nmodel\\_id: Union[str, os.PathLike]\nadapter\\_name: str = 'default'\nis\\_trainable: bool = False\nconfig: Optional[PeftConfig] = None\n\\*\\*kwargs: Any\n\n\n* **model** (PreTrainedModel) —\nThe model to be adapted. The model should be initialized with the\nfrom\\_pretrained method from the 🤗 Transformers library.\n* **model\\_id** (`str` or `os.PathLike`) —\nThe\n\n==================\n Document 3 \n----------------\n## class peft.PrefixEncoder\n\n\n* **config** (PrefixTuningConfig) — The configuration of the prefix encoder.\n\nThe `torch.nn` model to encode the prefix.\n\n```\n>>> from peft import PrefixEncoder, PrefixTuningConfig\n\n>>> config = PrefixTuningConfig(\n...     peft_type=\"PREFIX\\_TUNING\",\n...     task_type=\"SEQ\\_2\\_SEQ\\_LM\",\n...     num_virtual_tokens=20,\n...     token_dim=768,\n...     num_transformer_submodules=1,\n...     num_attention_heads=12,\n...     num_layers=12,\n...     encoder_hidden_size=768,\n... )\n>>> prefix_encoder = PrefixEncoder(config)\n```\n\n\n* **embedding** (`torch.nn.Embedding`) — The embedding layer of the prefix encoder.\n* **transform** (`torch.nn.Sequential`) — The two-layer MLP to transform the prefix embeddings if\n`prefix_projection` is `True`.\n* **prefix\\_projection** (`bool`) — Whether to project the prefix embeddings.\n\n\nInput shape: (`batch_size`, `num_virtual_tokens`)\n\n\nOutput shape: (`batch_size`, `num_virtual_tokens`, `2*layers*hidden`)\n\n\n## \nPrompt tuning\n\n### class peft.PromptTuningConfig\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\nnum\\_virtual\\_tokens: int = None\ntoken\\_dim: int = None\nnum\\_transformer\\_submodules: typing.Optional[int] = None\nnum\\_attention\\_heads: typing.Optional[int] = None\nnum\\_layers: typing.Optional[int] = None\nprompt\\_tuning\\_init: typing.Union[peft.tuners.prompt\\_tuning.config.PromptTuningInit, str]\n\n==================\n Document 4 \n----------------\n## class peft.PeftModelForSequenceClassification\n\n<\nsource\n>\n(\nmodel\npeft\\_config: PeftConfig\nadapter\\_name = 'default'\n\n\n* **model** (PreTrainedModel) — Base transformer model.\n* **peft\\_config** (PeftConfig) — Peft config.\n\nPeft model for sequence classification tasks.\n\n\n* **config** (PretrainedConfig) — The configuration object of the base model.\n* **cls\\_layer\\_name** (`str`) — The name of the classification layer.\n\nExample:\n\n\n Copied\n\n```\n>>> from transformers import AutoModelForSequenceClassification\n>>> from peft import PeftModelForSequenceClassification, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"PREFIX\\_TUNING\",\n...     \"task\\_type\": \"SEQ\\_CLS\",\n...     \"inference\\_mode\": False,\n...     \"num\\_virtual\\_tokens\": 20,\n...     \"token\\_dim\": 768,\n...     \"num\\_transformer\\_submodules\": 1,\n...     \"num\\_attention\\_heads\": 12,\n...     \"num\\_layers\": 12,\n...     \"encoder\\_hidden\\_size\": 768,\n...     \"prefix\\_projection\": False,\n...     \"postprocess\\_past\\_key\\_value\\_function\": None,\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n>>> peft_model = PeftModelForSequenceClassification(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 370178 || all params: 108680450 || trainable%: 0.3406113979101117\n```\n\n## \nPeftModelForTokenClassification\n\n\nA `PeftModel` for token classification tasks.\n\n\n### class peft.PeftModelForTokenClassification\n\n<\nsource\n>\n(\nmodel\npeft\\_config: PeftConfig = None\nadapter\\_name = 'default'\n\nPeft model for token classification tasks.\n\n```\n>>> from transformers import AutoModelForSequenceClassification\n>>> from peft import PeftModelForTokenClassification, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"PREFIX\\_TUNING\",\n...     \"task\\_type\": \"TOKEN\\_CLS\",\n...     \"inference\\_mode\": False,\n...     \"num\\_virtual\\_tokens\": 20,\n...     \"token\\_dim\": 768,\n...     \"num\\_transformer\\_submodules\": 1,\n...     \"num\\_attention\\_heads\": 12,\n...     \"num\\_layers\": 12,\n...     \"encoder\\_hidden\\_size\": 768,\n...     \"prefix\\_projection\": False,\n...     \"postprocess\\_past\\_key\\_value\\_function\": None,\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\")\n>>> peft_model = PeftModelForTokenClassification(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 370178 || all params: 108680450 || trainable%: 0.3406113979101117\n```\n\n\n## \nPeftModelForCausalLM\n\n\nA `PeftModel` for causal language modeling.\n\n\n### class peft.PeftModelForCausalLM\n\nPeft model for causal language modeling.\n\n```\n>>> from transformers import AutoModelForCausalLM\n>>> from peft import PeftModelForCausalLM, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"PREFIX\\_TUNING\",\n...     \"task\\_type\": \"CAUSAL\\_LM\",\n...     \"inference\\_mode\": False,\n...     \"num\\_virtual\\_tokens\": 20,\n...     \"token\\_dim\": 1280,\n...     \"num\\_transformer\\_submodules\": 1,\n...     \"num\\_attention\\_heads\": 20,\n...     \"num\\_layers\": 36,\n...     \"encoder\\_hidden\\_size\": 1280,\n...     \"prefix\\_projection\": False,\n...     \"postprocess\\_past\\_key\\_value\\_function\": None,\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n>>> peft_model = PeftModelForCausalLM(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 1843200 || all params: 775873280 || trainable%: 0.23756456724479544\n```\n\n\n## \nPeftModelForSeq2SeqLM\n\n\nA `PeftModel` for sequence-to-sequence language modeling.\n\n\n### class peft.PeftModelForSeq2SeqLM\n\nPeft model for sequence-to-sequence language modeling.\n\n```\n>>> from transformers import AutoModelForSeq2SeqLM\n>>> from peft import PeftModelForSeq2SeqLM, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"LORA\",\n...     \"task\\_type\": \"SEQ\\_2\\_SEQ\\_LM\",\n...     \"inference\\_mode\": False,\n...     \"r\": 8,\n...     \"target\\_modules\": [\"q\", \"v\"],\n...     \"lora\\_alpha\": 32,\n...     \"lora\\_dropout\": 0.1,\n...     \"fan\\_in\\_fan\\_out\": False,\n...     \"enable\\_lora\": None,\n...     \"bias\": \"none\",\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n>>> peft_model = PeftModelForSeq2SeqLM(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 884736 || all params: 223843584 || trainable%: 0.3952474242013566\n```\n\n\n## \nPeftModelForQuestionAnswering\n\n\nA `PeftModel` for question answering.\n\n\n### class peft.PeftModelForQuestionAnswering\n\nPeft model for extractive question answering.\n\n```\n>>> from transformers import AutoModelForQuestionAnswering\n>>> from peft import PeftModelForQuestionAnswering, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"LORA\",\n...     \"task\\_type\": \"QUESTION\\_ANS\",\n...     \"inference\\_mode\": False,\n...     \"r\": 16,\n...     \"target\\_modules\": [\"query\", \"value\"],\n...     \"lora\\_alpha\": 32,\n...     \"lora\\_dropout\": 0.05,\n...     \"fan\\_in\\_fan\\_out\": False,\n...     \"bias\": \"none\",\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-cased\")\n>>> peft_model = PeftModelForQuestionAnswering(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 592900 || all params: 108312580 || trainable%: 0.5473971721475013\n```\n\n\n## \nPeftModelForFeatureExtraction\n\n\nA `PeftModel` for getting extracting features/embeddings from transformer models.\n\n\n### class peft.PeftModelForFeatureExtraction\n\nPeft model for extracting features/embeddings from transformer models\n\n\n* **config** (PretrainedConfig) — The configuration object of the base model.\n\n```\n>>> from transformers import AutoModel\n>>> from peft import PeftModelForFeatureExtraction, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"LORA\",\n...     \"task\\_type\": \"FEATURE\\_EXTRACTION\",\n...     \"inference\\_mode\": False,\n...     \"r\": 16,\n...     \"target\\_modules\": [\"query\", \"value\"],\n...     \"lora\\_alpha\": 32,\n...     \"lora\\_dropout\": 0.05,\n...     \"fan\\_in\\_fan\\_out\": False,\n...     \"bias\": \"none\",\n... }\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModel.from_pretrained(\"bert-base-cased\")\n>>> peft_model = PeftModelForFeatureExtraction(model, peft_config)\n>>> peft_model.print_trainable_parameters()\n```\n\n\n# \nConfiguration\n\n\nThe configuration classes stores the configuration of a PeftModel, PEFT adapter models, and the configurations of `PrefixTuning`, `PromptTuning`, and PromptEncoder. They contain methods for saving and loading model configurations from the Hub, specifying the PEFT method to use, type of task to perform, and model configurations like number of layers and number of attention heads.\n\n\n\n## \nPeftConfigMixin\n\n\n### class peft.config.PeftConfigMixin\n\n<\nsource\n>\n(\npeft\\_type: typing.Optional[peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\n\n\n* **peft\\_type** (Union[`~peft.utils.config.PeftType`, `str`]) — The type of Peft method to use.\n\nThis is the base configuration class for PEFT adapter models. It contains all the methods that are common to all\nPEFT adapter models. This class inherits from PushToHubMixin which contains the methods to\npush your model to the Hub. The method `save_pretrained` will save the configuration of your adapter model in a\ndirectory. The method `from_pretrained` will load the configuration of your adapter model from a directory.\n\n#### from\\_json\\_file\n\n<\nsource\n>\n(\npath\\_json\\_file\n\\*\\*kwargs\n\n\n* **path\\_json\\_file** (`str`) —\nThe path to the json file.\n\nLoads a configuration file from a json file.\n\n<\nsource\n>\n(\npretrained\\_model\\_name\\_or\\_path\nsubfolder = None\n\\*\\*kwargs\n\n\n* **pretrained\\_model\\_name\\_or\\_path** (`str`) —\nThe directory or the Hub repository id where the configuration is saved.\n* **kwargs** (additional keyword arguments, *optional*) —\nAdditional keyword"
    },
    {
        "instruction": "Create a Python program that utilizes the 'peft' API to generate SQL queries based on a given description of a database query. Use the 'gpt2' model for SQL query generation. The program should accept high-level descriptions of queries and produce SQL queries that retrieve the requested data from a database.",
        "api": "peft",
        "output": "# Install necessary libraries\n#!pip install transformers\n#!pip install peft\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Load Models\nbase_model = \"gpt2\"\npeft_model = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(base_model)\n\n# Load peft\nmodel = PeftModel.from_pretrained(model, peft_model)\nmodel = model.eval()\n\n# Query description\nquery_description = \"Retrieve all orders placed by customer 'John Smith' in the year 2022.\"\n\n# Generate SQL query\ngenerated_query = model.generate_sql_query(query_description, max_length=200)\n\n# Print generated SQL query\nprint(generated_query)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n### from\\_json\\_file\n\n<\nsource\n>\n(\npath\\_json\\_file\n\\*\\*kwargs\n\n\n* **path\\_json\\_file** (`str`) —\nThe path to the json file.\n\nLoads a configuration file from a json file.\n\n<\nsource\n>\n(\npretrained\\_model\\_name\\_or\\_path\nsubfolder = None\n\\*\\*kwargs\n\n\n* **pretrained\\_model\\_name\\_or\\_path** (`str`) —\nThe directory or the Hub repository id where the configuration is saved.\n* **kwargs** (additional keyword arguments, *optional*) —\nAdditional keyword arguments passed along to the child class initialization.\n\nThis method loads the configuration of your adapter model from a directory.\n\n<\nsource\n>\n(\nsave\\_directory\n\\*\\*kwargs\n\n\n* **save\\_directory** (`str`) —\nThe directory where the configuration will be saved.\n* **kwargs** (additional keyword arguments, *optional*) —\nAdditional keyword arguments passed along to the push\\_to\\_hub\nmethod.\n\nThis method saves the configuration of your adapter model in a directory.\n\n\n## \nPeftConfig\n\n\n### class peft.PeftConfig\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\n\n\n* **peft\\_type** (Union[`~peft.utils.config.PeftType`, `str`]) — The type of Peft method to use.\n* **task\\_type** (Union[`~peft.utils.config.TaskType`, `str`]) — The type of task to perform.\n* **inference\\_mode** (`bool`, defaults to `False`) — Whether to use the Peft model in inference mode.\n\nThis is the base configuration class to store the configuration of a PeftModel.\n\n\n\n## \nPromptLearningConfig\n\n### class peft.PromptLearningConfig\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\nnum\\_virtual\\_tokens: int = None\ntoken\\_dim: int = None\nnum\\_transformer\\_submodules: typing.Optional[int] = None\nnum\\_attention\\_heads: typing.Optional[int] = None\nnum\\_layers: typing.Optional[int] = None\n\n\n* **num\\_virtual\\_tokens** (`int`)\n\n==================\n Document 1 \n----------------\n\n\n \n\n# \nModels\n\n\nPeftModel is the base model class for specifying the base Transformer model and configuration to apply a PEFT method to. The base `PeftModel` contains methods for loading and saving models from the Hub, and supports the PromptEncoder for prompt learning.\n\n\n\n## \nPeftModel\n\n### class peft.PeftModel\n\n<\nsource\n>\n(\nmodel: PreTrainedModel\npeft\\_config: PeftConfig\nadapter\\_name: str = 'default'\n\n)\n\n\nParameters \n\n\n* **model** (PreTrainedModel) — The base transformer model used for Peft.\n* **peft\\_config** (PeftConfig) — The configuration of the Peft model.\n* **adapter\\_name** (`str`) — The name of the adapter, defaults to `\"default\"`.\n\nBase model\n\n==================\n Document 2 \n----------------\n## class peft.PeftModelForSequenceClassification\n\n<\nsource\n>\n(\nmodel\npeft\\_config: PeftConfig\nadapter\\_name = 'default'\n\n\n* **model** (PreTrainedModel) — Base transformer model.\n* **peft\\_config** (PeftConfig) — Peft config.\n\nPeft model for sequence classification tasks.\n\n\n* **config** (PretrainedConfig) — The configuration object of the base model.\n* **cls\\_layer\\_name** (`str`) — The name of the classification layer.\n\nExample:\n\n\n Copied\n\n```\n>>> from transformers import AutoModelForSequenceClassification\n>>> from peft import PeftModelForSequenceClassification, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"PREFIX\\_TUNING\",\n...     \"task\\_type\": \"SEQ\\_CLS\",\n...     \"inference\\_mode\": False,\n...     \"num\\_virtual\\_tokens\": 20,\n...     \"token\\_dim\": 768,\n...     \"num\\_transformer\\_submodules\": 1,\n...     \"num\\_attention\\_heads\": 12,\n...     \"num\\_layers\": 12,\n...     \"encoder\\_hidden\\_size\": 768,\n...     \"prefix\\_projection\": False,\n...     \"postprocess\\_past\\_key\\_value\\_function\": None,\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n>>> peft_model = PeftModelForSequenceClassification(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 370178 || all params: 108680450 || trainable%: 0.3406113979101117\n```\n\n## \nPeftModelForTokenClassification\n\n\nA `PeftModel` for token classification tasks.\n\n\n### class peft.PeftModelForTokenClassification\n\n<\nsource\n>\n(\nmodel\npeft\\_config: PeftConfig = None\nadapter\\_name = 'default'\n\nPeft model for token classification tasks.\n\n```\n>>> from transformers import AutoModelForSequenceClassification\n>>> from peft import PeftModelForTokenClassification, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"PREFIX\\_TUNING\",\n...     \"task\\_type\": \"TOKEN\\_CLS\",\n...     \"inference\\_mode\": False,\n...     \"num\\_virtual\\_tokens\": 20,\n...     \"token\\_dim\": 768,\n...     \"num\\_transformer\\_submodules\": 1,\n...     \"num\\_attention\\_heads\": 12,\n...     \"num\\_layers\": 12,\n...     \"encoder\\_hidden\\_size\": 768,\n...     \"prefix\\_projection\": False,\n...     \"postprocess\\_past\\_key\\_value\\_function\": None,\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\")\n>>> peft_model = PeftModelForTokenClassification(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 370178 || all params: 108680450 || trainable%: 0.3406113979101117\n```\n\n\n## \nPeftModelForCausalLM\n\n\nA `PeftModel` for causal language modeling.\n\n\n### class peft.PeftModelForCausalLM\n\nPeft model for causal language modeling.\n\n```\n>>> from transformers import AutoModelForCausalLM\n>>> from peft import PeftModelForCausalLM, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"PREFIX\\_TUNING\",\n...     \"task\\_type\": \"CAUSAL\\_LM\",\n...     \"inference\\_mode\": False,\n...     \"num\\_virtual\\_tokens\": 20,\n...     \"token\\_dim\": 1280,\n...     \"num\\_transformer\\_submodules\": 1,\n...     \"num\\_attention\\_heads\": 20,\n...     \"num\\_layers\": 36,\n...     \"encoder\\_hidden\\_size\": 1280,\n...     \"prefix\\_projection\": False,\n...     \"postprocess\\_past\\_key\\_value\\_function\": None,\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n>>> peft_model = PeftModelForCausalLM(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 1843200 || all params: 775873280 || trainable%: 0.23756456724479544\n```\n\n\n## \nPeftModelForSeq2SeqLM\n\n\nA `PeftModel` for sequence-to-sequence language modeling.\n\n\n### class peft.PeftModelForSeq2SeqLM\n\nPeft model for sequence-to-sequence language modeling.\n\n```\n>>> from transformers import AutoModelForSeq2SeqLM\n>>> from peft import PeftModelForSeq2SeqLM, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"LORA\",\n...     \"task\\_type\": \"SEQ\\_2\\_SEQ\\_LM\",\n...     \"inference\\_mode\": False,\n...     \"r\": 8,\n...     \"target\\_modules\": [\"q\", \"v\"],\n...     \"lora\\_alpha\": 32,\n...     \"lora\\_dropout\": 0.1,\n...     \"fan\\_in\\_fan\\_out\": False,\n...     \"enable\\_lora\": None,\n...     \"bias\": \"none\",\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n>>> peft_model = PeftModelForSeq2SeqLM(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 884736 || all params: 223843584 || trainable%: 0.3952474242013566\n```\n\n\n## \nPeftModelForQuestionAnswering\n\n\nA `PeftModel` for question answering.\n\n\n### class peft.PeftModelForQuestionAnswering\n\nPeft model for extractive question answering.\n\n```\n>>> from transformers import AutoModelForQuestionAnswering\n>>> from peft import PeftModelForQuestionAnswering, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"LORA\",\n...     \"task\\_type\": \"QUESTION\\_ANS\",\n...     \"inference\\_mode\": False,\n...     \"r\": 16,\n...     \"target\\_modules\": [\"query\", \"value\"],\n...     \"lora\\_alpha\": 32,\n...     \"lora\\_dropout\": 0.05,\n...     \"fan\\_in\\_fan\\_out\": False,\n...     \"bias\": \"none\",\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-cased\")\n>>> peft_model = PeftModelForQuestionAnswering(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 592900 || all params: 108312580 || trainable%: 0.5473971721475013\n```\n\n\n## \nPeftModelForFeatureExtraction\n\n\nA `PeftModel` for getting extracting features/embeddings from transformer models.\n\n\n### class peft.PeftModelForFeatureExtraction\n\nPeft model for extracting features/embeddings from transformer models\n\n\n* **config** (PretrainedConfig) — The configuration object of the base model.\n\n```\n>>> from transformers import AutoModel\n>>> from peft import PeftModelForFeatureExtraction, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"LORA\",\n...     \"task\\_type\": \"FEATURE\\_EXTRACTION\",\n...     \"inference\\_mode\": False,\n...     \"r\": 16,\n...     \"target\\_modules\": [\"query\", \"value\"],\n...     \"lora\\_alpha\": 32,\n...     \"lora\\_dropout\": 0.05,\n...     \"fan\\_in\\_fan\\_out\": False,\n...     \"bias\": \"none\",\n... }\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModel.from_pretrained(\"bert-base-cased\")\n>>> peft_model = PeftModelForFeatureExtraction(model, peft_config)\n>>> peft_model.print_trainable_parameters()\n```\n\n\n# \nConfiguration\n\n\nThe configuration classes stores the configuration of a PeftModel, PEFT adapter models, and the configurations of `PrefixTuning`, `PromptTuning`, and PromptEncoder. They contain methods for saving and loading model configurations from the Hub, specifying the PEFT method to use, type of task to perform, and model configurations like number of layers and number of attention heads.\n\n\n\n## \nPeftConfigMixin\n\n\n### class peft.config.PeftConfigMixin\n\n<\nsource\n>\n(\npeft\\_type: typing.Optional[peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\n\n\n* **peft\\_type** (Union[`~peft.utils.config.PeftType`, `str`]) — The type of Peft method to use.\n\nThis is the base configuration class for PEFT adapter models. It contains all the methods that are common to all\nPEFT adapter models. This class inherits from PushToHubMixin which contains the methods to\npush your model to the Hub. The method `save_pretrained` will save the configuration of your adapter model in a\ndirectory. The method `from_pretrained` will load the configuration of your adapter model from a directory.\n\n#### from\\_json\\_file\n\n<\nsource\n>\n(\npath\\_json\\_file\n\\*\\*kwargs\n\n\n* **path\\_json\\_file** (`str`) —\nThe path to the json file.\n\nLoads a configuration file from a json file.\n\n<\nsource\n>\n(\npretrained\\_model\\_name\\_or\\_path\nsubfolder = None\n\\*\\*kwargs\n\n\n* **pretrained\\_model\\_name\\_or\\_path** (`str`) —\nThe directory or the Hub repository id where the configuration is saved.\n* **kwargs** (additional keyword arguments, *optional*) —\nAdditional keyword\n\n==================\n Document 3 \n----------------\n## class peft.PrefixEncoder\n\n\n* **config** (PrefixTuningConfig) — The configuration of the prefix encoder.\n\nThe `torch.nn` model to encode the prefix.\n\n```\n>>> from peft import PrefixEncoder, PrefixTuningConfig\n\n>>> config = PrefixTuningConfig(\n...     peft_type=\"PREFIX\\_TUNING\",\n...     task_type=\"SEQ\\_2\\_SEQ\\_LM\",\n...     num_virtual_tokens=20,\n...     token_dim=768,\n...     num_transformer_submodules=1,\n...     num_attention_heads=12,\n...     num_layers=12,\n...     encoder_hidden_size=768,\n... )\n>>> prefix_encoder = PrefixEncoder(config)\n```\n\n\n* **embedding** (`torch.nn.Embedding`) — The embedding layer of the prefix encoder.\n* **transform** (`torch.nn.Sequential`) — The two-layer MLP to transform the prefix embeddings if\n`prefix_projection` is `True`.\n* **prefix\\_projection** (`bool`) — Whether to project the prefix embeddings.\n\n\nInput shape: (`batch_size`, `num_virtual_tokens`)\n\n\nOutput shape: (`batch_size`, `num_virtual_tokens`, `2*layers*hidden`)\n\n\n## \nPrompt tuning\n\n### class peft.PromptTuningConfig\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\nnum\\_virtual\\_tokens: int = None\ntoken\\_dim: int = None\nnum\\_transformer\\_submodules: typing.Optional[int] = None\nnum\\_attention\\_heads: typing.Optional[int] = None\nnum\\_layers: typing.Optional[int] = None\nprompt\\_tuning\\_init: typing.Union[peft.tuners.prompt\\_tuning.config.PromptTuningInit, str]\n\n==================\n Document 4 \n----------------\n## class peft.PromptEmbedding\n\n<\nsource\n>\n(\nconfig\nword\\_embeddings\n\n\n* **config** (PromptTuningConfig) — The configuration of the prompt embedding.\n* **word\\_embeddings** (`torch.nn.Module`) — The word embeddings of the base transformer model.\n\nThe model to encode virtual tokens into prompt embeddings.\n\n\n* **embedding** (`torch.nn.Embedding`) — The embedding layer of the prompt embedding.\n\n```\n>>> from peft import PromptEmbedding, PromptTuningConfig\n\n>>> config = PromptTuningConfig(\n...     peft_type=\"PROMPT\\_TUNING\",\n...     task_type=\"SEQ\\_2\\_SEQ\\_LM\",\n...     num_virtual_tokens=20,\n...     token_dim=768,\n...     num_transformer_submodules=1,\n...     num_attention_heads=12,\n...     num_layers=12,\n...     prompt_tuning_init=\"TEXT\",\n...     prompt_tuning_init_text=\"Predict if sentiment of this review is positive, negative or neutral\",\n...     tokenizer_name_or_path=\"t5-base\",\n... )\n\n>>> # t5\\_model.shared is the word embeddings of the base model\n>>> prompt_embedding = PromptEmbedding(config, t5_model.shared)\n```\n\nInput Shape: (`batch_size`, `total_virtual_tokens`)\n\n\nOutput Shape: (`batch_size`, `total_virtual_tokens`, `token_dim`)\n\n\n## \nIA3\n\n### class peft.IA3Config\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\ntarget\\_modules: typing.Union[typing.List[str], str, NoneType] = None\nfeedforward\\_modules: typing.Union[typing.List[str], str, NoneType] = None\nfan\\_in\\_fan\\_out: bool = False\nmodules\\_to\\_save: typing.Optional[typing.List[str]] = None\ninit\\_ia3\\_weights: bool"
    },
    {
        "instruction": "Develop a Python program using the 'peft' API to generate code for a basic note-taking application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a note-taking application that allows users to create, edit, and organize notes.",
        "api": "peft",
        "output": "# Install necessary libraries\n#!pip install transformers\n#!pip install peft\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Load Models\nbase_model = \"gpt-3.5-turbo\"\npeft_model = \"gpt-3.5-turbo\"\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(base_model)\n\n# Load peft\nmodel = PeftModel.from_pretrained(model, peft_model)\nmodel = model.eval()\n\n# Generate code for a note-taking application\nnote_app_code = model.generate_note_app_code(max_length=500)\n\n# Print generated note-taking application code\nprint(note_app_code)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n### from\\_json\\_file\n\n<\nsource\n>\n(\npath\\_json\\_file\n\\*\\*kwargs\n\n\n* **path\\_json\\_file** (`str`) —\nThe path to the json file.\n\nLoads a configuration file from a json file.\n\n<\nsource\n>\n(\npretrained\\_model\\_name\\_or\\_path\nsubfolder = None\n\\*\\*kwargs\n\n\n* **pretrained\\_model\\_name\\_or\\_path** (`str`) —\nThe directory or the Hub repository id where the configuration is saved.\n* **kwargs** (additional keyword arguments, *optional*) —\nAdditional keyword arguments passed along to the child class initialization.\n\nThis method loads the configuration of your adapter model from a directory.\n\n<\nsource\n>\n(\nsave\\_directory\n\\*\\*kwargs\n\n\n* **save\\_directory** (`str`) —\nThe directory where the configuration will be saved.\n* **kwargs** (additional keyword arguments, *optional*) —\nAdditional keyword arguments passed along to the push\\_to\\_hub\nmethod.\n\nThis method saves the configuration of your adapter model in a directory.\n\n\n## \nPeftConfig\n\n\n### class peft.PeftConfig\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\n\n\n* **peft\\_type** (Union[`~peft.utils.config.PeftType`, `str`]) — The type of Peft method to use.\n* **task\\_type** (Union[`~peft.utils.config.TaskType`, `str`]) — The type of task to perform.\n* **inference\\_mode** (`bool`, defaults to `False`) — Whether to use the Peft model in inference mode.\n\nThis is the base configuration class to store the configuration of a PeftModel.\n\n\n\n## \nPromptLearningConfig\n\n### class peft.PromptLearningConfig\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\nnum\\_virtual\\_tokens: int = None\ntoken\\_dim: int = None\nnum\\_transformer\\_submodules: typing.Optional[int] = None\nnum\\_attention\\_heads: typing.Optional[int] = None\nnum\\_layers: typing.Optional[int] = None\n\n\n* **num\\_virtual\\_tokens** (`int`)\n\n==================\n Document 1 \n----------------\n\n\n \n\n# \nModels\n\n\nPeftModel is the base model class for specifying the base Transformer model and configuration to apply a PEFT method to. The base `PeftModel` contains methods for loading and saving models from the Hub, and supports the PromptEncoder for prompt learning.\n\n\n\n## \nPeftModel\n\n### class peft.PeftModel\n\n<\nsource\n>\n(\nmodel: PreTrainedModel\npeft\\_config: PeftConfig\nadapter\\_name: str = 'default'\n\n)\n\n\nParameters \n\n\n* **model** (PreTrainedModel) — The base transformer model used for Peft.\n* **peft\\_config** (PeftConfig) — The configuration of the Peft model.\n* **adapter\\_name** (`str`) — The name of the adapter, defaults to `\"default\"`.\n\nBase model\n\n==================\n Document 2 \n----------------\n## class peft.PeftModel\n\n<\nsource\n>\n(\nmodel: PreTrainedModel\npeft\\_config: PeftConfig\nadapter\\_name: str = 'default'\n\n)\n\n\nParameters \n\n\n* **model** (PreTrainedModel) — The base transformer model used for Peft.\n* **peft\\_config** (PeftConfig) — The configuration of the Peft model.\n* **adapter\\_name** (`str`) — The name of the adapter, defaults to `\"default\"`.\n\nBase model encompassing various Peft methods.\n\n\n**Attributes**:\n\n\n* **base\\_model** (PreTrainedModel) — The base transformer model used for Peft.\n* **peft\\_config** (PeftConfig) — The configuration of the Peft model.\n* **modules\\_to\\_save** (`list` of `str`) — The list of sub-module names to save when\nsaving the model.\n* **prompt\\_encoder** (PromptEncoder) — The prompt encoder used for Peft if\nusing PromptLearningConfig.\n* **prompt\\_tokens** (`torch.Tensor`) — The virtual prompt tokens used for Peft if\nusing PromptLearningConfig.\n* **transformer\\_backbone\\_name** (`str`) — The name of the transformer\nbackbone in the base model if using PromptLearningConfig.\n* **word\\_embeddings** (`torch.nn.Embedding`) — The word embeddings of the transformer backbone\nin the base model if using PromptLearningConfig.\n\n#### create\\_or\\_update\\_model\\_card\n\n<\nsource\n>\n(\noutput\\_dir: str\n\nUpdates or create model card to include information about peft:\n\n\n1. Adds `peft` library tag\n2. Adds peft version\n3. Adds quantization information if it was used\n\n\n\n#### disable\\_adapter\n\n<\nsource\n>\n(\n)\n\nDisables the adapter module.\n\n\n#### forward\n\n<\nsource\n>\n(\n\\*args: Any\n\\*\\*kwargs: Any\n\nForward pass of the model.\n\n#### from\\_pretrained\n\n<\nsource\n>\n(\nmodel: PreTrainedModel\nmodel\\_id: Union[str, os.PathLike]\nadapter\\_name: str = 'default'\nis\\_trainable: bool = False\nconfig: Optional[PeftConfig] = None\n\\*\\*kwargs: Any\n\n\n* **model** (PreTrainedModel) —\nThe model to be adapted. The model should be initialized with the\nfrom\\_pretrained method from the 🤗 Transformers library.\n* **model\\_id** (`str` or `os.PathLike`) —\nThe\n\n==================\n Document 3 \n----------------\n## class peft.PromptLearningConfig\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\nnum\\_virtual\\_tokens: int = None\ntoken\\_dim: int = None\nnum\\_transformer\\_submodules: typing.Optional[int] = None\nnum\\_attention\\_heads: typing.Optional[int] = None\nnum\\_layers: typing.Optional[int] = None\n\n\n* **num\\_virtual\\_tokens** (`int`) — The number of virtual tokens to use.\n* **token\\_dim** (`int`) — The hidden embedding dimension of the base transformer model.\n* **num\\_transformer\\_submodules** (`int`) — The number of transformer submodules in the base transformer model.\n* **num\\_attention\\_heads** (`int`) — The number of attention heads in the base transformer model.\n* **num\\_layers** (`int`) — The number of layers in the base transformer model.\n\nThis is the base configuration class to store the configuration of `PrefixTuning`, PromptEncoder, or\n`PromptTuning`.\n\n\n \n\n# \nTuners\n\n\nEach tuner (or PEFT method) has a configuration and model.\n\n\n\n## \nLoRA\n\n\nFor finetuning a model with LoRA.\n\n### class peft.LoraConfig\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\nr: int = 8\ntarget\\_modules: typing.Union[typing.List[str], str, NoneType] = None\nlora\\_alpha: int = 8\nlora\\_dropout: float = 0.0\nfan\\_in\\_fan\\_out: bool = False\nbias:\n\n==================\n Document 4 \n----------------\n## class peft.PrefixEncoder\n\n\n* **config** (PrefixTuningConfig) — The configuration of the prefix encoder.\n\nThe `torch.nn` model to encode the prefix.\n\n```\n>>> from peft import PrefixEncoder, PrefixTuningConfig\n\n>>> config = PrefixTuningConfig(\n...     peft_type=\"PREFIX\\_TUNING\",\n...     task_type=\"SEQ\\_2\\_SEQ\\_LM\",\n...     num_virtual_tokens=20,\n...     token_dim=768,\n...     num_transformer_submodules=1,\n...     num_attention_heads=12,\n...     num_layers=12,\n...     encoder_hidden_size=768,\n... )\n>>> prefix_encoder = PrefixEncoder(config)\n```\n\n\n* **embedding** (`torch.nn.Embedding`) — The embedding layer of the prefix encoder.\n* **transform** (`torch.nn.Sequential`) — The two-layer MLP to transform the prefix embeddings if\n`prefix_projection` is `True`.\n* **prefix\\_projection** (`bool`) — Whether to project the prefix embeddings.\n\n\nInput shape: (`batch_size`, `num_virtual_tokens`)\n\n\nOutput shape: (`batch_size`, `num_virtual_tokens`, `2*layers*hidden`)\n\n\n## \nPrompt tuning\n\n### class peft.PromptTuningConfig\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\nnum\\_virtual\\_tokens: int = None\ntoken\\_dim: int = None\nnum\\_transformer\\_submodules: typing.Optional[int] = None\nnum\\_attention\\_heads: typing.Optional[int] = None\nnum\\_layers: typing.Optional[int] = None\nprompt\\_tuning\\_init: typing.Union[peft.tuners.prompt\\_tuning.config.PromptTuningInit, str]\n\n==================\n Document 5 \n----------------\n## class peft.PeftModelForSequenceClassification\n\n<\nsource\n>\n(\nmodel\npeft\\_config: PeftConfig\nadapter\\_name = 'default'\n\n\n* **model** (PreTrainedModel) — Base transformer model.\n* **peft\\_config** (PeftConfig) — Peft config.\n\nPeft model for sequence classification tasks.\n\n\n* **config** (PretrainedConfig) — The configuration object of the base model.\n* **cls\\_layer\\_name** (`str`) — The name of the classification layer.\n\nExample:\n\n\n Copied\n\n```\n>>> from transformers import AutoModelForSequenceClassification\n>>> from peft import PeftModelForSequenceClassification, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"PREFIX\\_TUNING\",\n...     \"task\\_type\": \"SEQ\\_CLS\",\n...     \"inference\\_mode\": False,\n...     \"num\\_virtual\\_tokens\": 20,\n...     \"token\\_dim\": 768,\n...     \"num\\_transformer\\_submodules\": 1,\n...     \"num\\_attention\\_heads\": 12,\n...     \"num\\_layers\": 12,\n...     \"encoder\\_hidden\\_size\": 768,\n...     \"prefix\\_projection\": False,\n...     \"postprocess\\_past\\_key\\_value\\_function\": None,\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n>>> peft_model = PeftModelForSequenceClassification(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 370178 || all params: 108680450 || trainable%: 0.3406113979101117\n```\n\n## \nPeftModelForTokenClassification\n\n\nA `PeftModel` for token classification tasks.\n\n\n### class peft.PeftModelForTokenClassification\n\n<\nsource\n>\n(\nmodel\npeft\\_config: PeftConfig = None\nadapter\\_name = 'default'\n\nPeft model for token classification tasks.\n\n```\n>>> from transformers import AutoModelForSequenceClassification\n>>> from peft import PeftModelForTokenClassification, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"PREFIX\\_TUNING\",\n...     \"task\\_type\": \"TOKEN\\_CLS\",\n...     \"inference\\_mode\": False,\n...     \"num\\_virtual\\_tokens\": 20,\n...     \"token\\_dim\": 768,\n...     \"num\\_transformer\\_submodules\": 1,\n...     \"num\\_attention\\_heads\": 12,\n...     \"num\\_layers\": 12,\n...     \"encoder\\_hidden\\_size\": 768,\n...     \"prefix\\_projection\": False,\n...     \"postprocess\\_past\\_key\\_value\\_function\": None,\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\")\n>>> peft_model = PeftModelForTokenClassification(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 370178 || all params: 108680450 || trainable%: 0.3406113979101117\n```\n\n\n## \nPeftModelForCausalLM\n\n\nA `PeftModel` for causal language modeling.\n\n\n### class peft.PeftModelForCausalLM\n\nPeft model for causal language modeling.\n\n```\n>>> from transformers import AutoModelForCausalLM\n>>> from peft import PeftModelForCausalLM, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"PREFIX\\_TUNING\",\n...     \"task\\_type\": \"CAUSAL\\_LM\",\n...     \"inference\\_mode\": False,\n...     \"num\\_virtual\\_tokens\": 20,\n...     \"token\\_dim\": 1280,\n...     \"num\\_transformer\\_submodules\": 1,\n...     \"num\\_attention\\_heads\": 20,\n...     \"num\\_layers\": 36,\n...     \"encoder\\_hidden\\_size\": 1280,\n...     \"prefix\\_projection\": False,\n...     \"postprocess\\_past\\_key\\_value\\_function\": None,\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n>>> peft_model = PeftModelForCausalLM(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 1843200 || all params: 775873280 || trainable%: 0.23756456724479544\n```\n\n\n## \nPeftModelForSeq2SeqLM\n\n\nA `PeftModel` for sequence-to-sequence language modeling.\n\n\n### class peft.PeftModelForSeq2SeqLM\n\nPeft model for sequence-to-sequence language modeling.\n\n```\n>>> from transformers import AutoModelForSeq2SeqLM\n>>> from peft import PeftModelForSeq2SeqLM, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"LORA\",\n...     \"task\\_type\": \"SEQ\\_2\\_SEQ\\_LM\",\n...     \"inference\\_mode\": False,\n...     \"r\": 8,\n...     \"target\\_modules\": [\"q\", \"v\"],\n...     \"lora\\_alpha\": 32,\n...     \"lora\\_dropout\": 0.1,\n...     \"fan\\_in\\_fan\\_out\": False,\n...     \"enable\\_lora\": None,\n...     \"bias\": \"none\",\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n>>> peft_model = PeftModelForSeq2SeqLM(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 884736 || all params: 223843584 || trainable%: 0.3952474242013566\n```\n\n\n## \nPeftModelForQuestionAnswering\n\n\nA `PeftModel` for question answering.\n\n\n### class peft.PeftModelForQuestionAnswering\n\nPeft model for extractive question answering.\n\n```\n>>> from transformers import AutoModelForQuestionAnswering\n>>> from peft import PeftModelForQuestionAnswering, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"LORA\",\n...     \"task\\_type\": \"QUESTION\\_ANS\",\n...     \"inference\\_mode\": False,\n...     \"r\": 16,\n...     \"target\\_modules\": [\"query\", \"value\"],\n...     \"lora\\_alpha\": 32,\n...     \"lora\\_dropout\": 0.05,\n...     \"fan\\_in\\_fan\\_out\": False,\n...     \"bias\": \"none\",\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-cased\")\n>>> peft_model = PeftModelForQuestionAnswering(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 592900 || all params: 108312580 || trainable%: 0.5473971721475013\n```\n\n\n## \nPeftModelForFeatureExtraction\n\n\nA `PeftModel` for getting extracting features/embeddings from transformer models.\n\n\n### class peft.PeftModelForFeatureExtraction\n\nPeft model for extracting features/embeddings from transformer models\n\n\n* **config** (PretrainedConfig) — The configuration object of the base model.\n\n```\n>>> from transformers import AutoModel\n>>> from peft import PeftModelForFeatureExtraction, get_peft_config\n\n>>> config = {\n...     \"peft\\_type\": \"LORA\",\n...     \"task\\_type\": \"FEATURE\\_EXTRACTION\",\n...     \"inference\\_mode\": False,\n...     \"r\": 16,\n...     \"target\\_modules\": [\"query\", \"value\"],\n...     \"lora\\_alpha\": 32,\n...     \"lora\\_dropout\": 0.05,\n...     \"fan\\_in\\_fan\\_out\": False,\n...     \"bias\": \"none\",\n... }\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModel.from_pretrained(\"bert-base-cased\")\n>>> peft_model = PeftModelForFeatureExtraction(model, peft_config)\n>>> peft_model.print_trainable_parameters()\n```\n\n\n# \nConfiguration\n\n\nThe configuration classes stores the configuration of a PeftModel, PEFT adapter models, and the configurations of `PrefixTuning`, `PromptTuning`, and PromptEncoder. They contain methods for saving and loading model configurations from the Hub, specifying the PEFT method to use, type of task to perform, and model configurations like number of layers and number of attention heads.\n\n\n\n## \nPeftConfigMixin\n\n\n### class peft.config.PeftConfigMixin\n\n<\nsource\n>\n(\npeft\\_type: typing.Optional[peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\n\n\n* **peft\\_type** (Union[`~peft.utils.config.PeftType`, `str`]) — The type of Peft method to use.\n\nThis is the base configuration class for PEFT adapter models. It contains all the methods that are common to all\nPEFT adapter models. This class inherits from PushToHubMixin which contains the methods to\npush your model to the Hub. The method `save_pretrained` will save the configuration of your adapter model in a\ndirectory. The method `from_pretrained` will load the configuration of your adapter model from a directory.\n\n#### from\\_json\\_file\n\n<\nsource\n>\n(\npath\\_json\\_file\n\\*\\*kwargs\n\n\n* **path\\_json\\_file** (`str`) —\nThe path to the json file.\n\nLoads a configuration file from a json file.\n\n<\nsource\n>\n(\npretrained\\_model\\_name\\_or\\_path\nsubfolder = None\n\\*\\*kwargs\n\n\n* **pretrained\\_model\\_name\\_or\\_path** (`str`) —\nThe directory or the Hub repository id where the configuration is saved.\n* **kwargs** (additional keyword arguments, *optional*) —\nAdditional keyword"
    },
    {
        "instruction": "Create a Python program using the 'peft' API to generate text in the style of William Shakespeare. Utilize the 'gpt2' model for text generation. The program should take a starting phrase and generate a coherent and Shakespearean-style text.",
        "api": "peft",
        "output": "# Install necessary libraries\n#!pip install transformers\n#!pip install peft\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Load Models\nbase_model = \"gpt2\"\npeft_model = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(base_model)\n\n# Load peft\nmodel = PeftModel.from_pretrained(model, peft_model)\nmodel = model.eval()\n\n# Starting phrase\nstarting_phrase = \"To be or not to be, that is the\"\n\n# Generate text\ngenerated_text = model.generate_text(starting_phrase, max_length=200)\n\n# Print generated text\nprint(generated_text)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n \n\n# \nModels\n\n\nPeftModel is the base model class for specifying the base Transformer model and configuration to apply a PEFT method to. The base `PeftModel` contains methods for loading and saving models from the Hub, and supports the PromptEncoder for prompt learning.\n\n\n\n## \nPeftModel\n\n### class peft.PeftModel\n\n<\nsource\n>\n(\nmodel: PreTrainedModel\npeft\\_config: PeftConfig\nadapter\\_name: str = 'default'\n\n)\n\n\nParameters \n\n\n* **model** (PreTrainedModel) — The base transformer model used for Peft.\n* **peft\\_config** (PeftConfig) — The configuration of the Peft model.\n* **adapter\\_name** (`str`) — The name of the adapter, defaults to `\"default\"`.\n\nBase model\n\n==================\n Document 1 \n----------------\n### from\\_json\\_file\n\n<\nsource\n>\n(\npath\\_json\\_file\n\\*\\*kwargs\n\n\n* **path\\_json\\_file** (`str`) —\nThe path to the json file.\n\nLoads a configuration file from a json file.\n\n<\nsource\n>\n(\npretrained\\_model\\_name\\_or\\_path\nsubfolder = None\n\\*\\*kwargs\n\n\n* **pretrained\\_model\\_name\\_or\\_path** (`str`) —\nThe directory or the Hub repository id where the configuration is saved.\n* **kwargs** (additional keyword arguments, *optional*) —\nAdditional keyword arguments passed along to the child class initialization.\n\nThis method loads the configuration of your adapter model from a directory.\n\n<\nsource\n>\n(\nsave\\_directory\n\\*\\*kwargs\n\n\n* **save\\_directory** (`str`) —\nThe directory where the configuration will be saved.\n* **kwargs** (additional keyword arguments, *optional*) —\nAdditional keyword arguments passed along to the push\\_to\\_hub\nmethod.\n\nThis method saves the configuration of your adapter model in a directory.\n\n\n## \nPeftConfig\n\n\n### class peft.PeftConfig\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\n\n\n* **peft\\_type** (Union[`~peft.utils.config.PeftType`, `str`]) — The type of Peft method to use.\n* **task\\_type** (Union[`~peft.utils.config.TaskType`, `str`]) — The type of task to perform.\n* **inference\\_mode** (`bool`, defaults to `False`) — Whether to use the Peft model in inference mode.\n\nThis is the base configuration class to store the configuration of a PeftModel.\n\n\n\n## \nPromptLearningConfig\n\n### class peft.PromptLearningConfig\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\nnum\\_virtual\\_tokens: int = None\ntoken\\_dim: int = None\nnum\\_transformer\\_submodules: typing.Optional[int] = None\nnum\\_attention\\_heads: typing.Optional[int] = None\nnum\\_layers: typing.Optional[int] = None\n\n\n* **num\\_virtual\\_tokens** (`int`)\n\n==================\n Document 2 \n----------------\n## class peft.PeftModel\n\n<\nsource\n>\n(\nmodel: PreTrainedModel\npeft\\_config: PeftConfig\nadapter\\_name: str = 'default'\n\n)\n\n\nParameters \n\n\n* **model** (PreTrainedModel) — The base transformer model used for Peft.\n* **peft\\_config** (PeftConfig) — The configuration of the Peft model.\n* **adapter\\_name** (`str`) — The name of the adapter, defaults to `\"default\"`.\n\nBase model encompassing various Peft methods.\n\n\n**Attributes**:\n\n\n* **base\\_model** (PreTrainedModel) — The base transformer model used for Peft.\n* **peft\\_config** (PeftConfig) — The configuration of the Peft model.\n* **modules\\_to\\_save** (`list` of `str`) — The list of sub-module names to save when\nsaving the model.\n* **prompt\\_encoder** (PromptEncoder) — The prompt encoder used for Peft if\nusing PromptLearningConfig.\n* **prompt\\_tokens** (`torch.Tensor`) — The virtual prompt tokens used for Peft if\nusing PromptLearningConfig.\n* **transformer\\_backbone\\_name** (`str`) — The name of the transformer\nbackbone in the base model if using PromptLearningConfig.\n* **word\\_embeddings** (`torch.nn.Embedding`) — The word embeddings of the transformer backbone\nin the base model if using PromptLearningConfig.\n\n#### create\\_or\\_update\\_model\\_card\n\n<\nsource\n>\n(\noutput\\_dir: str\n\nUpdates or create model card to include information about peft:\n\n\n1. Adds `peft` library tag\n2. Adds peft version\n3. Adds quantization information if it was used\n\n\n\n#### disable\\_adapter\n\n<\nsource\n>\n(\n)\n\nDisables the adapter module.\n\n\n#### forward\n\n<\nsource\n>\n(\n\\*args: Any\n\\*\\*kwargs: Any\n\nForward pass of the model.\n\n#### from\\_pretrained\n\n<\nsource\n>\n(\nmodel: PreTrainedModel\nmodel\\_id: Union[str, os.PathLike]\nadapter\\_name: str = 'default'\nis\\_trainable: bool = False\nconfig: Optional[PeftConfig] = None\n\\*\\*kwargs: Any\n\n\n* **model** (PreTrainedModel) —\nThe model to be adapted. The model should be initialized with the\nfrom\\_pretrained method from the 🤗 Transformers library.\n* **model\\_id** (`str` or `os.PathLike`) —\nThe\n\n==================\n Document 3 \n----------------\n## class peft.PromptEmbedding\n\n<\nsource\n>\n(\nconfig\nword\\_embeddings\n\n\n* **config** (PromptTuningConfig) — The configuration of the prompt embedding.\n* **word\\_embeddings** (`torch.nn.Module`) — The word embeddings of the base transformer model.\n\nThe model to encode virtual tokens into prompt embeddings.\n\n\n* **embedding** (`torch.nn.Embedding`) — The embedding layer of the prompt embedding.\n\n```\n>>> from peft import PromptEmbedding, PromptTuningConfig\n\n>>> config = PromptTuningConfig(\n...     peft_type=\"PROMPT\\_TUNING\",\n...     task_type=\"SEQ\\_2\\_SEQ\\_LM\",\n...     num_virtual_tokens=20,\n...     token_dim=768,\n...     num_transformer_submodules=1,\n...     num_attention_heads=12,\n...     num_layers=12,\n...     prompt_tuning_init=\"TEXT\",\n...     prompt_tuning_init_text=\"Predict if sentiment of this review is positive, negative or neutral\",\n...     tokenizer_name_or_path=\"t5-base\",\n... )\n\n>>> # t5\\_model.shared is the word embeddings of the base model\n>>> prompt_embedding = PromptEmbedding(config, t5_model.shared)\n```\n\nInput Shape: (`batch_size`, `total_virtual_tokens`)\n\n\nOutput Shape: (`batch_size`, `total_virtual_tokens`, `token_dim`)\n\n\n## \nIA3\n\n### class peft.IA3Config\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\ntarget\\_modules: typing.Union[typing.List[str], str, NoneType] = None\nfeedforward\\_modules: typing.Union[typing.List[str], str, NoneType] = None\nfan\\_in\\_fan\\_out: bool = False\nmodules\\_to\\_save: typing.Optional[typing.List[str]] = None\ninit\\_ia3\\_weights: bool\n\n==================\n Document 4 \n----------------\n## class peft.PrefixEncoder\n\n\n* **config** (PrefixTuningConfig) — The configuration of the prefix encoder.\n\nThe `torch.nn` model to encode the prefix.\n\n```\n>>> from peft import PrefixEncoder, PrefixTuningConfig\n\n>>> config = PrefixTuningConfig(\n...     peft_type=\"PREFIX\\_TUNING\",\n...     task_type=\"SEQ\\_2\\_SEQ\\_LM\",\n...     num_virtual_tokens=20,\n...     token_dim=768,\n...     num_transformer_submodules=1,\n...     num_attention_heads=12,\n...     num_layers=12,\n...     encoder_hidden_size=768,\n... )\n>>> prefix_encoder = PrefixEncoder(config)\n```\n\n\n* **embedding** (`torch.nn.Embedding`) — The embedding layer of the prefix encoder.\n* **transform** (`torch.nn.Sequential`) — The two-layer MLP to transform the prefix embeddings if\n`prefix_projection` is `True`.\n* **prefix\\_projection** (`bool`) — Whether to project the prefix embeddings.\n\n\nInput shape: (`batch_size`, `num_virtual_tokens`)\n\n\nOutput shape: (`batch_size`, `num_virtual_tokens`, `2*layers*hidden`)\n\n\n## \nPrompt tuning\n\n### class peft.PromptTuningConfig\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\nnum\\_virtual\\_tokens: int = None\ntoken\\_dim: int = None\nnum\\_transformer\\_submodules: typing.Optional[int] = None\nnum\\_attention\\_heads: typing.Optional[int] = None\nnum\\_layers: typing.Optional[int] = None\nprompt\\_tuning\\_init: typing.Union[peft.tuners.prompt\\_tuning.config.PromptTuningInit, str]\n\n==================\n Document 5 \n----------------\n## class peft.PromptTuningConfig\n\n<\nsource\n>\n(\npeft\\_type: typing.Union[str, peft.utils.peft\\_types.PeftType] = None\nauto\\_mapping: typing.Optional[dict] = None\nbase\\_model\\_name\\_or\\_path: str = None\nrevision: str = None\ntask\\_type: typing.Union[str, peft.utils.peft\\_types.TaskType] = None\ninference\\_mode: bool = False\nnum\\_virtual\\_tokens: int = None\ntoken\\_dim: int = None\nnum\\_transformer\\_submodules: typing.Optional[int] = None\nnum\\_attention\\_heads: typing.Optional[int] = None\nnum\\_layers: typing.Optional[int] = None\nprompt\\_tuning\\_init: typing.Union[peft.tuners.prompt\\_tuning.config.PromptTuningInit, str] = <PromptTuningInit.RANDOM: 'RANDOM'>\nprompt\\_tuning\\_init\\_text: typing.Optional[str] = None\ntokenizer\\_name\\_or\\_path: typing.Optional[str] = None\n\n\n* **prompt\\_tuning\\_init** (Union[`PromptTuningInit`, `str`]) — The initialization of the prompt embedding.\n* **prompt\\_tuning\\_init\\_text** (`str`, *optional*) —\nThe text to initialize the prompt embedding. Only used if `prompt_tuning_init` is `TEXT`.\n* **tokenizer\\_name\\_or\\_path** (`str`, *optional*) —\nThe name or path of the tokenizer. Only used if `prompt_tuning_init` is `TEXT`.\n\nThis is the configuration class to store the configuration of a PromptEmbedding.\n### class peft.PromptEmbedding\n\n<\nsource\n>\n(\nconfig\nword\\_embeddings\n\n\n* **config** (PromptTuningConfig) — The configuration of the prompt embedding.\n* **word\\_embeddings** (`torch.nn.Module`) — The word embeddings of the base transformer model.\n\nThe model to encode virtual tokens into prompt embeddings.\n\n\n* **embedding** (`torch.nn.Embedding`) — The embedding layer of the prompt"
    },
    {
        "instruction": "Create a Python program using the 'plotly' API to generate a scatter plot with a color scale. The program should define data points for the x and y axes, as well as the color values for each data point. The program should create the scatter plot with a color scale, customize the chart title, X-axis label, and Y-axis label, and display the color scale on the chart. Finally, save the chart as a PDF file and display it on the screen.",
        "api": "plotly",
        "output": "#!pip install plotly\nimport plotly.graph_objects as go\n\n# Define data points for the x and y axes\nx = [1, 2, 3, 4, 5]\ny = [10, 16, 5, 11, 8]\n\n# Define color values for each data point\ncolor = [1, 2, 3, 4, 5]\n\n# Create a scatter plot with a color scale using Plotly Graph Objects\nfig = go.Figure(data=go.Scatter(x=x, y=y, mode='markers', marker=dict(color=color, colorscale='Viridis', showscale=True)))\nfig.update_layout(title=\"Scatter Plot with Color Scale\", xaxis_title=\"X-axis\", yaxis_title=\"Y-axis\")\n\n# Save the chart as a PDF file\nfig.write_image(\"scatter_plot_with_color_scale.pdf\")\n\n# Display the chart on the screen\nfig.show()\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n Plotly Python Open Source Graphing Library Scientific Charts\n\n\nPlotly's Python graphing library makes interactive, publication-quality graphs online. Examples of how to make scientific charts such as contour plots, heatmaps, dendrograms, polar charts, and ternary plots.\n\n\n* Contour Plots\n\nView Tutorial\n\n* Heatmaps\n\nView Tutorial\n\n* Imshow\n\nView Tutorial\n\n* Ternary Plots\n\nView Tutorial\n\n* Log Plots\n\nView Tutorial\n\n* Dendrograms\n\nView Tutorial\n\n* Annotated Heatmaps\n\nView Tutorial\n\n* Ternary Overlay\n\nView Tutorial\n\n* Parallel Coordinates Plot\n\nView Tutorial\n\n* Quiver Plots\n\nView Tutorial\n\n* Streamline Plots\n\nView Tutorial\n\n* Network Graphs\n\nView Tutorial\n\n* Carpet Plots\n\nView Tutorial\n\n* Carpet Contour Plot\n\nView Tutorial\n\n* Carpet Scatter Plot\n\nView Tutorial\n\n* Polar Charts\n\nView Tutorial\n\n* Radar Charts\n\nView Tutorial\n\n* Ternary contours\n\nView Tutorial\n\n* Wind Rose and Polar Bar Charts\n\nView Tutorial\n\n* Plotly and Datashader\n\nView Tutorial\n\n* Smith Charts\n\nView Tutorial\n\n\n \n \n > Scientific Charts\n \n \n > Contour Plots\n\n\n# \n Contour Plots\n \n in \n Python\n\n\nHow to make Contour plots in Python with Plotly. \n\n\n\n### Basic Contour Plot¶\n\nA 2D contour plot shows the contour lines of a 2D numerical array `z`, i.e. interpolated lines of isovalues of `z`.\n\nfig = go.Figure(data =\n    go.Contour(\n        z=[[10, 10.625, 12.5, 15.625, 20],\n           [5.625, 6.25, 8.125, 11.25, 15.625],\n           [2.5, 3.125, 5., 8.125, 12.5],\n           [0.625, 1.25, 3.125, 6.25, 10.625],\n           [0, 0.625, 2.5, 5.625, 10]]\n    ))\nfig.show()\n\n\n\n### Setting X and Y Coordinates in a Contour Plot¶\n\nfig = go.Figure(data =\n    go.Contour(\n        z=[[10, 10.625, 12.5, 15.625, 20],\n           [5.625, 6.25, 8.125, 11.25, 15.625],\n           [2.5, 3.125, 5., 8.125, 12.5],\n           [0.625, 1.25, 3.125, 6.25, 10.625],\n           [0, 0.625, 2.5, 5.625, 10]],\n        x=[-9, -6, -5 , -3, -1], # horizontal axis\n        y=[0, 1, 4, 5, 7] # vertical axis\n    ))\nfig.show()\n\n\n\n### Colorscale for Contour Plot¶\n\nfig = go.Figure(data =\n     go.Contour(\n        z=[[10, 10.625, 12.5, 15.625, 20],\n           [5.625, 6.25, 8.125, 11.25, 15.625],\n           [2.5, 3.125, 5., 8.125, 12.5],\n           [0.625, 1.25, 3.125, 6.25, 10.625],\n           [0, 0.625, 2.5, 5.625, 10]],\n        colorscale='Electric',\n    ))\nfig.show()\n\n\n\n### Customizing Size and Range of a Contour Plot's Contours¶\n\nfig = go.Figure(data =\n    go.Contour(\n        z=[[10, 10.625, 12.5, 15.625, 20],\n           [5.625, 6.25, 8.125, 11.25, 15.625],\n           [2.5, 3.125, 5., 8.125, 12.5],\n           [0.625, 1.25, 3.125, 6.25, 10.625],\n           [0, 0.625, 2.5, 5.625, 10]],\n        colorscale='Hot',\n        contours=dict(\n            start=0,\n            end=8,\n            size=2,\n        ),\n    ))\n\n\n\n### Customizing Spacing Between X and Y Axis Ticks¶\n\nfig = go.Figure(data =\n    go.Contour(\n        z= [[10, 10.625, 12.5, 15.625, 20],\n              [5.625, 6.25, 8.125, 11.25, 15.625],\n              [2.5, 3.125, 5., 8.125, 12.5],\n              [0.625, 1.25, 3.125, 6.25, 10.625],\n              [0, 0.625, 2.5, 5.625, 10]],\n        dx=10,\n        x0=5,\n        dy=10,\n        y0=10,\n    )\n)\n\n\n### Connect the Gaps Between None Values in the Z Matrix¶\n\n```\nimport plotly.graph\\_objs as go\nfrom plotly.subplots import make\\_subplots\n\nfig = make\\_subplots(rows=2, cols=2, subplot\\_titles=('connectgaps = False',\n\n==================\n Document 1 \n----------------\n Plotly Python Open Source Graphing Library Basic Charts\n\n\nPlotly's Python graphing library makes interactive, publication-quality graphs online. Examples of how to make basic charts.\n\n\n* Scatter Plots\n\nView Tutorial\n\n* Line Charts\n\nView Tutorial\n\n* Bar Charts\n\nView Tutorial\n\n* Pie Charts\n\nView Tutorial\n\n* Bubble Charts\n\nView Tutorial\n\n* Dot Plots\n\nView Tutorial\n\n* Filled Area Plots\n\nView Tutorial\n\n* Horizontal Bar Charts\n\nView Tutorial\n\n* Gantt Charts\n\nView Tutorial\n\n* Sunburst Charts\n\nView Tutorial\n\n* Tables\n\nView Tutorial\n\n* Sankey Diagram\n\nView Tutorial\n\n* Treemap Charts\n\nView Tutorial\n\n* WebGL vs SVG\n\nView Tutorial\n\n* Figure Factory Tables\n\nView Tutorial\n\n* Categorical Axes\n\nView Tutorial\n\n* Icicle Charts\n\nView Tutorial\n\n* Patterns, Hatching, Texture\n\nView Tutorial\n\n* Dumbbell Plots\n\nView Tutorial\n\n\n \n \n > Basic Charts\n \n \n > Scatter Plots\n\n\n# \n Scatter Plots\n \n in \n Python\n\n\nHow to make scatter plots in Python with Plotly. \n\n\n\n## Scatter plots with Plotly Express¶\n\nPlotly Express is the easy-to-use, high-level interface to Plotly, which operates on a variety of types of data and produces easy-to-style figures.\n\n\nWith `px.scatter`, each data point is represented as a marker point, whose location is given by the `x` and `y` columns.\n\n```\n\n# x and y given as array\\_like objects\nimport plotly.express as px\nfig = px.scatter(x=[0, 1, 2, 3, 4], y=[0, 1, 4, 9, 16])\nfig.show()\n\n```\n\n# x and y given as DataFrame columns\nimport plotly.express as px\ndf = px.data.iris() # iris is a pandas DataFrame\nfig = px.scatter(df, x=\"sepal\\_width\", y=\"sepal\\_length\")\nfig.show()\n\n\n#### Setting size and color with column names¶\n\nScatter plots with variable-sized circular markers are often known as bubble charts. Note that `color` and `size` data are added to hover information. You can add other columns to hover data with the\n\n==================\n Document 2 \n----------------\n \n Plotly Express\n \n in \n Python\n\n\nPlotly Express is a terse, consistent, high-level API for creating figures. \n\nThe `plotly.express` module (usually imported as `px`) contains functions that can create entire figures at once, and is referred to as Plotly Express or PX. Plotly Express is a built-in part of the `plotly` library, and is the recommended starting point for creating most common figures. Every Plotly Express function uses graph objects internally and returns a `plotly.graph_objects.Figure` instance. Throughout the `plotly` documentation, you will find the Plotly Express way of building figures at the top of any applicable page, followed by a section on how to use graph objects to build similar figures. Any figure created in a single function call with Plotly Express could be created using graph objects alone, but with between 5 and 100 times more code.\n\n\nPlotly Express provides more than 30 functions for creating different types of figures. The API for these functions was carefully designed to be as consistent and easy to learn as possible, making it easy to switch from a scatter plot to a bar chart to a histogram to a sunburst chart throughout a data exploration session. *Scroll down for a gallery of Plotly Express plots, each made in a single function call.*\n\n\nHere is a talk from the SciPy 2021 conference that gives a good introduction to Plotly Express and Dash:\n\n\nPlotly Express currently includes the following functions:\n\n\n* **Basics**: `scatter`, `line`, `area`, `bar`, `funnel`, `timeline`\n* **Part-of-Whole**: `pie`, `sunburst`, `treemap`, `icicle`, `funnel_area`\n* **1D Distributions**: `histogram`, `box`, `violin`, `strip`, `ecdf`\n* **2D Distributions**: `density_heatmap`, `density_contour`\n* **Matrix or Image Input**: `imshow`\n* **3-Dimensional**: `scatter_3d`, `line_3d`\n* **Multidimensional**: `scatter_matrix`, `parallel_coordinates`, `parallel_categories`\n* **Tile Maps**: `scatter_mapbox`, `line_mapbox`, `choropleth_mapbox`, `density_mapbox`\n* **Outline Maps**: `scatter_geo`, `line_geo`, `choropleth`\n* **Polar Charts**: `scatter_polar`, `line_polar`, `bar_polar`\n* **Ternary Charts**: `scatter_ternary`, `line_ternary`\n\n### High-Level Features¶\n\nThe Plotly Express API in general offers the following features:\n\n\n* **A single entry point into `plotly`**: just `import plotly.express as px` and get access to all the plotting functions, plus built-in demo datasets under `px.data` and built-in color\n\n==================\n Document 3 \n----------------\n## Data Order in Scatter and Line Charts¶\n\nPlotly line charts are implemented as connected scatterplots (see below), meaning that the points are plotted and connected with lines **in the order they are provided, with no automatic reordering**.\n\n\nThis makes it possible to make charts like the one below, but also means that it may be required to explicitly sort data before passing it to Plotly to avoid lines moving \"backwards\" across the chart.\n\ndf = pd.DataFrame(dict(\n    x = [1, 3, 2, 4],\n    y = [1, 2, 3, 4]\n))\nfig = px.line(df, x=\"x\", y=\"y\", title=\"Unsorted Input\") \nfig.show()\n\ndf = df.sort\\_values(by=\"x\")\nfig = px.line(df, x=\"x\", y=\"y\", title=\"Sorted Input\") \nfig.show()\n\n\n### Connected Scatterplots¶\n\nIn a connected scatterplot, two continuous variables are plotted against each other, with a line connecting them in some meaningful order, usually a time variable. In the plot below, we show the \"trajectory\" of a pair of countries through a space defined by GDP per Capita and Life Expectancy. Botswana's life expectancy\n\ndf = px.data.gapminder().query(\"country in ['Canada', 'Botswana']\")\n\nfig = px.line(df, x=\"lifeExp\", y=\"gdpPercap\", color=\"country\", text=\"year\")\nfig.update\\_traces(textposition=\"bottom right\")\nfig.show()\n\n\n\n## Scatter and line plots with go.Scatter¶\n\nIf Plotly Express does not provide a good starting point, it is possible to use the more generic `go.Scatter` class from `plotly.graph_objects`. Whereas `plotly.express` has two functions `scatter` and `line`, `go.Scatter` can be used both for plotting points (makers) or lines, depending on the value of `mode`. The different options of `go.Scatter` are documented in its reference page.\n\n\n\n#### Simple Scatter Plot¶\n\n```\nimport plotly.graph\\_objects as go\nimport numpy as np\n\nN = 1000\nt = np.linspace(0, 10, 100)\ny = np.sin(t)\n\nfig = go.Figure(data=go.Scatter(x=t, y=y, mode='markers'))\n\n\n\n#### Line and Scatter Plots¶\n\nUse `mode` argument to choose between markers, lines, or a combination of both. For more options about line plots, see also the line charts notebook and the filled area plots notebook.\n\n\n# Create random data with numpy\nimport numpy as np\nnp.random.seed(1)\n\nN = 100\nrandom\\_x = np.linspace(0, 1, N)\nrandom\\_y0 = np.random.randn(N) + 5\nrandom\\_y1 = np.random.randn(N)\nrandom\\_y2 = np.random.randn(N) - 5\n\n\n# Add traces\nfig.add\\_trace(go.Scatter(x=random\\_x, y=random\\_y0,\n                    mode='markers',\n                    name='markers'))\nfig.add\\_trace(go.Scatter(x=random\\_x, y=random\\_y1,\n                    mode='lines+markers',\n                    name='lines+markers'))\nfig.add\\_trace(go.Scatter(x=random\\_x, y=random\\_y2,\n                    mode='lines',\n                    name='lines'))\n\n\n\n#### Bubble Scatter Plots¶\n\nIn bubble charts, a third dimension of the data is shown through the size of markers. For more examples, see the bubble chart notebook\n\nfig = go.Figure(data=go.Scatter(\n    x=[1, 2, 3, 4],\n    y=[10, 11, 12, 13],\n    mode='markers',\n    marker=dict(size=[40, 60, 80, 100],\n                color=[0, 1, 2, 3])\n))\n\n\n\n#### Style Scatter Plots¶\n\n\nt = np.linspace(0, 10, 100)\n\nfig.add\\_trace(go.Scatter(\n    x=t, y=np.sin(t),\n    name='sin',\n    mode='markers',\n    marker\\_color='rgba(152, 0, 0, .8)'\n))\n\nfig.add\\_trace(go.Scatter(\n    x=t, y=np.cos(t),\n    name='cos',\n    marker\\_color='rgba(255, 182, 193, .9)'\n))\n\n\n# Set options common to all traces with fig.update\\_traces\nfig.update\\_traces(mode='markers', marker\\_line\\_width=2, marker\\_size=10)\nfig.update\\_layout(title='Styled Scatter',\n                  yaxis\\_zeroline=False, xaxis\\_zeroline=False)\n\n\nfig.show()\n\n\n\n#### Data Labels on Hover¶\n\n```\nimport plotly.graph\\_objects as go\nimport pandas as pd\n\ndata= pd.read\\_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/2014\\_usa\\_states.csv\")\n\nfig = go.Figure(data=go.Scatter(x=data['Postal'],\n                                y=data['Population'],\n                                mode='markers',\n                                marker\\_color=data['Population'],\n                                text=data['State'])) # hover text goes here\n\nfig.update\\_layout(title='Population of USA States')\nfig.show()\n\n\n\n#### Scatter with a Color Dimension¶\n\nfig = go.Figure(data=go.Scatter(\n    y = np.random.randn(500),\n    mode='markers',\n    marker=dict(\n        size=16,\n        color=np.random.randn(500), #set color equal to a variable\n        colorscale='Viridis', # one of plotly colorscales\n        showscale=True\n    )\n))\n\n\n\n#### Large Data Sets¶\n\nNow in Plotly you can implement WebGL with `Scattergl()` in place of `Scatter()`   \n\nfor increased speed, improved interactivity, and the ability to plot even more data!\n\nN = 100000\nfig = go.Figure(data=go.Scattergl(\n    x = np.random.randn(N),\n    y = np.random.randn(N),\n    mode='markers',\n    marker=dict(\n        color=np.random.randn(N),\n        colorscale='Viridis',\n        line\\_width=1\n    )\n))\n\nN = 100000\nr = np.random.uniform(0, 1, N)\ntheta = np.random.uniform(0, 2\\*np.pi, N)\n\nfig = go.Figure(data=go.Scattergl(\n    x = r \\* np.cos(theta), # non-uniform distribution\n    y = r \\* np.sin(theta), # zoom to see more points at the center\n    mode='markers',\n    marker=dict(\n        color=np.random.randn(N),\n        colorscale='Viridis',\n        line\\_width=1\n    )\n))\n\n\n\n### Reference¶\n\nSee function reference for `px.scatter()` or https://plotly.com/python/reference/scatter/ or https://plotly.com/python/reference/scattergl/ for more information and chart attribute options!\n\n \n \n > Basic Charts\n \n \n > Line Charts\n\n\n\n# \n Line Charts\n \n in \n Python\n\n\nHow to make line charts in Python with Plotly. Examples on creating and styling line charts in Python with Plotly. \n\n\n\n### Line Plots with plotly.express¶\n\nPlotly Express is the easy-to-use, high-level interface to Plotly, which operates on a variety of types of data and produces easy-to-style figures. With `px.line`, each data point is represented as a vertex (which location is given by the `x` and `y` columns) of a **polyline mark** in 2D space.\n\n\nFor more examples of line plots, see the line and scatter notebook.\n\ndf = px.data.gapminder().query(\"country=='Canada'\")\nfig = px.line(df, x=\"year\", y=\"lifeExp\", title='Life expectancy in Canada')\nfig.show()\n\n\n\n### Line Plots with column encoding color¶\n\ndf = px.data.gapminder().query(\"continent=='Oceania'\")\nfig = px.line(df, x=\"year\", y=\"lifeExp\", color='country')\nfig.show()\n\n\n\n### Line charts in Dash¶\n\n\n\n### Data Order in Line Charts¶\n\n\n\n### Line charts with markers¶\n\nThe `markers` argument can be set to `True` to show markers on lines.\n\n\n\n### Sparklines with Plotly Express¶\n\nSparklines are scatter plots inside subplots, with gridlines, axis lines, and ticks removed.\n\n```\nimport plotly.express as px\ndf = px.data.stocks(indexed=True)\nfig = px.line(df, facet\\_row=\"company\", facet\\_row\\_spacing=0.01, height=200, width=200)\n\n\n# hide and lock down axes\nfig.update\\_xaxes(visible=False, fixedrange=True)\nfig.update\\_yaxes(visible=False, fixedrange=True)\n\n\n# remove facet/subplot labels\nfig.update\\_layout(annotations=[], overwrite=True)\n\n\n# strip down the rest of the plot\nfig.update\\_layout(\n    showlegend=False,\n    plot\\_bgcolor=\"white\",\n    margin=dict(t=10,l=10,b=10,r=10)\n)\n\n\n# disable the modebar for such a small plot\nfig.show(config=dict(displayModeBar=False))\n\n\n\n### Line Plot with go.Scatter¶\n\n\n\n#### Simple Line Plot¶\n\nx = np.arange(10)\n\nfig = go.Figure(data=go.Scatter(x=x, y=x\\*\\*2))\nfig.show()\n\n\n\n#### Line Plot Modes¶\n\n\n# Create traces\nfig = go.Figure()\nfig.add\\_trace(go.Scatter(x=random\\_x, y=random\\_y0,\n                    mode='lines',\n                    name='lines'))\nfig.add\\_trace(go.Scatter(x=random\\_x, y=random\\_y1,\n                    mode='lines+markers',\n                    name='lines+markers'))\nfig.add\\_trace(go.Scatter(x=random\\_x, y=random\\_y2,\n                    mode='markers', name='markers'))\n\n\n\n#### Style Line Plots¶\n\nThis example styles the color and dash of the traces, adds trace names,\nmodifies line width, and adds plot and axes titles.\n\n# Add data\nmonth = ['January', 'February', 'March', 'April', 'May', 'June', 'July',\n         'August', 'September', 'October', 'November', 'December']\nhigh\\_2000 = [32.5, 37.6, 49.9, 53.0, 69.1, 75.4, 76.5, 76.6, 70.7, 60.6, 45.1, 29.3]\nlow\\_2000 = [13.8, 22.3,\n\n==================\n Document 4 \n----------------\n### Setting size and color with column names¶\n\nScatter plots with variable-sized circular markers are often known as bubble charts. Note that `color` and `size` data are added to hover information. You can add other columns to hover data with the `hover_data` argument of `px.scatter`.\n\n```\nimport plotly.express as px\ndf = px.data.iris()\nfig = px.scatter(df, x=\"sepal\\_width\", y=\"sepal\\_length\", color=\"species\",\n                 size='petal\\_length', hover\\_data=['petal\\_width'])\nfig.show()\n\n\nColor can be continuous as follows, or discrete/categorical as above.\n\n```\nimport plotly.express as px\ndf = px.data.iris()\nfig = px.scatter(df, x=\"sepal\\_width\", y=\"sepal\\_length\", color='petal\\_length')\nfig.show()\n\n\nThe `symbol` argument can be mapped to a column as well. A wide variety of symbols are available.\n\n```\nimport plotly.express as px\ndf = px.data.iris()\nfig = px.scatter(df, x=\"sepal\\_width\", y=\"sepal\\_length\", color=\"species\", symbol=\"species\")\nfig.show()\n\n\n## Scatter plots in Dash¶\n\nOut[6]:\n\n\n\n### Scatter plots and Categorical Axes¶\n\nScatter plots can be made using any type of cartesian axis, including linear, logarithmic, categorical or date axes.\n\n\nScatter plots where one axis is categorical are often known as dot plots.\n\nfig = px.scatter(df, y=\"nation\", x=\"count\", color=\"medal\", symbol=\"medal\")\nfig.update\\_traces(marker\\_size=10)\nfig.show()\n\n\n### Grouped Scatter Points¶\n\n*New in 5.12*\n\n\nBy default, scatter points at the same location are overlayed. We can see this in the previous example, with the values for Canada for bronze and silver. Set `scattermode='group'` to plot scatter points next to\n\n==================\n Document 5 \n----------------\n \n ML Regression\n \n in \n Python\n\n\nVisualize regression in scikit-learn with Plotly. \n\n\nThis page shows how to use Plotly charts for displaying various types of regression models, starting from simple models like Linear Regression, and progressively move towards models like Decision Tree and Polynomial Features. We highlight various capabilities of plotly, such as comparative analysis of the same model with different parameters, displaying Latex, surface plots for 3D data, and enhanced prediction error analysis with Plotly Express.\n\n\nWe will use Scikit-learn to split and preprocess our data and train various regression models. Scikit-learn is a popular Machine Learning (ML) library that offers various tools for creating and training ML algorithms, feature engineering, data cleaning, and evaluating and testing models. It was designed to be accessible, and to work seamlessly with popular libraries like NumPy and Pandas.\n\n\n## Basic linear regression plots¶\n\nIn this section, we show you how to apply a simple regression model for predicting tips a server will receive based on various client attributes (such as sex, time of the week, and whether they are a smoker).\n\n\nWe will be using the Linear Regression, which is a simple model that fit an intercept (the mean tip received by a server), and add a slope for each feature we use, such as the value of the total bill. We show you how to do that with both Plotly Express and Scikit-learn.\n\n\n\n### Ordinary Least Square (OLS) with `plotly.express`¶\n\nThis example shows how to use `plotly.express`'s `trendline` parameter to train a simply Ordinary Least Square (OLS) for predicting the tips waiters will receive based on the value of the total bill.\n\ndf = px.data.tips()\nfig = px.scatter(\n    df, x='total\\_bill', y='tip', opacity=0.65,\n    trendline='ols', trendline\\_color\\_override='darkblue'\n)\nfig.show()\n\n\n\n### Linear Regression with scikit-learn¶\n\nYou can also perform the same prediction using scikit-learn's `LinearRegression`.\n\n```\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph\\_objects as go\nfrom sklearn.linear\\_model import LinearRegression\n\ndf = px.data.tips()\nX = df.total\\_bill.values.reshape(-1, 1)\n\nmodel = LinearRegression()\nmodel.fit(X, df.tip)\n\nx\\_range = np.linspace(X.min(), X.max(), 100)\ny\\_range = model.predict(x\\_range.reshape(-1, 1))\n\nfig = px.scatter(df, x='total\\_bill', y='tip', opacity=0.65)\nfig.add\\_traces(go.Scatter(x=x\\_range, y=y\\_range, name='Regression Fit'))\nfig.show()\n\n\n\n### ML Regression in Dash¶\n\n\n## Model generalization on unseen data¶\n\nWith `go.Scatter`, you can easily color your plot based on a predefined data split. By coloring the training and the testing data points with different colors, you can easily see if whether the model generalizes\n\n==================\n Document 6 \n----------------\n### Scatter, Line, Area and Bar Charts¶\n\n**Read more about scatter plots and discrete color.**\n\n```\nimport plotly.express as px\ndf = px.data.iris()\nfig = px.scatter(df, x=\"sepal\\_width\", y=\"sepal\\_length\", color=\"species\")\nfig.show()\n\n\n**Read more about trendlines and templates and marginal distribution plots.**\n\n```\nimport plotly.express as px\ndf = px.data.iris()\nfig = px.scatter(df, x=\"sepal\\_width\", y=\"sepal\\_length\", color=\"species\", marginal\\_y=\"violin\",\n           marginal\\_x=\"box\", trendline=\"ols\", template=\"simple\\_white\")\nfig.show()\n\n\n**Read more about error bars.**\n\n```\nimport plotly.express as px\ndf = px.data.iris()\ndf[\"e\"] = df[\"sepal\\_width\"]/100\nfig = px.scatter(df, x=\"sepal\\_width\", y=\"sepal\\_length\", color=\"species\", error\\_x=\"e\", error\\_y=\"e\")\nfig.show()\n\n\n**Read more about bar charts.**\n\n```\nimport plotly.express as px\ndf = px.data.tips()\nfig = px.bar(df, x=\"sex\", y=\"total\\_bill\", color=\"smoker\", barmode=\"group\")\nfig.show()\n\n```\nimport plotly.express as px\ndf = px.data.medals\\_long()\n\nfig = px.bar(df, x=\"medal\", y=\"count\", color=\"nation\",\n             pattern\\_shape=\"nation\", pattern\\_shape\\_sequence=[\".\", \"x\", \"+\"])\nfig.show()\n\n\n**Read more about facet plots.**\n\n```\nimport plotly.express as px\ndf = px.data.tips()\nfig = px.bar(df, x=\"sex\", y=\"total\\_bill\", color=\"smoker\", barmode=\"group\", facet\\_row=\"time\", facet\\_col=\"day\",\n       category\\_orders={\"day\": [\"Thur\", \"Fri\", \"Sat\", \"Sun\"], \"time\": [\"Lunch\", \"Dinner\"]})\nfig.show()\n\n\n**Read more about scatterplot matrices (SPLOMs).**\n\n```\nimport plotly.express as px\ndf = px.data.iris()\nfig = px.scatter\\_matrix(df, dimensions=[\"sepal\\_width\", \"sepal\\_length\", \"petal\\_width\", \"petal\\_length\"], color=\"species\")\nfig.show()\n\n\n**Read more about parallel coordinates and parallel categories, as well as continuous color.**\n\n```\nimport plotly.express as px\ndf = px.data.iris()\nfig = px.parallel\\_coordinates(df, color=\"species\\_id\", labels={\"species\\_id\": \"Species\",\n                  \"sepal\\_width\": \"Sepal Width\", \"sepal\\_length\": \"Sepal Length\",\n                  \"petal\\_width\": \"Petal Width\", \"petal\\_length\": \"Petal Length\", },\n                    color\\_continuous\\_scale=px.colors.diverging.Tealrose, color\\_continuous\\_midpoint=2)\nfig.show()\n\n```\nimport plotly.express as px\ndf = px.data.tips()\nfig = px.parallel\\_categories(df, color=\"size\", color\\_continuous\\_scale=px.colors.sequential.Inferno)\nfig.show()\n\n\n**Read more about hover labels.**\n\n```\nimport plotly.express as px\ndf = px.data.gapminder()\nfig = px.scatter(df.query(\"year==2007\"), x=\"gdpPercap\", y=\"lifeExp\", size=\"pop\", color=\"continent\",\n           hover\\_name=\"country\", log\\_x=True, size\\_max=60)\nfig.show()\n\n\n**Read more about animations.**\n\n```\nimport plotly.express as px\ndf = px.data.gapminder()\nfig = px.scatter(df, x=\"gdpPercap\", y=\"lifeExp\", animation\\_frame=\"year\", animation\\_group=\"country\",\n           size=\"pop\", color=\"continent\", hover\\_name=\"country\", facet\\_col=\"continent\",\n           log\\_x=True, size\\_max=45, range\\_x=[100,100000], range\\_y=[25,90])\nfig.show()\n\n\n**Read more about line charts.**\n\n```\nimport plotly.express as px\ndf = px.data.gapminder()\nfig = px.line(df, x=\"year\", y=\"lifeExp\", color=\"continent\", line\\_group=\"country\", hover\\_name=\"country\",\n        line\\_shape=\"spline\", render\\_mode=\"svg\")\nfig.show()\n\n\n**Read more about area charts.**\n\n```\nimport plotly.express as px\ndf = px.data.gapminder()\nfig = px.area(df, x=\"year\", y=\"pop\", color=\"continent\", line\\_group=\"country\")\nfig.show()\n\n\n**Read more about timeline/Gantt charts.**\n\n```\nimport plotly.express as px\nimport pandas as pd\n\ndf = pd.DataFrame([\n    dict(Task=\"Job A\", Start='2009-01-01', Finish='2009-02-28', Resource=\"Alex\"),\n    dict(Task=\"Job B\", Start='2009-03-05', Finish='2009-04-15', Resource=\"Alex\"),\n    dict(Task=\"Job C\", Start='2009-02-20', Finish='2009-05-30', Resource=\"Max\")\n])\n\nfig = px.timeline(df, x\\_start=\"Start\", x\\_end=\"Finish\", y=\"Resource\", color=\"Resource\")\nfig.show()\n\n\n**Read more about funnel charts.**\n\n```\nimport plotly.express as px\ndata = dict(\n    number=[39, 27.4, 20.6, 11, 2],\n    stage=[\"Website visit\", \"Downloads\", \"Potential customers\", \"Requested price\", \"Invoice sent\"])\nfig = px.funnel(data, x='number', y='stage')\nfig.show()\n\n### Part to Whole Charts¶\n\n**Read more about pie charts.**\n\n```\nimport plotly.express as px\ndf = px.data.gapminder().query(\"year == 2007\").query(\"continent == 'Europe'\")\ndf.loc[df['pop'] < 2.e6, 'country'] = 'Other countries' # Represent only large countries\nfig = px.pie(df, values='pop', names='country', title='Population of European continent')\nfig.show()\n\n\n**Read more about sunburst"
    },
    {
        "instruction": "Create a Python program using the 'plotly' API to generate a bubble chart. The program should define data points for the x, y, and z axes, create the chart, customize the chart title, X-axis label, Y-axis label, and size of the bubbles. Finally, save the chart as a PNG image file and display it on the screen.",
        "api": "plotly",
        "output": "#!pip install plotly\nimport plotly.graph_objects as go\n\n# Define data points for the x, y, and z axes\nx = [1, 2, 3, 4, 5]\ny = [10, 16, 5, 11, 8]\nz = [7, 3, 9, 5, 2]\n\n# Define the size of the bubbles\nsize = [30, 50, 20, 40, 10]\n\n# Create a bubble chart using Plotly Graph Objects\nfig = go.Figure(data=go.Scatter(x=x, y=y, mode='markers', marker=dict(size=size)))\nfig.update_layout(title=\"Bubble Chart\", xaxis_title=\"X-axis\", yaxis_title=\"Y-axis\")\n\n# Save the chart as a PNG image file\nfig.write_image(\"bubble_chart.png\")\n\n# Display the chart on the screen\nfig.show()\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n# Bubble chart with plotly.express¶\n\nA bubble chart is a scatter plot in which a third dimension of the data is shown through the size of markers. For other types of scatter plot, see the scatter plot documentation.\n\n\nWe first show a bubble chart example using Plotly Express. Plotly Express is the easy-to-use, high-level interface to Plotly, which operates on a variety of types of data and produces easy-to-style figures. The size of markers is set from the dataframe column given as the `size` parameter.\n\n```\nimport plotly.express as px\ndf = px.data.gapminder()\n\nfig = px.scatter(df.query(\"year==2007\"), x=\"gdpPercap\", y=\"lifeExp\",\n\t         size=\"pop\", color=\"continent\",\n                 hover\\_name=\"country\", log\\_x=True, size\\_max=60)\nfig.show()\n\n\n## Bubble Chart with plotly.graph\\_objects¶\n\nIf Plotly Express does not provide a good starting point, it is also possible to use the more generic `go.Scatter` class from `plotly.graph_objects`, and define the size of markers to create a bubble chart. All of the available options are described in the scatter section of the reference page: https://plotly.com/python/reference#scatter.\n\n\n\n### Simple Bubble Chart¶\n\nfig = go.Figure(data=[go.Scatter(\n    x=[1, 2, 3, 4], y=[10, 11, 12, 13],\n    mode='markers',\n    marker\\_size=[40, 60, 80, 100])\n])\n\n\n\n### Setting Marker Size and Color¶\n\nfig = go.Figure(data=[go.Scatter(\n    x=[1, 2, 3, 4], y=[10, 11, 12, 13],\n    mode='markers',\n    marker=dict(\n        color=['rgb(93, 164, 214)', 'rgb(255, 144, 14)',\n               'rgb(44, 160, 101)', 'rgb(255, 65, 54)'],\n        opacity=[1, 0.8, 0.6, 0.4],\n        size=[40, 60, 80, 100],\n    )\n)])\n\n\n### Scaling the Size of Bubble Charts¶\n\nTo scale the bubble size, use the attribute `sizeref`. We recommend using the following formula to calculate a `sizeref` value:  \n\n`sizeref = 2. * max(array of size values) / (desired maximum marker size\n\n==================\n Document 1 \n----------------\n### United States Bubble Map¶\n\nNote about `sizeref`:\n\n\nTo scale the bubble size, use the attribute sizeref. We recommend using the following formula to calculate a sizeref value:\n\n\n`sizeref = 2. * max(array of size values) / (desired maximum marker size ** 2)`\n\n\nNote that setting `sizeref` to a value greater than $1$, decreases the rendered marker sizes, while setting `sizeref` to less than $1$, increases the rendered marker sizes.\n\n\nSee https://plotly.com/python/reference/scatter/#scatter-marker-sizeref for more information. Additionally, we recommend setting the sizemode attribute: https://plotly.com/python/reference/scatter/#scatter-marker-sizemode to area.\n\ndf = pd.read\\_csv('https://raw.githubusercontent.com/plotly/datasets/master/2014\\_us\\_cities.csv')\ndf.head()\n\ndf['text'] = df['name'] + '<br>Population ' + (df['pop']/1e6).astype(str)+' million'\nlimits = [(0,2),(3,10),(11,20),(21,50),(50,3000)]\ncolors = [\"royalblue\",\"crimson\",\"lightseagreen\",\"orange\",\"lightgrey\"]\ncities = []\nscale = 5000\n\nfor i in range(len(limits)):\n    lim = limits[i]\n    df\\_sub = df[lim[0]:lim[1]]\n    fig.add\\_trace(go.Scattergeo(\n        locationmode = 'USA-states',\n        lon = df\\_sub['lon'],\n        lat = df\\_sub['lat'],\n        text = df\\_sub['text'],\n        marker = dict(\n            size = df\\_sub['pop']/scale,\n            color = colors[i],\n            line\\_color='rgb(40,40,40)',\n            line\\_width=0.5,\n            sizemode = 'area'\n        ),\n        name = '{0} - {1}'.format(lim[0],lim[1])))\n\nfig.update\\_layout(\n        title\\_text = '2014 US city populations<br>(Click legend to toggle traces)',\n        showlegend = True,\n        geo = dict(\n            scope = 'usa',\n            landcolor = 'rgb(217, 217, 217)',\n        )\n    )\n\n\n#### Ebola Cases in West Africa¶\n\ndf = pd.read\\_csv('https://raw.githubusercontent.com/plotly/datasets/master/2014\\_ebola.csv')\ndf.head()\n\ncolors = ['rgb(239,243,255)','rgb(189,215,231)','rgb(107,174,214)','rgb(33,113,181)']\nmonths = {6:'June',7:'July',8:'Aug',9:'Sept'}\n\nfor i in range(6,10)[::-1]:\n    df\\_month = df.query('Month == %d' %i)\n    fig.add\\_trace(go.Scattergeo(\n            lon = df\\_month['Lon'],\n            lat = df\\_month['Lat'],\n            text = df\\_month['Value'],\n            name = months[i],\n            marker = dict(\n                size = df\\_month['Value']/50,\n                color = colors[i-6],\n                line\\_width = 0\n            )))\n\ndf\\_sept = df.query('Month == 9')\nfig['data'][0].update(mode='markers+text', textposition='bottom center',\n                      text=df\\_sept['Value'].map('{:.0f}'.format).astype(str)+' '+\\\n                      df\\_sept['Country'])\n\n# Inset\nfig.add\\_trace(go.Choropleth(\n        locationmode = 'country names',\n        locations = df\\_sept['Country'],\n        z = df\\_sept['Value'],\n\n==================\n Document 2 \n----------------\n### Plot chart with area proportional to total count¶\n\nPlots in the same `scalegroup` are represented with an area proportional to their total size.\n\nlabels = [\"Asia\", \"Europe\", \"Africa\", \"Americas\", \"Oceania\"]\n\nfig = make\\_subplots(1, 2, specs=[[{'type':'domain'}, {'type':'domain'}]],\n                    subplot\\_titles=['1980', '2007'])\nfig.add\\_trace(go.Pie(labels=labels, values=[4, 7, 1, 7, 0.5], scalegroup='one',\n                     name=\"World GDP 1980\"), 1, 1)\nfig.add\\_trace(go.Pie(labels=labels, values=[21, 15, 3, 19, 1], scalegroup='one',\n                     name=\"World GDP 2007\"), 1, 2)\n\nfig.update\\_layout(title\\_text='World GDP')\nfig.show()\n\n*New in 5.15*\n\n\nPie charts support patterns (also known as hatching or texture) in addition to color.\n\nlabels = [\"Oxygen\", \"Hydrogen\", \"Carbon\\_Dioxide\", \"Nitrogen\"]\nvalues = [4500, 2500, 1053, 500]\ncolors = [\"gold\", \"mediumturquoise\", \"darkorange\", \"lightgreen\"]\n\nfig = go.Figure(\n    data=[\n        go.Pie(\n            labels=labels,\n            values=values,\n            textfont\\_size=20,\n            marker=dict(colors=colors, pattern=dict(shape=[\".\", \"x\", \"+\", \"-\"]))\n        )\n    ]\n)\n\n\n### See Also: Sunburst charts¶\n\nFor multilevel pie charts representing hierarchical data, you can use the `Sunburst` chart. A simple example is given below, for more information see the tutorial on Sunburst charts.\n\nfig =go.Figure(go.Sunburst(\n    labels=[\"Eve\", \"Cain\", \"Seth\", \"Enos\", \"Noam\", \"Abel\", \"Awan\", \"Enoch\", \"Azura\"],\n    parents=[\"\", \"Eve\", \"Eve\", \"Seth\", \"Seth\", \"Eve\", \"Eve\", \"Awan\", \"Eve\" ],\n    values=[10, 14, 12, 10, 2, 6, 6, 4, 4],\n))\nfig.update\\_layout(margin = dict(t=0, l=0, r=0, b=0))\n\nSee function reference for `px.pie()` or https://plotly.com/python/reference/pie/ for more information and chart attribute options!\n\n \n \n > Basic Charts\n \n \n > Bubble Charts\n\n\n\n# \n Bubble Charts\n \n in \n Python\n\n\nHow to make bubble charts in Python with Plotly. \n\n\n## Bubble chart with plotly.express¶\n\nA bubble chart is a scatter plot in which a third dimension of the data is shown through the size of markers. For other types of scatter plot, see the scatter plot documentation.\n\n\nWe first show a\n\n==================\n Document 3 \n----------------\n Displaying Figures¶\n\nPlotly's Python graphing library, `plotly.py`, gives you a wide range of options for how and where to display your figures.\n\n\nIn general, there are five different approaches you can take in order to display `plotly` figures:\n\n\n1. Using the `renderers` framework in the context of a script or notebook (the main topic of this page)\n2. Using Dash in a web app context\n3. Using a `FigureWidget` rather than a `Figure` in an `ipywidgets` context\n4. By exporting to an HTML file and loading that file in a browser immediately or later\n5. By rendering the figure to a static image file using Kaleido such as PNG, JPEG, SVG, PDF or EPS and loading the resulting file in any viewer\n\n\nEach of the first three approaches is discussed below.\n\n### Displaying Figures Using The `renderers` Framework¶\n\nThe renderers framework is a flexible approach for displaying `plotly.py` figures in a variety of contexts. To display a figure using the renderers framework, you call the `.show()` method on a graph object figure,\n\n==================\n Document 4 \n----------------\n \n Plot CSV Data\n \n in \n Python\n\n\nHow to create charts from csv files with Plotly and Python \n\n\nCSV or comma-delimited-values is a very popular format for storing structured data. In this tutorial, we will see how to plot beautiful graphs using csv data, and Pandas. We will learn how to import csv data from an external source (a url), and plot it using Plotly and pandas.\n\n\nFirst we import the data and look at it.\n\n```\nimport pandas as pd\ndf = pd.read\\_csv('https://raw.githubusercontent.com/plotly/datasets/master/2014\\_apple\\_stock.csv')\ndf.head()\n\n\n|  | AAPL\\_x | AAPL\\_y |\n| --- | --- | --- |\n| 0 | 2014-01-02 | 77.445395 |\n| 1 | 2014-01-03 | 77.045575 |\n| 2 | 2014-01-06 | 74.896972 |\n| 3 | 2014-01-07 | 75.856461 |\n| 4 | 2014-01-08 | 75.091947 |\n\n### Plot from CSV with Plotly Express¶\n\ndf = pd.read\\_csv('https://raw.githubusercontent.com/plotly/datasets/master/2014\\_apple\\_stock.csv')\n\nfig = px.line(df, x = 'AAPL\\_x', y = 'AAPL\\_y', title='Apple Share Prices over time (2014)')\nfig.show()\n\n\n\n### Plot from CSV in Dash¶\n\n\n\n### Plot from CSV with `graph_objects`¶\n\n```\nimport pandas as pd\nimport plotly.graph\\_objects as go\n\nfig = go.Figure(go.Scatter(x = df['AAPL\\_x'], y = df['AAPL\\_y'],\n                  name='Share Prices (in USD)'))\n\nfig.update\\_layout(title='Apple Share Prices over time (2014)',\n                   plot\\_bgcolor='rgb(230, 230,230)',\n                   showlegend=True)\n\nSee https://plotly.com/python/getting-started for more information about Plotly's Python API!\n\n \n \n > \n \n \n > Random Walk\n\n\n\n# \n Random Walk\n \n in \n Python\n\n\nLearn how to use Python to make a Random Walk \n\n\nA random walk can be thought of as a random process in which a token or a marker is randomly moved around some space, that is, a space with a metric used to compute distance. It is more commonly conceptualized in one dimension ($\\mathbb{Z}$), two dimensions ($\\mathbb{Z}^2$) or three dimensions ($\\mathbb{Z}^3$) in Cartesian space, where $\\mathbb{Z}$ represents the set of integers. In the visualizations below, we will be using scatter plots as well as a colorscale to denote the time sequence of the walk.\n\n\n\n#### Random Walk in 1D¶\n\nThe jitter in the data points along the x and y axes are meant to illuminate where the points are being drawn and what the tendency of the random walk is.\n\nl = 100\nsteps = np.random.choice([-1, 1], size=l) + 0.05 \\* np.random.randn(l) # l steps\nposition = np.cumsum(steps) # integrate the position by summing steps values\ny = 0.05 \\* np.random.randn(l)\n\nfig = go.Figure(data=go.Scatter(\n    x=position,\n    y=y,\n    mode='markers',\n    name='Random Walk in 1D',\n    marker=dict(\n        color=np.arange(l),\n        size=7,\n        colorscale='Reds',\n        showscale=True,\n    )\n))\n\nfig.update\\_layout(yaxis\\_range=[-1, 1])\nfig.show()\n\n\n\n#### Random Walk in 2D¶\n\nl = 1000\nx\\_steps = np.random.choice([-1, 1], size=l) + 0.2 \\* np.random.randn(l) # l steps\ny\\_steps = np.random.choice([-1, 1], size=l) + 0.2 \\* np.random.randn(l) # l steps\nx\\_position = np.cumsum(x\\_steps) # integrate the position by summing steps values\ny\\_position = np.cumsum(y\\_steps) # integrate the position by summing steps values\n\nfig = go.Figure(data=go.Scatter(\n    x=x\\_position,\n    y=y\\_position,\n    mode='markers',\n    name='Random Walk',\n    marker=dict(\n        color=np.arange(l),\n        size=8,\n        colorscale='Greens',\n        showscale=True\n    )\n))\n\n\n#### Random walk and diffusion¶\n\nIn the two following charts we show the link between random walks and diffusion. We compute a large number `N` of random walks representing for examples molecules in a small drop of chemical. While all trajectories\n\n==================\n Document 5 \n----------------\n Plotly Python Open Source Graphing Library Scientific Charts\n\n\nPlotly's Python graphing library makes interactive, publication-quality graphs online. Examples of how to make scientific charts such as contour plots, heatmaps, dendrograms, polar charts, and ternary plots.\n\n\n* Contour Plots\n\nView Tutorial\n\n* Heatmaps\n\nView Tutorial\n\n* Imshow\n\nView Tutorial\n\n* Ternary Plots\n\nView Tutorial\n\n* Log Plots\n\nView Tutorial\n\n* Dendrograms\n\nView Tutorial\n\n* Annotated Heatmaps\n\nView Tutorial\n\n* Ternary Overlay\n\nView Tutorial\n\n* Parallel Coordinates Plot\n\nView Tutorial\n\n* Quiver Plots\n\nView Tutorial\n\n* Streamline Plots\n\nView Tutorial\n\n* Network Graphs\n\nView Tutorial\n\n* Carpet Plots\n\nView Tutorial\n\n* Carpet Contour Plot\n\nView Tutorial\n\n* Carpet Scatter Plot\n\nView Tutorial\n\n* Polar Charts\n\nView Tutorial\n\n* Radar Charts\n\nView Tutorial\n\n* Ternary contours\n\nView Tutorial\n\n* Wind Rose and Polar Bar Charts\n\nView Tutorial\n\n* Plotly and Datashader\n\nView Tutorial\n\n* Smith Charts\n\nView Tutorial\n\n\n \n \n > Scientific Charts\n \n \n > Contour Plots\n\n\n# \n Contour Plots\n \n in \n Python\n\n\nHow to make Contour plots in Python with Plotly. \n\n\n\n### Basic Contour Plot¶\n\nA 2D contour plot shows the contour lines of a 2D numerical array `z`, i.e. interpolated lines of isovalues of `z`.\n\nfig = go.Figure(data =\n    go.Contour(\n        z=[[10, 10.625, 12.5, 15.625, 20],\n           [5.625, 6.25, 8.125, 11.25, 15.625],\n           [2.5, 3.125, 5., 8.125, 12.5],\n           [0.625, 1.25, 3.125, 6.25, 10.625],\n           [0, 0.625, 2.5, 5.625, 10]]\n    ))\nfig.show()\n\n\n\n### Setting X and Y Coordinates in a Contour Plot¶\n\nfig = go.Figure(data =\n    go.Contour(\n        z=[[10, 10.625, 12.5, 15.625, 20],\n           [5.625, 6.25, 8.125, 11.25, 15.625],\n           [2.5, 3.125, 5., 8.125, 12.5],\n           [0.625, 1.25, 3.125, 6.25, 10.625],\n           [0, 0.625, 2.5, 5.625, 10]],\n        x=[-9, -6, -5 , -3, -1], # horizontal axis\n        y=[0, 1, 4, 5, 7] # vertical axis\n    ))\nfig.show()\n\n\n\n### Colorscale for Contour Plot¶\n\nfig = go.Figure(data =\n     go.Contour(\n        z=[[10, 10.625, 12.5, 15.625, 20],\n           [5.625, 6.25, 8.125, 11.25, 15.625],\n           [2.5, 3.125, 5., 8.125, 12.5],\n           [0.625, 1.25, 3.125, 6.25, 10.625],\n           [0, 0.625, 2.5, 5.625, 10]],\n        colorscale='Electric',\n    ))\nfig.show()\n\n\n\n### Customizing Size and Range of a Contour Plot's Contours¶\n\nfig = go.Figure(data =\n    go.Contour(\n        z=[[10, 10.625, 12.5, 15.625, 20],\n           [5.625, 6.25, 8.125, 11.25, 15.625],\n           [2.5, 3.125, 5., 8.125, 12.5],\n           [0.625, 1.25, 3.125, 6.25, 10.625],\n           [0, 0.625, 2.5, 5.625, 10]],\n        colorscale='Hot',\n        contours=dict(\n            start=0,\n            end=8,\n            size=2,\n        ),\n    ))\n\n\n\n### Customizing Spacing Between X and Y Axis Ticks¶\n\nfig = go.Figure(data =\n    go.Contour(\n        z= [[10, 10.625, 12.5, 15.625, 20],\n              [5.625, 6.25, 8.125, 11.25, 15.625],\n              [2.5, 3.125, 5., 8.125, 12.5],\n              [0.625, 1.25, 3.125, 6.25, 10.625],\n              [0, 0.625, 2.5, 5.625, 10]],\n        dx=10,\n        x0=5,\n        dy=10,\n        y0=10,\n    )\n)\n\n\n### Connect the Gaps Between None Values in the Z Matrix¶\n\n```\nimport plotly.graph\\_objs as go\nfrom plotly.subplots import make\\_subplots\n\nfig = make\\_subplots(rows=2, cols=2, subplot\\_titles=('connectgaps = False',\n\n==================\n Document 6 \n----------------\n \n Candlestick Charts\n \n in \n Python\n\n\nHow to make interactive candlestick charts in Python with Plotly. Six examples of candlestick charts with Pandas, time series, and yahoo finance data. \n\n\nThe candlestick chart is a style of financial chart describing open, high, low and close for a given `x` coordinate (most likely\ntime). The boxes represent the spread between the `open` and `close` values and the lines represent the spread between the `low` and `high` values. Sample points where the close value is higher (lower) then the open value are called increasing (decreasing). By default, increasing candles are drawn in green whereas decreasing are drawn in red.\n\n\n#### Simple Candlestick with Pandas¶\n\nimport pandas as pd\nfrom datetime import datetime\n\nfig = go.Figure(data=[go.Candlestick(x=df['Date'],\n                open=df['AAPL.Open'],\n                high=df['AAPL.High'],\n                low=df['AAPL.Low'],\n                close=df['AAPL.Close'])])\n\n\n\n#### Candlestick without Rangeslider¶\n\nfig = go.Figure(data=[go.Candlestick(x=df['Date'],\n                open=df['AAPL.Open'], high=df['AAPL.High'],\n                low=df['AAPL.Low'], close=df['AAPL.Close'])\n                     ])\n\nfig.update\\_layout(xaxis\\_rangeslider\\_visible=False)\nfig.show()\n\n\n\n#### Candlestick in Dash¶\n\n\n\n#### Adding Customized Text and Annotations¶\n\n\ndf = pd.read\\_csv('https://raw.githubusercontent.com/plotly/datasets/master/finance-charts-apple.csv')\n\nfig = go.Figure(data=[go.Candlestick(x=df['Date'],\n                open=df['AAPL.Open'], high=df['AAPL.High'],\n                low=df['AAPL.Low'], close=df['AAPL.Close'])\n                      ])\n\nfig.update\\_layout(\n    title='The Great Recession',\n    yaxis\\_title='AAPL Stock',\n    shapes = [dict(\n        x0='2016-12-09', x1='2016-12-09', y0=0, y1=1, xref='x', yref='paper',\n        line\\_width=2)],\n    annotations=[dict(\n        x='2016-12-09', y=0.05, xref='x', yref='paper',\n        showarrow=False, xanchor='left', text='Increase Period Begins')]\n)\n\n\n\n#### Custom Candlestick Colors¶\n\nfig = go.Figure(data=[go.Candlestick(\n    x=df['Date'],\n    open=df['AAPL.Open'], high=df['AAPL.High'],\n    low=df['AAPL.Low'], close=df['AAPL.Close'],\n    increasing\\_line\\_color= 'cyan', decreasing\\_line\\_color= 'gray'\n)])\n\n\n\n#### Simple Example with `datetime` Objects¶\n\n```\nimport plotly.graph\\_objects as go\nfrom datetime import datetime\n\nopen\\_data = [33.0, 33.3, 33.5, 33.0, 34.1]\nhigh\\_data = [33.1, 33.3, 33.6, 33.2, 34.8]\nlow\\_data = [32.7, 32.7, 32.8, 32.6, 32.8]\nclose\\_data = [33.0, 32.9, 33.3, 33.1, 33.1]\ndates = [datetime(year=2013, month=10, day=10),\n         datetime(year=2013, month=11, day=10),\n         datetime(year=2013, month=12, day=10),\n         datetime(year=2014, month=1, day=10),\n         datetime(year=2014, month=2, day=10)]\n\nfig = go.Figure(data=[go.Candlestick(x=dates,\n                       open=open\\_data, high=high\\_data,\n                       low=low\\_data, close=close\\_data)])\n\nFor more information on candlestick attributes, see: https://plotly.com/python/reference/candlestick/\n\n \n \n > Financial Charts\n \n \n > Waterfall Charts\n\n\n\n# \n Waterfall Charts\n \n in \n Python\n\n\nHow to make waterfall plots in Python with Plotly. \n\n\n\n### Simple Waterfall Chart¶\n\nfig = go.Figure(go.Waterfall(\n    name = \"20\", orientation = \"v\",\n    measure = [\"relative\", \"relative\", \"total\", \"relative\", \"relative\", \"total\"],\n    x = [\"Sales\", \"Consulting\", \"Net revenue\", \"Purchases\", \"Other expenses\", \"Profit before tax\"],\n    textposition = \"outside\",\n    text = [\"+60\", \"+80\", \"\", \"-40\", \"-20\", \"Total\"],\n    y = [60, 80, 0, -40, -20, 0],\n    connector = {\"line\":{\"color\":\"rgb(63, 63, 63)\"}},\n))\n\nfig.update\\_layout(\n        title = \"Profit and loss statement 2018\",\n        showlegend = True\n)\n\n\n### Multi Category Waterfall Chart¶\n\nThis example uses the waterfallgroupgap attribute, which sets a gap between bars.\n\nfig.add\\_trace(go.Waterfall(\n    x = [[\"2016\", \"2017\", \"2017\", \"2017\", \"2017\", \"2018\", \"2018\", \"2018\", \"2018\"],\n        [\"initial\", \"q1\", \"q2\",\n\n==================\n Document 7 \n----------------\n \n Plotly Express\n \n in \n Python\n\n\nPlotly Express is a terse, consistent, high-level API for creating figures. \n\nThe `plotly.express` module (usually imported as `px`) contains functions that can create entire figures at once, and is referred to as Plotly Express or PX. Plotly Express is a built-in part of the `plotly` library, and is the recommended starting point for creating most common figures. Every Plotly Express function uses graph objects internally and returns a `plotly.graph_objects.Figure` instance. Throughout the `plotly` documentation, you will find the Plotly Express way of building figures at the top of any applicable page, followed by a section on how to use graph objects to build similar figures. Any figure created in a single function call with Plotly Express could be created using graph objects alone, but with between 5 and 100 times more code.\n\n\nPlotly Express provides more than 30 functions for creating different types of figures. The API for these functions was carefully designed to be as consistent and easy to learn as possible, making it easy to switch from a scatter plot to a bar chart to a histogram to a sunburst chart throughout a data exploration session. *Scroll down for a gallery of Plotly Express plots, each made in a single function call.*\n\n\nHere is a talk from the SciPy 2021 conference that gives a good introduction to Plotly Express and Dash:\n\n\nPlotly Express currently includes the following functions:\n\n\n* **Basics**: `scatter`, `line`, `area`, `bar`, `funnel`, `timeline`\n* **Part-of-Whole**: `pie`, `sunburst`, `treemap`, `icicle`, `funnel_area`\n* **1D Distributions**: `histogram`, `box`, `violin`, `strip`, `ecdf`\n* **2D Distributions**: `density_heatmap`, `density_contour`\n* **Matrix or Image Input**: `imshow`\n* **3-Dimensional**: `scatter_3d`, `line_3d`\n* **Multidimensional**: `scatter_matrix`, `parallel_coordinates`, `parallel_categories`\n* **Tile Maps**: `scatter_mapbox`, `line_mapbox`, `choropleth_mapbox`, `density_mapbox`\n* **Outline Maps**: `scatter_geo`, `line_geo`, `choropleth`\n* **Polar Charts**: `scatter_polar`, `line_polar`, `bar_polar`\n* **Ternary Charts**: `scatter_ternary`, `line_ternary`\n\n### High-Level Features¶\n\nThe Plotly Express API in general offers the following features:\n\n\n* **A single entry point into `plotly`**: just `import plotly.express as px` and get access to all the plotting functions, plus built-in demo datasets under `px.data` and built-in color"
    },
    {
        "instruction": "Create a Python program using the 'plotly' API to generate a scatter plot with error bars. The program should define data points for the x and y axes, as well as the error values for each data point. The program should create the scatter plot with error bars, customize the chart title, X-axis label, and Y-axis label, and display the error bars on the chart. Finally, save the chart as a PDF file and display it on the screen.",
        "api": "plotly",
        "output": "#!pip install plotly\nimport plotly.graph_objects as go\n\n# Define data points for the x and y axes\nx = [1, 2, 3, 4, 5]\ny = [10, 16, 5, 11, 8]\n\n# Define error values for each data point\nerror = [1, 2, 0.5, 1.5, 1]\n\n# Create a scatter plot with error bars using Plotly Graph Objects\nfig = go.Figure(data=go.Scatter(x=x, y=y, mode='markers', error_y=dict(type='data', array=error, visible=True)))\nfig.update_layout(title=\"Scatter Plot with Error Bars\", xaxis_title=\"X-axis\", yaxis_title=\"Y-axis\")\n\n# Save the chart as a PDF file\nfig.write_image(\"scatter_plot_with_error_bars.pdf\")\n\n# Display the chart on the screen\nfig.show()\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n  Plotly Python Open Source Graphing Library Statistical Charts\n\n\nPlotly's Python graphing library makes interactive, publication-quality graphs online. Examples of how to make statistical charts such as box plots, histograms, and distrubution plots.\n\n\n* Error Bars\n\nView Tutorial\n\n* Box Plots\n\nView Tutorial\n\n* Histograms\n\nView Tutorial\n\n* Distplots\n\nView Tutorial\n\n* 2D Histograms\n\nView Tutorial\n\n* Scatterplot Matrix\n\nView Tutorial\n\n* Facet and Trellis Plots\n\nView Tutorial\n\n* Parallel Categories Diagram\n\nView Tutorial\n\n* Tree-plots\n\nView Tutorial\n\n* Violin Plots\n\nView Tutorial\n\n* 2D Histogram Contour\n\nView Tutorial\n\n* Linear and Non-Linear Trendlines\n\nView Tutorial\n\n* Marginal Distribution Plots\n\nView Tutorial\n\n* Strip Charts\n\nView Tutorial\n\n* Continuous Error Bands\n\nView Tutorial\n\n* Empirical Cumulative Distribution Plots\n\nView Tutorial\n\n\n\n### Statistical charts in Dash\n\n \n \n > Statistical Charts\n \n \n > Error Bars\n\n\n\n# \n Error Bars\n \n in \n Python\n\n\nHow to add error-bars to charts in Python with Plotly. \n\n\n\n### Error Bars with Plotly Express¶\n\nPlotly Express is the easy-to-use, high-level interface to Plotly, which operates on a variety of types of data and produces easy-to-style figures. For functions representing 2D data points such as `px.scatter`, `px.line`, `px.bar` etc., error bars are given as a column name which is the value of the `error_x` (for the error on x position) and `error_y` (for the error on y position).\n\n\n\n#### Asymmetric Error Bars with Plotly Express¶\n\n```\nimport plotly.express as px\ndf = px.data.iris()\ndf[\"e\\_plus\"] = df[\"sepal\\_width\"]/100\ndf[\"e\\_minus\"] = df[\"sepal\\_width\"]/40\nfig = px.scatter(df, x=\"sepal\\_width\", y=\"sepal\\_length\", color=\"species\",\n                 error\\_y=\"e\\_plus\", error\\_y\\_minus=\"e\\_minus\")\nfig.show()\n\n\n\n### Error Bars with graph\\_objects¶\n\n\n#### Basic Symmetric Error Bars¶\n\nfig = go.Figure(data=go.Scatter(\n        x=[0, 1, 2],\n        y=[6, 10, 2],\n        error\\_y=dict(\n            type='data', # value of error bar given in data coordinates\n            array=[1, 2, 3],\n            visible=True)\n    ))\nfig.show()\n\n\n\n#### Asymmetric Error Bars¶\n\nfig = go.Figure(data=go.Scatter(\n        x=[1, 2, 3, 4],\n        y=[2, 1, 3, 4],\n        error\\_y=dict(\n            type='data',\n            symmetric=False,\n            array=[0.1, 0.2, 0.1, 0.1],\n            arrayminus=[0.2, 0.4, 1, 0.2])\n        ))\nfig.show()\n\n\n\n#### Error Bars as a Percentage of the y Value¶\n\nfig = go.Figure(data=go.Scatter(\n        x=[0, 1, 2],\n        y=[6, 10, 2],\n        error\\_y=dict(\n            type='percent', # value of error bar given as percentage of y value\n            value=50,\n            visible=True)\n    ))\nfig.show()\n\n\n\n#### Asymmetric Error Bars with a Constant Offset¶\n\nfig = go.Figure(data=go.Scatter(\n        x=[1, 2, 3, 4],\n        y=[2, 1, 3, 4],\n        error\\_y=dict(\n            type='percent',\n            symmetric=False,\n            value=15,\n            valueminus=25)\n    ))\nfig.show()\n\n\n\n#### Horizontal Error Bars¶\n\nfig = go.Figure(data=go.Scatter(\n        x=[1, 2, 3, 4],\n        y=[2, 1, 3, 4],\n        error\\_x=dict(\n            type='percent',\n            value=10)\n    ))\nfig.show()\n\n\n\n#### Bar Chart with Error Bars¶\n\nfig = go.Figure()\nfig.add\\_trace(go.Bar(\n    name='Control',\n    x=['Trial 1', 'Trial 2', 'Trial 3'], y=[3, 6, 4],\n    error\\_y=dict(type='data', array=[1, 0.5, 1.5])\n))\nfig.add\\_trace(go.Bar(\n    name='Experimental',\n    x=['Trial 1', 'Trial 2', 'Trial 3'], y=[4, 7, 3],\n    error\\_y=dict(type='data', array=[0.5, 1, 2])\n))\nfig.update\\_layout(barmode='group')\nfig.show()\n\n\n\n#### Colored and Styled Error Bars¶\n\nx\\_theo = np.linspace(-4, 4, 100)\nsincx = np.sinc(x\\_theo)\nx = [-3.8, -3.03, -1.91, -1.46, -0.89, -0.24, -0.0, 0.41, 0.89, 1.01, 1.91, 2.28, 2.79, 3.56]\ny = [-0.02, 0.04, -0.01, -0.27, 0.36, 0.75, 1.03, 0.65, 0.28, 0.02, -0.11, 0.16, 0.04, -0.15]\n\nfig = go.Figure()\nfig.add\\_trace(go.Scatter(\n    x=x\\_theo, y=sincx,\n    name='sinc(x)'\n))\nfig.add\\_trace(go.Scatter(\n    x=x, y=y,\n    mode='markers',\n    name='measured',\n    error\\_y=dict(\n        type='constant',\n        value=0.1,\n        color='purple',\n        thickness=1.5,\n        width=3,\n    ),\n    error\\_x=dict(\n        type='constant',\n        value=0.2,\n        color='purple',\n        thickness=1.5,\n        width=3,\n    ),\n    marker=dict(color='purple', size=8)\n))\nfig.show()\n\n \n \n > Statistical Charts\n \n \n > Box Plots\n\n\n\n# \n Box Plots\n \n in \n Python\n\n\nHow to make Box Plots in Python with Plotly. \n\n\nA box plot is a statistical representation of the distribution of a variable through its quartiles. The ends of the box represent the lower and upper quartiles, while the median (second quartile) is marked by a line inside the box. For other statistical representations of numerical data, see other statistical charts.\n\n\nAlternatives to box plots for visualizing distributions include histograms, violin plots, ECDF plots and strip charts.\n\n\n\n## Box Plot with `plotly.express`¶\n\n\nIn a box plot created by `px.box`, the distribution of the column given as `y` argument is represented.\n\n```\nimport plotly.express as px\ndf = px.data.tips()\nfig = px.box(df, y=\"total\\_bill\")\nfig.show()\n\n\nIf a column name is given as `x` argument, a box plot is drawn for each value of `x`.\n\n```\nimport plotly.express as px\ndf = px.data.tips()\nfig = px.box(df, x=\"time\", y=\"total\\_bill\")\nfig.show()\n\n\n\n### Box Plots in Dash¶\n\n\n\n### Display the underlying data¶\n\nWith the `points` argument, display underlying data points with either all points (`all`), outliers only (`outliers`, default), or none of them (`False`).\n\n```\nimport plotly.express as px\ndf = px.data.tips()\nfig = px.box(df, x=\"time\", y=\"total\\_bill\", points=\"all\")\nfig.show()\n\n\n### Choosing The Algorithm For Computing Quartiles¶\n\nBy default, quartiles for box plots are computed using the `linear` method (for more about linear interpolation, see #10 listed on http://jse.amstat.org/v14n3/langford.html and https://en.wikipedia.org/wiki/Quartile for more details).\n\n\nHowever, you can also choose to use an\n\n==================\n Document 1 \n----------------\n Plotly Python Open Source Graphing Library Scientific Charts\n\n\nPlotly's Python graphing library makes interactive, publication-quality graphs online. Examples of how to make scientific charts such as contour plots, heatmaps, dendrograms, polar charts, and ternary plots.\n\n\n* Contour Plots\n\nView Tutorial\n\n* Heatmaps\n\nView Tutorial\n\n* Imshow\n\nView Tutorial\n\n* Ternary Plots\n\nView Tutorial\n\n* Log Plots\n\nView Tutorial\n\n* Dendrograms\n\nView Tutorial\n\n* Annotated Heatmaps\n\nView Tutorial\n\n* Ternary Overlay\n\nView Tutorial\n\n* Parallel Coordinates Plot\n\nView Tutorial\n\n* Quiver Plots\n\nView Tutorial\n\n* Streamline Plots\n\nView Tutorial\n\n* Network Graphs\n\nView Tutorial\n\n* Carpet Plots\n\nView Tutorial\n\n* Carpet Contour Plot\n\nView Tutorial\n\n* Carpet Scatter Plot\n\nView Tutorial\n\n* Polar Charts\n\nView Tutorial\n\n* Radar Charts\n\nView Tutorial\n\n* Ternary contours\n\nView Tutorial\n\n* Wind Rose and Polar Bar Charts\n\nView Tutorial\n\n* Plotly and Datashader\n\nView Tutorial\n\n* Smith Charts\n\nView Tutorial\n\n\n \n \n > Scientific Charts\n \n \n > Contour Plots\n\n\n# \n Contour Plots\n \n in \n Python\n\n\nHow to make Contour plots in Python with Plotly. \n\n\n\n### Basic Contour Plot¶\n\nA 2D contour plot shows the contour lines of a 2D numerical array `z`, i.e. interpolated lines of isovalues of `z`.\n\nfig = go.Figure(data =\n    go.Contour(\n        z=[[10, 10.625, 12.5, 15.625, 20],\n           [5.625, 6.25, 8.125, 11.25, 15.625],\n           [2.5, 3.125, 5., 8.125, 12.5],\n           [0.625, 1.25, 3.125, 6.25, 10.625],\n           [0, 0.625, 2.5, 5.625, 10]]\n    ))\nfig.show()\n\n\n\n### Setting X and Y Coordinates in a Contour Plot¶\n\nfig = go.Figure(data =\n    go.Contour(\n        z=[[10, 10.625, 12.5, 15.625, 20],\n           [5.625, 6.25, 8.125, 11.25, 15.625],\n           [2.5, 3.125, 5., 8.125, 12.5],\n           [0.625, 1.25, 3.125, 6.25, 10.625],\n           [0, 0.625, 2.5, 5.625, 10]],\n        x=[-9, -6, -5 , -3, -1], # horizontal axis\n        y=[0, 1, 4, 5, 7] # vertical axis\n    ))\nfig.show()\n\n\n\n### Colorscale for Contour Plot¶\n\nfig = go.Figure(data =\n     go.Contour(\n        z=[[10, 10.625, 12.5, 15.625, 20],\n           [5.625, 6.25, 8.125, 11.25, 15.625],\n           [2.5, 3.125, 5., 8.125, 12.5],\n           [0.625, 1.25, 3.125, 6.25, 10.625],\n           [0, 0.625, 2.5, 5.625, 10]],\n        colorscale='Electric',\n    ))\nfig.show()\n\n\n\n### Customizing Size and Range of a Contour Plot's Contours¶\n\nfig = go.Figure(data =\n    go.Contour(\n        z=[[10, 10.625, 12.5, 15.625, 20],\n           [5.625, 6.25, 8.125, 11.25, 15.625],\n           [2.5, 3.125, 5., 8.125, 12.5],\n           [0.625, 1.25, 3.125, 6.25, 10.625],\n           [0, 0.625, 2.5, 5.625, 10]],\n        colorscale='Hot',\n        contours=dict(\n            start=0,\n            end=8,\n            size=2,\n        ),\n    ))\n\n\n\n### Customizing Spacing Between X and Y Axis Ticks¶\n\nfig = go.Figure(data =\n    go.Contour(\n        z= [[10, 10.625, 12.5, 15.625, 20],\n              [5.625, 6.25, 8.125, 11.25, 15.625],\n              [2.5, 3.125, 5., 8.125, 12.5],\n              [0.625, 1.25, 3.125, 6.25, 10.625],\n              [0, 0.625, 2.5, 5.625, 10]],\n        dx=10,\n        x0=5,\n        dy=10,\n        y0=10,\n    )\n)\n\n\n### Connect the Gaps Between None Values in the Z Matrix¶\n\n```\nimport plotly.graph\\_objs as go\nfrom plotly.subplots import make\\_subplots\n\nfig = make\\_subplots(rows=2, cols=2, subplot\\_titles=('connectgaps = False',\n\n==================\n Document 2 \n----------------\n## Grouped Scatter Points¶\n\n*New in 5.12*\n\n\nBy default, scatter points at the same location are overlayed. We can see this in the previous example, with the values for Canada for bronze and silver. Set `scattermode='group'` to plot scatter points next to one another, centered around the shared location.\n\ndf = px.data.medals\\_long()\n\nfig = px.scatter(df, y=\"count\", x=\"nation\", color=\"medal\")\nfig.update\\_traces(marker\\_size=10)\nfig.update\\_layout(scattermode=\"group\")\nfig.show()\n\n\n*New in 5.12*\n\n\nYou can configure the gap between groups of scatter points using `scattergap`. Here we set it to `0.75`, which brings the points closer together by allocating more space to the gap between groups. If you don't set `scattergap`, a default value of `0` is used, unless you have `bargap` set. If you have `bargap` set, the `scattergap` defaults to that value.\n\nfig = px.scatter(df, y=\"count\", x=\"nation\", color=\"medal\")\nfig.update\\_traces(marker\\_size=10)\nfig.update\\_layout(scattermode=\"group\", scattergap=0.75)\nfig.show()\n\n\n### Error Bars¶\n\nScatter plots support error bars.\n\n```\nimport plotly.express as px\ndf = px.data.iris()\ndf[\"e\"] = df[\"sepal\\_width\"]/100\nfig = px.scatter(df, x=\"sepal\\_width\", y=\"sepal\\_length\", color=\"species\",\n                 error\\_x=\"e\", error\\_y=\"e\")\nfig.show()\n\n\n\n### Marginal Distribution Plots¶\n\nScatter plots support marginal distribution plots\n\n```\nimport plotly.express as px\ndf = px.data.iris()\nfig = px.scatter(df, x=\"sepal\\_length\", y=\"sepal\\_width\", marginal\\_x=\"histogram\", marginal\\_y=\"rug\")\nfig.show()\n\n\n\n### Facetting¶\n\nScatter plots support faceting.\n\n```\nimport plotly.express as px\ndf = px.data.tips()\nfig = px.scatter(df, x=\"total\\_bill\", y=\"tip\", color=\"smoker\", facet\\_col=\"sex\", facet\\_row=\"time\")\nfig.show()\n\n\n\n### Linear Regression and Other Trendlines¶\n\nScatter plots support linear and non-linear trendlines.\n\ndf = px.data.tips()\nfig = px.scatter(df, x=\"total\\_bill\", y=\"tip\", trendline=\"ols\")\nfig.show()\n\n\n## Line plots with Plotly Express¶\n\n```\nimport plotly.express as px\nimport numpy as np\n\nt = np.linspace(0, 2\\*np.pi, 100)\n\nfig = px.line(x=t, y=np.cos(t), labels={'x':'t', 'y':'cos(t)'})\nfig.show()\n\n```\nimport plotly.express as px\ndf = px.data.gapminder().query(\"continent == 'Oceania'\")\nfig = px.line(df, x='year', y='lifeExp', color='country')\nfig.show()\n\n\nThe `markers` argument can be set to `True`\n\n==================\n Document 3 \n----------------\n## Data Order in Scatter and Line Charts¶\n\nPlotly line charts are implemented as connected scatterplots (see below), meaning that the points are plotted and connected with lines **in the order they are provided, with no automatic reordering**.\n\n\nThis makes it possible to make charts like the one below, but also means that it may be required to explicitly sort data before passing it to Plotly to avoid lines moving \"backwards\" across the chart.\n\ndf = pd.DataFrame(dict(\n    x = [1, 3, 2, 4],\n    y = [1, 2, 3, 4]\n))\nfig = px.line(df, x=\"x\", y=\"y\", title=\"Unsorted Input\") \nfig.show()\n\ndf = df.sort\\_values(by=\"x\")\nfig = px.line(df, x=\"x\", y=\"y\", title=\"Sorted Input\") \nfig.show()\n\n\n### Connected Scatterplots¶\n\nIn a connected scatterplot, two continuous variables are plotted against each other, with a line connecting them in some meaningful order, usually a time variable. In the plot below, we show the \"trajectory\" of a pair of countries through a space defined by GDP per Capita and Life Expectancy. Botswana's life expectancy\n\ndf = px.data.gapminder().query(\"country in ['Canada', 'Botswana']\")\n\nfig = px.line(df, x=\"lifeExp\", y=\"gdpPercap\", color=\"country\", text=\"year\")\nfig.update\\_traces(textposition=\"bottom right\")\nfig.show()\n\n\n\n## Scatter and line plots with go.Scatter¶\n\nIf Plotly Express does not provide a good starting point, it is possible to use the more generic `go.Scatter` class from `plotly.graph_objects`. Whereas `plotly.express` has two functions `scatter` and `line`, `go.Scatter` can be used both for plotting points (makers) or lines, depending on the value of `mode`. The different options of `go.Scatter` are documented in its reference page.\n\n\n\n#### Simple Scatter Plot¶\n\n```\nimport plotly.graph\\_objects as go\nimport numpy as np\n\nN = 1000\nt = np.linspace(0, 10, 100)\ny = np.sin(t)\n\nfig = go.Figure(data=go.Scatter(x=t, y=y, mode='markers'))\n\n\n\n#### Line and Scatter Plots¶\n\nUse `mode` argument to choose between markers, lines, or a combination of both. For more options about line plots, see also the line charts notebook and the filled area plots notebook.\n\n\n# Create random data with numpy\nimport numpy as np\nnp.random.seed(1)\n\nN = 100\nrandom\\_x = np.linspace(0, 1, N)\nrandom\\_y0 = np.random.randn(N) + 5\nrandom\\_y1 = np.random.randn(N)\nrandom\\_y2 = np.random.randn(N) - 5\n\n\n# Add traces\nfig.add\\_trace(go.Scatter(x=random\\_x, y=random\\_y0,\n                    mode='markers',\n                    name='markers'))\nfig.add\\_trace(go.Scatter(x=random\\_x, y=random\\_y1,\n                    mode='lines+markers',\n                    name='lines+markers'))\nfig.add\\_trace(go.Scatter(x=random\\_x, y=random\\_y2,\n                    mode='lines',\n                    name='lines'))\n\n\n\n#### Bubble Scatter Plots¶\n\nIn bubble charts, a third dimension of the data is shown through the size of markers. For more examples, see the bubble chart notebook\n\nfig = go.Figure(data=go.Scatter(\n    x=[1, 2, 3, 4],\n    y=[10, 11, 12, 13],\n    mode='markers',\n    marker=dict(size=[40, 60, 80, 100],\n                color=[0, 1, 2, 3])\n))\n\n\n\n#### Style Scatter Plots¶\n\n\nt = np.linspace(0, 10, 100)\n\nfig.add\\_trace(go.Scatter(\n    x=t, y=np.sin(t),\n    name='sin',\n    mode='markers',\n    marker\\_color='rgba(152, 0, 0, .8)'\n))\n\nfig.add\\_trace(go.Scatter(\n    x=t, y=np.cos(t),\n    name='cos',\n    marker\\_color='rgba(255, 182, 193, .9)'\n))\n\n\n# Set options common to all traces with fig.update\\_traces\nfig.update\\_traces(mode='markers', marker\\_line\\_width=2, marker\\_size=10)\nfig.update\\_layout(title='Styled Scatter',\n                  yaxis\\_zeroline=False, xaxis\\_zeroline=False)\n\n\nfig.show()\n\n\n\n#### Data Labels on Hover¶\n\n```\nimport plotly.graph\\_objects as go\nimport pandas as pd\n\ndata= pd.read\\_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/2014\\_usa\\_states.csv\")\n\nfig = go.Figure(data=go.Scatter(x=data['Postal'],\n                                y=data['Population'],\n                                mode='markers',\n                                marker\\_color=data['Population'],\n                                text=data['State'])) # hover text goes here\n\nfig.update\\_layout(title='Population of USA States')\nfig.show()\n\n\n\n#### Scatter with a Color Dimension¶\n\nfig = go.Figure(data=go.Scatter(\n    y = np.random.randn(500),\n    mode='markers',\n    marker=dict(\n        size=16,\n        color=np.random.randn(500), #set color equal to a variable\n        colorscale='Viridis', # one of plotly colorscales\n        showscale=True\n    )\n))\n\n\n\n#### Large Data Sets¶\n\nNow in Plotly you can implement WebGL with `Scattergl()` in place of `Scatter()`   \n\nfor increased speed, improved interactivity, and the ability to plot even more data!\n\nN = 100000\nfig = go.Figure(data=go.Scattergl(\n    x = np.random.randn(N),\n    y = np.random.randn(N),\n    mode='markers',\n    marker=dict(\n        color=np.random.randn(N),\n        colorscale='Viridis',\n        line\\_width=1\n    )\n))\n\nN = 100000\nr = np.random.uniform(0, 1, N)\ntheta = np.random.uniform(0, 2\\*np.pi, N)\n\nfig = go.Figure(data=go.Scattergl(\n    x = r \\* np.cos(theta), # non-uniform distribution\n    y = r \\* np.sin(theta), # zoom to see more points at the center\n    mode='markers',\n    marker=dict(\n        color=np.random.randn(N),\n        colorscale='Viridis',\n        line\\_width=1\n    )\n))\n\n\n\n### Reference¶\n\nSee function reference for `px.scatter()` or https://plotly.com/python/reference/scatter/ or https://plotly.com/python/reference/scattergl/ for more information and chart attribute options!\n\n \n \n > Basic Charts\n \n \n > Line Charts\n\n\n\n# \n Line Charts\n \n in \n Python\n\n\nHow to make line charts in Python with Plotly. Examples on creating and styling line charts in Python with Plotly. \n\n\n\n### Line Plots with plotly.express¶\n\nPlotly Express is the easy-to-use, high-level interface to Plotly, which operates on a variety of types of data and produces easy-to-style figures. With `px.line`, each data point is represented as a vertex (which location is given by the `x` and `y` columns) of a **polyline mark** in 2D space.\n\n\nFor more examples of line plots, see the line and scatter notebook.\n\ndf = px.data.gapminder().query(\"country=='Canada'\")\nfig = px.line(df, x=\"year\", y=\"lifeExp\", title='Life expectancy in Canada')\nfig.show()\n\n\n\n### Line Plots with column encoding color¶\n\ndf = px.data.gapminder().query(\"continent=='Oceania'\")\nfig = px.line(df, x=\"year\", y=\"lifeExp\", color='country')\nfig.show()\n\n\n\n### Line charts in Dash¶\n\n\n\n### Data Order in Line Charts¶\n\n\n\n### Line charts with markers¶\n\nThe `markers` argument can be set to `True` to show markers on lines.\n\n\n\n### Sparklines with Plotly Express¶\n\nSparklines are scatter plots inside subplots, with gridlines, axis lines, and ticks removed.\n\n```\nimport plotly.express as px\ndf = px.data.stocks(indexed=True)\nfig = px.line(df, facet\\_row=\"company\", facet\\_row\\_spacing=0.01, height=200, width=200)\n\n\n# hide and lock down axes\nfig.update\\_xaxes(visible=False, fixedrange=True)\nfig.update\\_yaxes(visible=False, fixedrange=True)\n\n\n# remove facet/subplot labels\nfig.update\\_layout(annotations=[], overwrite=True)\n\n\n# strip down the rest of the plot\nfig.update\\_layout(\n    showlegend=False,\n    plot\\_bgcolor=\"white\",\n    margin=dict(t=10,l=10,b=10,r=10)\n)\n\n\n# disable the modebar for such a small plot\nfig.show(config=dict(displayModeBar=False))\n\n\n\n### Line Plot with go.Scatter¶\n\n\n\n#### Simple Line Plot¶\n\nx = np.arange(10)\n\nfig = go.Figure(data=go.Scatter(x=x, y=x\\*\\*2))\nfig.show()\n\n\n\n#### Line Plot Modes¶\n\n\n# Create traces\nfig = go.Figure()\nfig.add\\_trace(go.Scatter(x=random\\_x, y=random\\_y0,\n                    mode='lines',\n                    name='lines'))\nfig.add\\_trace(go.Scatter(x=random\\_x, y=random\\_y1,\n                    mode='lines+markers',\n                    name='lines+markers'))\nfig.add\\_trace(go.Scatter(x=random\\_x, y=random\\_y2,\n                    mode='markers', name='markers'))\n\n\n\n#### Style Line Plots¶\n\nThis example styles the color and dash of the traces, adds trace names,\nmodifies line width, and adds plot and axes titles.\n\n# Add data\nmonth = ['January', 'February', 'March', 'April', 'May', 'June', 'July',\n         'August', 'September', 'October', 'November', 'December']\nhigh\\_2000 = [32.5, 37.6, 49.9, 53.0, 69.1, 75.4, 76.5, 76.6, 70.7, 60.6, 45.1, 29.3]\nlow\\_2000 = [13.8, 22.3,\n\n==================\n Document 4 \n----------------\n### Scatter, Line, Area and Bar Charts¶\n\n**Read more about scatter plots and discrete color.**\n\n```\nimport plotly.express as px\ndf = px.data.iris()\nfig = px.scatter(df, x=\"sepal\\_width\", y=\"sepal\\_length\", color=\"species\")\nfig.show()\n\n\n**Read more about trendlines and templates and marginal distribution plots.**\n\n```\nimport plotly.express as px\ndf = px.data.iris()\nfig = px.scatter(df, x=\"sepal\\_width\", y=\"sepal\\_length\", color=\"species\", marginal\\_y=\"violin\",\n           marginal\\_x=\"box\", trendline=\"ols\", template=\"simple\\_white\")\nfig.show()\n\n\n**Read more about error bars.**\n\n```\nimport plotly.express as px\ndf = px.data.iris()\ndf[\"e\"] = df[\"sepal\\_width\"]/100\nfig = px.scatter(df, x=\"sepal\\_width\", y=\"sepal\\_length\", color=\"species\", error\\_x=\"e\", error\\_y=\"e\")\nfig.show()\n\n\n**Read more about bar charts.**\n\n```\nimport plotly.express as px\ndf = px.data.tips()\nfig = px.bar(df, x=\"sex\", y=\"total\\_bill\", color=\"smoker\", barmode=\"group\")\nfig.show()\n\n```\nimport plotly.express as px\ndf = px.data.medals\\_long()\n\nfig = px.bar(df, x=\"medal\", y=\"count\", color=\"nation\",\n             pattern\\_shape=\"nation\", pattern\\_shape\\_sequence=[\".\", \"x\", \"+\"])\nfig.show()\n\n\n**Read more about facet plots.**\n\n```\nimport plotly.express as px\ndf = px.data.tips()\nfig = px.bar(df, x=\"sex\", y=\"total\\_bill\", color=\"smoker\", barmode=\"group\", facet\\_row=\"time\", facet\\_col=\"day\",\n       category\\_orders={\"day\": [\"Thur\", \"Fri\", \"Sat\", \"Sun\"], \"time\": [\"Lunch\", \"Dinner\"]})\nfig.show()\n\n\n**Read more about scatterplot matrices (SPLOMs).**\n\n```\nimport plotly.express as px\ndf = px.data.iris()\nfig = px.scatter\\_matrix(df, dimensions=[\"sepal\\_width\", \"sepal\\_length\", \"petal\\_width\", \"petal\\_length\"], color=\"species\")\nfig.show()\n\n\n**Read more about parallel coordinates and parallel categories, as well as continuous color.**\n\n```\nimport plotly.express as px\ndf = px.data.iris()\nfig = px.parallel\\_coordinates(df, color=\"species\\_id\", labels={\"species\\_id\": \"Species\",\n                  \"sepal\\_width\": \"Sepal Width\", \"sepal\\_length\": \"Sepal Length\",\n                  \"petal\\_width\": \"Petal Width\", \"petal\\_length\": \"Petal Length\", },\n                    color\\_continuous\\_scale=px.colors.diverging.Tealrose, color\\_continuous\\_midpoint=2)\nfig.show()\n\n```\nimport plotly.express as px\ndf = px.data.tips()\nfig = px.parallel\\_categories(df, color=\"size\", color\\_continuous\\_scale=px.colors.sequential.Inferno)\nfig.show()\n\n\n**Read more about hover labels.**\n\n```\nimport plotly.express as px\ndf = px.data.gapminder()\nfig = px.scatter(df.query(\"year==2007\"), x=\"gdpPercap\", y=\"lifeExp\", size=\"pop\", color=\"continent\",\n           hover\\_name=\"country\", log\\_x=True, size\\_max=60)\nfig.show()\n\n\n**Read more about animations.**\n\n```\nimport plotly.express as px\ndf = px.data.gapminder()\nfig = px.scatter(df, x=\"gdpPercap\", y=\"lifeExp\", animation\\_frame=\"year\", animation\\_group=\"country\",\n           size=\"pop\", color=\"continent\", hover\\_name=\"country\", facet\\_col=\"continent\",\n           log\\_x=True, size\\_max=45, range\\_x=[100,100000], range\\_y=[25,90])\nfig.show()\n\n\n**Read more about line charts.**\n\n```\nimport plotly.express as px\ndf = px.data.gapminder()\nfig = px.line(df, x=\"year\", y=\"lifeExp\", color=\"continent\", line\\_group=\"country\", hover\\_name=\"country\",\n        line\\_shape=\"spline\", render\\_mode=\"svg\")\nfig.show()\n\n\n**Read more about area charts.**\n\n```\nimport plotly.express as px\ndf = px.data.gapminder()\nfig = px.area(df, x=\"year\", y=\"pop\", color=\"continent\", line\\_group=\"country\")\nfig.show()\n\n\n**Read more about timeline/Gantt charts.**\n\n```\nimport plotly.express as px\nimport pandas as pd\n\ndf = pd.DataFrame([\n    dict(Task=\"Job A\", Start='2009-01-01', Finish='2009-02-28', Resource=\"Alex\"),\n    dict(Task=\"Job B\", Start='2009-03-05', Finish='2009-04-15', Resource=\"Alex\"),\n    dict(Task=\"Job C\", Start='2009-02-20', Finish='2009-05-30', Resource=\"Max\")\n])\n\nfig = px.timeline(df, x\\_start=\"Start\", x\\_end=\"Finish\", y=\"Resource\", color=\"Resource\")\nfig.show()\n\n\n**Read more about funnel charts.**\n\n```\nimport plotly.express as px\ndata = dict(\n    number=[39, 27.4, 20.6, 11, 2],\n    stage=[\"Website visit\", \"Downloads\", \"Potential customers\", \"Requested price\", \"Invoice sent\"])\nfig = px.funnel(data, x='number', y='stage')\nfig.show()\n\n### Part to Whole Charts¶\n\n**Read more about pie charts.**\n\n```\nimport plotly.express as px\ndf = px.data.gapminder().query(\"year == 2007\").query(\"continent == 'Europe'\")\ndf.loc[df['pop'] < 2.e6, 'country'] = 'Other countries' # Represent only large countries\nfig = px.pie(df, values='pop', names='country', title='Population of European continent')\nfig.show()\n\n\n**Read more about sunburst\n\n==================\n Document 5 \n----------------\n# Line plots with Plotly Express¶\n\n```\nimport plotly.express as px\nimport numpy as np\n\nt = np.linspace(0, 2\\*np.pi, 100)\n\nfig = px.line(x=t, y=np.cos(t), labels={'x':'t', 'y':'cos(t)'})\nfig.show()\n\n```\nimport plotly.express as px\ndf = px.data.gapminder().query(\"continent == 'Oceania'\")\nfig = px.line(df, x='year', y='lifeExp', color='country')\nfig.show()\n\n\nThe `markers` argument can be set to `True` to show markers on lines.\n\n```\nimport plotly.express as px\ndf = px.data.gapminder().query(\"continent == 'Oceania'\")\nfig = px.line(df, x='year', y='lifeExp', color='country', markers=True)\nfig.show()\n\n\nThe `symbol` argument can be used to map a data field to the marker symbol. A wide variety of symbols are available.\n\n```\nimport plotly.express as px\ndf = px.data.gapminder().query(\"continent == 'Oceania'\")\nfig = px.line(df, x='year', y='lifeExp', color='country', symbol=\"country\")\nfig.show()\n\n\n### Line plots on Date axes¶\n\nLine plots can be made on using any type of cartesian axis, including linear, logarithmic, categorical or date axes. Line plots on date axes are often called time-series charts.\n\n\nPlotly auto-sets the axis type to a date format when the corresponding data are either ISO-formatted date strings or if they're a date pandas column or datetime NumPy array.\n\ndf = px.data.stocks()\nfig = px.line(df, x='date', y=\"GOOG\")\nfig.show()\n\n\n### Data Order in Scatter and Line Charts¶\n\nPlotly line charts are implemented as connected scatterplots (see below), meaning that the points are plotted and connected with lines **in the order they are provided, with no automatic reordering**.\n\n\nThis makes it possible"
    },
    {
        "instruction": "Create a Python program using the 'plotly' API to generate a choropleth map. The program should define data points for the locations and values, create the map, customize the map title, and display the data points on the map. Finally, save the map as an HTML file and display it on the screen.",
        "api": "plotly",
        "output": "#!pip install plotly\nimport plotly.graph_objects as go\n\n# Define data points for the locations and values\nlocations = ['USA', 'Canada', 'Mexico']\nvalues = [10, 5, 7]\n\n# Create a choropleth map using Plotly Graph Objects\nfig = go.Figure(data=go.Choropleth(locations=locations, z=values, locationmode='country names'))\nfig.update_layout(title=\"Choropleth Map\")\n\n# Save the map as an HTML file\nfig.write_html(\"choropleth_map.html\")\n\n# Display the map on the screen\nfig.show()\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n Plotly Python Open Source Graphing Library Maps\n\n\nPlotly's Python graphing library makes interactive, publication-quality maps online. Examples of how to make maps with Plotly and Mapbox.\n\n\n* Mapbox Choropleth Maps\n\nView Tutorial\n\n* Lines on Mapbox\n\nView Tutorial\n\n* Filled Area on Maps\n\nView Tutorial\n\n* Bubble Maps\n\nView Tutorial\n\n* Mapbox Density Heatmap\n\nView Tutorial\n\n* Lines on Maps\n\nView Tutorial\n\n* Choropleth Maps\n\nView Tutorial\n\n* Mapbox Map Layers\n\nView Tutorial\n\n* Scatter Plots on Mapbox\n\nView Tutorial\n\n* USA County Choropleth Maps\n\nView Tutorial\n\n* Scatter Plots on Maps\n\nView Tutorial\n\n* Map Configuration and Styling\n\nView Tutorial\n\n* Hexbin Mapbox\n\nView Tutorial\n\n\n\n### Maps in Dash\n\n \n \n > Maps\n \n \n > Mapbox Choropleth Maps\n\n\n\n# \n Mapbox Choropleth Maps\n \n in \n Python\n\n\nHow to make a Mapbox Choropleth Map of US Counties in Python with Plotly. \n\n\nA Choropleth Map is a map composed of colored polygons. It is used to represent spatial variations of a quantity. This page documents how to build **tile-map** choropleth maps, but you can also build **outline** choropleth maps using our non-Mapbox trace types.\n\n\nBelow we show how to create Choropleth Maps using either Plotly Express' `px.choropleth_mapbox` function or the lower-level `go.Choroplethmapbox` graph object.\n\n\n\n#### Mapbox Access Tokens and Base Map Configuration¶\n\nTo plot on Mapbox maps with Plotly you *may* need a Mapbox account and a public Mapbox Access Token. See our Mapbox Map Layers documentation for more information.\n\n\n### Introduction: main parameters for choropleth tile maps¶\n\nMaking choropleth Mapbox maps requires two main types of input:\n\n\n1. GeoJSON-formatted geometry information where each feature has either an `id` field or some identifying value in `properties`.\n2. A list of values indexed by\n\n==================\n Document 1 \n----------------\n \n Plotly Express\n \n in \n Python\n\n\nPlotly Express is a terse, consistent, high-level API for creating figures. \n\nThe `plotly.express` module (usually imported as `px`) contains functions that can create entire figures at once, and is referred to as Plotly Express or PX. Plotly Express is a built-in part of the `plotly` library, and is the recommended starting point for creating most common figures. Every Plotly Express function uses graph objects internally and returns a `plotly.graph_objects.Figure` instance. Throughout the `plotly` documentation, you will find the Plotly Express way of building figures at the top of any applicable page, followed by a section on how to use graph objects to build similar figures. Any figure created in a single function call with Plotly Express could be created using graph objects alone, but with between 5 and 100 times more code.\n\n\nPlotly Express provides more than 30 functions for creating different types of figures. The API for these functions was carefully designed to be as consistent and easy to learn as possible, making it easy to switch from a scatter plot to a bar chart to a histogram to a sunburst chart throughout a data exploration session. *Scroll down for a gallery of Plotly Express plots, each made in a single function call.*\n\n\nHere is a talk from the SciPy 2021 conference that gives a good introduction to Plotly Express and Dash:\n\n\nPlotly Express currently includes the following functions:\n\n\n* **Basics**: `scatter`, `line`, `area`, `bar`, `funnel`, `timeline`\n* **Part-of-Whole**: `pie`, `sunburst`, `treemap`, `icicle`, `funnel_area`\n* **1D Distributions**: `histogram`, `box`, `violin`, `strip`, `ecdf`\n* **2D Distributions**: `density_heatmap`, `density_contour`\n* **Matrix or Image Input**: `imshow`\n* **3-Dimensional**: `scatter_3d`, `line_3d`\n* **Multidimensional**: `scatter_matrix`, `parallel_coordinates`, `parallel_categories`\n* **Tile Maps**: `scatter_mapbox`, `line_mapbox`, `choropleth_mapbox`, `density_mapbox`\n* **Outline Maps**: `scatter_geo`, `line_geo`, `choropleth`\n* **Polar Charts**: `scatter_polar`, `line_polar`, `bar_polar`\n* **Ternary Charts**: `scatter_ternary`, `line_ternary`\n\n### High-Level Features¶\n\nThe Plotly Express API in general offers the following features:\n\n\n* **A single entry point into `plotly`**: just `import plotly.express as px` and get access to all the plotting functions, plus built-in demo datasets under `px.data` and built-in color\n\n==================\n Document 2 \n----------------\n Plotly Python Open Source Graphing Library Scientific Charts\n\n\nPlotly's Python graphing library makes interactive, publication-quality graphs online. Examples of how to make scientific charts such as contour plots, heatmaps, dendrograms, polar charts, and ternary plots.\n\n\n* Contour Plots\n\nView Tutorial\n\n* Heatmaps\n\nView Tutorial\n\n* Imshow\n\nView Tutorial\n\n* Ternary Plots\n\nView Tutorial\n\n* Log Plots\n\nView Tutorial\n\n* Dendrograms\n\nView Tutorial\n\n* Annotated Heatmaps\n\nView Tutorial\n\n* Ternary Overlay\n\nView Tutorial\n\n* Parallel Coordinates Plot\n\nView Tutorial\n\n* Quiver Plots\n\nView Tutorial\n\n* Streamline Plots\n\nView Tutorial\n\n* Network Graphs\n\nView Tutorial\n\n* Carpet Plots\n\nView Tutorial\n\n* Carpet Contour Plot\n\nView Tutorial\n\n* Carpet Scatter Plot\n\nView Tutorial\n\n* Polar Charts\n\nView Tutorial\n\n* Radar Charts\n\nView Tutorial\n\n* Ternary contours\n\nView Tutorial\n\n* Wind Rose and Polar Bar Charts\n\nView Tutorial\n\n* Plotly and Datashader\n\nView Tutorial\n\n* Smith Charts\n\nView Tutorial\n\n\n \n \n > Scientific Charts\n \n \n > Contour Plots\n\n\n# \n Contour Plots\n \n in \n Python\n\n\nHow to make Contour plots in Python with Plotly. \n\n\n\n### Basic Contour Plot¶\n\nA 2D contour plot shows the contour lines of a 2D numerical array `z`, i.e. interpolated lines of isovalues of `z`.\n\nfig = go.Figure(data =\n    go.Contour(\n        z=[[10, 10.625, 12.5, 15.625, 20],\n           [5.625, 6.25, 8.125, 11.25, 15.625],\n           [2.5, 3.125, 5., 8.125, 12.5],\n           [0.625, 1.25, 3.125, 6.25, 10.625],\n           [0, 0.625, 2.5, 5.625, 10]]\n    ))\nfig.show()\n\n\n\n### Setting X and Y Coordinates in a Contour Plot¶\n\nfig = go.Figure(data =\n    go.Contour(\n        z=[[10, 10.625, 12.5, 15.625, 20],\n           [5.625, 6.25, 8.125, 11.25, 15.625],\n           [2.5, 3.125, 5., 8.125, 12.5],\n           [0.625, 1.25, 3.125, 6.25, 10.625],\n           [0, 0.625, 2.5, 5.625, 10]],\n        x=[-9, -6, -5 , -3, -1], # horizontal axis\n        y=[0, 1, 4, 5, 7] # vertical axis\n    ))\nfig.show()\n\n\n\n### Colorscale for Contour Plot¶\n\nfig = go.Figure(data =\n     go.Contour(\n        z=[[10, 10.625, 12.5, 15.625, 20],\n           [5.625, 6.25, 8.125, 11.25, 15.625],\n           [2.5, 3.125, 5., 8.125, 12.5],\n           [0.625, 1.25, 3.125, 6.25, 10.625],\n           [0, 0.625, 2.5, 5.625, 10]],\n        colorscale='Electric',\n    ))\nfig.show()\n\n\n\n### Customizing Size and Range of a Contour Plot's Contours¶\n\nfig = go.Figure(data =\n    go.Contour(\n        z=[[10, 10.625, 12.5, 15.625, 20],\n           [5.625, 6.25, 8.125, 11.25, 15.625],\n           [2.5, 3.125, 5., 8.125, 12.5],\n           [0.625, 1.25, 3.125, 6.25, 10.625],\n           [0, 0.625, 2.5, 5.625, 10]],\n        colorscale='Hot',\n        contours=dict(\n            start=0,\n            end=8,\n            size=2,\n        ),\n    ))\n\n\n\n### Customizing Spacing Between X and Y Axis Ticks¶\n\nfig = go.Figure(data =\n    go.Contour(\n        z= [[10, 10.625, 12.5, 15.625, 20],\n              [5.625, 6.25, 8.125, 11.25, 15.625],\n              [2.5, 3.125, 5., 8.125, 12.5],\n              [0.625, 1.25, 3.125, 6.25, 10.625],\n              [0, 0.625, 2.5, 5.625, 10]],\n        dx=10,\n        x0=5,\n        dy=10,\n        y0=10,\n    )\n)\n\n\n### Connect the Gaps Between None Values in the Z Matrix¶\n\n```\nimport plotly.graph\\_objs as go\nfrom plotly.subplots import make\\_subplots\n\nfig = make\\_subplots(rows=2, cols=2, subplot\\_titles=('connectgaps = False',\n\n==================\n Document 3 \n----------------\n \n Heatmaps\n \n in \n Python\n\n\nHow to make Heatmaps in Python with Plotly. \n\n\nThe term \"heatmap\" usually refers to a cartesian plot with data visualized as colored rectangular tiles, which is the subject of this page. It is also sometimes used to refer to actual maps with density data displayed as color intensity.\n\n\nPlotly supports two different types of colored-tile heatmaps:\n\n\n1. **Matrix Heatmaps** accept a 2-dimensional matrix or array of data and visualizes it directly. This type of heatmap is the subject of this page.\n2. **Density Heatmaps** accept data as a list and visualizes aggregated quantities like counts or sums of this data. Please refer to the 2D Histogram documentation for this kind of figure.\n\n### Heatmaps with Plotly Express¶\n\nPlotly Express is the easy-to-use, high-level interface to Plotly, which operates on a variety of types of data and produces easy-to-style figures. With `px.imshow`, each value of the input array or data frame is represented as\n\n==================\n Document 4 \n----------------\n Displaying Figures¶\n\nPlotly's Python graphing library, `plotly.py`, gives you a wide range of options for how and where to display your figures.\n\n\nIn general, there are five different approaches you can take in order to display `plotly` figures:\n\n\n1. Using the `renderers` framework in the context of a script or notebook (the main topic of this page)\n2. Using Dash in a web app context\n3. Using a `FigureWidget` rather than a `Figure` in an `ipywidgets` context\n4. By exporting to an HTML file and loading that file in a browser immediately or later\n5. By rendering the figure to a static image file using Kaleido such as PNG, JPEG, SVG, PDF or EPS and loading the resulting file in any viewer\n\n\nEach of the first three approaches is discussed below.\n\n### Displaying Figures Using The `renderers` Framework¶\n\nThe renderers framework is a flexible approach for displaying `plotly.py` figures in a variety of contexts. To display a figure using the renderers framework, you call the `.show()` method on a graph object figure,\n\n==================\n Document 5 \n----------------\n\n\n\n \n Python\n \n \n \n\n \n\n \n \n > Is Plotly for Python Free?\n\nSuggest an edit to this page\n\n\n# \n Is Plotly for Python Free?\n\n\nPlotly's open-source graphing libraries are free to use, work offline and don't require any account registration. Plotly also has commercial offerings, such as Dash Enterprise and Chart Studio Enterprise. \n\n\n  \n\nNew to Plotly?\nPlotly is a free and open-source graphing library for Python. We recommend you read our Getting Started guide for the latest installation or upgrade instructions, then move on to our Plotly Fundamentals tutorials or dive straight in to some Basic Charts tutorials.\n\n\n#### Is Plotly for Python Free?\n\n\n    **Yes.**   Plotly for Python is free and open-source software, licensed under the **MIT license**. It costs nothing to install and use. You can view the source, report issues or contribute using our Github repository.\n\n\n\n#### Can I use Plotly for Python without signing up to any service?\n\n\n    **Yes.**   You can use Plotly for Python to make, view, and distribute charts and maps without registering for any service,\nobtaining any token, or creating any account. The one exception is that to view tile maps\nwhich use tiles from the Mapbox service (which is optional, as you can use other tile servers), you will need to have a Mapbox token.\n\n\n\n#### Can I use Plotly for Python offline, without being connected to the internet?\n\n\n    **Yes.**   You can use Plotly for Python to make, view, and distribute graphics totally offline. The one exception is that to view tile maps\nwhich use tiles from a cloud-hosted service, such as Open Street Maps or Mapbox, you will need a connection to that service. You can view tile maps totally offline if you run your own local tile server and use its tiles.\n\n\n\n#### Is Dash free?\n\n\n    **Yes.**   Plotly's Dash analytics application framework is also free and open-source software, licensed under the **MIT license**.\n\n\n\n#### Does Plotly also make commercial software?\n\n\n    **Yes.**   Plotly has commercial offerings such as Dash Enterprise and Chart Studio Enterprise.\n\n\n\n \n Python\n \n \n \n\n \n \n > Fundamentals\n\n# Plotly Python Open Source Graphing Library Fundamentals\n\n\nPlotly's Python graphing library makes interactive, publication-quality graphs online. Tutorials and tips about fundamental features of Plotly's python API.\n\n  \n\nDeploy Python AI Dash apps on private Kubernetes clusters: \n Pricing\n  | \n Demo"
    },
    {
        "instruction": "Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'country' column is 'Canada', select specific columns ('name', 'age', 'country'), and calculate the maximum age. Finally, display the resulting maximum age.",
        "api": "polars",
        "output": "#!pip install polars\nimport polars as pl\n\n# Load a sample CSV file into a Polars DataFrame\ndata = pl.read_csv(\"sample_data.csv\")\n\n# Filter rows\nfiltered_data = data.filter(data['country'] == 'Canada')\n\n# Select specific columns\nselected_columns = filtered_data.select(['name', 'age', 'country'])\n\n# Calculate the maximum age\nmax_age = selected_columns['age'].max()\n\n# Display the resulting maximum age\nprint(max_age)\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n# polars.read\\_csv#\n\n\npolars.read\\_csv(\n\n*source: str | TextIO | BytesIO | Path | BinaryIO | bytes*,\n*\\**,\n*has\\_header: bool = True*,\n*columns: Sequence[int] | Sequence[str] | None = None*,\n*new\\_columns: Sequence[str] | None = None*,\n*separator: str = ','*,\n*comment\\_char: str | None = None*,\n*quote\\_char: str | None =\n\n==================\n Document 1 \n----------------\n polars.scan\\_csv#\n\n\npolars.scan\\_csv(\n\n*source: str | Path*,\n*\\**,\n*has\\_header: bool = True*,\n*separator: str = ','*,\n*comment\\_char: str | None = None*,\n*quote\\_char: str | None = '\"'*,\n*skip\\_rows: int = 0*,\n*dtypes: SchemaDict | Sequence[PolarsDataType] | None = None*,\n*schema: SchemaDict | None = None*,\n*null\\_values: str | Sequence[str] | dict[str, str] | None = None*,\n*missing\\_utf8\\_is\\_empty\\_string: bool = False*,\n*ignore\\_errors: bool = False*,\n*cache: bool = True*,\n*with\\_column\\_names: Callable[[list[str]], list[str]] | None = None*,\n*infer\\_schema\\_length: int | None = 100*,\n*n\\_rows: int | None = None*,\n*encoding: CsvEncoding = 'utf8'*,\n*low\\_memory: bool = False*,\n*rechunk: bool = True*,\n*skip\\_rows\\_after\\_header: int = 0*,\n*row\\_count\\_name: str | None = None*,\n*row\\_count\\_offset: int = 0*,\n*try\\_parse\\_dates: bool = False*,\n*eol\\_char: str = '\\n'*,\n*new\\_columns: Sequence[str] | None = None*,\n*raise\\_if\\_empty: bool = True*,\n*truncate\\_ragged\\_lines: bool = False*,\n\n) → LazyFrame[source]#\nLazily read from a CSV file or multiple files via glob patterns.\n\n\nThis allows the query optimizer to push down predicates and\nprojections to the scan level, thereby potentially reducing\nmemory overhead.\n\n**source**Path to a file.\n\n**skip\\_rows**Start reading after `skip\\_rows` lines. The header will be parsed at this\noffset.\n\n**dtypes**Overwrite dtypes during inference; should be a {colname:dtype,} dict or,\nif providing a list of strings to `new\\_columns`, a list of dtypes of\nthe same length.\n\n**cache**Cache the result after reading.\n\n**with\\_column\\_names**Apply a function over the column names just in time (when they are determined);\nthis function will receive (and should return) a list of column names.\n\n**n\\_rows**Stop reading from CSV file after reading `n\\_rows`.\n\n**encoding**{‘utf8’, ‘utf8-lossy’}Lossy means that invalid utf8 values are replaced with `�`\ncharacters. Defaults to “utf8”.\n\n**low\\_memory**Reduce memory usage in expense of performance.\n\n**rechunk**Reallocate to contiguous memory when all chunks/ files are parsed.\n\n**try\\_parse\\_dates**Try to automatically parse dates. Most ISO8601-like formats\ncan be inferred, as well as a handful of others. If this does not succeed,\nthe column remains of data type `pl.Utf8`.\n\n**eol\\_char**Single byte end of line character\n\n**new\\_columns**Provide an explicit list of string column names to use (for example, when\nscanning a headerless CSV file). Note that unlike `read\\_csv` it is considered\nan error to provide fewer column names than there are columns in the file.\n\n**raise\\_if\\_empty**When there is no data in the source,``NoDataError`` is raised. If this parameter\nis set to False, an empty LazyFrame (with no columns) is returned instead.\n\nLazyFrame\n\n`read\\_csv`Read a CSV file into a DataFrame.\n\n```\n>>> import pathlib\n>>>\n>>> (\n...     pl.scan\\_csv(\"my\\_long\\_file.csv\")  # lazy, doesn't do a thing\n...     .select(\n...         [\"a\", \"c\"]\n...     )  # select only 2 columns (other columns will not be read)\n...     .filter(\n...         pl.col(\"a\") > 10\n...     )  # the filter is pushed down the scan, so less data is read into memory\n...     .fetch(100)  # pushed a limit of 100 rows to the scan level\n... )  \n\n\nWe can use `with\\_column\\_names` to modify the header before scanning:\n\n```\n>>> df = pl.DataFrame(\n...     {\"BrEeZaH\": [1, 2, 3, 4], \"LaNgUaGe\": [\"is\", \"hard\", \"to\", \"read\"]}\n... )\n>>> path: pathlib.Path = dirpath / \"mydf.csv\"\n>>> df.write\\_csv(path)\n>>> pl.scan\\_csv(\n...     path, with\\_column\\_names=lambda cols: [col.lower() for col in cols]\n... ).collect()\nshape: (4, 2)\n┌─────────┬──────────┐\n│ breezah ┆ language │\n│ --- ┆ --- │\n│ i64 ┆ str │\n╞═════════╪══════════╡\n│ 1 ┆ is │\n│ 2 ┆ hard │\n│ 3 ┆ to │\n│ 4 ┆ read │\n└─────────┴──────────┘\n\n\nYou can also simply replace column names (or provide them if the file has none)\nby passing a list of new column names to the `new\\_columns` parameter:\n\n```\n>>> df.write\\_csv(path)\n>>> pl.scan\\_csv(\n...     path,\n...     new\\_columns=[\"idx\", \"txt\"],\n...     dtypes=[pl.UInt16, pl.Utf8],\n... ).collect()\nshape: (4, 2)\n┌─────┬──────┐\n│ idx ┆ txt │\n│ --- ┆ --- │\n│ u16 ┆ str │\n╞═════╪══════╡\n│ 1 ┆ is │\n│ 2 ┆ hard │\n│ 3 ┆ to │\n│ 4 ┆ read │\n└─────┴──────┘\n\n# polars.DataFrame.write\\_csv#\n\n\nDataFrame.write\\_csv(\n\n*file: None = None*,\n*\\**,\n*has\\_header: bool = True*,\n*separator: str = ','*,\n*line\\_terminator: str = '\\n'*,\n*quote: str = '\"'*,\n*batch\\_size: int = 1024*,\n*datetime\\_format: str | None = None*,\n*date\\_format: str | None = None*,\n*time\\_format: str | None = None*,\n*float\\_precision: int | None = None*,\n*null\\_value:\n\n==================\n Document 2 \n----------------\n polars.scan\\_pyarrow\\_dataset#\n\n\npolars.scan\\_pyarrow\\_dataset(\n\n*source: pa.dataset.Dataset*,\n*\\**,\n*allow\\_pyarrow\\_filter: bool = True*,\n*batch\\_size: int | None = None*,\n\n) → LazyFrame[source]#\nScan a pyarrow dataset.\n\n\nThis can be useful to connect to cloud or partitioned datasets.\n\n**source**Pyarrow dataset to scan.\n\n**allow\\_pyarrow\\_filter**Allow predicates to be pushed down to pyarrow. This can lead to different\nresults if comparisons are done with null values as pyarrow handles this\ndifferent than polars does.\n\n**batch\\_size**The maximum row count for scanned pyarrow record batches.\n\nWarning\n\n\nThis API is experimental and may change without it being considered a breaking\nchange.\n\n\nWhen using partitioning, the appropriate `partitioning` option must be set on\n`pyarrow.dataset.dataset` before passing to Polars or the partitioned-on column(s)\nmay not get passed to Polars.\n\n```\n>>> import pyarrow.dataset as ds\n>>> dset = ds.dataset(\"s3://my-partitioned-folder/\", format=\"ipc\")  \n>>> (\n...     pl.scan\\_pyarrow\\_dataset(dset)\n...     .filter(\"bools\")\n...     .select([\"bools\", \"floats\", \"date\"])\n...     .collect()\n... )  \nshape: (1, 3)\n┌───────┬────────┬────────────┐\n│ bools ┆ floats ┆ date │\n│ --- ┆ --- ┆ --- │\n│ bool ┆ f64 ┆ date │\n╞═══════╪════════╪════════════╡\n│ true ┆ 2.0 ┆ 1970-05-04 │\n└───────┴────────┴────────────┘\n\n\n# polars.io.csv.batched\\_reader.BatchedCsvReader.next\\_batches#\n\n\nBatchedCsvReader.next\\_batches(*n: int*) → list[DataFrame] | None[source]#\nRead `n` batches from the reader.\n\n\nThe `n` chunks will be parallelized over the\navailable threads.\n\n**n**Number of chunks to fetch.\nThis is ideally >= number of threads\n\nlist of DataFrames\n\n```\n>>> reader = pl.read\\_csv\\_batched(\n...     \"./tpch/tables\\_scale\\_100/lineitem.tbl\",\n...     separator=\"|\",\n...     try\\_parse\\_dates=True,\n... )  \n>>> reader.next\\_batches(5)  \n\n\n# polars.api.register\\_expr\\_namespace#\n\n\npolars.api.register\\_expr\\_namespace(\n\n*name: str*,\n\n) → Callable[[type[NS]], type[NS]][source]#\nDecorator for registering custom functionality with a polars Expr.\n\n**name**Name under which the functionality will be accessed.\n\n`register\\_dataframe\\_namespace`Register functionality on a DataFrame.\n\n`register\\_lazyframe\\_namespace`Register functionality on a LazyFrame.\n\n`register\\_series\\_namespace`Register functionality on a Series.\n\n```\n>>> @pl.api.register\\_expr\\_namespace(\"pow\\_n\")\n... class PowersOfN:\n...\n\n==================\n Document 3 \n----------------\n polars.testing.parametric.dataframes#\n\n\npolars.testing.parametric.dataframes(\n\n*cols: int | column | Sequence[column] | None = None*,\n*\\**,\n*lazy: Literal[False] = False*,\n*min\\_cols: int | None = 0*,\n*max\\_cols: int | None = MAX\\_COLS*,\n*size: int | None = None*,\n*min\\_size: int | None = 0*,\n*max\\_size: int | None = MAX\\_DATA\\_SIZE*,\n*chunked: bool | None = None*,\n*include\\_cols: Sequence[column] | column | None = None*,\n*null\\_probability: float | dict[str, float] = 0.0*,\n*allow\\_infinities: bool = True*,\n*allowed\\_dtypes: Collection[PolarsDataType] | PolarsDataType | None = None*,\n*excluded\\_dtypes: Collection[PolarsDataType] | PolarsDataType | None = None*,\n\n) → SearchStrategy[DataFrame][source]#\n\npolars.testing.parametric.dataframes(\n\n*cols: int | column | Sequence[column] | None = None*,\n*\\**,\n*lazy: Literal[True]*,\n*min\\_cols: int | None = 0*,\n*max\\_cols: int | None = MAX\\_COLS*,\n*size: int | None = None*,\n*min\\_size: int | None = 0*,\n*max\\_size: int | None = MAX\\_DATA\\_SIZE*,\n*chunked: bool | None = None*,\n*include\\_cols: Sequence[column] | column | None = None*,\n*null\\_probability: float | dict[str, float] = 0.0*,\n*allow\\_infinities: bool = True*,\n*allowed\\_dtypes: Collection[PolarsDataType] | PolarsDataType | None = None*,\n*excluded\\_dtypes: Collection[PolarsDataType] | PolarsDataType | None = None*,\n\n) → SearchStrategy[LazyFrame]\nHypothesis strategy for producing polars DataFrames or LazyFrames.\n\n**cols**{int, columns}, optionalinteger number of columns to create, or a sequence of column objects\nthat describe the desired DataFrame column data.\n\n**lazy**bool, optionalproduce a LazyFrame instead of a DataFrame.\n\n**min\\_cols**int, optionalif not passing an exact size, can set a minimum here (defaults to 0).\n\n**max\\_cols**int, optionalif not passing an exact size, can set a maximum value here (defaults to\nMAX\\_COLS).\n\n**size**int, optionalif set, will create a DataFrame of exactly this size (and ignore\nthe min\\_size/max\\_size len params).\n\n**min\\_size**int, optionalif not passing an exact size, set the minimum number of rows in the\nDataFrame.\n\n**max\\_size**int, optionalif not passing an exact size, set the maximum number of rows in the\nDataFrame.\n\n**chunked**bool, optionalensure that DataFrames with more than row have `n\\_chunks` > 1. if\nomitted, chunking will be randomised at the level of individual Series.\n\n**include\\_cols**[column], optionala list of column objects to include in the generated DataFrame. note that\nexplicitly provided columns are appended onto the list of existing columns\n(if any present).\n\n**null\\_probability**{float, dict[str,float]}, optionalpercentage chance (expressed between 0.0 => 1.0) that a generated value is\nNone. this is applied independently of any None values generated by the\nunderlying strategy, and can be applied either on a per-column basis (if\ngiven as a `{col:pct}` dict), or globally. if null\\_probability is defined\non a column, it takes precedence over the global value.\n\n**allow\\_infinities**bool, optionaloptionally disallow generation of +/-inf values for floating-point dtypes.\n\n**allowed\\_dtypes**{list,set}, optionalwhen automatically generating data, allow only these dtypes.\n\n**excluded\\_dtypes**{list,set}, optionalwhen automatically generating data, exclude these dtypes.\n\n\nIn actual usage this is deployed as a unit test decorator, providing a strategy\nthat generates DataFrames or LazyFrames with the given characteristics for\nthe unit test. While developing a strategy/test, it can also be useful to\ncall .example() directly on a given strategy to see concrete instances of\nthe generated data.\n\n\nUse column or columns to specify the schema of the types of DataFrame to\ngenerate. Note: in actual use the strategy is applied as a test decorator, not\nused standalone.\n\n```\n>>> from polars.testing.parametric import column, columns, dataframes\n>>> from hypothesis import given\n\n\nGenerate arbitrary DataFrames (as part of a unit test):\n\n```\n>>> @given(df=dataframes())\n... def test\\_repr(df: pl.DataFrame) -> None:\n...     assert isinstance(repr(df), str)\n\n\nGenerate LazyFrames with at least 1 column, random dtypes, and specific size:\n\n```\n>>> dfs = dataframes(min\\_cols=1, max\\_size=5, lazy=True)\n>>> dfs.example()  \n<polars.LazyFrame object at 0x11F561580>\n\n\nGenerate DataFrames with known colnames, random dtypes (per test, not per-frame):\n\n```\n>>> dfs = dataframes(columns([\"x\", \"y\", \"z\"]))\n>>> dfs.example()  \nshape: (3, 3)\n┌────────────┬───────┬────────────────────────────┐\n│ x ┆ y ┆ z │\n│ --- ┆ --- ┆ --- │\n│ date ┆ u16 ┆ datetime[μs] │\n╞════════════╪═══════╪════════════════════════════╡\n│ 0565-08-12 ┆ 34715 ┆ 5844-09-20 00:33:31.076854 │\n│ 3382-10-17 ┆ 48662 ┆ 7540-01-29 11:20:14.836271 │\n│ 4063-06-17 ┆ 39092 ┆ 1889-05-05 13:25:41.874455 │\n└────────────┴───────┴────────────────────────────┘\n\n\nGenerate frames with explicitly named/typed columns and a fixed size:\n\n```\n>>> dfs = dataframes(\n...     [\n...         column(\"x\", dtype=pl.Int32),\n...         column(\"y\", dtype=pl.Float64),\n...     ],\n...     size=2,\n... )\n>>> dfs.example()  \nshape: (2, 2)\n┌───────────┬────────────┐\n│ x ┆ y │\n│ --- ┆ --- │\n│ i32 ┆ f64 │\n╞═══════════╪════════════╡\n│ -15836 ┆ 1.1755e-38 │\n│ 575050513 ┆ NaN │\n└───────────┴────────────┘\n\n# polars.testing.parametric.series#\n\n\npolars.testing.parametric.series(\n\n*\\**,\n*name: str | SearchStrategy[str] | None = None*,\n*dtype: PolarsDataType | None = None*,\n*size: int | None = None*,\n*min\\_size: int | None = 0*,\n*max\\_size: int | None = 10*,\n*strategy: SearchStrategy[object] | None = None*,\n*null\\_probability: float = 0.0*,\n*allow\\_infinities: bool = True*,\n*unique:\n\n==================\n Document 4 \n----------------\n polars.testing.parametric.create\\_list\\_strategy#\n\n\npolars.testing.parametric.create\\_list\\_strategy(\n\n*inner\\_dtype: PolarsDataType | None*,\n*\\**,\n*select\\_from: Sequence[Any] | None = None*,\n*size: int | None = None*,\n*min\\_size: int | None = None*,\n*max\\_size: int | None = None*,\n*unique: bool = False*,\n\n) → SearchStrategy[list[Any]][source]#\nHypothesis strategy for producing polars List data.\n\n**inner\\_dtype**PolarsDataTypetype of the inner list elements (can also be another List).\n\n**select\\_from**list, optionalrandomly select the innermost values from this list (otherwise\nthe default strategy associated with the innermost dtype is used).\n\n**size**int, optionalif set, generated lists will be of exactly this size (and\nignore the min\\_size/max\\_size params).\n\n**min\\_size**int, optionalset the minimum size of the generated lists (default: 0 if unset).\n\n**max\\_size**int, optionalset the maximum size of the generated lists (default: 3 if\nmin\\_size is unset or zero, otherwise 2x min\\_size).\n\n**unique**bool, optionalensure that the generated lists contain unique values.\n\n\nCreate a strategy that generates a list of i32 values:\n\n```\n>>> lst = create\\_list\\_strategy(inner\\_dtype=pl.Int32)\n>>> lst.example()  \n[-11330, 24030, 116]\n\n\nCreate a strategy that generates lists of lists of specific strings:\n\n```\n>>> lst = create\\_list\\_strategy(\n...     inner\\_dtype=pl.List(pl.Utf8),\n...     select\\_from=[\"xx\", \"yy\", \"zz\"],\n... )\n>>> lst.example()  \n[['yy', 'xx'], [], ['zz']]\n\n\nCreate a UInt8 dtype strategy as a hypothesis composite that generates\npairs of small int values where the first is always <= the second:\n\n```\n>>> from hypothesis.strategies import composite\n>>>\n>>> @composite\n... def uint8\\_pairs(draw, uints=create\\_list\\_strategy(pl.UInt8, size=2)):\n...     pairs = list(zip(draw(uints), draw(uints)))\n...     return [sorted(ints) for ints in pairs]\n...\n>>> uint8\\_pairs().example()  \n[(12, 22), (15, 131)]\n>>> uint8\\_pairs().example()  \n[(59, 176), (149, 149)]\n\n\n# polars.testing.parametric.load\\_profile#\n\n\npolars.testing.parametric.load\\_profile(\n\n*profile: Literal['fast', 'balanced', 'expensive'] | int = 'fast'*,\n*\\**,\n*set\\_environment: bool = False*,\n\n) → None[source]#\nLoad a named (or custom) hypothesis profile for use with the parametric tests.\n\n**profile**{str, int}, optionalName of the profile to load; one of “fast”, “balanced”, “expensive”, or\nthe integer number of iterations to run (which will create and register\na custom profile with that value).\n\n**set\\_environment**bool, default FalseIf True, also set the environment variable `POLARS\\_HYPOTHESIS\\_PROFILE`\nto the given profile name/value.\n\n```\n>>> # load a custom profile that will run with 1500 iterations\n>>> from polars.testing.parametric.profiles import load\\_profile\n>>> load\\_profile(1500)\n\n\n\n# polars.testing.parametric.set\\_profile#\n\n\npolars.testing.parametric.set\\_profile(*profile: Literal['fast', 'balanced', 'expensive'] | int*) → None[source]#\nSet the env var `POLARS\\_HYPOTHESIS\\_PROFILE` to the given profile name/value.\n\n```\n>>> # prefer the 'balanced' profile for running parametric tests\n>>> from polars.testing.parametric.profiles import set\\_profile\n>>> set\\_profile(\"balanced\")\n\n\n\n# polars.build\\_info#\n\n\npolars.build\\_info() → dict[str, Any][source]#\nReturn a dict with Polars build information.\n\n\nIf Polars was compiled with “build\\_info” feature gate return the full build info,\notherwise only version is included. The full build information dict contains\nthe following keys [‘build’, ‘info-time’, ‘dependencies’, ‘features’, ‘host’,\n‘target’, ‘git’, ‘version’].\n\n\n\n# polars.get\\_index\\_type#\n\n\npolars.get\\_index\\_type() → DataTypeClass[source]#\nGet the datatype used for Polars indexing.\n\nDataType`UInt32` in regular Polars, `UInt64` in bigidx Polars.\n\n\n\n# polars.show\\_versions#\n\n\npolars.show\\_versions() → None[source]#\nPrint out version of Polars and dependencies to stdout.\n\n```\n>>> pl.show\\_versions()  \n--------Version info---------\nPolars: 0.17.11\nIndex type: UInt32\nPlatform: Linux-5.15.90.1-microsoft-standard-WSL2-x86\\_64-with-glibc2.35\nPython: 3.11.3 (main, Apr 15 2023, 14:44:51) [GCC 11.3.0]\n\\b\n----Optional dependencies----\nnumpy: 1.24.2\npandas: 2.0.0\npyarrow: 11.0.0\nconnectorx: <not installed>\ndeltalake: 0.8.1\nfsspec: 2023.4.0\nmatplotlib: 3.7.1\nxlsx2csv: 0.8.1\nxlsxwriter: 3.1.0\n\n\n\n# polars.threadpool\\_size#\n\n\npolars.threadpool\\_size() → int[source]#\nGet the number of threads in the Polars thread pool.\n\n\nThe threadpool size can be overridden by setting the `POLARS\\_MAX\\_THREADS`\nenvironment variable before process start. (The thread pool is not behind a\nlock, so it cannot be modified once set). A reasonable use-case for this might\nbe temporarily setting max threads to a low value before importing polars in a\npyspark UDF or similar context. Otherwise, it is strongly recommended not to\noverride this value as it will be set automatically by the engine.\n\n\n# polars.SQLContext.execute#\n\n\nSQLContext.execute(*query: str*, *eager: None = None*) → DataFrame[source]#\n\nSQLContext.execute(\n\n*query: str*,\n*eager: Literal[False]*,\n\n) → LazyFrame\n\n*query: str*,\n*eager: Literal[True]*,\n\nSQLContext.execute(*query: str*, *eager: None = None*) → LazyFrame\n\n) → DataFrame\nParse the given SQL query and execute it against the registered frame data.\n\n**query**A valid string SQL query.\n\n**eager**Apply"
    },
    {
        "instruction": "Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'country' column is 'USA', select specific columns ('name', 'age', 'country'), and calculate the average age. Finally, display the resulting average age.",
        "api": "polars",
        "output": "#!pip install polars\nimport polars as pl\n\n# Load a sample CSV file into a Polars DataFrame\ndata = pl.read_csv(\"sample_data.csv\")\n\n# Filter rows\nfiltered_data = data.filter(data['country'] == 'USA')\n\n# Select specific columns\nselected_columns = filtered_data.select(['name', 'age', 'country'])\n\n# Calculate the average age\naverage_age = selected_columns['age'].mean()\n\n# Display the resulting average age\nprint(average_age)\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n# polars.read\\_csv#\n\n\npolars.read\\_csv(\n\n*source: str | TextIO | BytesIO | Path | BinaryIO | bytes*,\n*\\**,\n*has\\_header: bool = True*,\n*columns: Sequence[int] | Sequence[str] | None = None*,\n*new\\_columns: Sequence[str] | None = None*,\n*separator: str = ','*,\n*comment\\_char: str | None = None*,\n*quote\\_char: str | None =\n\n==================\n Document 1 \n----------------\n polars.scan\\_csv#\n\n\npolars.scan\\_csv(\n\n*source: str | Path*,\n*\\**,\n*has\\_header: bool = True*,\n*separator: str = ','*,\n*comment\\_char: str | None = None*,\n*quote\\_char: str | None = '\"'*,\n*skip\\_rows: int = 0*,\n*dtypes: SchemaDict | Sequence[PolarsDataType] | None = None*,\n*schema: SchemaDict | None = None*,\n*null\\_values: str | Sequence[str] | dict[str, str] | None = None*,\n*missing\\_utf8\\_is\\_empty\\_string: bool = False*,\n*ignore\\_errors: bool = False*,\n*cache: bool = True*,\n*with\\_column\\_names: Callable[[list[str]], list[str]] | None = None*,\n*infer\\_schema\\_length: int | None = 100*,\n*n\\_rows: int | None = None*,\n*encoding: CsvEncoding = 'utf8'*,\n*low\\_memory: bool = False*,\n*rechunk: bool = True*,\n*skip\\_rows\\_after\\_header: int = 0*,\n*row\\_count\\_name: str | None = None*,\n*row\\_count\\_offset: int = 0*,\n*try\\_parse\\_dates: bool = False*,\n*eol\\_char: str = '\\n'*,\n*new\\_columns: Sequence[str] | None = None*,\n*raise\\_if\\_empty: bool = True*,\n*truncate\\_ragged\\_lines: bool = False*,\n\n) → LazyFrame[source]#\nLazily read from a CSV file or multiple files via glob patterns.\n\n\nThis allows the query optimizer to push down predicates and\nprojections to the scan level, thereby potentially reducing\nmemory overhead.\n\n**source**Path to a file.\n\n**skip\\_rows**Start reading after `skip\\_rows` lines. The header will be parsed at this\noffset.\n\n**dtypes**Overwrite dtypes during inference; should be a {colname:dtype,} dict or,\nif providing a list of strings to `new\\_columns`, a list of dtypes of\nthe same length.\n\n**cache**Cache the result after reading.\n\n**with\\_column\\_names**Apply a function over the column names just in time (when they are determined);\nthis function will receive (and should return) a list of column names.\n\n**n\\_rows**Stop reading from CSV file after reading `n\\_rows`.\n\n**encoding**{‘utf8’, ‘utf8-lossy’}Lossy means that invalid utf8 values are replaced with `�`\ncharacters. Defaults to “utf8”.\n\n**low\\_memory**Reduce memory usage in expense of performance.\n\n**rechunk**Reallocate to contiguous memory when all chunks/ files are parsed.\n\n**try\\_parse\\_dates**Try to automatically parse dates. Most ISO8601-like formats\ncan be inferred, as well as a handful of others. If this does not succeed,\nthe column remains of data type `pl.Utf8`.\n\n**eol\\_char**Single byte end of line character\n\n**new\\_columns**Provide an explicit list of string column names to use (for example, when\nscanning a headerless CSV file). Note that unlike `read\\_csv` it is considered\nan error to provide fewer column names than there are columns in the file.\n\n**raise\\_if\\_empty**When there is no data in the source,``NoDataError`` is raised. If this parameter\nis set to False, an empty LazyFrame (with no columns) is returned instead.\n\nLazyFrame\n\n`read\\_csv`Read a CSV file into a DataFrame.\n\n```\n>>> import pathlib\n>>>\n>>> (\n...     pl.scan\\_csv(\"my\\_long\\_file.csv\")  # lazy, doesn't do a thing\n...     .select(\n...         [\"a\", \"c\"]\n...     )  # select only 2 columns (other columns will not be read)\n...     .filter(\n...         pl.col(\"a\") > 10\n...     )  # the filter is pushed down the scan, so less data is read into memory\n...     .fetch(100)  # pushed a limit of 100 rows to the scan level\n... )  \n\n\nWe can use `with\\_column\\_names` to modify the header before scanning:\n\n```\n>>> df = pl.DataFrame(\n...     {\"BrEeZaH\": [1, 2, 3, 4], \"LaNgUaGe\": [\"is\", \"hard\", \"to\", \"read\"]}\n... )\n>>> path: pathlib.Path = dirpath / \"mydf.csv\"\n>>> df.write\\_csv(path)\n>>> pl.scan\\_csv(\n...     path, with\\_column\\_names=lambda cols: [col.lower() for col in cols]\n... ).collect()\nshape: (4, 2)\n┌─────────┬──────────┐\n│ breezah ┆ language │\n│ --- ┆ --- │\n│ i64 ┆ str │\n╞═════════╪══════════╡\n│ 1 ┆ is │\n│ 2 ┆ hard │\n│ 3 ┆ to │\n│ 4 ┆ read │\n└─────────┴──────────┘\n\n\nYou can also simply replace column names (or provide them if the file has none)\nby passing a list of new column names to the `new\\_columns` parameter:\n\n```\n>>> df.write\\_csv(path)\n>>> pl.scan\\_csv(\n...     path,\n...     new\\_columns=[\"idx\", \"txt\"],\n...     dtypes=[pl.UInt16, pl.Utf8],\n... ).collect()\nshape: (4, 2)\n┌─────┬──────┐\n│ idx ┆ txt │\n│ --- ┆ --- │\n│ u16 ┆ str │\n╞═════╪══════╡\n│ 1 ┆ is │\n│ 2 ┆ hard │\n│ 3 ┆ to │\n│ 4 ┆ read │\n└─────┴──────┘\n\n# polars.DataFrame.write\\_csv#\n\n\nDataFrame.write\\_csv(\n\n*file: None = None*,\n*\\**,\n*has\\_header: bool = True*,\n*separator: str = ','*,\n*line\\_terminator: str = '\\n'*,\n*quote: str = '\"'*,\n*batch\\_size: int = 1024*,\n*datetime\\_format: str | None = None*,\n*date\\_format: str | None = None*,\n*time\\_format: str | None = None*,\n*float\\_precision: int | None = None*,\n*null\\_value:\n\n==================\n Document 2 \n----------------\n polars.read\\_json#\n\n\npolars.read\\_json(\n\n*source: str | Path | IOBase | bytes*,\n*\\**,\n*schema: SchemaDefinition | None = None*,\n*schema\\_overrides: SchemaDefinition | None = None*,\n\n) → DataFrame[source]#\nRead into a DataFrame from a JSON file.\n\n**schema**Sequence of str, (str,DataType) pairs, or a {str:DataType,} dictThe DataFrame schema may be declared in several ways:\n\n\n* As a dict of {name:type} pairs; if type is None, it will be auto-inferred.\n* As a list of column names; in this case types are automatically inferred.\n* As a list of (name,type) pairs; this is equivalent to the dictionary form.\n\n\nIf you supply a list of column names that does not match the names in the\nunderlying data, the names given here will overwrite them. The number\nof names given in the schema should match the underlying data dimensions.\n\n**schema\\_overrides**dict, default NoneSupport type specification or override of one or more columns; note that\nany dtypes inferred from the schema param will be overridden.\nunderlying data, the names given here will overwrite them.\n\n`read\\_ndjson`\n\n\n# polars.read\\_ndjson#\n\n\npolars.read\\_ndjson(\n\n*source: str | Path | IOBase | bytes*,\n*\\**,\n*schema: SchemaDefinition | None = None*,\n*schema\\_overrides: SchemaDefinition | None = None*,\n*ignore\\_errors: bool = False*,\n\n) → DataFrame[source]#\nRead into a DataFrame from a newline delimited JSON file.\n\n**ignore\\_errors**Return Null if parsing fails because of schema mismatches.\n\n\n\n# polars.scan\\_ndjson#\n\n\npolars.scan\\_ndjson(\n\n*source: str | Path*,\n*\\**,\n*infer\\_schema\\_length: int | None = 100*,\n*batch\\_size: int | None = 1024*,\n*n\\_rows: int | None = None*,\n*low\\_memory: bool = False*,\n*rechunk: bool = True*,\n*row\\_count\\_name: str | None = None*,\n*row\\_count\\_offset: int = 0*,\n\n) → LazyFrame[source]#\nLazily read from a newline delimited JSON file or multiple files via glob patterns.\n\n**infer\\_schema\\_length**Infer the schema from the first `infer\\_schema\\_length` rows.\n\n**batch\\_size**Number of rows to read in each batch.\n\n**n\\_rows**Stop reading from JSON file after reading `n\\_rows`.\n\n\n\n# polars.DataFrame.write\\_json#\n\n\nDataFrame.write\\_json(\n\n*file: None = None*,\n*\\**,\n*pretty: bool = False*,\n*row\\_oriented: bool = False*,\n\nDataFrame.write\\_json(\n\n*file: IOBase | str | Path*,\n*\\**,\n*pretty: bool = False*,\n*row\\_oriented: bool = False*,\n\n) → None\nSerialize to JSON representation.\n\n**pretty**Pretty serialize json.\n\n**row\\_oriented**Write to row oriented json. This is slower, but more common.\n\n`DataFrame.write\\_ndjson`\n\n```\n>>> df = pl.DataFrame(\n...     {\n...         \"foo\": [1, 2, 3],\n...         \"bar\": [6, 7, 8],\n...     }\n... )\n>>> df.write\\_json()\n'{\"columns\":[{\"name\":\"foo\",\"datatype\":\"Int64\",\"bit\\_settings\":\"\",\"values\":[1,2,3]},{\"name\":\"bar\",\"datatype\":\"Int64\",\"bit\\_settings\":\"\",\"values\":[6,7,8]}]}'\n>>> df.write\\_json(row\\_oriented=True)\n'[{\"foo\":1,\"bar\":6},{\"foo\":2,\"bar\":7},{\"foo\":3,\"bar\":8}]'\n\n\n\n# polars.DataFrame.write\\_ndjson#\n\n\nDataFrame.write\\_ndjson(*file: None = None*) → str[source]#\n\nDataFrame.write\\_ndjson(*file: IOBase | str | Path*) → None\nSerialize to newline delimited JSON representation.\n\n```\n>>> df = pl.DataFrame(\n...     {\n...         \"foo\": [1, 2, 3],\n...         \"bar\": [6, 7, 8],\n...     }\n... )\n>>> df.write\\_ndjson()\n'{\"foo\":1,\"bar\":6}\\n{\"foo\":2,\"bar\":7}\\n{\"foo\":3,\"bar\":8}\\n'\n\n\n\n# polars.read\\_avro#\n\n\npolars.read\\_avro(\n\n*source: str | Path | BytesIO | BinaryIO*,\n*\\**,\n*columns: list[int] | list[str] | None = None*,\n*n\\_rows: int | None = None*,\n\n) → DataFrame[source]#\nRead into a DataFrame from Apache Avro format.\n\n**n\\_rows**Stop reading from Apache Avro file after reading `n\\_rows`.\n\n\n# polars.DataFrame.write\\_avro#\n\n\nDataFrame.write\\_avro(\n\n*file: BinaryIO | BytesIO | str | Path*,\n*compression: AvroCompression = 'uncompressed'*,\n\n) → None[source]#\nWrite to Apache Avro file.\n\n**compression**{‘uncompressed’, ‘snappy’, ‘deflate’}Compression method. Defaults to “uncompressed”.\n\n```\n>>> import pathlib\n>>>\n>>> df = pl.DataFrame(\n...     {\n...         \"foo\": [1, 2, 3, 4, 5],\n...         \"bar\": [6, 7, 8, 9, 10],\n...         \"ham\": [\"a\", \"b\", \"c\", \"d\", \"e\"],\n...     }\n... )\n>>> path: pathlib.Path = dirpath / \"new\\_file.avro\"\n>>> df.write\\_avro(path)\n\n\n# polars.read\\_excel#\n\n\npolars.read\\_excel(\n\n*source: str | BytesIO | Path | BinaryIO | bytes*,\n*\\**,\n*sheet\\_id: None = None*,\n*sheet\\_name: str*,\n*engine: Literal['xlsx2csv', 'openpyxl'] | None = None*,\n*xlsx2csv\\_options: dict[str, Any] | None = None*,\n*read\\_csv\\_options: dict[str, Any] | None = None*,\n*schema\\_overrides: SchemaDict | None = None*,\n*raise\\_if\\_empty: bool =\n\n==================\n Document 3 \n----------------\n polars.api.register\\_lazyframe\\_namespace#\n\n\npolars.api.register\\_lazyframe\\_namespace(\n\n) → Callable[[type[NS]], type[NS]][source]#\nDecorator for registering custom functionality with a polars LazyFrame.\n\n```\n>>> @pl.api.register\\_lazyframe\\_namespace(\"types\")\n... class DTypeOperations:\n...     def \\_\\_init\\_\\_(self, ldf: pl.LazyFrame):\n...         self.\\_ldf = ldf\n...\n...     def split\\_by\\_column\\_dtypes(self) -> list[pl.LazyFrame]:\n...         return [\n...             self.\\_ldf.select(pl.col(tp))\n...             for tp in dict.fromkeys(self.\\_ldf.dtypes)\n...         ]\n...\n...     def upcast\\_integer\\_types(self) -> pl.LazyFrame:\n...         return self.\\_ldf.with\\_columns(\n...             pl.col(tp).cast(pl.Int64) for tp in (pl.Int8, pl.Int16, pl.Int32)\n...         )\n...\n>>>\n>>> ldf = pl.DataFrame(\n...     data={\"a\": [1, 2], \"b\": [3, 4], \"c\": [5.6, 6.7]},\n...     schema=[(\"a\", pl.Int16), (\"b\", pl.Int32), (\"c\", pl.Float32)],\n... ).lazy()\n>>>\n>>> ldf.collect()\nshape: (2, 3)\n┌─────┬─────┬─────┐\n│ a ┆ b ┆ c │\n│ --- ┆ --- ┆ --- │\n│ i16 ┆ i32 ┆ f32 │\n╞═════╪═════╪═════╡\n│ 1 ┆ 3 ┆ 5.6 │\n│ 2 ┆ 4 ┆ 6.7 │\n└─────┴─────┴─────┘\n>>> ldf.types.upcast\\_integer\\_types().collect()\nshape: (2, 3)\n┌─────┬─────┬─────┐\n│ a ┆ b ┆ c │\n│ --- ┆ --- ┆ --- │\n│ i64 ┆ i64 ┆ f32 │\n╞═════╪═════╪═════╡\n│ 1 ┆ 3 ┆ 5.6 │\n│ 2 ┆ 4 ┆ 6.7 │\n└─────┴─────┴─────┘\n>>>\n>>> ldf = pl.DataFrame(\n...     data=[[\"xx\", 2, 3, 4], [\"xy\", 4, 5, 6], [\"yy\", 5, 6, 7], [\"yz\", 6, 7, 8]],\n...     schema=[\"a1\", \"a2\", \"b1\", \"b2\"],\n...     orient=\"row\",\n... ).lazy()\n>>>\n>>> ldf.collect()\nshape: (4, 4)\n┌─────┬─────┬─────┬─────┐\n│ a1 ┆ a2 ┆ b1 ┆ b2 │\n│ --- ┆ --- ┆ --- ┆ --- │\n│ str ┆ i64 ┆ i64 ┆ i64 │\n╞═════╪═════╪═════╪═════╡\n│ xx ┆ 2 ┆ 3 ┆ 4 │\n│ xy ┆ 4 ┆ 5 ┆ 6 │\n│ yy ┆ 5 ┆ 6 ┆ 7 │\n│ yz ┆ 6 ┆ 7 ┆ 8 │\n└─────┴─────┴─────┴─────┘\n>>> pl.collect\\_all(ldf.types.split\\_by\\_column\\_dtypes())\n[shape: (4, 1)\n┌─────┐\n│ a1 │\n│ --- │\n│ str │\n╞═════╡\n│ xx │\n│ xy │\n│ yy │\n│ yz │\n└─────┘, shape: (4, 3)\n┌─────┬─────┬─────┐\n│ a2 ┆ b1 ┆ b2 │\n│ --- ┆ --- ┆ --- │\n│ i64 ┆ i64 ┆ i64 │\n╞═════╪═════╪═════╡\n│ 2 ┆ 3 ┆ 4 │\n│ 4 ┆ 5 ┆ 6 │\n│ 5 ┆ 6 ┆ 7 │\n│ 6 ┆ 7 ┆ 8 │\n└─────┴─────┴─────┘]\n\n# polars.api.register\\_series\\_namespace#\n\n\npolars.api.register\\_series\\_namespace(\n\n) → Callable[[type[NS]], type[NS]][source]#\nDecorator for registering custom functionality with a polars Series.\n\n```\n>>> @pl.api.register\\_series\\_namespace(\"math\")\n... class MathShortcuts:\n...     def \\_\\_init\\_\\_(self, s: pl.Series):\n...         self.\\_s = s\n...\n...     def\n\n==================\n Document 4 \n----------------\n polars.Struct#\n\n\n*class* polars.Struct(*\\*args: Any*, *\\*\\*kwargs: Any*)[source]#\nStruct composite type.\n\n\n\\_\\_init\\_\\_(*fields: Sequence[Field] | SchemaDict*)[source]#\nStruct composite type.\n\n**fields**The sequence of fields that make up the struct\n\n```\n>>> s = pl.Series(\n...     \"struct\\_series\",\n...     [{\"a\": [1], \"b\": [2], \"c\": [3]}, {\"a\": [4], \"b\": [5], \"c\": [6]}],\n... )\n>>> s\nshape: (2,)\nSeries: 'struct\\_series' [struct[3]]\n[\n {[1],[2],[3]}\n {[4],[5],[6]}\n]\n\n\n|  |  |\n| --- | --- |\n| `\\_\\_init\\_\\_`(fields) | Struct composite type. |\n| `base\\_type`() | Return this DataType's fundamental/root type class. |\n| `is\\_`(other) | Check if this DataType is the same as another DataType. |\n| `is\\_not`(other) | Check if this DataType is NOT the same as another DataType. |\n| `to\\_schema`() | Return Struct dtype as a schema dict. |\n\n\n# polars.Boolean#\n\n\n*class* polars.Boolean(*\\*args: Any*, *\\*\\*kwargs: Any*)[source]#\nBoolean type.\n\n\n\n# polars.Binary#\n\n\n*class* polars.Binary(*\\*args: Any*, *\\*\\*kwargs: Any*)[source]#\nBinary type.\n\n\n\n# polars.Categorical#\n\n\n*class* polars.Categorical(*\\*args: Any*, *\\*\\*kwargs: Any*)[source]#\nA categorical encoding of a set of strings.\n\n\n\n# polars.Null#\n\n\n*class* polars.Null(*\\*args: Any*, *\\*\\*kwargs: Any*)[source]#\nType representing Null / None values.\n\n\n\n# polars.Object#\n\n\n*class* polars.Object(*\\*args: Any*, *\\*\\*kwargs: Any*)[source]#\nType for wrapping arbitrary Python objects.\n\n\n\n# polars.Utf8#\n\n\n*class* polars.Utf8(*\\*args: Any*, *\\*\\*kwargs: Any*)[source]#\nUTF-8 encoded string type.\n\n\n\n# polars.Unknown#\n\n\n*class* polars.Unknown(*\\*args: Any*, *\\*\\*kwargs: Any*)[source]#\nType representing Datatype values that could not be determined statically.\n\n\n\n# polars.Config.activate\\_decimals#\n\n\n*classmethod* Config.activate\\_decimals(*active: bool | None = True*) → type[Config][source]#\nActivate `Decimal` data types.\n\n\nThis is a temporary setting that will be removed once the `Decimal` type\nstabilizes (`Decimal` is currently considered to be in beta testing).\n\n\n\n# polars.Config.set\\_ascii\\_tables#\n\n\n*classmethod* Config.set\\_ascii\\_tables(*active: bool | None = True*) → type[Config][source]#\nUse ASCII characters to display table outlines.\n\n\nSet False to revert to the default UTF8\\_FULL\\_CONDENSED formatting style.\n\n```\n>>> df = pl.DataFrame({\"abc\": [1.0, 2.5, 5.0], \"xyz\": [True, False, True]})\n>>> pl.Config.set\\_ascii\\_tables(True)  \n\n# ...\n\n# shape: (3, 2) shape: (3, 2)\n\n# ┌─────┬───────┐ +-----+-------+\n\n# │ abc ┆ xyz │ | abc | xyz |\n\n# │ --- ┆ --- │ | --- | --- |\n\n# │ f64 ┆ bool │ | f64 | bool |\n\n# ╞═════╪═══════╡ +=============+\n\n# │ 1.0 ┆ true │ >> | 1.0 | true |\n\n# │ 2.5 ┆ false │ | 2.5 | false |\n\n# │ 5.0 ┆ true │ | 5.0 | true |\n\n# └─────┴───────┘ +-----+-------+\n\n\n\n# polars.Config.set\\_fmt\\_float#\n\n\n*classmethod* Config.set\\_fmt\\_float(*fmt: FloatFmt | None = 'mixed'*) → type[Config][source]#\nControl how floating point values are displayed.\n\n**fmt**{“mixed”, “full”}How to format floating point numbers\n\n\n# polars.Config.set\\_fmt\\_str\\_lengths#\n\n\n*classmethod* Config.set\\_fmt\\_str\\_lengths(*n: int | None*) → type[Config][source]#\nSet the number of characters used to display string values.\n\n**n**intnumber of characters to display\n\n```\n>>> df = pl.DataFrame(\n...     {\n...         \"txt\": [\n...\n\n==================\n Document 5 \n----------------\n polars.testing.parametric.create\\_list\\_strategy#\n\n\npolars.testing.parametric.create\\_list\\_strategy(\n\n*inner\\_dtype: PolarsDataType | None*,\n*\\**,\n*select\\_from: Sequence[Any] | None = None*,\n*size: int | None = None*,\n*min\\_size: int | None = None*,\n*max\\_size: int | None = None*,\n*unique: bool = False*,\n\n) → SearchStrategy[list[Any]][source]#\nHypothesis strategy for producing polars List data.\n\n**inner\\_dtype**PolarsDataTypetype of the inner list elements (can also be another List).\n\n**select\\_from**list, optionalrandomly select the innermost values from this list (otherwise\nthe default strategy associated with the innermost dtype is used).\n\n**size**int, optionalif set, generated lists will be of exactly this size (and\nignore the min\\_size/max\\_size params).\n\n**min\\_size**int, optionalset the minimum size of the generated lists (default: 0 if unset).\n\n**max\\_size**int, optionalset the maximum size of the generated lists (default: 3 if\nmin\\_size is unset or zero, otherwise 2x min\\_size).\n\n**unique**bool, optionalensure that the generated lists contain unique values.\n\n\nCreate a strategy that generates a list of i32 values:\n\n```\n>>> lst = create\\_list\\_strategy(inner\\_dtype=pl.Int32)\n>>> lst.example()  \n[-11330, 24030, 116]\n\n\nCreate a strategy that generates lists of lists of specific strings:\n\n```\n>>> lst = create\\_list\\_strategy(\n...     inner\\_dtype=pl.List(pl.Utf8),\n...     select\\_from=[\"xx\", \"yy\", \"zz\"],\n... )\n>>> lst.example()  \n[['yy', 'xx'], [], ['zz']]\n\n\nCreate a UInt8 dtype strategy as a hypothesis composite that generates\npairs of small int values where the first is always <= the second:\n\n```\n>>> from hypothesis.strategies import composite\n>>>\n>>> @composite\n... def uint8\\_pairs(draw, uints=create\\_list\\_strategy(pl.UInt8, size=2)):\n...     pairs = list(zip(draw(uints), draw(uints)))\n...     return [sorted(ints) for ints in pairs]\n...\n>>> uint8\\_pairs().example()  \n[(12, 22), (15, 131)]\n>>> uint8\\_pairs().example()  \n[(59, 176), (149, 149)]\n\n\n# polars.testing.parametric.load\\_profile#\n\n\npolars.testing.parametric.load\\_profile(\n\n*profile: Literal['fast', 'balanced', 'expensive'] | int = 'fast'*,\n*\\**,\n*set\\_environment: bool = False*,\n\n) → None[source]#\nLoad a named (or custom) hypothesis profile for use with the parametric tests.\n\n**profile**{str, int}, optionalName of the profile to load; one of “fast”, “balanced”, “expensive”, or\nthe integer number of iterations to run (which will create and register\na custom profile with that value).\n\n**set\\_environment**bool, default FalseIf True, also set the environment variable `POLARS\\_HYPOTHESIS\\_PROFILE`\nto the given profile name/value.\n\n```\n>>> # load a custom profile that will run with 1500 iterations\n>>> from polars.testing.parametric.profiles import load\\_profile\n>>> load\\_profile(1500)\n\n\n\n# polars.testing.parametric.set\\_profile#\n\n\npolars.testing.parametric.set\\_profile(*profile: Literal['fast', 'balanced', 'expensive'] | int*) → None[source]#\nSet the env var `POLARS\\_HYPOTHESIS\\_PROFILE` to the given profile name/value.\n\n```\n>>> # prefer the 'balanced' profile for running parametric tests\n>>> from polars.testing.parametric.profiles import set\\_profile\n>>> set\\_profile(\"balanced\")\n\n\n\n# polars.build\\_info#\n\n\npolars.build\\_info() → dict[str, Any][source]#\nReturn a dict with Polars build information.\n\n\nIf Polars was compiled with “build\\_info” feature gate return the full build info,\notherwise only version is included. The full build information dict contains\nthe following keys [‘build’, ‘info-time’, ‘dependencies’, ‘features’, ‘host’,\n‘target’, ‘git’, ‘version’].\n\n\n\n# polars.get\\_index\\_type#\n\n\npolars.get\\_index\\_type() → DataTypeClass[source]#\nGet the datatype used for Polars indexing.\n\nDataType`UInt32` in regular Polars, `UInt64` in bigidx Polars.\n\n\n\n# polars.show\\_versions#\n\n\npolars.show\\_versions() → None[source]#\nPrint out version of Polars and dependencies to stdout.\n\n```\n>>> pl.show\\_versions()  \n--------Version info---------\nPolars: 0.17.11\nIndex type: UInt32\nPlatform: Linux-5.15.90.1-microsoft-standard-WSL2-x86\\_64-with-glibc2.35\nPython: 3.11.3 (main, Apr 15 2023, 14:44:51) [GCC 11.3.0]\n\\b\n----Optional dependencies----\nnumpy: 1.24.2\npandas: 2.0.0\npyarrow: 11.0.0\nconnectorx: <not installed>\ndeltalake: 0.8.1\nfsspec: 2023.4.0\nmatplotlib: 3.7.1\nxlsx2csv: 0.8.1\nxlsxwriter: 3.1.0\n\n\n\n# polars.threadpool\\_size#\n\n\npolars.threadpool\\_size() → int[source]#\nGet the number of threads in the Polars thread pool.\n\n\nThe threadpool size can be overridden by setting the `POLARS\\_MAX\\_THREADS`\nenvironment variable before process start. (The thread pool is not behind a\nlock, so it cannot be modified once set). A reasonable use-case for this might\nbe temporarily setting max threads to a low value before importing polars in a\npyspark UDF or similar context. Otherwise, it is strongly recommended not to\noverride this value as it will be set automatically by the engine.\n\n\n# polars.SQLContext.execute#\n\n\nSQLContext.execute(*query: str*, *eager: None = None*) → DataFrame[source]#\n\nSQLContext.execute(\n\n*query: str*,\n*eager: Literal[False]*,\n\n) → LazyFrame\n\n*query: str*,\n*eager: Literal[True]*,\n\nSQLContext.execute(*query: str*, *eager: None = None*) → LazyFrame\n\n) → DataFrame\nParse the given SQL query and execute it against the registered frame data.\n\n**query**A valid string SQL query.\n\n**eager**Apply"
    },
    {
        "instruction": "Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'score' column is greater than 80, select specific columns ('name', 'score', 'grade'), and calculate the average score. Finally, display the resulting average score.",
        "api": "polars",
        "output": "#!pip install polars\nimport polars as pl\n\n# Load a sample CSV file into a Polars DataFrame\ndata = pl.read_csv(\"sample_data.csv\")\n\n# Filter rows\nfiltered_data = data.filter(data['score'] > 80)\n\n# Select specific columns\nselected_columns = filtered_data.select(['name', 'score', 'grade'])\n\n# Calculate the average score\naverage_score = selected_columns['score'].mean()\n\n# Display the resulting average score\nprint(average_score)\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n# polars.read\\_csv#\n\n\npolars.read\\_csv(\n\n*source: str | TextIO | BytesIO | Path | BinaryIO | bytes*,\n*\\**,\n*has\\_header: bool = True*,\n*columns: Sequence[int] | Sequence[str] | None = None*,\n*new\\_columns: Sequence[str] | None = None*,\n*separator: str = ','*,\n*comment\\_char: str | None = None*,\n*quote\\_char: str | None =\n\n==================\n Document 1 \n----------------\n polars.scan\\_csv#\n\n\npolars.scan\\_csv(\n\n*source: str | Path*,\n*\\**,\n*has\\_header: bool = True*,\n*separator: str = ','*,\n*comment\\_char: str | None = None*,\n*quote\\_char: str | None = '\"'*,\n*skip\\_rows: int = 0*,\n*dtypes: SchemaDict | Sequence[PolarsDataType] | None = None*,\n*schema: SchemaDict | None = None*,\n*null\\_values: str | Sequence[str] | dict[str, str] | None = None*,\n*missing\\_utf8\\_is\\_empty\\_string: bool = False*,\n*ignore\\_errors: bool = False*,\n*cache: bool = True*,\n*with\\_column\\_names: Callable[[list[str]], list[str]] | None = None*,\n*infer\\_schema\\_length: int | None = 100*,\n*n\\_rows: int | None = None*,\n*encoding: CsvEncoding = 'utf8'*,\n*low\\_memory: bool = False*,\n*rechunk: bool = True*,\n*skip\\_rows\\_after\\_header: int = 0*,\n*row\\_count\\_name: str | None = None*,\n*row\\_count\\_offset: int = 0*,\n*try\\_parse\\_dates: bool = False*,\n*eol\\_char: str = '\\n'*,\n*new\\_columns: Sequence[str] | None = None*,\n*raise\\_if\\_empty: bool = True*,\n*truncate\\_ragged\\_lines: bool = False*,\n\n) → LazyFrame[source]#\nLazily read from a CSV file or multiple files via glob patterns.\n\n\nThis allows the query optimizer to push down predicates and\nprojections to the scan level, thereby potentially reducing\nmemory overhead.\n\n**source**Path to a file.\n\n**skip\\_rows**Start reading after `skip\\_rows` lines. The header will be parsed at this\noffset.\n\n**dtypes**Overwrite dtypes during inference; should be a {colname:dtype,} dict or,\nif providing a list of strings to `new\\_columns`, a list of dtypes of\nthe same length.\n\n**cache**Cache the result after reading.\n\n**with\\_column\\_names**Apply a function over the column names just in time (when they are determined);\nthis function will receive (and should return) a list of column names.\n\n**n\\_rows**Stop reading from CSV file after reading `n\\_rows`.\n\n**encoding**{‘utf8’, ‘utf8-lossy’}Lossy means that invalid utf8 values are replaced with `�`\ncharacters. Defaults to “utf8”.\n\n**low\\_memory**Reduce memory usage in expense of performance.\n\n**rechunk**Reallocate to contiguous memory when all chunks/ files are parsed.\n\n**try\\_parse\\_dates**Try to automatically parse dates. Most ISO8601-like formats\ncan be inferred, as well as a handful of others. If this does not succeed,\nthe column remains of data type `pl.Utf8`.\n\n**eol\\_char**Single byte end of line character\n\n**new\\_columns**Provide an explicit list of string column names to use (for example, when\nscanning a headerless CSV file). Note that unlike `read\\_csv` it is considered\nan error to provide fewer column names than there are columns in the file.\n\n**raise\\_if\\_empty**When there is no data in the source,``NoDataError`` is raised. If this parameter\nis set to False, an empty LazyFrame (with no columns) is returned instead.\n\nLazyFrame\n\n`read\\_csv`Read a CSV file into a DataFrame.\n\n```\n>>> import pathlib\n>>>\n>>> (\n...     pl.scan\\_csv(\"my\\_long\\_file.csv\")  # lazy, doesn't do a thing\n...     .select(\n...         [\"a\", \"c\"]\n...     )  # select only 2 columns (other columns will not be read)\n...     .filter(\n...         pl.col(\"a\") > 10\n...     )  # the filter is pushed down the scan, so less data is read into memory\n...     .fetch(100)  # pushed a limit of 100 rows to the scan level\n... )  \n\n\nWe can use `with\\_column\\_names` to modify the header before scanning:\n\n```\n>>> df = pl.DataFrame(\n...     {\"BrEeZaH\": [1, 2, 3, 4], \"LaNgUaGe\": [\"is\", \"hard\", \"to\", \"read\"]}\n... )\n>>> path: pathlib.Path = dirpath / \"mydf.csv\"\n>>> df.write\\_csv(path)\n>>> pl.scan\\_csv(\n...     path, with\\_column\\_names=lambda cols: [col.lower() for col in cols]\n... ).collect()\nshape: (4, 2)\n┌─────────┬──────────┐\n│ breezah ┆ language │\n│ --- ┆ --- │\n│ i64 ┆ str │\n╞═════════╪══════════╡\n│ 1 ┆ is │\n│ 2 ┆ hard │\n│ 3 ┆ to │\n│ 4 ┆ read │\n└─────────┴──────────┘\n\n\nYou can also simply replace column names (or provide them if the file has none)\nby passing a list of new column names to the `new\\_columns` parameter:\n\n```\n>>> df.write\\_csv(path)\n>>> pl.scan\\_csv(\n...     path,\n...     new\\_columns=[\"idx\", \"txt\"],\n...     dtypes=[pl.UInt16, pl.Utf8],\n... ).collect()\nshape: (4, 2)\n┌─────┬──────┐\n│ idx ┆ txt │\n│ --- ┆ --- │\n│ u16 ┆ str │\n╞═════╪══════╡\n│ 1 ┆ is │\n│ 2 ┆ hard │\n│ 3 ┆ to │\n│ 4 ┆ read │\n└─────┴──────┘\n\n# polars.DataFrame.write\\_csv#\n\n\nDataFrame.write\\_csv(\n\n*file: None = None*,\n*\\**,\n*has\\_header: bool = True*,\n*separator: str = ','*,\n*line\\_terminator: str = '\\n'*,\n*quote: str = '\"'*,\n*batch\\_size: int = 1024*,\n*datetime\\_format: str | None = None*,\n*date\\_format: str | None = None*,\n*time\\_format: str | None = None*,\n*float\\_precision: int | None = None*,\n*null\\_value:\n\n==================\n Document 2 \n----------------\n polars.testing.parametric.create\\_list\\_strategy#\n\n\npolars.testing.parametric.create\\_list\\_strategy(\n\n*inner\\_dtype: PolarsDataType | None*,\n*\\**,\n*select\\_from: Sequence[Any] | None = None*,\n*size: int | None = None*,\n*min\\_size: int | None = None*,\n*max\\_size: int | None = None*,\n*unique: bool = False*,\n\n) → SearchStrategy[list[Any]][source]#\nHypothesis strategy for producing polars List data.\n\n**inner\\_dtype**PolarsDataTypetype of the inner list elements (can also be another List).\n\n**select\\_from**list, optionalrandomly select the innermost values from this list (otherwise\nthe default strategy associated with the innermost dtype is used).\n\n**size**int, optionalif set, generated lists will be of exactly this size (and\nignore the min\\_size/max\\_size params).\n\n**min\\_size**int, optionalset the minimum size of the generated lists (default: 0 if unset).\n\n**max\\_size**int, optionalset the maximum size of the generated lists (default: 3 if\nmin\\_size is unset or zero, otherwise 2x min\\_size).\n\n**unique**bool, optionalensure that the generated lists contain unique values.\n\n\nCreate a strategy that generates a list of i32 values:\n\n```\n>>> lst = create\\_list\\_strategy(inner\\_dtype=pl.Int32)\n>>> lst.example()  \n[-11330, 24030, 116]\n\n\nCreate a strategy that generates lists of lists of specific strings:\n\n```\n>>> lst = create\\_list\\_strategy(\n...     inner\\_dtype=pl.List(pl.Utf8),\n...     select\\_from=[\"xx\", \"yy\", \"zz\"],\n... )\n>>> lst.example()  \n[['yy', 'xx'], [], ['zz']]\n\n\nCreate a UInt8 dtype strategy as a hypothesis composite that generates\npairs of small int values where the first is always <= the second:\n\n```\n>>> from hypothesis.strategies import composite\n>>>\n>>> @composite\n... def uint8\\_pairs(draw, uints=create\\_list\\_strategy(pl.UInt8, size=2)):\n...     pairs = list(zip(draw(uints), draw(uints)))\n...     return [sorted(ints) for ints in pairs]\n...\n>>> uint8\\_pairs().example()  \n[(12, 22), (15, 131)]\n>>> uint8\\_pairs().example()  \n[(59, 176), (149, 149)]\n\n\n# polars.testing.parametric.load\\_profile#\n\n\npolars.testing.parametric.load\\_profile(\n\n*profile: Literal['fast', 'balanced', 'expensive'] | int = 'fast'*,\n*\\**,\n*set\\_environment: bool = False*,\n\n) → None[source]#\nLoad a named (or custom) hypothesis profile for use with the parametric tests.\n\n**profile**{str, int}, optionalName of the profile to load; one of “fast”, “balanced”, “expensive”, or\nthe integer number of iterations to run (which will create and register\na custom profile with that value).\n\n**set\\_environment**bool, default FalseIf True, also set the environment variable `POLARS\\_HYPOTHESIS\\_PROFILE`\nto the given profile name/value.\n\n```\n>>> # load a custom profile that will run with 1500 iterations\n>>> from polars.testing.parametric.profiles import load\\_profile\n>>> load\\_profile(1500)\n\n\n\n# polars.testing.parametric.set\\_profile#\n\n\npolars.testing.parametric.set\\_profile(*profile: Literal['fast', 'balanced', 'expensive'] | int*) → None[source]#\nSet the env var `POLARS\\_HYPOTHESIS\\_PROFILE` to the given profile name/value.\n\n```\n>>> # prefer the 'balanced' profile for running parametric tests\n>>> from polars.testing.parametric.profiles import set\\_profile\n>>> set\\_profile(\"balanced\")\n\n\n\n# polars.build\\_info#\n\n\npolars.build\\_info() → dict[str, Any][source]#\nReturn a dict with Polars build information.\n\n\nIf Polars was compiled with “build\\_info” feature gate return the full build info,\notherwise only version is included. The full build information dict contains\nthe following keys [‘build’, ‘info-time’, ‘dependencies’, ‘features’, ‘host’,\n‘target’, ‘git’, ‘version’].\n\n\n\n# polars.get\\_index\\_type#\n\n\npolars.get\\_index\\_type() → DataTypeClass[source]#\nGet the datatype used for Polars indexing.\n\nDataType`UInt32` in regular Polars, `UInt64` in bigidx Polars.\n\n\n\n# polars.show\\_versions#\n\n\npolars.show\\_versions() → None[source]#\nPrint out version of Polars and dependencies to stdout.\n\n```\n>>> pl.show\\_versions()  \n--------Version info---------\nPolars: 0.17.11\nIndex type: UInt32\nPlatform: Linux-5.15.90.1-microsoft-standard-WSL2-x86\\_64-with-glibc2.35\nPython: 3.11.3 (main, Apr 15 2023, 14:44:51) [GCC 11.3.0]\n\\b\n----Optional dependencies----\nnumpy: 1.24.2\npandas: 2.0.0\npyarrow: 11.0.0\nconnectorx: <not installed>\ndeltalake: 0.8.1\nfsspec: 2023.4.0\nmatplotlib: 3.7.1\nxlsx2csv: 0.8.1\nxlsxwriter: 3.1.0\n\n\n\n# polars.threadpool\\_size#\n\n\npolars.threadpool\\_size() → int[source]#\nGet the number of threads in the Polars thread pool.\n\n\nThe threadpool size can be overridden by setting the `POLARS\\_MAX\\_THREADS`\nenvironment variable before process start. (The thread pool is not behind a\nlock, so it cannot be modified once set). A reasonable use-case for this might\nbe temporarily setting max threads to a low value before importing polars in a\npyspark UDF or similar context. Otherwise, it is strongly recommended not to\noverride this value as it will be set automatically by the engine.\n\n\n# polars.SQLContext.execute#\n\n\nSQLContext.execute(*query: str*, *eager: None = None*) → DataFrame[source]#\n\nSQLContext.execute(\n\n*query: str*,\n*eager: Literal[False]*,\n\n) → LazyFrame\n\n*query: str*,\n*eager: Literal[True]*,\n\nSQLContext.execute(*query: str*, *eager: None = None*) → LazyFrame\n\n) → DataFrame\nParse the given SQL query and execute it against the registered frame data.\n\n**query**A valid string SQL query.\n\n**eager**Apply\n\n==================\n Document 3 \n----------------\n polars.testing.parametric.dataframes#\n\n\npolars.testing.parametric.dataframes(\n\n*cols: int | column | Sequence[column] | None = None*,\n*\\**,\n*lazy: Literal[False] = False*,\n*min\\_cols: int | None = 0*,\n*max\\_cols: int | None = MAX\\_COLS*,\n*size: int | None = None*,\n*min\\_size: int | None = 0*,\n*max\\_size: int | None = MAX\\_DATA\\_SIZE*,\n*chunked: bool | None = None*,\n*include\\_cols: Sequence[column] | column | None = None*,\n*null\\_probability: float | dict[str, float] = 0.0*,\n*allow\\_infinities: bool = True*,\n*allowed\\_dtypes: Collection[PolarsDataType] | PolarsDataType | None = None*,\n*excluded\\_dtypes: Collection[PolarsDataType] | PolarsDataType | None = None*,\n\n) → SearchStrategy[DataFrame][source]#\n\npolars.testing.parametric.dataframes(\n\n*cols: int | column | Sequence[column] | None = None*,\n*\\**,\n*lazy: Literal[True]*,\n*min\\_cols: int | None = 0*,\n*max\\_cols: int | None = MAX\\_COLS*,\n*size: int | None = None*,\n*min\\_size: int | None = 0*,\n*max\\_size: int | None = MAX\\_DATA\\_SIZE*,\n*chunked: bool | None = None*,\n*include\\_cols: Sequence[column] | column | None = None*,\n*null\\_probability: float | dict[str, float] = 0.0*,\n*allow\\_infinities: bool = True*,\n*allowed\\_dtypes: Collection[PolarsDataType] | PolarsDataType | None = None*,\n*excluded\\_dtypes: Collection[PolarsDataType] | PolarsDataType | None = None*,\n\n) → SearchStrategy[LazyFrame]\nHypothesis strategy for producing polars DataFrames or LazyFrames.\n\n**cols**{int, columns}, optionalinteger number of columns to create, or a sequence of column objects\nthat describe the desired DataFrame column data.\n\n**lazy**bool, optionalproduce a LazyFrame instead of a DataFrame.\n\n**min\\_cols**int, optionalif not passing an exact size, can set a minimum here (defaults to 0).\n\n**max\\_cols**int, optionalif not passing an exact size, can set a maximum value here (defaults to\nMAX\\_COLS).\n\n**size**int, optionalif set, will create a DataFrame of exactly this size (and ignore\nthe min\\_size/max\\_size len params).\n\n**min\\_size**int, optionalif not passing an exact size, set the minimum number of rows in the\nDataFrame.\n\n**max\\_size**int, optionalif not passing an exact size, set the maximum number of rows in the\nDataFrame.\n\n**chunked**bool, optionalensure that DataFrames with more than row have `n\\_chunks` > 1. if\nomitted, chunking will be randomised at the level of individual Series.\n\n**include\\_cols**[column], optionala list of column objects to include in the generated DataFrame. note that\nexplicitly provided columns are appended onto the list of existing columns\n(if any present).\n\n**null\\_probability**{float, dict[str,float]}, optionalpercentage chance (expressed between 0.0 => 1.0) that a generated value is\nNone. this is applied independently of any None values generated by the\nunderlying strategy, and can be applied either on a per-column basis (if\ngiven as a `{col:pct}` dict), or globally. if null\\_probability is defined\non a column, it takes precedence over the global value.\n\n**allow\\_infinities**bool, optionaloptionally disallow generation of +/-inf values for floating-point dtypes.\n\n**allowed\\_dtypes**{list,set}, optionalwhen automatically generating data, allow only these dtypes.\n\n**excluded\\_dtypes**{list,set}, optionalwhen automatically generating data, exclude these dtypes.\n\n\nIn actual usage this is deployed as a unit test decorator, providing a strategy\nthat generates DataFrames or LazyFrames with the given characteristics for\nthe unit test. While developing a strategy/test, it can also be useful to\ncall .example() directly on a given strategy to see concrete instances of\nthe generated data.\n\n\nUse column or columns to specify the schema of the types of DataFrame to\ngenerate. Note: in actual use the strategy is applied as a test decorator, not\nused standalone.\n\n```\n>>> from polars.testing.parametric import column, columns, dataframes\n>>> from hypothesis import given\n\n\nGenerate arbitrary DataFrames (as part of a unit test):\n\n```\n>>> @given(df=dataframes())\n... def test\\_repr(df: pl.DataFrame) -> None:\n...     assert isinstance(repr(df), str)\n\n\nGenerate LazyFrames with at least 1 column, random dtypes, and specific size:\n\n```\n>>> dfs = dataframes(min\\_cols=1, max\\_size=5, lazy=True)\n>>> dfs.example()  \n<polars.LazyFrame object at 0x11F561580>\n\n\nGenerate DataFrames with known colnames, random dtypes (per test, not per-frame):\n\n```\n>>> dfs = dataframes(columns([\"x\", \"y\", \"z\"]))\n>>> dfs.example()  \nshape: (3, 3)\n┌────────────┬───────┬────────────────────────────┐\n│ x ┆ y ┆ z │\n│ --- ┆ --- ┆ --- │\n│ date ┆ u16 ┆ datetime[μs] │\n╞════════════╪═══════╪════════════════════════════╡\n│ 0565-08-12 ┆ 34715 ┆ 5844-09-20 00:33:31.076854 │\n│ 3382-10-17 ┆ 48662 ┆ 7540-01-29 11:20:14.836271 │\n│ 4063-06-17 ┆ 39092 ┆ 1889-05-05 13:25:41.874455 │\n└────────────┴───────┴────────────────────────────┘\n\n\nGenerate frames with explicitly named/typed columns and a fixed size:\n\n```\n>>> dfs = dataframes(\n...     [\n...         column(\"x\", dtype=pl.Int32),\n...         column(\"y\", dtype=pl.Float64),\n...     ],\n...     size=2,\n... )\n>>> dfs.example()  \nshape: (2, 2)\n┌───────────┬────────────┐\n│ x ┆ y │\n│ --- ┆ --- │\n│ i32 ┆ f64 │\n╞═══════════╪════════════╡\n│ -15836 ┆ 1.1755e-38 │\n│ 575050513 ┆ NaN │\n└───────────┴────────────┘\n\n# polars.testing.parametric.series#\n\n\npolars.testing.parametric.series(\n\n*\\**,\n*name: str | SearchStrategy[str] | None = None*,\n*dtype: PolarsDataType | None = None*,\n*size: int | None = None*,\n*min\\_size: int | None = 0*,\n*max\\_size: int | None = 10*,\n*strategy: SearchStrategy[object] | None = None*,\n*null\\_probability: float = 0.0*,\n*allow\\_infinities: bool = True*,\n*unique:\n\n==================\n Document 4 \n----------------\n polars.Struct#\n\n\n*class* polars.Struct(*\\*args: Any*, *\\*\\*kwargs: Any*)[source]#\nStruct composite type.\n\n\n\\_\\_init\\_\\_(*fields: Sequence[Field] | SchemaDict*)[source]#\nStruct composite type.\n\n**fields**The sequence of fields that make up the struct\n\n```\n>>> s = pl.Series(\n...     \"struct\\_series\",\n...     [{\"a\": [1], \"b\": [2], \"c\": [3]}, {\"a\": [4], \"b\": [5], \"c\": [6]}],\n... )\n>>> s\nshape: (2,)\nSeries: 'struct\\_series' [struct[3]]\n[\n {[1],[2],[3]}\n {[4],[5],[6]}\n]\n\n\n|  |  |\n| --- | --- |\n| `\\_\\_init\\_\\_`(fields) | Struct composite type. |\n| `base\\_type`() | Return this DataType's fundamental/root type class. |\n| `is\\_`(other) | Check if this DataType is the same as another DataType. |\n| `is\\_not`(other) | Check if this DataType is NOT the same as another DataType. |\n| `to\\_schema`() | Return Struct dtype as a schema dict. |\n\n\n# polars.Boolean#\n\n\n*class* polars.Boolean(*\\*args: Any*, *\\*\\*kwargs: Any*)[source]#\nBoolean type.\n\n\n\n# polars.Binary#\n\n\n*class* polars.Binary(*\\*args: Any*, *\\*\\*kwargs: Any*)[source]#\nBinary type.\n\n\n\n# polars.Categorical#\n\n\n*class* polars.Categorical(*\\*args: Any*, *\\*\\*kwargs: Any*)[source]#\nA categorical encoding of a set of strings.\n\n\n\n# polars.Null#\n\n\n*class* polars.Null(*\\*args: Any*, *\\*\\*kwargs: Any*)[source]#\nType representing Null / None values.\n\n\n\n# polars.Object#\n\n\n*class* polars.Object(*\\*args: Any*, *\\*\\*kwargs: Any*)[source]#\nType for wrapping arbitrary Python objects.\n\n\n\n# polars.Utf8#\n\n\n*class* polars.Utf8(*\\*args: Any*, *\\*\\*kwargs: Any*)[source]#\nUTF-8 encoded string type.\n\n\n\n# polars.Unknown#\n\n\n*class* polars.Unknown(*\\*args: Any*, *\\*\\*kwargs: Any*)[source]#\nType representing Datatype values that could not be determined statically.\n\n\n\n# polars.Config.activate\\_decimals#\n\n\n*classmethod* Config.activate\\_decimals(*active: bool | None = True*) → type[Config][source]#\nActivate `Decimal` data types.\n\n\nThis is a temporary setting that will be removed once the `Decimal` type\nstabilizes (`Decimal` is currently considered to be in beta testing).\n\n\n\n# polars.Config.set\\_ascii\\_tables#\n\n\n*classmethod* Config.set\\_ascii\\_tables(*active: bool | None = True*) → type[Config][source]#\nUse ASCII characters to display table outlines.\n\n\nSet False to revert to the default UTF8\\_FULL\\_CONDENSED formatting style.\n\n```\n>>> df = pl.DataFrame({\"abc\": [1.0, 2.5, 5.0], \"xyz\": [True, False, True]})\n>>> pl.Config.set\\_ascii\\_tables(True)  \n\n# ...\n\n# shape: (3, 2) shape: (3, 2)\n\n# ┌─────┬───────┐ +-----+-------+\n\n# │ abc ┆ xyz │ | abc | xyz |\n\n# │ --- ┆ --- │ | --- | --- |\n\n# │ f64 ┆ bool │ | f64 | bool |\n\n# ╞═════╪═══════╡ +=============+\n\n# │ 1.0 ┆ true │ >> | 1.0 | true |\n\n# │ 2.5 ┆ false │ | 2.5 | false |\n\n# │ 5.0 ┆ true │ | 5.0 | true |\n\n# └─────┴───────┘ +-----+-------+\n\n\n\n# polars.Config.set\\_fmt\\_float#\n\n\n*classmethod* Config.set\\_fmt\\_float(*fmt: FloatFmt | None = 'mixed'*) → type[Config][source]#\nControl how floating point values are displayed.\n\n**fmt**{“mixed”, “full”}How to format floating point numbers\n\n\n# polars.Config.set\\_fmt\\_str\\_lengths#\n\n\n*classmethod* Config.set\\_fmt\\_str\\_lengths(*n: int | None*) → type[Config][source]#\nSet the number of characters used to display string values.\n\n**n**intnumber of characters to display\n\n```\n>>> df = pl.DataFrame(\n...     {\n...         \"txt\": [\n..."
    },
    {
        "instruction": "Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'city' column is 'Toronto', select specific columns ('name', 'age', 'city'), and calculate the minimum age. Finally, display the resulting minimum age.",
        "api": "polars",
        "output": "#!pip install polars\nimport polars as pl\n\n# Load a sample CSV file into a Polars DataFrame\ndata = pl.read_csv(\"sample_data.csv\")\n\n# Filter rows\nfiltered_data = data.filter(data['city'] == 'Toronto')\n\n# Select specific columns\nselected_columns = filtered_data.select(['name', 'age', 'city'])\n\n# Calculate the minimum age\nmin_age = selected_columns['age'].min()\n\n# Display the resulting minimum age\nprint(min_age)\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n# polars.read\\_csv#\n\n\npolars.read\\_csv(\n\n*source: str | TextIO | BytesIO | Path | BinaryIO | bytes*,\n*\\**,\n*has\\_header: bool = True*,\n*columns: Sequence[int] | Sequence[str] | None = None*,\n*new\\_columns: Sequence[str] | None = None*,\n*separator: str = ','*,\n*comment\\_char: str | None = None*,\n*quote\\_char: str | None =\n\n==================\n Document 1 \n----------------\n polars.testing.parametric.dataframes#\n\n\npolars.testing.parametric.dataframes(\n\n*cols: int | column | Sequence[column] | None = None*,\n*\\**,\n*lazy: Literal[False] = False*,\n*min\\_cols: int | None = 0*,\n*max\\_cols: int | None = MAX\\_COLS*,\n*size: int | None = None*,\n*min\\_size: int | None = 0*,\n*max\\_size: int | None = MAX\\_DATA\\_SIZE*,\n*chunked: bool | None = None*,\n*include\\_cols: Sequence[column] | column | None = None*,\n*null\\_probability: float | dict[str, float] = 0.0*,\n*allow\\_infinities: bool = True*,\n*allowed\\_dtypes: Collection[PolarsDataType] | PolarsDataType | None = None*,\n*excluded\\_dtypes: Collection[PolarsDataType] | PolarsDataType | None = None*,\n\n) → SearchStrategy[DataFrame][source]#\n\npolars.testing.parametric.dataframes(\n\n*cols: int | column | Sequence[column] | None = None*,\n*\\**,\n*lazy: Literal[True]*,\n*min\\_cols: int | None = 0*,\n*max\\_cols: int | None = MAX\\_COLS*,\n*size: int | None = None*,\n*min\\_size: int | None = 0*,\n*max\\_size: int | None = MAX\\_DATA\\_SIZE*,\n*chunked: bool | None = None*,\n*include\\_cols: Sequence[column] | column | None = None*,\n*null\\_probability: float | dict[str, float] = 0.0*,\n*allow\\_infinities: bool = True*,\n*allowed\\_dtypes: Collection[PolarsDataType] | PolarsDataType | None = None*,\n*excluded\\_dtypes: Collection[PolarsDataType] | PolarsDataType | None = None*,\n\n) → SearchStrategy[LazyFrame]\nHypothesis strategy for producing polars DataFrames or LazyFrames.\n\n**cols**{int, columns}, optionalinteger number of columns to create, or a sequence of column objects\nthat describe the desired DataFrame column data.\n\n**lazy**bool, optionalproduce a LazyFrame instead of a DataFrame.\n\n**min\\_cols**int, optionalif not passing an exact size, can set a minimum here (defaults to 0).\n\n**max\\_cols**int, optionalif not passing an exact size, can set a maximum value here (defaults to\nMAX\\_COLS).\n\n**size**int, optionalif set, will create a DataFrame of exactly this size (and ignore\nthe min\\_size/max\\_size len params).\n\n**min\\_size**int, optionalif not passing an exact size, set the minimum number of rows in the\nDataFrame.\n\n**max\\_size**int, optionalif not passing an exact size, set the maximum number of rows in the\nDataFrame.\n\n**chunked**bool, optionalensure that DataFrames with more than row have `n\\_chunks` > 1. if\nomitted, chunking will be randomised at the level of individual Series.\n\n**include\\_cols**[column], optionala list of column objects to include in the generated DataFrame. note that\nexplicitly provided columns are appended onto the list of existing columns\n(if any present).\n\n**null\\_probability**{float, dict[str,float]}, optionalpercentage chance (expressed between 0.0 => 1.0) that a generated value is\nNone. this is applied independently of any None values generated by the\nunderlying strategy, and can be applied either on a per-column basis (if\ngiven as a `{col:pct}` dict), or globally. if null\\_probability is defined\non a column, it takes precedence over the global value.\n\n**allow\\_infinities**bool, optionaloptionally disallow generation of +/-inf values for floating-point dtypes.\n\n**allowed\\_dtypes**{list,set}, optionalwhen automatically generating data, allow only these dtypes.\n\n**excluded\\_dtypes**{list,set}, optionalwhen automatically generating data, exclude these dtypes.\n\n\nIn actual usage this is deployed as a unit test decorator, providing a strategy\nthat generates DataFrames or LazyFrames with the given characteristics for\nthe unit test. While developing a strategy/test, it can also be useful to\ncall .example() directly on a given strategy to see concrete instances of\nthe generated data.\n\n\nUse column or columns to specify the schema of the types of DataFrame to\ngenerate. Note: in actual use the strategy is applied as a test decorator, not\nused standalone.\n\n```\n>>> from polars.testing.parametric import column, columns, dataframes\n>>> from hypothesis import given\n\n\nGenerate arbitrary DataFrames (as part of a unit test):\n\n```\n>>> @given(df=dataframes())\n... def test\\_repr(df: pl.DataFrame) -> None:\n...     assert isinstance(repr(df), str)\n\n\nGenerate LazyFrames with at least 1 column, random dtypes, and specific size:\n\n```\n>>> dfs = dataframes(min\\_cols=1, max\\_size=5, lazy=True)\n>>> dfs.example()  \n<polars.LazyFrame object at 0x11F561580>\n\n\nGenerate DataFrames with known colnames, random dtypes (per test, not per-frame):\n\n```\n>>> dfs = dataframes(columns([\"x\", \"y\", \"z\"]))\n>>> dfs.example()  \nshape: (3, 3)\n┌────────────┬───────┬────────────────────────────┐\n│ x ┆ y ┆ z │\n│ --- ┆ --- ┆ --- │\n│ date ┆ u16 ┆ datetime[μs] │\n╞════════════╪═══════╪════════════════════════════╡\n│ 0565-08-12 ┆ 34715 ┆ 5844-09-20 00:33:31.076854 │\n│ 3382-10-17 ┆ 48662 ┆ 7540-01-29 11:20:14.836271 │\n│ 4063-06-17 ┆ 39092 ┆ 1889-05-05 13:25:41.874455 │\n└────────────┴───────┴────────────────────────────┘\n\n\nGenerate frames with explicitly named/typed columns and a fixed size:\n\n```\n>>> dfs = dataframes(\n...     [\n...         column(\"x\", dtype=pl.Int32),\n...         column(\"y\", dtype=pl.Float64),\n...     ],\n...     size=2,\n... )\n>>> dfs.example()  \nshape: (2, 2)\n┌───────────┬────────────┐\n│ x ┆ y │\n│ --- ┆ --- │\n│ i32 ┆ f64 │\n╞═══════════╪════════════╡\n│ -15836 ┆ 1.1755e-38 │\n│ 575050513 ┆ NaN │\n└───────────┴────────────┘\n\n# polars.testing.parametric.series#\n\n\npolars.testing.parametric.series(\n\n*\\**,\n*name: str | SearchStrategy[str] | None = None*,\n*dtype: PolarsDataType | None = None*,\n*size: int | None = None*,\n*min\\_size: int | None = 0*,\n*max\\_size: int | None = 10*,\n*strategy: SearchStrategy[object] | None = None*,\n*null\\_probability: float = 0.0*,\n*allow\\_infinities: bool = True*,\n*unique:\n\n==================\n Document 2 \n----------------\n polars.testing.parametric.create\\_list\\_strategy#\n\n\npolars.testing.parametric.create\\_list\\_strategy(\n\n*inner\\_dtype: PolarsDataType | None*,\n*\\**,\n*select\\_from: Sequence[Any] | None = None*,\n*size: int | None = None*,\n*min\\_size: int | None = None*,\n*max\\_size: int | None = None*,\n*unique: bool = False*,\n\n) → SearchStrategy[list[Any]][source]#\nHypothesis strategy for producing polars List data.\n\n**inner\\_dtype**PolarsDataTypetype of the inner list elements (can also be another List).\n\n**select\\_from**list, optionalrandomly select the innermost values from this list (otherwise\nthe default strategy associated with the innermost dtype is used).\n\n**size**int, optionalif set, generated lists will be of exactly this size (and\nignore the min\\_size/max\\_size params).\n\n**min\\_size**int, optionalset the minimum size of the generated lists (default: 0 if unset).\n\n**max\\_size**int, optionalset the maximum size of the generated lists (default: 3 if\nmin\\_size is unset or zero, otherwise 2x min\\_size).\n\n**unique**bool, optionalensure that the generated lists contain unique values.\n\n\nCreate a strategy that generates a list of i32 values:\n\n```\n>>> lst = create\\_list\\_strategy(inner\\_dtype=pl.Int32)\n>>> lst.example()  \n[-11330, 24030, 116]\n\n\nCreate a strategy that generates lists of lists of specific strings:\n\n```\n>>> lst = create\\_list\\_strategy(\n...     inner\\_dtype=pl.List(pl.Utf8),\n...     select\\_from=[\"xx\", \"yy\", \"zz\"],\n... )\n>>> lst.example()  \n[['yy', 'xx'], [], ['zz']]\n\n\nCreate a UInt8 dtype strategy as a hypothesis composite that generates\npairs of small int values where the first is always <= the second:\n\n```\n>>> from hypothesis.strategies import composite\n>>>\n>>> @composite\n... def uint8\\_pairs(draw, uints=create\\_list\\_strategy(pl.UInt8, size=2)):\n...     pairs = list(zip(draw(uints), draw(uints)))\n...     return [sorted(ints) for ints in pairs]\n...\n>>> uint8\\_pairs().example()  \n[(12, 22), (15, 131)]\n>>> uint8\\_pairs().example()  \n[(59, 176), (149, 149)]\n\n\n# polars.testing.parametric.load\\_profile#\n\n\npolars.testing.parametric.load\\_profile(\n\n*profile: Literal['fast', 'balanced', 'expensive'] | int = 'fast'*,\n*\\**,\n*set\\_environment: bool = False*,\n\n) → None[source]#\nLoad a named (or custom) hypothesis profile for use with the parametric tests.\n\n**profile**{str, int}, optionalName of the profile to load; one of “fast”, “balanced”, “expensive”, or\nthe integer number of iterations to run (which will create and register\na custom profile with that value).\n\n**set\\_environment**bool, default FalseIf True, also set the environment variable `POLARS\\_HYPOTHESIS\\_PROFILE`\nto the given profile name/value.\n\n```\n>>> # load a custom profile that will run with 1500 iterations\n>>> from polars.testing.parametric.profiles import load\\_profile\n>>> load\\_profile(1500)\n\n\n\n# polars.testing.parametric.set\\_profile#\n\n\npolars.testing.parametric.set\\_profile(*profile: Literal['fast', 'balanced', 'expensive'] | int*) → None[source]#\nSet the env var `POLARS\\_HYPOTHESIS\\_PROFILE` to the given profile name/value.\n\n```\n>>> # prefer the 'balanced' profile for running parametric tests\n>>> from polars.testing.parametric.profiles import set\\_profile\n>>> set\\_profile(\"balanced\")\n\n\n\n# polars.build\\_info#\n\n\npolars.build\\_info() → dict[str, Any][source]#\nReturn a dict with Polars build information.\n\n\nIf Polars was compiled with “build\\_info” feature gate return the full build info,\notherwise only version is included. The full build information dict contains\nthe following keys [‘build’, ‘info-time’, ‘dependencies’, ‘features’, ‘host’,\n‘target’, ‘git’, ‘version’].\n\n\n\n# polars.get\\_index\\_type#\n\n\npolars.get\\_index\\_type() → DataTypeClass[source]#\nGet the datatype used for Polars indexing.\n\nDataType`UInt32` in regular Polars, `UInt64` in bigidx Polars.\n\n\n\n# polars.show\\_versions#\n\n\npolars.show\\_versions() → None[source]#\nPrint out version of Polars and dependencies to stdout.\n\n```\n>>> pl.show\\_versions()  \n--------Version info---------\nPolars: 0.17.11\nIndex type: UInt32\nPlatform: Linux-5.15.90.1-microsoft-standard-WSL2-x86\\_64-with-glibc2.35\nPython: 3.11.3 (main, Apr 15 2023, 14:44:51) [GCC 11.3.0]\n\\b\n----Optional dependencies----\nnumpy: 1.24.2\npandas: 2.0.0\npyarrow: 11.0.0\nconnectorx: <not installed>\ndeltalake: 0.8.1\nfsspec: 2023.4.0\nmatplotlib: 3.7.1\nxlsx2csv: 0.8.1\nxlsxwriter: 3.1.0\n\n\n\n# polars.threadpool\\_size#\n\n\npolars.threadpool\\_size() → int[source]#\nGet the number of threads in the Polars thread pool.\n\n\nThe threadpool size can be overridden by setting the `POLARS\\_MAX\\_THREADS`\nenvironment variable before process start. (The thread pool is not behind a\nlock, so it cannot be modified once set). A reasonable use-case for this might\nbe temporarily setting max threads to a low value before importing polars in a\npyspark UDF or similar context. Otherwise, it is strongly recommended not to\noverride this value as it will be set automatically by the engine.\n\n\n# polars.SQLContext.execute#\n\n\nSQLContext.execute(*query: str*, *eager: None = None*) → DataFrame[source]#\n\nSQLContext.execute(\n\n*query: str*,\n*eager: Literal[False]*,\n\n) → LazyFrame\n\n*query: str*,\n*eager: Literal[True]*,\n\nSQLContext.execute(*query: str*, *eager: None = None*) → LazyFrame\n\n) → DataFrame\nParse the given SQL query and execute it against the registered frame data.\n\n**query**A valid string SQL query.\n\n**eager**Apply"
    },
    {
        "instruction": "Create a Python program using the 'pyglove' API to perform network analysis. The program should analyze a network (e.g., social network or computer network) by loading a dataset, applying network analysis techniques, and outputting relevant statistics or insights. Additionally, provide a unit test that verifies the accuracy of the network analysis results.",
        "api": "pyglove",
        "output": "#!pip install pyglove\nimport pyglove as pg\nimport networkx as nx\n\ndef load_and_analyze_network():\n  # Replace this with your code to load and analyze a network\n  # The example below creates a simple social network and computes network statistics\n  G = nx.Graph()\n  G.add_edges_from([(1, 2), (2, 3), (3, 4), (4, 5), (5, 1)])\n\n  num_nodes = G.number_of_nodes()\n  num_edges = G.number_of_edges()\n  average_degree = sum(dict(G.degree()).values()) / num_nodes\n  network_density = nx.density(G)\n\n  return num_nodes, num_edges, average_degree, network_density\n\nif __name__ == '__main__':\n  num_nodes, num_edges, average_degree, network_density = load_and_analyze_network()\n\n  print(f'Number of Nodes: {num_nodes}')\n  print(f'Number of Edges: {num_edges}')\n  print(f'Average Degree: {average_degree:.2f}')\n  print(f'Network Density: {network_density:.2f}')\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# Public API: pyglove#\n\n\n## Modules#\n\n\n### core#\n\n* pg.detouring\n* pg.geno\n* pg.hyper\n* pg.object\\_utils\n* pg.patching\n* pg.symbolic\n* pg.tuning\n* pg.typing\n\n\n### ext#\n\n* pg.early\\_stopping\n* pg.evolution\n* pg.mutfun\n* pg.scalars\n\n\n\n## Top-level shortcurts#\n\n\n### Objects#\n\n\n* pg.MISSING\\_VALUE\n\n\n\n### Classes#\n\n\n* pg.ClassWrapper\n* pg.CustomTyping\n* pg.DNA\n* pg.DNAGenerator\n* pg.DNASpec\n* pg.Dict\n* pg.Diff\n* pg.DocStr\n* pg.Field\n* pg.FieldUpdate\n* pg.Formattable\n* pg.Functor\n* pg.Inferentiable\n* pg.InferredValue\n* pg.Insertion\n* pg.JSONConvertible\n* pg.KeyPath\n* pg.KeySpec\n* pg.List\n* pg.MaybePartial\n* pg.Object\n* pg.ObjectFactory\n* pg.Origin\n* pg.PureSymbolic\n* pg.Ref\n* pg.Schema\n* pg.Symbolic\n* pg.TraverseAction\n* pg.ValueSpec\n* pg.WritePermissionError\n* pg.dict\n* pg.list\n\n\n### Functions#\n\n\n* pg.allow\\_empty\\_field\\_description\n* pg.allow\\_partial\n* pg.allow\\_repeated\\_class\\_registration\n* pg.allow\\_writable\\_accessors\n* pg.apply\\_wrappers\n* pg.as\\_sealed\n* pg.auto\\_call\\_functors\n* pg.boilerplate\\_class\n* pg.catch\\_errors\n* pg.clone\n* pg.compound\n* pg.compound\\_class\n* pg.contains\n* pg.detour\n* pg.diff\n* pg.dna\\_spec\n* pg.docstr\n* pg.enable\\_type\\_check\n* pg.eq\n* pg.evolve\n* pg.explicit\\_method\\_override\n* pg.float\\_value\n* pg.floatv\n* pg.format\n* pg.from\\_json\n* pg.from\\_json\\_str\n* pg.functor\n* pg.functor\\_class\n* pg.get\\_converter\n* pg.get\\_load\\_handler\n* pg.get\\_save\\_handler\n* pg.get\\_signature\n* pg.gt\n* pg.hash\n* pg.is\\_abstract\n* pg.is\\_deterministic\n* pg.is\\_partial\n* pg.is\\_pure\\_symbolic\n*\n\n==================\n Document 1 \n----------------\n# Overview#\n\n\n`pg.object\\_utils` sits at the bottom of all PyGlove modules and empowers other\nmodules with the following features:\n\n> \n> \n> \n> \n> | Functionality | API |\n> | --- | --- |\n> | Formatting | `pg.Formattable`,\n> `pg.format`,\n> `pg.print`,\n> `pg.object\\_utils.kvlist\\_str`,\n> `pg.object\\_utils.quote\\_if\\_str`,\n> `pg.object\\_utils.message\\_on\\_path` |\n> | Serialization | `pg.JSONConvertible`,\n> `pg.registered\\_types`,\n> `pg.object\\_utils.to\\_json`,\n> `pg.object\\_utils.from\\_json`, |\n> | Partial construction | `pg.MaybePartial`,\n> `pg.MISSING\\_VALUE`. |\n> | Hierarchical key\n> representation | `pg.KeyPath` |\n> | Hierarchical object\n> traversal | `pg.object\\_utils.traverse` |\n> | Hierarchical object\n> transformation | `pg.object\\_utils.transform`,\n> `pg.object\\_utils.merge`,\n> `pg.object\\_utils.canonicalize`,\n> `pg.object\\_utils.flatten` |\n> | Code generation | `pg.object\\_utils.make\\_function` |\n> | Docstr handling | `pg.docstr`, |\n> | Error handling | `pg.catch\\_errors`, |\n> \n> \n> \n> \n\n\n## Objects#\n\n* MISSING\\_VALUE\n\n## Classes#\n\n* BracketType\n* CatchErrorsContext\n* DocStr\n* DocStrArgument\n* DocStrEntry\n* DocStrExample\n* DocStrRaises\n* DocStrReturns\n* DocStrStyle\n* Formattable\n* Functor\n* JSONConvertible\n* KeyPath\n* MaybePartial\n* MissingValue\n* StrKey\n\n* auto\\_plural\n* bracket\\_chars\n* canonicalize\n* catch\\_errors\n* comma\\_delimited\\_str\n* docstr\n* ensure\\_explicit\\_method\\_override\n* explicit\\_method\\_override\n* flatten\n* format\n* from\\_json\n* is\\_partial\n* kvlist\\_str\n* make\\_function\n* merge\n* merge\\_tree\n* message\\_on\\_path\n* print\n* quote\\_if\\_str\n* registered\\_types\n* thread\\_local\\_decrement\n* thread\\_local\\_del\n*\n\n==================\n Document 2 \n----------------\n pg.typing#\n\n\nSymbolic typing.\n\n\nTo enable symbolic programmability on classes and functions, PyGlove intercepts\nthe assignment operation on the attributes of symbolic objects, to achieve two\ngoals:\n\n> \n> * Enable automatic type checking, value validation, conversion and\n> transformation on attribute values. See Runtime typing for more details.\n> * Allow symbolic attributes to be placeheld by special symbols, in order to\n> represent abstract concepts such as a space of objects. E.g. hyper\n> primitives like `pg.oneof` placeholds a symbolic attribute to\n> create a space of values for that attribute. See Symbolic placeholding\n> for more details.\n> \n> \n> \n\n### Runtime typing#\n\n\nSymbolic objects are intended to be manipulated after creation. Without a\nruntime typing system, things can go wrong easily. For instance, an int\nattribute which was mistakenly modified at early program stages can be very\ndifficut to debug at later stages. PyGlove introduces a runtime type system\nthat automatically validates symbolic objects upon creation and modification,\nminimizing boilerplated code for input validation, so the developer can focus on\nthe main business logic.\n\n#### Understanding Schema#\n\n\nPyGlove’s runtime type system is based on the concept of `Schema` (\nclass `pg.Schema`), which defines what symbolic attributes are held\nby a symbolic type (e.g. a symbolic dict, a symbolic list or a symbolic class)\nand what values each attribute\n\n==================\n Document 3 \n----------------\n### Understanding Schema#\n\n\nPyGlove’s runtime type system is based on the concept of `Schema` (\nclass `pg.Schema`), which defines what symbolic attributes are held\nby a symbolic type (e.g. a symbolic dict, a symbolic list or a symbolic class)\nand what values each attribute accepts. A `Schema` object consists of a list\nof `Field` (class `pg.Field`), which define the acceptable\nkeys (class `pg.KeySpec`) and their values (class\n`pg.ValueSpec`) for these types. A `Schema` object is usually\ncreated automatically and associated with a symbolic type upon its declaration,\nthrough decorators such as `pg.members`, `pg.symbolize` or\n`pg.functor`. For example:\n\n```\n@pg.members([\n    ('x', pg.typing.Int(default=1)),\n    ('y', pg.typing.Float().noneable())\n])\nclass A(pg.Object):\n  pass\n\nprint(A.\\_\\_schema\\_\\_)\n\n@pg.symbolize([\n    ('a', pg.typing.Int()),\n    ('b', pg.typing.Float())\n])\ndef foo(a, b):\n  return a + b\n\nprint(foo.\\_\\_schema\\_\\_)\n\n\nThe first argument of all the decorators takes a list of field definitions,\nwith each described by a tuple of 4 items:\n\n```\n(key specification, value specification, doc string, field metadata)\n\n\nThe **key specification** and **value specification** are required, while the\ndoc string and the field metadata are optional. The key specification defines\nacceptable identifiers for this field, and the value specification defines the\nattribute’s value type, its default value, validation rules. The doc string will\nserve as the description for the field, and the field metadata can be used for\nfield-based code generation.\n\n\nThe following code snippet illustrates all supported `KeySpec` and\n`ValueSpec` subclasses and their usage with a manually created schema:\n\n```\nschema = pg.typing.create\\_schema([\n    # Primitive types.\n    ('a', pg.typing.Bool(default=True).noneable()),\n    ('b', True),       # Equivalent to ('b', pg.typing.Bool(default=True)).\n    ('c', pg.typing.Int()),\n    ('d', 0),          # Equivalent to ('d', pg.typing.Int(default=0)).\n    ('e', pg.typing.Int(\n        min\\_value=0,\n        max\\_value=10).noneable()),\n    ('f', pg.typing.Float()),\n    ('g', 1.0),        # Equivalent to ('g', pg.typing.Float(default=0.0)).\n    ('h', pg.typing.Str()),\n    ('i', 'foo'),      # Equivalent to ('i', pg.typing.Str(default='foo').\n    ('j', pg.typing.Str(regex='foo.\\*')),\n\n    # Enum type.\n    ('l', pg.typing.Enum('foo', ['foo', 'bar', 0, 1]))\n\n    # List type.\n    ('m', pg.typing.List(pg.typing.Int(), size=2, default=[])),\n    ('n', pg.typing.List(pg.typing.Dict([\n        ('n1', pg.typing.List(pg.typing.Int())),\n        ('n2', pg.typing.Str().noneable())\n    ]), min\\_size=1, max\\_size=10, default=[])),\n\n    # Dict type.\n    ('o', pg.typing.Dict([\n        ('o1', pg.typing.Int()),\n        ('o2', pg.typing.List(pg.typing.Dict([\n            ('o21', 1),\n            ('o22', 1.0),\n        ]))),\n        ('o3', pg.typing.Dict([\n            # Use of regex key,\n            (pg.typing.StrKey('n3.\\*'), pg.typing.Int())\n        ]))\n    ]))\n\n    # Tuple type.\n    ('p', pg.typing.Tuple([\n        ('p1', pg.typing.Int()),\n        ('p2', pg.typing.Str())\n    ]))\n\n    # Object type.\n    ('q', pg.typing.Object(A, default=A()))\n\n    # Type type.\n    ('r', pg.typing.Type(int))\n\n    # Callable type.\n    ('s', pg.typing.Callable([pg.typing.Int(), pg.typing.Int()],\n                              kw=[('a', pg.typing.Str())])),\n\n    # Functor type (same as Callable, but only for symbolic.Functor).\n    ('t', pg.typing.Functor([pg.typing.Str()],\n                             kwargs=[('a', pg.typing.Str())]))\n\n    # Union type.\n    ('u', pg.typing.Union([\n        pg.typing.Int(),\n        pg.typing.Str()\n    ], default=1),\n\n    # Any type.\n    ('v', pg.typing.Any(default=1))\n])\n##### Schema inheritance#\n\n\nSymbolic attributes can be inherited during subclassing. Accordingly, the\nschema that defines a symbolic class’ attributes can be inherited too by its\nsubclasses. The fields from the bases’ schema will be carried over into the\nsubclasses’ schema, while the subclass can"
    },
    {
        "instruction": "Create a Python program using the 'pyglove' API to perform topic modeling on a collection of text documents. The program should download and preprocess a collection of text documents, apply Latent Dirichlet Allocation (LDA) to the documents, and output the topics and their associated words. Additionally, provide a unit test that verifies the presence of topics and associated words in the output.",
        "api": "pyglove",
        "output": "#!pip install pyglove\nimport pyglove as pg\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport numpy as np\n\ndef download_and_preprocess_documents():\n  # Replace this with your code to download and preprocess a collection of text documents\n  documents = [\n    \"This is the first document.\",\n    \"This document is the second document.\",\n    \"And this is the third one.\",\n    \"Is this the first document?\"\n  ]\n  return documents\n\ndef apply_lda(documents, num_topics=5):\n  # Convert documents to document-term matrix\n  vectorizer = CountVectorizer()\n  dtm = vectorizer.fit_transform(documents)\n\n  # Apply Latent Dirichlet Allocation\n  lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)\n  lda.fit(dtm)\n\n  return vectorizer, lda\n\ndef extract_topics_and_words(vectorizer, lda, num_words=10):\n  feature_names = vectorizer.get_feature_names_out()\n  topics = []\n  for topic_idx, topic in enumerate(lda.components_):\n    top_words_idx = topic.argsort()[:-num_words - 1:-1]\n    top_words = [feature_names[i] for i in top_words_idx]\n    topics.append({'topic': topic_idx, 'words': top_words})\n  return topics\n\nif __name__ == '__main__':\n  documents = download_and_preprocess_documents()\n  num_topics = 3\n  vectorizer, lda = apply_lda(documents, num_topics)\n  topics = extract_topics_and_words(vectorizer, lda)\n\n  for topic in topics:\n    print(f'Topic {topic[\"topic\"]}: {\", \".join(topic[\"words\"])}')\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# Public API: pyglove#\n\n\n## Modules#\n\n\n### core#\n\n* pg.detouring\n* pg.geno\n* pg.hyper\n* pg.object\\_utils\n* pg.patching\n* pg.symbolic\n* pg.tuning\n* pg.typing\n\n\n### ext#\n\n* pg.early\\_stopping\n* pg.evolution\n* pg.mutfun\n* pg.scalars\n\n\n\n## Top-level shortcurts#\n\n\n### Objects#\n\n\n* pg.MISSING\\_VALUE\n\n\n\n### Classes#\n\n\n* pg.ClassWrapper\n* pg.CustomTyping\n* pg.DNA\n* pg.DNAGenerator\n* pg.DNASpec\n* pg.Dict\n* pg.Diff\n* pg.DocStr\n* pg.Field\n* pg.FieldUpdate\n* pg.Formattable\n* pg.Functor\n* pg.Inferentiable\n* pg.InferredValue\n* pg.Insertion\n* pg.JSONConvertible\n* pg.KeyPath\n* pg.KeySpec\n* pg.List\n* pg.MaybePartial\n* pg.Object\n* pg.ObjectFactory\n* pg.Origin\n* pg.PureSymbolic\n* pg.Ref\n* pg.Schema\n* pg.Symbolic\n* pg.TraverseAction\n* pg.ValueSpec\n* pg.WritePermissionError\n* pg.dict\n* pg.list\n\n\n### Functions#\n\n\n* pg.allow\\_empty\\_field\\_description\n* pg.allow\\_partial\n* pg.allow\\_repeated\\_class\\_registration\n* pg.allow\\_writable\\_accessors\n* pg.apply\\_wrappers\n* pg.as\\_sealed\n* pg.auto\\_call\\_functors\n* pg.boilerplate\\_class\n* pg.catch\\_errors\n* pg.clone\n* pg.compound\n* pg.compound\\_class\n* pg.contains\n* pg.detour\n* pg.diff\n* pg.dna\\_spec\n* pg.docstr\n* pg.enable\\_type\\_check\n* pg.eq\n* pg.evolve\n* pg.explicit\\_method\\_override\n* pg.float\\_value\n* pg.floatv\n* pg.format\n* pg.from\\_json\n* pg.from\\_json\\_str\n* pg.functor\n* pg.functor\\_class\n* pg.get\\_converter\n* pg.get\\_load\\_handler\n* pg.get\\_save\\_handler\n* pg.get\\_signature\n* pg.gt\n* pg.hash\n* pg.is\\_abstract\n* pg.is\\_deterministic\n* pg.is\\_partial\n* pg.is\\_pure\\_symbolic\n*\n\n==================\n Document 1 \n----------------\n# Overview#\n\n\n`pg.object\\_utils` sits at the bottom of all PyGlove modules and empowers other\nmodules with the following features:\n\n> \n> \n> \n> \n> | Functionality | API |\n> | --- | --- |\n> | Formatting | `pg.Formattable`,\n> `pg.format`,\n> `pg.print`,\n> `pg.object\\_utils.kvlist\\_str`,\n> `pg.object\\_utils.quote\\_if\\_str`,\n> `pg.object\\_utils.message\\_on\\_path` |\n> | Serialization | `pg.JSONConvertible`,\n> `pg.registered\\_types`,\n> `pg.object\\_utils.to\\_json`,\n> `pg.object\\_utils.from\\_json`, |\n> | Partial construction | `pg.MaybePartial`,\n> `pg.MISSING\\_VALUE`. |\n> | Hierarchical key\n> representation | `pg.KeyPath` |\n> | Hierarchical object\n> traversal | `pg.object\\_utils.traverse` |\n> | Hierarchical object\n> transformation | `pg.object\\_utils.transform`,\n> `pg.object\\_utils.merge`,\n> `pg.object\\_utils.canonicalize`,\n> `pg.object\\_utils.flatten` |\n> | Code generation | `pg.object\\_utils.make\\_function` |\n> | Docstr handling | `pg.docstr`, |\n> | Error handling | `pg.catch\\_errors`, |\n> \n> \n> \n> \n\n\n## Objects#\n\n* MISSING\\_VALUE\n\n## Classes#\n\n* BracketType\n* CatchErrorsContext\n* DocStr\n* DocStrArgument\n* DocStrEntry\n* DocStrExample\n* DocStrRaises\n* DocStrReturns\n* DocStrStyle\n* Formattable\n* Functor\n* JSONConvertible\n* KeyPath\n* MaybePartial\n* MissingValue\n* StrKey\n\n* auto\\_plural\n* bracket\\_chars\n* canonicalize\n* catch\\_errors\n* comma\\_delimited\\_str\n* docstr\n* ensure\\_explicit\\_method\\_override\n* explicit\\_method\\_override\n* flatten\n* format\n* from\\_json\n* is\\_partial\n* kvlist\\_str\n* make\\_function\n* merge\n* merge\\_tree\n* message\\_on\\_path\n* print\n* quote\\_if\\_str\n* registered\\_types\n* thread\\_local\\_decrement\n* thread\\_local\\_del\n*\n\n==================\n Document 2 \n----------------\n### Understanding Schema#\n\n\nPyGlove’s runtime type system is based on the concept of `Schema` (\nclass `pg.Schema`), which defines what symbolic attributes are held\nby a symbolic type (e.g. a symbolic dict, a symbolic list or a symbolic class)\nand what values each attribute accepts. A `Schema` object consists of a list\nof `Field` (class `pg.Field`), which define the acceptable\nkeys (class `pg.KeySpec`) and their values (class\n`pg.ValueSpec`) for these types. A `Schema` object is usually\ncreated automatically and associated with a symbolic type upon its declaration,\nthrough decorators such as `pg.members`, `pg.symbolize` or\n`pg.functor`. For example:\n\n```\n@pg.members([\n    ('x', pg.typing.Int(default=1)),\n    ('y', pg.typing.Float().noneable())\n])\nclass A(pg.Object):\n  pass\n\nprint(A.\\_\\_schema\\_\\_)\n\n@pg.symbolize([\n    ('a', pg.typing.Int()),\n    ('b', pg.typing.Float())\n])\ndef foo(a, b):\n  return a + b\n\nprint(foo.\\_\\_schema\\_\\_)\n\n\nThe first argument of all the decorators takes a list of field definitions,\nwith each described by a tuple of 4 items:\n\n```\n(key specification, value specification, doc string, field metadata)\n\n\nThe **key specification** and **value specification** are required, while the\ndoc string and the field metadata are optional. The key specification defines\nacceptable identifiers for this field, and the value specification defines the\nattribute’s value type, its default value, validation rules. The doc string will\nserve as the description for the field, and the field metadata can be used for\nfield-based code generation.\n\n\nThe following code snippet illustrates all supported `KeySpec` and\n`ValueSpec` subclasses and their usage with a manually created schema:\n\n```\nschema = pg.typing.create\\_schema([\n    # Primitive types.\n    ('a', pg.typing.Bool(default=True).noneable()),\n    ('b', True),       # Equivalent to ('b', pg.typing.Bool(default=True)).\n    ('c', pg.typing.Int()),\n    ('d', 0),          # Equivalent to ('d', pg.typing.Int(default=0)).\n    ('e', pg.typing.Int(\n        min\\_value=0,\n        max\\_value=10).noneable()),\n    ('f', pg.typing.Float()),\n    ('g', 1.0),        # Equivalent to ('g', pg.typing.Float(default=0.0)).\n    ('h', pg.typing.Str()),\n    ('i', 'foo'),      # Equivalent to ('i', pg.typing.Str(default='foo').\n    ('j', pg.typing.Str(regex='foo.\\*')),\n\n    # Enum type.\n    ('l', pg.typing.Enum('foo', ['foo', 'bar', 0, 1]))\n\n    # List type.\n    ('m', pg.typing.List(pg.typing.Int(), size=2, default=[])),\n    ('n', pg.typing.List(pg.typing.Dict([\n        ('n1', pg.typing.List(pg.typing.Int())),\n        ('n2', pg.typing.Str().noneable())\n    ]), min\\_size=1, max\\_size=10, default=[])),\n\n    # Dict type.\n    ('o', pg.typing.Dict([\n        ('o1', pg.typing.Int()),\n        ('o2', pg.typing.List(pg.typing.Dict([\n            ('o21', 1),\n            ('o22', 1.0),\n        ]))),\n        ('o3', pg.typing.Dict([\n            # Use of regex key,\n            (pg.typing.StrKey('n3.\\*'), pg.typing.Int())\n        ]))\n    ]))\n\n    # Tuple type.\n    ('p', pg.typing.Tuple([\n        ('p1', pg.typing.Int()),\n        ('p2', pg.typing.Str())\n    ]))\n\n    # Object type.\n    ('q', pg.typing.Object(A, default=A()))\n\n    # Type type.\n    ('r', pg.typing.Type(int))\n\n    # Callable type.\n    ('s', pg.typing.Callable([pg.typing.Int(), pg.typing.Int()],\n                              kw=[('a', pg.typing.Str())])),\n\n    # Functor type (same as Callable, but only for symbolic.Functor).\n    ('t', pg.typing.Functor([pg.typing.Str()],\n                             kwargs=[('a', pg.typing.Str())]))\n\n    # Union type.\n    ('u', pg.typing.Union([\n        pg.typing.Int(),\n        pg.typing.Str()\n    ], default=1),\n\n    # Any type.\n    ('v', pg.typing.Any(default=1))\n])\n##### Schema inheritance#\n\n\nSymbolic attributes can be inherited during subclassing. Accordingly, the\nschema that defines a symbolic class’ attributes can be inherited too by its\nsubclasses. The fields from the bases’ schema will be carried over into the\nsubclasses’ schema, while the subclass can\n\n==================\n Document 3 \n----------------\n pg.typing#\n\n\nSymbolic typing.\n\n\nTo enable symbolic programmability on classes and functions, PyGlove intercepts\nthe assignment operation on the attributes of symbolic objects, to achieve two\ngoals:\n\n> \n> * Enable automatic type checking, value validation, conversion and\n> transformation on attribute values. See Runtime typing for more details.\n> * Allow symbolic attributes to be placeheld by special symbols, in order to\n> represent abstract concepts such as a space of objects. E.g. hyper\n> primitives like `pg.oneof` placeholds a symbolic attribute to\n> create a space of values for that attribute. See Symbolic placeholding\n> for more details.\n> \n> \n> \n\n### Runtime typing#\n\n\nSymbolic objects are intended to be manipulated after creation. Without a\nruntime typing system, things can go wrong easily. For instance, an int\nattribute which was mistakenly modified at early program stages can be very\ndifficut to debug at later stages. PyGlove introduces a runtime type system\nthat automatically validates symbolic objects upon creation and modification,\nminimizing boilerplated code for input validation, so the developer can focus on\nthe main business logic.\n\n#### Understanding Schema#\n\n\nPyGlove’s runtime type system is based on the concept of `Schema` (\nclass `pg.Schema`), which defines what symbolic attributes are held\nby a symbolic type (e.g. a symbolic dict, a symbolic list or a symbolic class)\nand what values each attribute"
    },
    {
        "instruction": "Create a Python program using the 'pymc' API to perform Bayesian linear regression on a dataset. The program should define prior distributions for the model's parameters, including the intercept and slope, and use a likelihood function to model the observed data. After setting up the model, perform Bayesian inference to draw posterior samples using NUTS sampling.",
        "api": "pymc",
        "output": "#!pip install pymc arviz\nfrom pymc import HalfCauchy, Model, Normal, sample\nimport arviz as az\nimport numpy as np\n\nrng = np.random.default_rng(8927)\nsize = 200\ntrue_intercept = 1\ntrue_slope = 2\n\nx = np.linspace(0, 1, size)\n# y = a + b*x\ntrue_regression_line = true_intercept + true_slope * x\n# add noise\ny = true_regression_line + rng.normal(scale=0.5, size=size)\n\n\nwith Model() as model:  \n    # Define priors\n    sigma = HalfCauchy(\"sigma\", beta=10)\n    intercept = Normal(\"Intercept\", 0, sigma=20)\n    slope = Normal(\"slope\", 0, sigma=20)\n\n    # Define likelihood\n    likelihood = Normal(\"y\", mu=intercept + slope * x, sigma=sigma, observed=y)\n\n    # Inference!\n    # draw 30 posterior samples using NUTS sampling\n    idata = sample(30)\n\nprint(\"30 posterior samples using NUTS sampling: \")\nprint(az.summary(idata, round_to=2))",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# API#\n\n* Distributions\n* Gaussian Processes\n* Model\n* Samplers\n* Variational Inference\n* Sequential Monte Carlo\n* Data\n* Ordinary differential equations (ODEs)\n* Probability\n* Tuning\n* Math\n* PyTensor utils\n* shape\\_utils\n* Storage backends\n* Other utils\n\n\n\n## Dimensionality#\n\n\nPyMC provides numerous methods, and syntactic sugar, to easily specify the dimensionality of\nRandom Variables in modeling. Refer to Distribution Dimensionality notebook to see examples\ndemonstrating the functionality.\n\n\n\n## API extensions#\n\n\n### Plots, stats and diagnostics#\n\n\nPlots, stats and diagnostics are delegated to the\nArviZ.\nlibrary, a general purpose library for\n“exploratory analysis of Bayesian models”.\n\n\n* Functions from the arviz.plots module are available through `pymc.<function>` or `pymc.plots.<function>`,\nbut for their API documentation please refer to the ArviZ documentation.\n* Functions from the arviz.stats module are available through `pymc.<function>` or `pymc.stats.<function>`,\nbut for their API documentation please refer to the ArviZ documentation.\n\n\nArviZ is a dependency of PyMC and so, in addition to the locations described above,\nimporting ArviZ and using `arviz.<function>` will also work without any extra installation.\n\n\n\n### Generalized Linear Models (GLMs)#\n\n\nGeneralized Linear Models are delegated to the\nBambi.\nlibrary, a high-level Bayesian model-building\ninterface built on top of PyMC.\n\n\nBambi is not a dependency of PyMC and should be installed in addition to PyMC\nto use it to generate PyMC models via formula syntax.\n\n\n# Learn PyMC & Bayesian modeling#\n\n* Installation\n* Notebooks on core features\n* Books\n* Videos and Podcasts\n* Consulting\n* Glossary\n\n\n\n## At a glance#\n\n\n### Beginner#\n\n\n* Book: Bayesian Methods for Hackers\n* Book: Bayesian Analysis with Python\n\n\n\n### Intermediate#\n\n\n* Introductory Overview of PyMC shows PyMC 4.0 code in action\n* Example notebooks: PyMC Example Gallery\n\n\n\t+ GLM: Linear regression\n\t+ Prior and Posterior Predictive Checks\n\t+ Comparing models: Model comparison\n\t+ Shapes and dimensionality Distribution Dimensionality\n* Videos and Podcasts\n* Book: Bayesian Modeling and Computation in Python\n\n\n\n### Advanced#\n\n\n* Experimental and cutting edge functionality: PyMC experimental library\n* PyMC internals guides (To be outlined and referenced here once pymc#5538\nis addressed)\n\n# Distributions#\n\n* Continuous\n\t+ pymc.AsymmetricLaplace\n\t+ pymc.Beta\n\t+ pymc.Cauchy\n\t+ pymc.ChiSquared\n\t+ pymc.ExGaussian\n\t+ pymc.Exponential\n\t+ pymc.Flat\n\t+ pymc.Gamma\n\t+ pymc.Gumbel\n\t+ pymc.HalfCauchy\n\t+ pymc.HalfFlat\n\t+ pymc.HalfNormal\n\t+ pymc.HalfStudentT\n\t+ pymc.Interpolated\n\t+ pymc.InverseGamma\n\t+ pymc.Kumaraswamy\n\t+ pymc.Laplace\n\t+ pymc.Logistic\n\t+ pymc.LogitNormal\n\t+ pymc.LogNormal\n\t+ pymc.Moyal\n\t+ pymc.Normal\n\t+ pymc.Pareto\n\t+ pymc.PolyaGamma\n\t+ pymc.Rice\n\t+ pymc.SkewNormal\n\t+ pymc.StudentT\n\t+ pymc.Triangular\n\t+ pymc.TruncatedNormal\n\t+ pymc.Uniform\n\t+ pymc.VonMises\n\t+ pymc.Wald\n\t+ pymc.Weibull\n* Discrete\n\t+ pymc.Bernoulli\n\t+ pymc.BetaBinomial\n\t+ pymc.Binomial\n\t+\n\n==================\n Document 1 \n----------------\n One dimensional column vectors of inputs\nX1 = np.linspace(0, 1, 10)[:, None]\nX2 = np.linspace(0, 2, 5)[:, None]\nXs = [X1, X2]\ny = np.random.randn(len(X1)\\*len(X2))  # toy data\nwith pm.Model() as model:\n    # Specify the covariance functions for each Xi\n    cov\\_func1 = pm.gp.cov.ExpQuad(1, ls=0.1)  # Must accept X1 without error\n    cov\\_func2 = pm.gp.cov.ExpQuad(1, ls=0.3)  # Must accept X2 without error\n\n    # Specify the GP. The default mean function is `Zero`.\n    gp = pm.gp.MarginalKron(cov\\_funcs=[cov\\_func1, cov\\_func2])\n\n    # Place a GP prior over the function f.\n    sigma = pm.HalfCauchy(\"sigma\", beta=3)\n    y\\_ = gp.marginal\\_likelihood(\"y\", Xs=Xs, y=y, sigma=sigma)\n\n\n|  |  |\n| --- | --- |\n| `MarginalKron.\\_\\_init\\_\\_`(\\*[, mean\\_func, cov\\_funcs]) |  |\n| `MarginalKron.conditional`(name, Xnew[, ...]) | Returns the conditional distribution evaluated over new input locations Xnew, just as in Marginal. |\n| `MarginalKron.marginal\\_likelihood`(name, Xs, ...) | Returns the marginal likelihood distribution, given the input locations cartesian(\\*Xs) and the data y. |\n| `MarginalKron.predict`(Xnew[, point, diag, ...]) | Return the mean vector and covariance matrix of the conditional distribution as numpy arrays, given a point, such as the MAP estimate or a sample from a trace. |\n| `MarginalKron.prior`(name, X, \\*args, \\*\\*kwargs) |  |\n\n\n|  |  |\n| --- | --- |\n| `Xs` |  |\n| `sigma` |  |\n| `y` |  |\n\n# pymc.gp.MarginalApprox#\n\n\n*class* pymc.gp.MarginalApprox(*approx='VFE'*, *\\**, *mean\\_func=<pymc.gp.mean.Zero object>*, *cov\\_func=<pymc.gp.cov.Constant object>*)[source]#\nApproximate marginal Gaussian process.\n\n\nThe gp.MarginalApprox class is an implementation of the sum of a GP\nprior and additive noise. It has marginal\\_likelihood, conditional\nand predict methods. This GP implementation can be used to\nimplement regression on\n\n==================\n Document 2 \n----------------\n Setup data\ntrue\\_colcov = np.array([[1.0, 0.5, 0.1],\n                        [0.5, 1.0, 0.2],\n                        [0.1, 0.2, 1.0]])\nm = 3\nn = true\\_colcov.shape[0]\ntrue\\_scale = 3\ntrue\\_rowcov = np.diag([true\\_scale\\*\\*(2\\*i) for i in range(m)])\nmu = np.zeros((m, n))\ntrue\\_kron = np.kron(true\\_rowcov, true\\_colcov)\ndata = np.random.multivariate\\_normal(mu.flatten(), true\\_kron)\ndata = data.reshape(m, n)\n\nwith pm.Model() as model:\n    # Setup right cholesky matrix\n    sd\\_dist = pm.HalfCauchy.dist(beta=2.5, shape=3)\n    colchol\\_packed = pm.LKJCholeskyCov('colcholpacked', n=3, eta=2,\n                                       sd\\_dist=sd\\_dist)\n    colchol = pm.expand\\_packed\\_triangular(3, colchol\\_packed)\n\n    # Setup left covariance matrix\n    scale = pm.LogNormal('scale', mu=np.log(true\\_scale), sigma=0.5)\n    rowcov = pt.diag([scale\\*\\*(2\\*i) for i in range(m)])\n\n    vals = pm.MatrixNormal('vals', mu=mu, colchol=colchol, rowcov=rowcov,\n                           observed=data)\n\n\n|  |  |\n| --- | --- |\n| `MatrixNormal.dist`(mu[, rowcov, rowchol, ...]) | Creates a tensor variable corresponding to the cls distribution. |\n\n# pymc.Multinomial#\n\n\n*class* pymc.Multinomial(*name*, *\\*args*, *\\*\\*kwargs*)[source]#\nMultinomial log-likelihood.\n\n\nGeneralizes binomial distribution, but instead of each trial resulting\nin “success” or “failure”, each one results in exactly one of some\nfixed finite number k of possible outcomes over n independent trials.\n‘x[i]’ indicates the number of times\n\n==================\n Document 3 \n----------------\n pymc.find\\_constrained\\_prior#\n\n\npymc.find\\_constrained\\_prior(*distribution*, *lower*, *upper*, *init\\_guess*, *mass=0.95*, *fixed\\_params=None*, *mass\\_below\\_lower=None*, *\\*\\*kwargs*)[source]#\nFind optimal parameters to get mass % of probability\nof a distribution between lower and upper.\n\n\nNote: only works for one- and two-parameter distributions, as there\nare exactly two constraints. Fix some combination of parameters\nif you want to use it on >=3-parameter distributions.\n\n**distribution**DistributionPyMC distribution you want to set a prior on.\nNeeds to have a `logcdf` method implemented in PyMC.\n\n**lower**`float`Lower bound to get mass % of probability of pm\\_dist.\n\n**upper**`float`Upper bound to get mass % of probability of pm\\_dist.\n\n**init\\_guess**`dict` of {`str``float`}Initial guess for `scipy.optimize.least\\_squares` to find the\noptimal parameters of pm\\_dist fitting the interval constraint.\nMust be a dictionary with the name of the PyMC distribution’s\nparameter as keys and the initial guess as values.\n\n**mass**`float`, default 0.95Share of the probability mass we want between `lower` and `upper`.\nDefaults to 95%.\n\n**fixed\\_params**`str` or `float`, optional, default `None`Only used when pm\\_dist has at least three parameters.\nDictionary of fixed parameters, so that there are only 2 to optimize.\nFor instance, for a StudentT, you fix nu to a constant and get the optimized\nmu and sigma.\n\n**mass\\_below\\_lower**`float`, optional, default `None`The probability mass below the `lower` bound. If `None`,\ndefaults to `(1 - mass) / 2`, which implies that the probability\nmass below the `lower` value will be equal to the probability\nmass above the `upper` value.\n\n**opt\\_params**`dict`The optimized distribution parameters as a dictionary.\nDictionary keys are the parameter names and\ndictionary values are the optimized parameter values.\n\n\nOptional keyword arguments can be passed to `find\\_constrained\\_prior`. These will be\ndelivered to the underlying call to `scipy.optimize.minimize()`.\n\n```\n# get parameters obeying constraints\nopt\\_params = pm.find\\_constrained\\_prior(\n    pm.Gamma, lower=0.1, upper=0.4, mass=0.75, init\\_guess={\"alpha\": 1, \"beta\": 10}\n)\n\n\n# use these parameters to draw random samples\nsamples = pm.Gamma.dist(\\*\\*opt\\_params, size=100).eval()\n\n\n# use these parameters in a model\nwith pm.Model():\n    x = pm.Gamma('x', \\*\\*opt\\_params)\n\n\n# specify fixed values before optimization\nopt\\_params = pm.find\\_constrained\\_prior(\n    pm.StudentT,\n    lower=0,\n    upper=1,\n    init\\_guess={\"mu\": 5, \"sigma\": 2},\n    fixed\\_params={\"nu\": 7},\n)\n\n\nUnder some circumstances, you might not want to have the same cumulative\nprobability below the `lower` threshold and above the `upper` threshold.\nFor example, you might want to constrain an Exponential distribution to\nfind the parameter that yields 90% of the mass below the `upper` bound,\nand have zero mass below `lower`. You can do that with the following call\nto `find\\_constrained\\_prior`\n\n```\nopt\\_params = pm.find\\_constrained\\_prior(\n    pm.Exponential,\n    lower=0,\n    upper=3.,\n    mass=0.9,\n    init\\_guess={\"lam\": 1},\n    mass\\_below\\_lower=0,\n)\n\n\n# pymc.DictToArrayBijection#\n\n\n*class* pymc.DictToArrayBijection[source]#\nMap between a `dict`s of variables to an array space.\n\n\nSaid array space consists of all the vars raveled and then concatenated.\n\n\n|  |  |\n| --- | --- |\n| `DictToArrayBijection.\\_\\_init\\_\\_`(\\*args, \\*\\*kwargs) |  |\n| `DictToArrayBijection.map`(var\\_dict) | Map a dictionary\n\n==================\n Document 4 \n----------------\n pymc.sampling.forward.sample\\_prior\\_predictive#\n\n\npymc.sampling.forward.sample\\_prior\\_predictive(*samples=500*, *model=None*, *var\\_names=None*, *random\\_seed=None*, *return\\_inferencedata=True*, *idata\\_kwargs=None*, *compile\\_kwargs=None*)[source]#\nGenerate samples from the prior predictive distribution.\n\n**samples**`int`Number of samples from the prior predictive to generate. Defaults to 500.\n\n**model**`Model` (optional `if` `in` `with` `context`)\n**var\\_names**`Iterable`[`str`]A list of names of variables for which to compute the prior predictive\nsamples. Defaults to both observed and unobserved RVs. Transformed values\nare not allowed.\n\n**random\\_seed**`int`, `RandomState` or `Generator`, optionalSeed for the random number generator.\n\n**return\\_inferencedata**boolWhether to return an `arviz.InferenceData` (True) object or a dictionary (False).\nDefaults to True.\n\n**idata\\_kwargs**`dict`, optionalKeyword arguments for `pymc.to\\_inference\\_data()`\n\n**compile\\_kwargs: dict, optional**Keyword arguments for `pymc.pytensorf.compile\\_pymc()`.\n\n`arviz.InferenceData` or `Dict`An ArviZ `InferenceData` object containing the prior and prior predictive samples (default),\nor a dictionary with variable names as keys and samples as numpy arrays.\n\n# pymc.sampling.forward.sample\\_posterior\\_predictive#\n\n\npymc.sampling.forward.sample\\_posterior\\_predictive(*trace*, *model=None*, *var\\_names=None*, *sample\\_dims=None*, *random\\_seed=None*, *progressbar=True*, *return\\_inferencedata=True*, *extend\\_inferencedata=False*, *predictions=False*, *idata\\_kwargs=None*, *compile\\_kwargs=None*)[source]#\nGenerate posterior predictive samples from a model given a trace.\n\n**trace**`backend`, `list`, `xarray.Dataset`, `arviz.InferenceData`, or `MultiTrace`Trace generated from MCMC sampling, or a list of dicts (eg. points or from find\\_MAP()),\nor\n\n==================\n Document 5 \n----------------\n pymc.Wald#\n\n\n*class* pymc.Wald(*name*, *\\*args*, *rng=None*, *dims=None*, *initval=None*, *observed=None*, *total\\_size=None*, *transform=UNSET*, *\\*\\*kwargs*)[source]#\nWald log-likelihood.\n\n\\[f(x \\mid \\mu, \\lambda) =\n \\left(\\frac{\\lambda}{2\\pi}\\right)^{1/2} x^{-3/2}\n \\exp\\left\\{\n -\\frac{\\lambda}{2x}\\left(\\frac{x-\\mu}{\\mu}\\right)^2\n \\right\\}\\]\n(`Source code`, `png`, `hires.png`, `pdf`)\n\n\n\n|  |  |\n| --- | --- |\n| Support | \\(x \\in (0, \\infty)\\) |\n| Mean | \\(\\mu\\) |\n| Variance | \\(\\dfrac{\\mu^3}{\\lambda}\\) |\n\n\nWald distribution can be parameterized either in terms of lam or phi.\nThe link between the two parametrizations is given by\n\n\\[\\phi = \\dfrac{\\lambda}{\\mu}\\]\n\n**mu**tensor\\_like of `float`, optionalMean of the distribution (mu > 0).\n\n**lam**tensor\\_like of `float`, optionalRelative precision (lam > 0).\n\n**phi**tensor\\_like of `float`, optionalAlternative shape parameter (phi > 0).\n\n**alpha**tensor\\_like of `float`, default 0Shift/location parameter (alpha >= 0).\n\n\nTo instantiate the distribution specify any of the following\n\n\n* only mu (in this case lam will be 1)\n* mu and lam\n* mu and phi\n* lam and phi\n\n\n[Tweedie1957]\nTweedie, M. C. K. (1957).\nStatistical Properties of Inverse Gaussian Distributions I.\nThe Annals of Mathematical Statistics, Vol. 28, No. 2, pp. 362-377\n\n\n[Michael1976]\nMichael, J. R., Schucany, W. R. and Hass, R. W. (1976).\nGenerating Random Variates Using Transformations with Multiple Roots.\nThe American Statistician, Vol. 30, No. 2, pp. 88-90\n\n\n[Giner2016]\nGöknur Giner, Gordon K. Smyth (2016)\nstatmod: Probability Calculations for the Inverse Gaussian Distribution\n\n\n|  |  |\n| --- | --- |\n| `Wald.dist`([mu, lam, phi, alpha]) | Creates a tensor variable corresponding to the cls distribution. |\n\n\n# pymc.Weibull#\n\n\n*class* pymc.Weibull(*name*, *\\*args*, *rng=None*, *dims=None*, *initval=None*, *observed=None*, *total\\_size=None*, *transform=UNSET*, *\\*\\*kwargs*)[source]#\nWeibull log-likelihood.\n\n\\[f(x \\mid \\alpha, \\beta) =\n \\frac{\\alpha x^{\\alpha - 1}\n \\exp(-(\\frac{x}{\\beta})^{\\alpha})}{\\beta^\\alpha}\\]\n(`Source code`, `png`, `hires.png`, `pdf`)\n\n\n\n|  |  |\n| --- | --- |\n| Support | \\(x \\in [0, \\infty)\\) |\n| Mean | \\(\\beta \\Gamma(1 + \\frac{1}{\\alpha})\\) |\n| Variance | \\(\\beta^2 \\Gamma(1 + \\frac{2}{\\alpha} - \\mu^2/\\beta^2)\\) |\n\n**alpha**`float`Shape parameter (alpha > 0).\n\n**beta**`float`Scale parameter (beta > 0).\n\n\n|  |  |\n| --- | --- |\n| `Weibull.dist`(alpha, beta, \\*args, \\*\\*kwargs) | Creates a tensor variable corresponding to the cls distribution. |\n\n\n# Discrete#\n\n\n|  |  |\n| --- | --- |\n| `Bernoulli`(name, \\*args, \\*\\*kwargs) | Bernoulli log-likelihood |\n| `BetaBinomial`(name, \\*args, \\*\\*kwargs) | Beta-binomial log-likelihood. |\n| `Binomial`(name, \\*args, \\*\\*kwargs) | Binomial log-likelihood. |\n| `Categorical`(name, \\*args, \\*\\*kwargs) | Categorical log-likelihood. |\n| `DiscreteUniform`(name, \\*args, \\*\\*kwargs) | Discrete uniform distribution. |\n| `DiscreteWeibull`(name, \\*args, \\*\\*kwargs) | Discrete Weibull\n\n==================\n Document 6 \n----------------\n pymc.model.transform.conditioning.observe#\n\n\npymc.model.transform.conditioning.observe(*model*, *vars\\_to\\_observations*)[source]#\nConvert free RVs or Deterministics to observed RVs.\n\n**model: PyMC Model**\n**vars\\_to\\_observations: Dict of variable or name to TensorLike**Dictionary that maps model variables (or names) to observed values.\nObserved values must have a shape and data type that is compatible\nwith the original model variable.\n\nnew\\_model: `PyMC` `model`A distinct PyMC model with the relevant variables observed.\nAll remaining variables are cloned and can be retrieved via new\\_model[“var\\_name”].\n\nwith pm.Model() as m:\n    x = pm.Normal(\"x\")\n    y = pm.Normal(\"y\", x)\n    z = pm.Normal(\"z\", y)\n\nm\\_new = pm.observe(m, {y: 0.5})\n\n\nDeterministic variables can also be observed.\nThis relies on PyMC ability to infer the logp of the underlying expression\n\nwith pm.Model() as m:\n    x = pm.Normal(\"x\")\n    y = pm.Normal.dist(x, shape=(5,))\n    y\\_censored = pm.Deterministic(\"y\\_censored\", pm.math.clip(y, -1, 1))\n\nnew\\_m = pm.observe(m, {y\\_censored: [0.9, 0.5, 0.3, 1, 1]})\n\n\n# pymc.model.transform.conditioning.remove\\_value\\_transforms#\n\n\npymc.model.transform.conditioning.remove\\_value\\_transforms(*model*, *vars=None*)[source]#\nRemove the value variables transforms in the model\n\n**model**`Model`\n**vars**`Model` `variables`, optionalModel variables for which to remove transforms. Defaults to all transformed variables\n\n**new\\_model**`Model`Model with the removed transformed value variables\n\n```\nimport pymc as pm\nfrom pymc.model.transform.conditioning import remove\\_value\\_transforms\n\nwith pm.Model() as transformed\\_m:\n    p = pm.Uniform(\"p\", 0, 1)\n    w = pm.Binomial(\"w\", n=9, p=p, observed=6)\n    mean\\_q = pm.find\\_MAP()\n\nwith remove\\_value\\_transforms(transformed\\_m) as untransformed\\_m:\n    new\\_p = untransformed\\_m[\"p\"]\n    std\\_q = ((1 / pm.find\\_hessian(mean\\_q, vars=[new\\_p])) \\*\\* 0.5)[0]\n    print(f\" Mean, Standard deviation\\np {mean\\_q['p']:.2}, {std\\_q[0]:.2}\")\n\n\n# Mean, Standard deviation\n\n# p 0.67, 0.16\n\n\n\n# FunctionGraph#\n\n\n|  |  |\n| --- | --- |\n| `clone\\_model`(model) | Clone a PyMC model. |\n| `fgraph\\_from\\_model`(model[, inlined\\_views]) | Convert Model to FunctionGraph. |\n| `model\\_from\\_fgraph`(fgraph) | Convert FunctionGraph to PyMC model. |\n\n\n# pymc.model.fgraph.clone\\_model#\n\n\npymc.model.fgraph.clone\\_model(*model*)[source]#\nClone a PyMC model.\n\n\nRecreates a PyMC model with clones of the original variables.\nShared variables will point to the same container but be otherwise different objects.\nConstants are not cloned.\n\n```\nimport pymc as pm\nfrom pymc.model.fgraph import clone\\_model\n\nwith pm.Model() as m:\n    p = pm.Beta(\"p\", 1, 1)\n    x = pm.Bernoulli(\"x\", p=p, shape=(3,))\n\nwith clone\\_model(m) as clone\\_m:\n    # Access cloned variables by name\n    clone\\_x = clone\\_m[\"x\"]\n\n    # z will be part of clone\\_m but not m\n    z = pm.Deterministic(\"z\", clone\\_x + 1)\n\n\n\n# pymc.model.fgraph.fgraph\\_from\\_model#\n\n\npymc.model.fgraph.fgraph\\_from\\_model(*model*, *inlined\\_views=False*)[source]#\nConvert Model to FunctionGraph.\n\n\nSee: model\\_from\\_fgraph\n\n**model: PyMC model**\n**inlined\\_views: bool, default False**Whether “view” variables (Deterministics and Data) should be inlined among RVs in the fgraph,\nor show up as separate branches.\n\nfgraph: `FunctionGraph`FunctionGraph that includes a copy of model variables, wrapped in dummy ModelVar Ops.\nIt should be possible to reconstruct a valid PyMC model using model\\_from\\_fgraph.\n\nmemo: `Dict`A dictionary mapping original model variables to the equivalent nodes in the fgraph.\n\n\n\n# pymc.model.fgraph.model\\_from\\_fgraph#\n\n\npymc.model.fgraph.model\\_from\\_fgraph(*fgraph*)[source]#\nConvert FunctionGraph to PyMC model.\n\n\nThis requires nodes to be properly tagged with ModelVar dummy Ops.\n\n\nSee: fgraph\\_from\\_model\n\n\n# Samplers#\n\n\nThis submodule contains functions for MCMC and forward sampling.\n\n\n|  |  |\n| --- | --- |\n| `sample\\_prior\\_predictive`([samples, model, ...]) | Generate samples from the prior predictive distribution. |\n| `sample\\_posterior\\_predictive`(trace[, model, ...]) | Generate posterior predictive samples from a model given a trace."
    },
    {
        "instruction": "Create a Python program using the 'pymc' API to perform Bayesian inference on a dataset of observed continuous outcomes. The program should define a prior distribution for the mean and standard deviation, and use a likelihood function to model the observed outcomes. After setting up the model, perform Bayesian inference to draw posterior samples using Hamiltonian Monte Carlo (HMC) sampling.",
        "api": "pymc",
        "output": "#!pip install pymc arviz\nfrom pymc import Model, Normal, sample\nimport arviz as az\nimport numpy as np\n\nrng = np.random.default_rng(8927)\nsize = 100\ntrue_mean = 0\ntrue_std = 1\n\n# Simulate observed continuous outcomes\noutcomes = rng.normal(true_mean, true_std, size=size)\n\nwith Model() as model:\n    # Define prior\n    mean = Normal(\"mean\", mu=0, sigma=10)\n    std = Normal(\"std\", mu=1, sigma=10)\n\n    # Define likelihood\n    likelihood = Normal(\"likelihood\", mu=mean, sigma=std, observed=outcomes)\n\n    # Inference!\n    # draw 1000 posterior samples using Hamiltonian Monte Carlo (HMC) sampling\n    idata = sample(1000, tune=500)\n\nprint(\"1000 posterior samples using Hamiltonian Monte Carlo (HMC) sampling: \")\nprint(az.summary(idata, round_to=2))",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# API#\n\n* Distributions\n* Gaussian Processes\n* Model\n* Samplers\n* Variational Inference\n* Sequential Monte Carlo\n* Data\n* Ordinary differential equations (ODEs)\n* Probability\n* Tuning\n* Math\n* PyTensor utils\n* shape\\_utils\n* Storage backends\n* Other utils\n\n\n\n## Dimensionality#\n\n\nPyMC provides numerous methods, and syntactic sugar, to easily specify the dimensionality of\nRandom Variables in modeling. Refer to Distribution Dimensionality notebook to see examples\ndemonstrating the functionality.\n\n\n\n## API extensions#\n\n\n### Plots, stats and diagnostics#\n\n\nPlots, stats and diagnostics are delegated to the\nArviZ.\nlibrary, a general purpose library for\n“exploratory analysis of Bayesian models”.\n\n\n* Functions from the arviz.plots module are available through `pymc.<function>` or `pymc.plots.<function>`,\nbut for their API documentation please refer to the ArviZ documentation.\n* Functions from the arviz.stats module are available through `pymc.<function>` or `pymc.stats.<function>`,\nbut for their API documentation please refer to the ArviZ documentation.\n\n\nArviZ is a dependency of PyMC and so, in addition to the locations described above,\nimporting ArviZ and using `arviz.<function>` will also work without any extra installation.\n\n\n\n### Generalized Linear Models (GLMs)#\n\n\nGeneralized Linear Models are delegated to the\nBambi.\nlibrary, a high-level Bayesian model-building\ninterface built on top of PyMC.\n\n\nBambi is not a dependency of PyMC and should be installed in addition to PyMC\nto use it to generate PyMC models via formula syntax.\n\n\n# Learn PyMC & Bayesian modeling#\n\n* Installation\n* Notebooks on core features\n* Books\n* Videos and Podcasts\n* Consulting\n* Glossary\n\n\n\n## At a glance#\n\n\n### Beginner#\n\n\n* Book: Bayesian Methods for Hackers\n* Book: Bayesian Analysis with Python\n\n\n\n### Intermediate#\n\n\n* Introductory Overview of PyMC shows PyMC 4.0 code in action\n* Example notebooks: PyMC Example Gallery\n\n\n\t+ GLM: Linear regression\n\t+ Prior and Posterior Predictive Checks\n\t+ Comparing models: Model comparison\n\t+ Shapes and dimensionality Distribution Dimensionality\n* Videos and Podcasts\n* Book: Bayesian Modeling and Computation in Python\n\n\n\n### Advanced#\n\n\n* Experimental and cutting edge functionality: PyMC experimental library\n* PyMC internals guides (To be outlined and referenced here once pymc#5538\nis addressed)\n\n# Distributions#\n\n* Continuous\n\t+ pymc.AsymmetricLaplace\n\t+ pymc.Beta\n\t+ pymc.Cauchy\n\t+ pymc.ChiSquared\n\t+ pymc.ExGaussian\n\t+ pymc.Exponential\n\t+ pymc.Flat\n\t+ pymc.Gamma\n\t+ pymc.Gumbel\n\t+ pymc.HalfCauchy\n\t+ pymc.HalfFlat\n\t+ pymc.HalfNormal\n\t+ pymc.HalfStudentT\n\t+ pymc.Interpolated\n\t+ pymc.InverseGamma\n\t+ pymc.Kumaraswamy\n\t+ pymc.Laplace\n\t+ pymc.Logistic\n\t+ pymc.LogitNormal\n\t+ pymc.LogNormal\n\t+ pymc.Moyal\n\t+ pymc.Normal\n\t+ pymc.Pareto\n\t+ pymc.PolyaGamma\n\t+ pymc.Rice\n\t+ pymc.SkewNormal\n\t+ pymc.StudentT\n\t+ pymc.Triangular\n\t+ pymc.TruncatedNormal\n\t+ pymc.Uniform\n\t+ pymc.VonMises\n\t+ pymc.Wald\n\t+ pymc.Weibull\n* Discrete\n\t+ pymc.Bernoulli\n\t+ pymc.BetaBinomial\n\t+ pymc.Binomial\n\t+\n\n==================\n Document 1 \n----------------\n Distributions#\n\n* Continuous\n\t+ pymc.AsymmetricLaplace\n\t+ pymc.Beta\n\t+ pymc.Cauchy\n\t+ pymc.ChiSquared\n\t+ pymc.ExGaussian\n\t+ pymc.Exponential\n\t+ pymc.Flat\n\t+ pymc.Gamma\n\t+ pymc.Gumbel\n\t+ pymc.HalfCauchy\n\t+ pymc.HalfFlat\n\t+ pymc.HalfNormal\n\t+ pymc.HalfStudentT\n\t+ pymc.Interpolated\n\t+ pymc.InverseGamma\n\t+ pymc.Kumaraswamy\n\t+ pymc.Laplace\n\t+ pymc.Logistic\n\t+ pymc.LogitNormal\n\t+ pymc.LogNormal\n\t+ pymc.Moyal\n\t+ pymc.Normal\n\t+ pymc.Pareto\n\t+ pymc.PolyaGamma\n\t+ pymc.Rice\n\t+ pymc.SkewNormal\n\t+ pymc.StudentT\n\t+ pymc.Triangular\n\t+ pymc.TruncatedNormal\n\t+ pymc.Uniform\n\t+ pymc.VonMises\n\t+ pymc.Wald\n\t+ pymc.Weibull\n* Discrete\n\t+ pymc.Bernoulli\n\t+ pymc.BetaBinomial\n\t+ pymc.Binomial\n\t+ pymc.Categorical\n\t+ pymc.DiscreteUniform\n\t+ pymc.DiscreteWeibull\n\t+ pymc.Geometric\n\t+ pymc.HyperGeometric\n\t+ pymc.NegativeBinomial\n\t+ pymc.OrderedLogistic\n\t+ pymc.OrderedProbit\n\t+ pymc.Poisson\n* Multivariate\n\t+ pymc.CAR\n\t+ pymc.Dirichlet\n\t+ pymc.DirichletMultinomial\n\t+ pymc.ICAR\n\t+ pymc.KroneckerNormal\n\t+ pymc.LKJCholeskyCov\n\t+ pymc.LKJCorr\n\t+ pymc.MatrixNormal\n\t+ pymc.Multinomial\n\t+ pymc.MvNormal\n\t+ pymc.MvStudentT\n\t+ pymc.OrderedMultinomial\n\t+ pymc.StickBreakingWeights\n\t+ pymc.Wishart\n\t+ pymc.WishartBartlett\n\t+ pymc.ZeroSumNormal\n* Mixture\n\t+ pymc.Mixture\n\t+ pymc.NormalMixture\n\t+ pymc.ZeroInflatedBinomial\n\t+ pymc.ZeroInflatedNegativeBinomial\n\t+ pymc.ZeroInflatedPoisson\n\t+ pymc.HurdlePoisson\n\t+ pymc.HurdleNegativeBinomial\n\t+ pymc.HurdleGamma\n\t+ pymc.HurdleLogNormal\n* Timeseries\n\t+ pymc.AR\n\t+ pymc.EulerMaruyama\n\t+ pymc.GARCH11\n\t+ pymc.GaussianRandomWalk\n\t+ pymc.MvGaussianRandomWalk\n\t+ pymc.MvStudentTRandomWalk\n* Truncated\n\t+ `Truncated`\n* Censored\n\t+ `Censored`\n* Simulator\n\t+ `Simulator`\n* Transformations\n\t+ Transform Instances\n\t+ Specific Transform Classes\n\t+ Transform Composition Classes\n* Distribution utilities\n\t+ pymc.Continuous\n\t+ pymc.CustomDist\n\t+ pymc.Discrete\n\t+ pymc.Distribution\n\t+ pymc.SymbolicRandomVariable\n\t+ pymc.DiracDelta\n\n# Continuous#\n\n\n|  |  |\n| --- | --- |\n| `AsymmetricLaplace`(name, \\*args[, rng, dims, ...]) | Asymmetric-Laplace log-likelihood. |\n| `Beta`(name, \\*args[, rng, dims, initval, ...]) | Beta log-likelihood. |\n| `Cauchy`(name, \\*args[, rng, dims, initval, ...]) | Cauchy log-likelihood. |\n| `ChiSquared`(name, nu, \\*\\*kwargs) | \\(\\chi^2\\) log-likelihood. |\n| `ExGaussian`(name, \\*args[, rng, dims, ...]) | Exponentially modified Gaussian log-likelihood. |\n| `Exponential`(name, \\*args[, rng, dims, ...]) | Exponential\n\n==================\n Document 2 \n----------------\n pymc.Binomial#\n\n\n*class* pymc.Binomial(*name*, *\\*args*, *\\*\\*kwargs*)[source]#\nBinomial log-likelihood.\n\n\nThe discrete probability distribution of the number of successes\nin a sequence of n independent yes/no experiments, each of which\nyields success with probability p.\nThe pmf of this distribution is\n\n\\[f(x \\mid n, p) = \\binom{n}{x} p^x (1-p)^{n-x}\\]\n(`Source code`, `png`, `hires.png`, `pdf`)\n\n\n\n|  |  |\n| --- | --- |\n| Support | \\(x \\in \\{0, 1, \\ldots, n\\}\\) |\n| Mean | \\(n p\\) |\n| Variance | \\(n p (1 - p)\\) |\n\n**p**tensor\\_like of `float`Probability of success in each trial (0 < p < 1).\n\n\n|  |  |\n| --- | --- |\n| `Binomial.dist`(n[, p, logit\\_p]) | Creates a tensor variable corresponding to the cls distribution. |\n\n\n# pymc.Categorical#\n\n\n*class* pymc.Categorical(*name*, *\\*args*, *\\*\\*kwargs*)[source]#\nCategorical log-likelihood.\n\n\nThe most general discrete distribution. The pmf of this distribution is\n\n\\[f(x \\mid p) = p\\_x\\]\n(`Source code`, `png`, `hires.png`, `pdf`)\n\n\n\n|  |  |\n| --- | --- |\n| Support | \\(x \\in \\{0, 1, \\ldots, |p|-1\\}\\) |\n\n**p**`array` of `floats`p > 0 and the elements of p must sum to 1.\n\n**logit\\_p**`float`Alternative log odds for the probability of success.\n\n\n|  |  |\n| --- | --- |\n| `Categorical.dist`([p, logit\\_p]) | Creates a tensor variable corresponding to the cls distribution. |\n\n\n\n# pymc.DiscreteUniform#\n\n\n*class* pymc.DiscreteUniform(*name*, *\\*args*, *\\*\\*kwargs*)[source]#\nDiscrete uniform distribution.\n\n\nThe pmf of this distribution is\n\n\\[f(x \\mid lower, upper) = \\frac{1}{upper-lower+1}\\]\n(`Source code`, `png`, `hires.png`, `pdf`)\n\n\n\n|  |  |\n| --- | --- |\n| Support | \\(x \\in {lower, lower + 1, \\ldots, upper}\\) |\n| Mean | \\(\\dfrac{lower + upper}{2}\\) |\n| Variance | \\(\\dfrac{(upper - lower)^2}{12}\\) |\n\n**lower**tensor\\_like of `int`Lower limit.\n\n**upper**tensor\\_like of `int`Upper limit (upper > lower).\n\n\n|  |  |\n| --- | --- |\n| `DiscreteUniform.dist`(lower, upper, \\*args, ...) | Creates a tensor variable corresponding to the cls distribution. |\n\n\n# pymc.DiscreteWeibull#\n\n\n*class* pymc.DiscreteWeibull(*name*, *\\*args*, *\\*\\*kwargs*)[source]#\nDiscrete Weibull log-likelihood.\n\n\nThe discrete Weibull distribution is a flexible model of count data that\ncan handle both over- and under-dispersion.\nThe pmf of this distribution is\n\n\\[f(x \\mid q, \\beta) = q^{x^{\\beta}} - q^{(x + 1)^{\\beta}}\\]\n(`Source code`, `png`, `hires.png`,\n\n==================\n Document 3 \n----------------\n pymc.gp.Latent#\n\n\n*class* pymc.gp.Latent(*\\**, *mean\\_func=<pymc.gp.mean.Zero object>*, *cov\\_func=<pymc.gp.cov.Constant object>*)[source]#\nLatent Gaussian process.\n\n\nThe gp.Latent class is a direct implementation of a GP. No additive\nnoise is assumed. It is called “Latent” because the underlying function\nvalues are treated as latent variables. It has a prior method and a\nconditional method. Given a mean and covariance function the\nfunction \\(f(x)\\) is modeled as,\n\n\\[f(x) \\sim \\mathcal{GP}\\left(\\mu(x), k(x, x')\\right)\\]\nUse the prior and conditional methods to actually construct random\nvariables representing the unknown, or latent, function whose\ndistribution is the GP prior or GP conditional. This GP implementation\ncan be used to implement regression on data that is not normally\ndistributed. For more information on the prior and conditional methods,\nsee their docstrings.\n\n**mean\\_func**`Mean`, default `Zero`The mean function.\n\n**cov\\_func**2D array\\_like, or `Covariance`, default `Constant`The covariance function.\n\n```# A one dimensional column vector of inputs.\nX = np.linspace(0, 1, 10)[:, None]\n\nwith pm.Model() as model:\n    # Specify the covariance function.\n    cov\\_func = pm.gp.cov.ExpQuad(1, ls=0.1)\n\n    # Specify the GP. The default\n\n==================\n Document 4 \n----------------\n pymc.gp.TP#\n\n\n*class* pymc.gp.TP(*\\**, *mean\\_func=<pymc.gp.mean.Zero object>*, *scale\\_func=<pymc.gp.cov.Constant object>*, *cov\\_func=None*, *nu=None*)[source]#\nStudent’s T process prior.\n\n\nThe usage is nearly identical to that of gp.Latent. The differences\nare that it must be initialized with a degrees of freedom parameter, and\nTP is not additive. Given a mean and covariance function, and a degrees of\nfreedom parameter, the function \\(f(x)\\) is modeled as,\n\n\\[f(X) \\sim \\mathcal{TP}\\left( \\mu(X), k(X, X'), \\nu \\right)\\]\n\n**scale\\_func**2D array\\_like, or `Covariance`, default `Constant`The covariance function.\n\n**cov\\_func**2D array\\_like, or `Covariance`, default `None`Deprecated, previous version of “scale\\_func”\n\n**nu**`float`The degrees of freedom\n\n\n* Shah, A., Wilson, A. G., and Ghahramani, Z. (2014). Student-t\nProcesses as Alternatives to Gaussian Processes. arXiv preprint arXiv:1402.4306.\n\n\n|  |  |\n| --- | --- |\n| `TP.\\_\\_init\\_\\_`(\\*[, mean\\_func, scale\\_func, ...]) |  |\n| `TP.conditional`(name, Xnew[, jitter]) | Returns the conditional distribution evaluated over new input locations Xnew. |\n| `TP.marginal\\_likelihood`(name, X, \\*args, \\*\\*kwargs) |  |\n| `TP.predict`(Xnew[, point, given, diag, model]) |  |\n| `TP.prior`(name, X[, reparameterize, jitter]) | Returns the TP prior distribution evaluated over the input locations X. |\n\n\n|  |  |\n| --- | --- |\n| `X` |  |\n| `f` |  |\n| `nu` |  |\n\n\n# Mean Functions#\n\n\n|  |  |\n| --- | --- |\n| `Zero`() | Zero mean function for Gaussian process. |\n| `Constant`([c]) | Constant mean function for Gaussian process. |\n| `Linear`(coeffs[, intercept]) | Linear mean function for Gaussian process. |\n\n\n# pymc.gp.mean.Zero#\n\n\n*class* pymc.gp.mean.Zero[source]#\nZero mean function for Gaussian process.\n\n\n|  |  |\n| --- | --- |\n| `Zero.\\_\\_init\\_\\_`(\\*args, \\*\\*kwargs) |  |\n\n\n\n# pymc.gp.mean.Constant#\n\n\n*class* pymc.gp.mean.Constant(*c=0*)[source]#\nConstant mean function for Gaussian process.\n\n**c: variable, array or integer**Constant mean value\n\n\n|  |  |\n| --- | --- |\n| `Constant.\\_\\_init\\_\\_`([c]) |  |\n\n\n\n# pymc.gp.mean.Linear#\n\n\n*class* pymc.gp.mean.Linear(*coeffs*, *intercept=0*)[source]#\nLinear mean function for Gaussian process.\n\n**coeffs: variables**Linear coefficients\n\n**intercept: variable, array or integer**Intercept for linear function (Defaults to zero)\n\n\n|  |  |\n| --- | --- |\n| `Linear.\\_\\_init\\_\\_`(coeffs[, intercept]) |  |\n\n\n# Covariance Functions#\n\n\n|  |  |\n| --- | --- |\n| `Constant`(c) | Constant valued covariance function. |\n| `WhiteNoise`(sigma) | White noise covariance function. |\n| `ExpQuad`(input\\_dim[, ls, ls\\_inv, active\\_dims]) | The Exponentiated Quadratic kernel. |\n| `RatQuad`(input\\_dim, alpha[, ls, ls\\_inv, ...]) | The Rational Quadratic kernel. |\n| `Exponential`(input\\_dim[, ls, ls\\_inv, active\\_dims])\n\n==================\n Document 5 \n----------------\n Samplers#\n\n\nThis submodule contains functions for MCMC and forward sampling.\n\n\n|  |  |\n| --- | --- |\n| `sample\\_prior\\_predictive`([samples, model, ...]) | Generate samples from the prior predictive distribution. |\n| `sample\\_posterior\\_predictive`(trace[, model, ...]) | Generate posterior predictive samples from a model given a trace. |\n| `draw`(vars[, draws, random\\_seed]) | Draw samples for one variable or a list of variables |\n\n\n|  |  |\n| --- | --- |\n| `sample`([draws, tune, chains, cores, ...]) | Draw samples from the posterior using the given step methods. |\n| `init\\_nuts`(\\*[, init, chains, n\\_init, model, ...]) | Set up the mass matrix initialization for NUTS. |\n\n\n|  |  |\n| --- | --- |\n| `sample\\_blackjax\\_nuts`([draws, tune, chains, ...]) | Draw samples from the posterior using the NUTS method from the `blackjax` library. |\n| `sample\\_numpyro\\_nuts`([draws, tune, chains, ...]) | Draw samples from the posterior using the NUTS method from the `numpyro` library. |\n\n## Step methods#\n\n\n### HMC family#\n\n\n|  |  |\n| --- | --- |\n| `NUTS`(\\*args, \\*\\*kwargs) | A sampler for continuous variables based on Hamiltonian mechanics. |\n| `HamiltonianMC`(\\*args, \\*\\*kwargs) | A sampler for continuous variables based on Hamiltonian mechanics. |\n\n\n### Metropolis family#\n\n\n|  |  |\n| --- | --- |\n| `BinaryGibbsMetropolis`(\\*args, \\*\\*kwargs) | A Metropolis-within-Gibbs step method optimized for binary variables |\n| `BinaryMetropolis`(\\*args, \\*\\*kwargs) | Metropolis-Hastings optimized for binary variables |\n| `CategoricalGibbsMetropolis`(\\*args, \\*\\*kwargs) | A Metropolis-within-Gibbs step method optimized for categorical variables.\n\n==================\n Document 6 \n----------------\n pymc.sampling.mcmc.sample#\n\n\npymc.sampling.mcmc.sample(*draws=1000*, *\\**, *tune=1000*, *chains=None*, *cores=None*, *random\\_seed=None*, *progressbar=True*, *step=None*, *nuts\\_sampler='pymc'*, *initvals=None*, *init='auto'*, *jitter\\_max\\_retries=10*, *n\\_init=200000*, *trace=None*, *discard\\_tuned\\_samples=True*, *compute\\_convergence\\_checks=True*, *keep\\_warning\\_stat=False*, *return\\_inferencedata=True*, *idata\\_kwargs=None*, *nuts\\_sampler\\_kwargs=None*, *callback=None*, *mp\\_ctx=None*, *model=None*, *\\*\\*kwargs*)[source]#\nDraw samples from the posterior using the given step methods.\n\n\nMultiple step methods are supported via compound step methods.\n\n**draws**`int`The number of samples to draw. Defaults to 1000. The number of tuned samples are discarded\nby default. See `discard\\_tuned\\_samples`.\n\n**tune**`int`Number of iterations to tune, defaults to 1000. Samplers adjust the step sizes, scalings or\nsimilar during tuning. Tuning samples will be drawn in addition to the number specified in\nthe `draws` argument, and will be discarded unless `discard\\_tuned\\_samples` is set to\nFalse.\n\n**chains**`int`The number of chains to sample. Running independent chains is important for some\nconvergence statistics and can also reveal multiple modes in the posterior. If `None`,\nthen set to either `cores` or 2, whichever is larger.\n\n**cores**`int`The number of chains to run in parallel. If `None`, set to the number of CPUs in the\nsystem, but at most 4.\n\n**random\\_seed**`int`, array\\_like of `int`, `RandomState` or `Generator`, optionalRandom seed(s) used by the sampling steps. If a list, tuple or array of ints\nis passed, each entry will be used to seed each chain. A ValueError will be\nraised if the length does not match the number of chains.\n\n**progressbar**bool, optional default=TrueWhether or not to display a progress bar in the command line. The bar shows the percentage\nof completion, the sampling speed in samples per second (SPS), and the estimated remaining\ntime until completion (“expected time of arrival”; ETA).\nOnly applicable to the pymc nuts sampler.\n\n**step**`function` or iterable of `functions`A step function or collection of functions. If there are variables without step methods,\nstep methods for those variables will be assigned automatically. By default the NUTS step\nmethod will be used, if appropriate to the model.\n\n**nuts\\_sampler**`str`Which NUTS implementation to run. One of [“pymc”, “nutpie”, “blackjax”, “numpyro”].\nThis requires the chosen sampler to be installed.\nAll samplers, except “pymc”, require the full model to be continuous.\n\n**initvals**optional, `dict`, `array` of `dict`Dict or list of dicts with initial value strategies to use instead of the defaults from\nModel.initial\\_values. The keys should be names of transformed random variables.\nInitialization methods for NUTS (see `init` keyword) can overwrite the default.\n\n**init**`str`Initialization method to use for auto-assigned NUTS samplers. See pm.init\\_nuts for a list\nof all options. This argument is ignored when manually passing the NUTS step method.\nOnly applicable to the pymc nuts sampler.\n\n**jitter\\_max\\_retries**`int`Maximum number of repeated attempts (per chain) at creating an initial matrix with uniform\njitter that yields a finite probability. This applies to `jitter+adapt\\_diag` and\n`jitter+adapt\\_full` init methods.\n\n**n\\_init**`int`Number of iterations of initializer. Only works for ‘ADVI’ init methods.\n\n**trace**`backend`, optionalA backend instance or None.\nIf None, the NDArray backend is used.\n\n**discard\\_tuned\\_samples**boolWhether to discard posterior samples of the tune interval.\n\n**compute\\_convergence\\_checks**bool, default=TrueWhether to compute sampler statistics like Gelman-Rubin and `effective\\_n`.\n\n**keep\\_warning\\_stat**boolIf `True` the “warning” stat emitted by, for example, HMC samplers will be kept\nin the returned `idata.sample\\_stat` group.\nThis leads to the `idata` not supporting `.to\\_netcdf()` or `.to\\_zarr()` and\nshould only be set to `True` if you intend to use the “warning” objects right away.\nDefaults to `False` such that `pm.drop\\_warning\\_stat` is applied automatically,\nmaking the `InferenceData` compatible with saving.\n\n**return\\_inferencedata**boolWhether to return the trace as an `arviz.InferenceData` (True) object or a\nMultiTrace (False). Defaults to True.\n\n**nuts\\_sampler\\_kwargs**`dict`, optionalKeyword arguments for the sampling library that implements nuts.\nOnly used when an external sampler is specified via the nuts\\_sampler kwarg.\n\n**callback**`function`, default=NoneA function which gets called for every sample from the trace of a chain. The function is\ncalled with the trace and the current draw and will contain all samples for a single trace.\nthe `draw.chain` argument can be used to determine which of the active chains the sample\nis drawn from.\nSampling can be interrupted by throwing a `KeyboardInterrupt` in the callback.\n\n**mp\\_ctx**`multiprocessing.context.BaseContent`A multiprocessing context for parallel sampling.\nSee multiprocessing documentation for details.\n\n**model**`Model` (optional `if` `in` `with` `context`)Model to sample from. The model needs to have free random variables.\n\n**trace**`pymc.backends.base.MultiTrace` or `arviz.InferenceData`A `MultiTrace` or ArviZ `InferenceData` object that contains the samples.\n\n\nOptional keyword arguments can be passed to `sample` to be delivered to the\n`step\\_method`s used during sampling.\n\n\nFor example:\n\n> \n> 1. `target\\_accept` to NUTS: nuts={‘target\\_accept’:0.9}\n> 2. `transit\\_p` to BinaryGibbsMetropolis: binary\\_gibbs\\_metropolis={‘transit\\_p’:.7}\n> \n> \n> \n\n\nNote that available step names are:\n\n\n`nuts`, `hmc`, `metropolis`, `binary\\_metropolis`,\n`binary\\_gibbs\\_metropolis`, `categorical\\_gibbs\\_metropolis`,\n`DEMetropolis`, `DEMetropolisZ`, `slice`\n\n\nThe NUTS step method has several options including:\n\n> \n> * target\\_accept : float in [0, 1]. The step size is tuned such that we\n> approximate this acceptance rate. Higher values like 0.9 or 0.95 often\n> work better for problematic posteriors. This argument can be passed directly to sample.\n> * max\\_treedepth : The maximum depth of the trajectory tree\n> * step\\_scale : float, default 0.25\n> The initial guess for the step size scaled down by \\(1/n\\*\\*(1/4)\\),\n> where n is the dimensionality of the parameter space\n> \n> \n> \n\nAlternatively, if you manually declare the `step\\_method`s, within the `step`kwarg, then you can address the `step\\_method` kwargs directly.\ne.g. for a CompoundStep comprising NUTS and BinaryGibbsMetropolis,\nyou could send\n\n```\nstep=[pm.NUTS([freeRV1, freeRV2], target\\_accept=0.9),\n      pm.BinaryGibbsMetropolis([freeRV3], transit\\_p=.7)]\n\n\nYou can find a full list of arguments in the docstring of the step methods.\n\n```\nIn [1]: import pymc as pm\n ...: n = 100\n ...: h = 61\n ...: alpha = 2\n ...: beta = 2\n\nIn [2]: with pm.Model() as model: # context management\n ...:     p = pm.Beta(\"p\", alpha=alpha, beta=beta)\n ...:     y = pm.Binomial(\"y\", n=n, p=p, observed=h)\n ...:     idata = pm.sample()\n\nIn [3]: az.summary(idata, kind=\"stats\")\n\nOut[3]:\n mean sd hdi\\_3% hdi\\_97%\np 0.609 0.047 0.528 0.699\n\n# pymc.sampling.mcmc.init\\_nuts#\n\n\npymc.sampling.mcmc.init\\_nuts(*\\**, *init='auto'*, *chains=1*, *n\\_init=500000*, *model=None*, *random\\_seed=None*, *progressbar=True*, *jitter\\_max\\_retries=10*, *tune=None*, *initvals=None*, *\\*\\*kwargs*)[source]#\nSet up the mass matrix initialization for NUTS.\n\n\nNUTS convergence and sampling speed is extremely dependent on the\nchoice of mass/scaling matrix. This function implements different\nmethods for choosing or adapting the"
    },
    {
        "instruction": "Create a Python program using the 'pypdf' API to extract and display the list of all embedded files (attachments) in a PDF file.",
        "api": "pypdf",
        "output": "#!pip install pypdf\nimport pypdf\n\n# Specify the path of the PDF file with embedded files\npdf_file_path = \"document_with_attachments.pdf\"\n\n# Open the PDF file in read-binary mode\nwith open(pdf_file_path, \"rb\") as pdf_file:\n    # Create a PDF reader object\n    pdf_reader = pypdf.PdfReader(pdf_file)\n\n    # Initialize a list to store the names of embedded files\n    embedded_files = []\n\n    # Check if the PDF has file attachments\n    if '/EmbeddedFiles' in pdf_reader.Info:\n        embedded_file_dict = pdf_reader.Info.EmbeddedFiles\n        for key, value in embedded_file_dict.items():\n            embedded_files.append(key)\n\n    # Display the list of embedded files\n    for file_name in embedded_files:\n        print(file_name)\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n Example: This will launch the print window when the PDF is opened.\n\n```\n\naddJS(*javascript: str*) → None[source]\nUse `add\\_js()` instead.\n\n\nadd\\_attachment(*filename: str*, *data: Union[str, bytes]*) → None[source]\nEmbed a file inside the PDF.\n\n\nReference:\nhttps://www.adobe.com/content/dam/Adobe/en/devnet/acrobat/pdfs/PDF32000\\_2008.pdf\nSection 7.11.3\n\nParameters\n* **filename** – The filename to display.\n* **data** – The data in the file.\n\naddAttachment(*fname: str*, *fdata: Union[str, bytes]*) → None[source]\nUse `add\\_attachment()` instead.\n\n\nappend\\_pages\\_from\\_reader(*reader: PdfReader*, *after\\_page\\_append: Optional[Callable[[PageObject], None]] = None*) → None[source]\nCopy pages from reader to writer. Includes an optional callback\nparameter which is invoked after pages are appended to the writer.\n\n\n`append` should be prefered.\n\nParameters\n* **reader** – a PdfReader object from which to copy page\nannotations to this writer object. The writer’s annots\nwill then be updated\n* **after\\_page\\_append** – Callback function that is invoked after each page is appended to\nthe writer. Signature includes a reference to the appended page\n(delegates to append\\_pages\\_from\\_reader). The single parameter of\nthe callback is a reference to the page just appended to the\ndocument.\n\nappendPagesFromReader(*reader: PdfReader*, *after\\_page\\_append: Optional[Callable[[PageObject], None]] = None*) → None[source]\nUse `append\\_pages\\_from\\_reader()` instead.\n\n\nupdate\\_page\\_form\\_field\\_values(*page: ~pypdf.\\_page.PageObject, fields: ~typing.Dict[str, ~typing.Any], flags: ~pypdf.constants.FieldFlag = FieldFlag.None, auto\\_regenerate: ~typing.Optional[bool] = True*) → None[source]\nUpdate the form field values for a given page from a fields dictionary.\n\n\nCopy field texts and values from fields to page.\nIf the field links to a parent object, add the information to the parent.\n\nParameters\n* **page** – Page reference from PDF writer where the\nannotations and field data will be updated.\n* **fields** – a Python dictionary of field names (/T) and text\nvalues (/V)\n* **flags** – An integer (0 to 7). The first bit sets ReadOnly, the\nsecond bit sets Required, the third bit sets NoExport. See\nPDF Reference Table 8.70 for details.\n* **auto\\_regenerate** – set/unset the need\\_appearances flag ;\nthe flag is unchanged if auto\\_regenerate is None\n\nupdatePageFormFieldValues(*page: ~pypdf.\\_page.PageObject, fields: ~typing.Dict[str, ~typing.Any], flags: ~pypdf.constants.FieldFlag = FieldFlag.None*) → None[source]\nUse `update\\_page\\_form\\_field\\_values()` instead.\n\n\nclone\\_reader\\_document\\_root(*reader: PdfReader*) → None[source]\nCopy the reader document root to the writer and all sub elements,\nincluding pages, threads, outlines,… For partial insertion, `append`\nshould be considered.\n\nParameters\n**reader** – PdfReader from the document root should be copied.\n\ncloneReaderDocumentRoot(*reader: PdfReader*) → None[source]\nUse `clone\\_reader\\_document\\_root()` instead.\n\n\nclone\\_document\\_from\\_reader(*reader: PdfReader*, *after\\_page\\_append: Optional[Callable[[PageObject], None]] = None*) → None[source]\nCreate a copy (clone) of a document from a PDF file reader cloning\nsection ‘/Root’ and ‘/Info’ and ‘/ID’ of the pdf.\n\nParameters\n* **reader** – PDF file reader instance from which the clone\nshould be created.\n* **after\\_page\\_append** – Callback function that is invoked after each page is appended to\nthe writer. Signature includes a reference to the appended page\n(delegates to append\\_pages\\_from\\_reader). The single parameter of\nthe callback is a reference to the page just appended to the\ndocument.\n\ncloneDocumentFromReader(*reader: PdfReader*, *after\\_page\\_append: Optional[Callable[[PageObject], None]] = None*) → None[source]\nUse `clone\\_document\\_from\\_reader()` instead.\n\n\ngenerate\\_file\\_identifiers() → None[source]\nGenerate an identifier for the PDF that will be written.\n\n\nThe only point of this is ensuring uniqueness. Reproducibility is not\nrequired; see 14.4 “File Identifiers”.\n\nencrypt(*user\\_password: Optional[str] = None*, *owner\\_password: Optional[str] = None*, *use\\_128bit: bool = True*, *permissions\\_flag: UserAccessPermissions = UserAccessPermissions.PRINT | MODIFY | EXTRACT | ADD\\_OR\\_MODIFY | R7 | R8 | FILL\\_FORM\\_FIELDS | EXTRACT\\_TEXT\\_AND\\_GRAPHICS | ASSEMBLE\\_DOC | PRINT\\_TO\\_REPRESENTATION | R13 | R14 | R15 | R16 | R17 | R18 | R19 | R20 | R21 | R22 | R23 | R24 | R25 | R26 | R27 | R28 | R29 | R30 | R31*, *user\\_pwd: Optional[str] = None*, *owner\\_pwd: Optional[str] = None*, *\\**, *algorithm: Optional[str] = None*) → None[source]\nEncrypt this PDF file with the PDF Standard encryption handler.\n\nParameters\n* **user\\_password** – The password which allows for opening\nand reading the PDF file with the restrictions provided.\n* **owner\\_password** – The password which allows for\nopening the PDF files without any restrictions. By default,\nthe owner password is the same as the user password.\n* **use\\_128bit** – flag as to whether to use 128bit\nencryption. When false, 40bit encryption will be used.\nBy default, this flag is on.\n* **permissions\\_flag** – permissions as described in\nTABLE 3.20 of the PDF 1.7 specification. A bit value of 1 means\nthe permission is grantend.\nHence an integer value of -1 will set all flags.\nBit position 3 is for printing, 4 is for modifying content,\n5 and 6 control annotations, 9 for form fields,\n10 for extraction of text and graphics.\n* **algorithm** – encrypt algorithm. Values maybe one of “RC4-40”, “RC4-128”,\n“AES-128”, “AES-256-R5”, “AES-256”. If it’s valid,\nuse\\_128bit will be ignored.\n\nwrite\\_stream(*stream: IO*) → None[source]\n\nwrite(*stream: Union[Path, str, IO]*) → Tuple[bool, IO][source]\nWrite the collection of pages added to this object out as a PDF file.\n\nParameters\n**stream** – An object to write the file to. The object can support\nthe write method and the tell method, similar to a file object, or\nbe a file path, just like the fileobj, just named it stream to keep\nexisting workflow.\n\nReturns\nA tuple (bool, IO)\n\nadd\\_metadata(*infos: Dict[str, Any]*) → None[source]\nAdd custom metadata to the output.\n\nParameters\n**infos** – a Python dictionary where each key is a field\nand each value is your new metadata.\n\naddMetadata(*infos: Dict[str, Any]*) → None[source]\nUse `add\\_metadata()` instead.\n\n\nget\\_reference(*obj: PdfObject*) → IndirectObject[source]\n\ngetReference(*obj: PdfObject*) → IndirectObject[source]\nUse `get\\_reference()` instead.\n\n\nget\\_outline\\_root() → TreeObject[source]\n\nget\\_threads\\_root() → ArrayObject[source]\nThe list of threads.\n\nReturns\nAn array (possibly empty) of Dictionaries with `/F` and\n`/I` properties.\n\n*property* threads*: pypdf.generic.\\_data\\_structures.ArrayObject*\nRead-only property for the list of threads.\n\n\nEach element is a dictionaries with `/F` and `/I` keys.\n\ngetOutlineRoot() → TreeObject[source]\nUse `get\\_outline\\_root()` instead.\n\n\nget\\_named\\_dest\\_root() → ArrayObject[source]\n\ngetNamedDestRoot() → ArrayObject[source]\nUse `get\\_named\\_dest\\_root()` instead.\n\n\nadd\\_outline\\_item\\_destination(*page\\_destination: Union[None, PageObject, TreeObject] = None*, *parent: Union[None, TreeObject, IndirectObject] = None*, *before: Union[None, TreeObject, IndirectObject] = None*, *is\\_open: bool = True*, *dest: Union[None, PageObject, TreeObject] = None*) → IndirectObject[source]\n\nadd\\_bookmark\\_destination(*dest: Union[PageObject, TreeObject]*, *parent: Union[None, TreeObject, IndirectObject] = None*) → IndirectObject[source]\nUse `add\\_outline\\_item\\_destination()` instead.\n\n\naddBookmarkDestination(*dest: PageObject*, *parent: Optional[TreeObject] = None*) → IndirectObject[source]\nUse `add\\_outline\\_item\\_destination()` instead.\n\n\nadd\\_outline\\_item\\_dict(*outline\\_item: Union[OutlineItem, Destination]*, *parent: Union[None, TreeObject, IndirectObject] = None*, *before: Union[None, TreeObject, IndirectObject] = None*, *is\\_open: bool = True*) → IndirectObject[source]\n\nadd\\_bookmark\\_dict(*outline\\_item: Union[OutlineItem, Destination]*, *parent: Optional[TreeObject] = None*) → IndirectObject[source]\nUse `add\\_outline\\_item\\_dict()` instead.\n\n\naddBookmarkDict(*outline\\_item: Union[OutlineItem, Destination]*, *parent: Optional[TreeObject] = None*) → IndirectObject[source]\nUse `add\\_outline\\_item\\_dict()` instead.\n\n\nadd\\_outline\\_item(*title: str, page\\_number: ~typing.Union[None, ~pypdf.\\_page.PageObject, ~pypdf.generic.\\_base.IndirectObject, int], parent: ~typing.Union[None, ~pypdf.generic.\\_data\\_structures.TreeObject, ~pypdf.generic.\\_base.IndirectObject] = None, before: ~typing.Union[None, ~pypdf.generic.\\_data\\_structures.TreeObject, ~pypdf.generic.\\_base.IndirectObject] = None, color: ~typing.Optional[~typing.Union[~typing.Tuple[float, float, float], str]] = None, bold: bool = False, italic: bool = False, fit: ~pypdf.generic.\\_fit.Fit = <pypdf.generic.\\_fit.Fit object>, is\\_open: bool = True, pagenum: ~typing.Optional[int] = None*) → IndirectObject[source]\nAdd an outline item (commonly referred to as a “Bookmark”) to the PDF file.\n\nParameters\n* **title** – Title to use for this outline item.\n* **page\\_number** – Page number this outline item will point to.\n* **parent** – A reference to a parent outline item to create nested\noutline items.\n* **before** –\n* **color** – Color of the outline item’s font as a red, green, blue tuple\nfrom 0.0 to 1.0 or as a Hex String (#RRGGBB)\n* **bold** – Outline item font is bold\n* **italic** – Outline item font is italic\n* **fit** – The fit of the destination page.\n\nReturns\nThe added outline item as an indirect object.\n\nadd\\_bookmark(*title: str*, *pagenum: int*, *parent: Union[None, TreeObject, IndirectObject] = None*, *color: Optional[Tuple[float, float, float]] = None*, *bold: bool = False*, *italic: bool = False*, *fit: Literal['/Fit', '/XYZ', '/FitH', '/FitV', '/FitR', '/FitB', '/FitBH', '/FitBV'] = '/Fit'*, *\\*args: Union[NumberObject, NullObject, float]*) → IndirectObject[source]\nUse `add\\_outline\\_item()` instead.\n\n\naddBookmark(*title: str*, *pagenum: int*, *parent: Union[None, TreeObject, IndirectObject] = None*, *color: Optional[Tuple[float, float, float]] = None*, *bold: bool = False*, *italic: bool = False*, *fit: Literal['/Fit', '/XYZ', '/FitH', '/FitV', '/FitR', '/FitB', '/FitBH', '/FitBV'] = '/Fit'*, *\\*args: Union[NumberObject, NullObject, float]*) → IndirectObject[source]\nUse `add\\_outline\\_item()` instead.\n\n\nadd\\_outline() → None[source]\n\nadd\\_named\\_destination\\_array(*title: TextStringObject*, *destination: Union[IndirectObject, ArrayObject]*) → None[source]\n\nadd\\_named\\_destination\\_object(*page\\_destination: Optional[PdfObject] = None*, *dest: Optional[PdfObject] = None*) → IndirectObject[source]\n\naddNamedDestinationObject(*dest: Destination*) → IndirectObject[source]\nUse `add\\_named\\_destination\\_object()` instead.\n\n\nadd\\_named\\_destination(*title: str*, *page\\_number: Optional[int] = None*, *pagenum: Optional[int] = None*) → IndirectObject[source]\n\naddNamedDestination(*title: str*, *pagenum: int*) → IndirectObject[source]\nUse `add\\_named\\_destination()` instead.\n\n\nremove\\_links() → None[source]\nRemove links and annotations from this output.\n\nremoveLinks() → None[source]\nUse `remove\\_links()` instead.\n\n\nremove\\_annotations(*subtypes: Optional[Union[Literal['/Text', '/Link', '/FreeText', '/Line', '/Square', '/Circle', '/Polygon', '/PolyLine', '/Highlight', '/Unterline', '/Squiggly', '/StrikeOut', '/Stamp', '/Caret', '/Ink', '/Popup', '/FileAttachment', '/Sound', '/Movie', '/Widget', '/Screen', '/PrinterMark', '/TrapNet', '/Watermark', '/3D', '/Redact'], Iterable[Literal['/Text', '/Link', '/FreeText', '/Line', '/Square', '/Circle', '/Polygon', '/PolyLine', '/Highlight', '/Unterline', '/Squiggly', '/StrikeOut', '/Stamp', '/Caret', '/Ink', '/Popup', '/FileAttachment', '/Sound', '/Movie', '/Widget', '/Screen', '/PrinterMark', '/TrapNet', '/Watermark', '/3D', '/Redact']]]]*) → None[source]\nRemove annotations by annotation subtype.\n\nParameters\n**subtypes** – SubType or list of SubTypes to be removed.\nExamples are: “/Link”, “/FileAttachment”, “/Sound”,\n“/Movie”, “/Screen”, …\nIf you want to remove all annotations, use subtypes=None.\n\nremove\\_objects\\_from\\_page(*page: Union[PageObject, DictionaryObject]*, *to\\_delete: Union[ObjectDeletionFlag, Iterable[ObjectDeletionFlag]]*) → None[source]\nRemove objects specified by `to\\_delete` from the given page.\n\nParameters\n* **page** – Page object to clean up.\n* **to\\_delete** – Objects to be deleted; can be a `ObjectDeletionFlag`\nor a list of ObjectDeletionFlag\n\nremove\\_images(*ignore\\_byte\\_string\\_object: Optional[bool] = None*) → None[source]\nRemove images from this output.\n\nParameters\n**ignore\\_byte\\_string\\_object** – deprecated\n\nremoveImages(*ignoreByteStringObject: bool = False*) → None[source]\nUse `remove\\_images()` instead.\n\n\nremove\\_text(*ignore\\_byte\\_string\\_object: Optional[bool] = None*) → None[source]\nRemove text from this output.\n\nremoveText(*ignoreByteStringObject: bool = False*) → None[source]\nUse `remove\\_text()` instead.\n\n\nadd\\_uri(*page\\_number: int*, *uri: str*, *rect: RectangleObject*, *border: Optional[ArrayObject] = None*, *pagenum: Optional[int] = None*) → None[source]\nAdd an URI from a rectangular area to the specified page.\n\n\nThis uses the basic structure of `add\\_link()`\n\nParameters\n* **page\\_number** – index of the page on which to place the URI action.\n* **uri** – URI of resource to link to.\n* **rect** – `RectangleObject` or\narray of four integers specifying the clickable rectangular area\n`[xLL, yLL, xUR, yUR]`, or string in the form\n`\"[ xLL yLL xUR yUR ]\"`.\n* **border** – if provided, an array describing border-drawing\nproperties. See the PDF spec for details. No border will be\ndrawn if this argument is omitted.\n\naddURI(*pagenum: int*, *uri: str*, *rect: RectangleObject*, *border: Optional[ArrayObject] = None*) → None[source]\nUse `add\\_uri()` instead.\n\n\nadd\\_link(*pagenum: int*, *page\\_destination: int*, *rect: RectangleObject*, *border: Optional[ArrayObject] = None*, *fit: Literal['/Fit', '/XYZ', '/FitH', '/FitV', '/FitR', '/FitB', '/FitBH', '/FitBV'] = '/Fit'*, *\\*args: Union[NumberObject, NullObject, float]*) → DictionaryObject[source]\n\naddLink(*pagenum: int*, *page\\_destination: int*, *rect: RectangleObject*, *border: Optional[ArrayObject] = None*, *fit: Literal['/Fit', '/XYZ', '/FitH', '/FitV', '/FitR', '/FitB', '/FitBH', '/FitBV'] = '/Fit'*, *\\*args: Union[NumberObject, NullObject, float]*) → None[source]\nUse `add\\_link()` instead.\n\n\ngetPageLayout() → Optional[Literal['/NoLayout', '/SinglePage', '/OneColumn', '/TwoColumnLeft', '/TwoColumnRight', '/TwoPageLeft', '/TwoPageRight']][source]\nUse `page\\_layout` instead.\n\n\nset\\_page\\_layout(*layout: Literal['/NoLayout', '/SinglePage', '/OneColumn', '/TwoColumnLeft', '/TwoColumnRight', '/TwoPageLeft', '/TwoPageRight']*) → None[source]\nSet the page layout.\n\nParameters\n**layout** – The page layout to be used\n\n\nValid `layout` arguments\n\nsetPageLayout(*layout: Literal['/NoLayout', '/SinglePage', '/OneColumn', '/TwoColumnLeft', '/TwoColumnRight', '/TwoPageLeft', '/TwoPageRight']*) → None[source]\nUse `page\\_layout` instead.\n\n\n*property* page\\_layout*: Optional[Literal['/NoLayout', '/SinglePage', '/OneColumn', '/TwoColumnLeft', '/TwoColumnRight', '/TwoPageLeft', '/TwoPageRight']]*\nPage layout property.\n\n*property* pageLayout*: Optional[Literal['/NoLayout', '/SinglePage', '/OneColumn', '/TwoColumnLeft', '/TwoColumnRight', '/TwoPageLeft', '/TwoPageRight']]*\nUse `page\\_layout` instead.\n\n\ngetPageMode() → Optional[Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']][source]\nUse `page\\_mode` instead.\n\n\nset\\_page\\_mode(*mode: Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']*) → None[source]\nUse `page\\_mode` instead.\n\n\nsetPageMode(*mode: Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']*) → None[source]\nUse `page\\_mode` instead.\n\n\n*property* page\\_mode*: Optional[Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']]*\nPage mode property.\n\n*property* pageMode*: Optional[Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']]*\nUse `page\\_mode` instead.\n\n\nadd\\_annotation(*page\\_number: Union[int, PageObject]*, *annotation: Dict[str, Any]*) → DictionaryObject[source]\nAdd a single annotation to the page.\nThe added annotation must be a new annotation.\nIt can not be recycled.\n\nParameters\n* **page\\_number** – PageObject or page index.\n* **annotation** – Annotation to be added (created with annotation).\n\nReturns\nThe inserted object\nThis can be used for pop-up creation, for example\n\nclean\\_page(*page: Union[PageObject, IndirectObject]*) → PageObject[source]\nPerform some clean up in the page.\nCurrently: convert NameObject nameddestination to TextStringObject\n(required for names/dests list)\n\nParameters\n**page** – \n\nReturns\nThe cleaned PageObject\n\nappend(*fileobj: Union[str, IO, PdfReader, Path]*, *outline\\_item: Union[str, None, PageRange, Tuple[int, int], Tuple[int, int, int], List[int]] = None*, *pages: Union[None, PageRange, Tuple[int, int], Tuple[int, int, int], List[int], List[PageObject]] = None*, *import\\_outline: bool = True*, *excluded\\_fields: Optional[Union[List[str], Tuple[str, ...]]] = None*) → None[source]\nIdentical to the `merge()` method, but assumes you want to\nconcatenate all pages onto the end of the file instead of specifying a\nposition.\n\nParameters\n* **fileobj** – A File Object or an object that supports the standard\nread and seek methods similar to a File Object. Could also be a\nstring representing a path to a PDF file.\n* **outline\\_item** – Optionally, you may specify a string to build an\noutline (aka ‘bookmark’) to identify the beginning of the\nincluded file.\n* **pages** – Can be a `PageRange`\nor a `(start, stop[, step])` tuple\nor a list of pages to be processed\nto merge only the specified range of pages from the source\ndocument into the output document.\n* **import\\_outline** – You may prevent the source document’s\noutline (collection of outline items, previously referred to as\n‘bookmarks’) from being imported by specifying this as `False`.\n* **excluded\\_fields** – Provide the list of fields/keys to be ignored\nif `/Annots` is part of the list, the annotation will be ignored\nif `/B` is part of the list, the articles will be ignored\n\nmerge(*position: Optional[int]*, *fileobj: Union[Path, str, IO, PdfReader]*, *outline\\_item: Optional[str] = None*, *pages: Optional[Union[str, PageRange, Tuple[int, int], Tuple[int, int, int], List[int], List[PageObject]]] = None*, *import\\_outline: bool = True*, *excluded\\_fields: Optional[Union[List[str], Tuple[str, ...]]] = ()*) → None[source]\nMerge the pages from the given file into the output file at the\nspecified page number.\n\nParameters\n* **position** – The *page number* to insert this file. File will\nbe inserted after the given number.\n* **fileobj** – A File Object or an object that supports the standard\nread and seek methods similar to a File Object. Could also be a\nstring representing a path to a PDF file.\n* **outline\\_item** – Optionally, you may specify a string to build an outline\n(aka ‘bookmark’) to identify the\nbeginning of the included file.\n* **pages** – can be a `PageRange`\nor a `(start, stop[, step])` tuple\nor a list of pages to be processed\nto merge only the specified range of pages from the source\ndocument into the output document.\n* **import\\_outline** – You may prevent the source document’s\noutline (collection of outline items, previously referred to as\n‘bookmarks’) from being imported by specifying this as `False`.\n* **excluded\\_fields** – provide the list of fields/keys to be ignored\nif `/Annots` is part of the list, the annotation will be ignored\nif `/B` is part of the list, the articles will be ignored\n\nRaises\n**TypeError** – The pages attribute is not configured properly\n\nadd\\_filtered\\_articles(*fltr: Union[Pattern, str]*, *pages: Dict[int, PageObject]*, *reader: PdfReader*) → None[source]\nAdd articles matching the defined criteria.\n\nParameters\n* **fltr** –\n* **pages** –\n* **reader** –\n\nclose() → None[source]\nTo match the functions from Merger.\n\nfind\\_outline\\_item(*outline\\_item: Dict[str, Any]*, *root: Optional[List[Union[Destination, List[Union[Destination, List[Destination]]]]]] = None*) → Optional[List[int]][source]\n\nfind\\_bookmark(*outline\\_item: Dict[str, Any]*, *root: Optional[List[Union[Destination, List[Union[Destination, List[Destination]]]]]] = None*) → Optional[List[int]][source]\n\nDeprecated since version 2.9.0: Use `find\\_outline\\_item()` instead.\n\n\nreset\\_translation(*reader: Union[None, PdfReader, IndirectObject] = None*) → None[source]\nReset the translation table between reader and the writer object.\n\n\nLate cloning will create new independent objects.\n\nParameters\n**reader** – PdfReader or IndirectObject refering a PdfReader object.\nif set to None or omitted, all tables will be reset.\n\nset\\_page\\_label(*page\\_index\\_from: int*, *page\\_index\\_to: int*, *style: Optional[PageLabelStyle] = None*, *prefix: Optional[str] = None*, *start: Optional[int] = 0*) → None[source]\nSet a page label to a range of pages.\n\n\nPage indexes must be given starting from 0.\nLabels must have a style, a prefix or both.\nIf to a range is not assigned any page label a decimal label starting from 1 is applied.\n\nParameters\n* **page\\_index\\_from** – page index of the beginning of the range starting from 0\n* **page\\_index\\_to** – page index of the beginning of the range starting from 0\n* **style** – The numbering style to be used for the numeric portion of each page label:\n‘/D’ Decimal arabic numerals\n‘/R’ Uppercase roman numerals\n‘/r’ Lowercase roman numerals\n‘/A’ Uppercase letters (A to Z for the first 26 pages,\n\n> \n> AA to ZZ for the next 26, and so on)\n> \n> \n> \n\n’/a’ Lowercase letters (a to z for the first 26 pages,aa to zz for the next 26, and so on)\n* **prefix** – The label prefix for page labels in this range.\n* **start** – The value of the numeric portion for the first page label\nin the range.\nSubsequent pages are numbered sequentially from this value,\nwhich must be greater than or equal to 1.\nDefault value: 1.\n# The PdfMerger Class\n\n\n*class* pypdf.PdfMerger(*strict: bool = False*, *fileobj: Union[Path, str, IO] = ''*)[source]\nBases: `object`\n\n\nUse `PdfWriter` instead.\n\nDeprecated since version 5.0.0.\n\nmerge(*page\\_number: Optional[int] = None*, *fileobj: Union[None, Path, str, IO, PdfReader] = None*, *outline\\_item: Optional[str] = None*, *pages: Optional[Union[str, PageRange, Tuple[int, int],\n\n==================\n Document 1 \n----------------\n The PdfMerger Class\n\n\n*class* pypdf.PdfMerger(*strict: bool = False*, *fileobj: Union[Path, str, IO] = ''*)[source]\nBases: `object`\n\n\nUse `PdfWriter` instead.\n\nDeprecated since version 5.0.0.\n\nmerge(*page\\_number: Optional[int] = None*, *fileobj: Union[None, Path, str, IO, PdfReader] = None*, *outline\\_item: Optional[str] = None*, *pages: Optional[Union[str, PageRange, Tuple[int, int], Tuple[int, int, int], List[int]]] = None*, *import\\_outline: bool = True*, *position: Optional[int] = None*) → None[source]\nMerge the pages from the given file into the output file at the\nspecified page number.\n\nParameters\n* **page\\_number** – The *page number* to insert this file. File will\nbe inserted after the given number.\n* **fileobj** – A File Object or an object that supports the standard\nread and seek methods similar to a File Object. Could also be a\nstring representing a path to a PDF file.\nNone as an argument is deprecated.\n* **outline\\_item** – Optionally, you may specify an outline item\n(previously referred to as a ‘bookmark’) to be applied at the\nbeginning of the included file by supplying the text of the outline item.\n* **pages** – \ncan be a `PageRange`or a `(start, stop[, step])` tuple\nto merge only the specified range of pages from the source\ndocument into the output document.\nCan also be a list of pages to merge.\n\nimport\\_outline: You may prevent the source document’soutline (collection of outline items, previously referred to as\n‘bookmarks’) from being imported by specifying this as `False`.\n\nappend(*fileobj: Union[str, IO, PdfReader, Path]*, *outline\\_item: Optional[str] = None*, *pages: Union[None, PageRange, Tuple[int, int], Tuple[int, int, int], List[int]] = None*, *import\\_outline: bool = True*) → None[source]\nIdentical to the `merge()` method, but assumes you want to\nconcatenate all pages onto the end of the file instead of specifying a\nposition.\n\nParameters\n* **fileobj** – A File Object or an object that supports the standard\nread and seek methods similar to a File Object. Could also be a\nstring representing a path to a PDF file.\n* **outline\\_item** – Optionally, you may specify an outline item\n(previously referred to as a ‘bookmark’) to be applied at the\nbeginning of the included file by supplying the text of the outline item.\n* **pages** – can be a `PageRange`\nor a `(start, stop[, step])` tuple\nto merge only the specified range of pages from the source\ndocument into the output document.\nCan also be a list of pages to append.\n* **import\\_outline** – You may prevent the source document’s\noutline (collection of outline items, previously referred to as\n‘bookmarks’) from being imported by specifying this as `False`.\n\nwrite(*fileobj: Union[Path, str, IO]*) → None[source]\nWrite all data that has been merged to the given output file.\n\nParameters\n**fileobj** – Output file. Can be a filename or any kind of\nfile-like object.\n\nclose() → None[source]\nShut all file descriptors (input and output) and clear all memory usage.\n\nParameters\n**infos** – a Python dictionary where each key is a field\nand each value is your new metadata.\nAn example is `{'/Title': 'My title'}`\n\n\nsetPageLayout(*layout: Literal['/NoLayout', '/SinglePage', '/OneColumn', '/TwoColumnLeft', '/TwoColumnRight', '/TwoPageLeft', '/TwoPageRight']*) → None[source]\nUse `set\\_page\\_layout()` instead.\n\nsetPageMode(*mode: Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']*) → None[source]\nUse `set\\_page\\_mode()` instead.\n\n\nset\\_page\\_mode(*mode: Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']*) → None[source]\nSet the page mode.\n\nParameters\n**mode** – The page mode to use.\n\n\nValid `mode` arguments\n\n\nadd\\_outline\\_item(*title: str*, *page\\_number: ~typing.Optional[int] = None*, *parent: ~typing.Union[None*, *~pypdf.generic.\\_data\\_structures.TreeObject*, *~pypdf.generic.\\_base.IndirectObject] = None*, *color: ~typing.Optional[~typing.Tuple[float*, *float*, *float]] = None*, *bold: bool = False*, *italic: bool = False*, *fit: ~pypdf.generic.\\_fit.Fit = <pypdf.generic.\\_fit.Fit object>*, *pagenum: ~typing.Optional[int] = None*) → IndirectObject[source]\nAdd an outline item (commonly referred to as a “Bookmark”) to this PDF file.\n\nParameters\n* **title** – Title to use for this outline item.\n* **page\\_number** – Page number this outline item will point to.\n* **parent** – A reference to a parent outline item to create nested\noutline items.\n* **color** – Color of the outline item’s font as a red, green, blue tuple\nfrom 0.0 to 1.0\n* **bold** – Outline item font is bold\n* **italic** – Outline item font is italic\n* **fit** – The fit of the destination page.\n\naddBookmark(*title: str*, *pagenum: int*, *parent: Union[None, TreeObject, IndirectObject] = None*, *color: Optional[Tuple[float, float, float]] = None*, *bold: bool = False*, *italic: bool = False*, *fit: Literal['/Fit', '/XYZ', '/FitH', '/FitV', '/FitR', '/FitB', '/FitBH', '/FitBV'] = '/Fit'*, *\\*args: Union[NumberObject, NullObject, float]*) → IndirectObject[source]\n\nDeprecated since version 1.28.0: Use `add\\_outline\\_item()` instead.\n\n\nadd\\_bookmark(*title: str*, *pagenum: int*, *parent: Union[None, TreeObject, IndirectObject] = None*, *color: Optional[Tuple[float, float, float]] = None*, *bold: bool = False*, *italic: bool = False*, *fit: Literal['/Fit', '/XYZ', '/FitH', '/FitV', '/FitR', '/FitB', '/FitBH', '/FitBV'] = '/Fit'*, *\\*args: Union[NumberObject, NullObject, float]*) → IndirectObject[source]\n\nDeprecated since version 2.9.0: Use `add\\_outline\\_item()` instead.\n\n\naddNamedDestination(*title: str*, *pagenum: int*) → None[source]\n\nDeprecated since version 1.28.0: Use `add\\_named\\_destination()` instead.\n\n\nadd\\_named\\_destination(*title: str*, *page\\_number: Optional[int] = None*, *pagenum: Optional[int] = None*) → None[source]\nAdd a destination to the output.\n\nParameters\n* **title** – Title to use\n* **page\\_number** – Page number this destination points at.\n# The PageObject Class\n\n\n*class* pypdf.\\_page.PageObject(*pdf: Union[None, PdfReaderProtocol, PdfWriterProtocol] = None*, *indirect\\_reference: Optional[IndirectObject] = None*, *indirect\\_ref: Optional[IndirectObject] = None*)[source]\nBases: `DictionaryObject`\n\n\nPageObject represents a single page within a PDF file.\n\n\nTypically this object will be created by accessing the\n`get\\_page()` method of the\n`PdfReader` class, but\n\n==================\n Document 2 \n----------------\n The annotations module\n\n\nPDF specifies several annotation types which pypdf makes available here.\n\n\nThe names of the annotations and their attributes do not reflect the names in\nthe specification in all cases. For example, the PDF standard defines a\n‘Square’ annotation that does not actually need to be square. For this reason,\npypdf calls it ‘Rectangle’.\n\n\nAt their core, all annotation types are DictionaryObjects. That means if pypdf\ndoes not implement a feature, users can easily extend the given functionality.\n\n\n*class* pypdf.annotations.AnnotationDictionary[source]\nBases: `DictionaryObject`, `ABC`\n\n\n*property* flags*: pypdf.constants.AnnotationFlag*\n\n\n*class* pypdf.annotations.MarkupAnnotation(*\\**, *title\\_bar: Optional[str] = None*)[source]\nBases: `AnnotationDictionary`, `ABC`\n\n\nBase class for all markup annotations.\n\nParameters\n**title\\_bar** – Text to be displayed in the title bar of the annotation;\nby convention this is the name of the author\n\n*class* pypdf.annotations.Ellipse(*rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *\\**, *interiour\\_color: Optional[str] = None*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.FreeText(*\\**, *text: str*, *rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *font: str = 'Helvetica'*, *bold: bool = False*, *italic: bool = False*, *font\\_size: str = '14pt'*, *font\\_color: str = '000000'*, *border\\_color: Optional[str] = '000000'*, *background\\_color: Optional[str] = 'ffffff'*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n\nA FreeText annotation\n\n*class* pypdf.annotations.Highlight(*\\**, *rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *quad\\_points: ArrayObject*, *highlight\\_color: str = 'ff0000'*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.Line(*p1: Tuple[float, float]*, *p2: Tuple[float, float]*, *rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *text: str = ''*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.Link(*\\*, rect: ~typing.Union[~pypdf.generic.\\_rectangle.RectangleObject, ~typing.Tuple[float, float, float, float]], border: ~typing.Optional[~pypdf.generic.\\_data\\_structures.ArrayObject] = None, url: ~typing.Optional[str] = None, target\\_page\\_index: ~typing.Optional[int] = None, fit: ~pypdf.generic.\\_fit.Fit = <pypdf.generic.\\_fit.Fit object>, \\*\\*kwargs: ~typing.Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.Polygon(*vertices: List[Tuple[float, float]]*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.PolyLine(*vertices: List[Tuple[float, float]]*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.Rectangle(*rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *\\**, *interiour\\_color: Optional[str] = None*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.Text(*\\*, rect: ~typing.Union[~pypdf.generic.\\_rectangle.RectangleObject, ~typing.Tuple[float, float, float, float]], text: str, open: bool = False, flags: int = AnnotationFlag.None, \\*\\*kwargs: ~typing.Any*)[source]\nBases: `MarkupAnnotation`\n\n\nA text annotation.\n\nParameters\n* **rect** – array of four integers `[xLL, yLL, xUR, yUR]`\nspecifying the clickable rectangular area\n* **text** – The text that is added to the document\n* **open** –\n* **flags** –\n\n*class* pypdf.annotations.Popup(*\\**, *rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *parent: Optional[DictionaryObject] = None*, *open: bool = False*, *\\*\\*kwargs: Any*)[source]\nBases: `AnnotationDictionary`\n\n# The Fit Class\n\n\n*class* pypdf.generic.Fit(*fit\\_type: str*, *fit\\_args: Tuple[Union[None, float, Any], ...] = ()*)[source]\nBases: `object`\n\n\n*classmethod* xyz(*left: Optional[float] = None*, *top: Optional[float] = None*, *zoom: Optional[float] = None*) → Fit[source]\nDisplay the page designated by page, with the coordinates (left , top)\npositioned at\n\n==================\n Document 3 \n----------------\n The DocumentInformation Class\n\n\n*class* pypdf.DocumentInformation[source]\nBases: `DictionaryObject`\n\n\nA class representing the basic document metadata provided in a PDF File.\nThis class is accessible through\n`PdfReader.metadata`.\n\n\nAll text properties of the document metadata have\n*two* properties, eg. author and author\\_raw. The non-raw property will\nalways return a `TextStringObject`, making it ideal for a case where\nthe metadata is being displayed. The raw property can sometimes return\na `ByteStringObject`, if pypdf was unable to decode the string’s\ntext encoding; this requires additional safety in the caller and\ntherefore is not as commonly accessed.\n\n\ngetText(*key: str*) → Optional[str][source]\nUse the attributes (e.g. `title` / `author`).\n\n\n*property* title*: Optional[str]*\nRead-only property accessing the document’s title.\n\n\nReturns a `TextStringObject` or `None` if the title is not\nspecified.\n\n*property* title\\_raw*: Optional[str]*\nThe “raw” version of title; can return a `ByteStringObject`.\n\n*property* author*: Optional[str]*\nRead-only property accessing the document’s author.\n\n\nReturns a `TextStringObject` or `None` if the author is not\nspecified.\n\n*property* author\\_raw*: Optional[str]*\nThe “raw” version of author; can return a `ByteStringObject`.\n\n*property* subject*: Optional[str]*\nRead-only property accessing the document’s subject.\n\n\nReturns a `TextStringObject` or `None` if the subject is not\nspecified.\n\n*property* subject\\_raw*: Optional[str]*\nThe “raw” version of subject; can return a `ByteStringObject`.\n\n*property* creator*: Optional[str]*\nRead-only property accessing the document’s creator.\n\n\nIf the document was converted to PDF from another format, this is the\nname of the application (e.g. OpenOffice) that created the original\ndocument from which it was converted. Returns a `TextStringObject` or\n`None` if the creator is not specified.\n\n*property* creator\\_raw*: Optional[str]*\nThe “raw” version of creator; can return a `ByteStringObject`.\n\n*property* producer*: Optional[str]*\nRead-only property accessing the document’s producer.\n\n\nIf the document was converted to PDF from another format, this is the\nname of the application (for example, OSX Quartz) that converted it to\nPDF. Returns a `TextStringObject` or `None` if the producer is not\nspecified.\n\n*property* producer\\_raw*: Optional[str]*\nThe “raw” version of producer; can return a `ByteStringObject`.\n\n*property* creation\\_date*: Optional[datetime]*\nRead-only property accessing the document’s creation date.\n\n*property* creation\\_date\\_raw*: Optional[str]*\nThe “raw” version of creation date; can return a `ByteStringObject`.\n\n\nTypically in the format `D:YYYYMMDDhhmmss[+Z-]hh'mm` where the suffix\nis the offset from UTC.\n\n*property* modification\\_date*: Optional[datetime]*\nRead-only property accessing the document’s modification date.\n\n\nThe date and time the document was most recently modified.\n\n*property* modification\\_date\\_raw*: Optional[str]*\nThe “raw” version of modification date; can return a\n`ByteStringObject`.\n# The XmpInformation Class\n\n\n*class* pypdf.xmp.XmpInformation(*stream: ContentStream*)[source]\nBases: `PdfObject`\n\n\nAn object that represents Adobe XMP metadata.\nUsually accessed by `xmp\\_metadata()`\n\nRaises\n**PdfReadError** – if XML is invalid\n\n\n*property* rdfRoot*: xml.dom.minidom.Element*\n\nwrite\\_to\\_stream(*stream: IO*, *encryption\\_key: Union[None, str, bytes] = None*) → None[source]\n\nwriteToStream(*stream: IO*, *encryption\\_key: Union[None, str, bytes]*) → None[source]\nUse `write\\_to\\_stream()`"
    },
    {
        "instruction": "Create a Python program using the 'pypdf' API to extract and display the page dimensions and orientation (portrait or landscape) of each page in a PDF file.",
        "api": "pypdf",
        "output": "#!pip install pypdf\nimport pypdf\n\n# Specify the path of the PDF file to analyze page dimensions\npdf_file_path = \"document.pdf\"\n\n# Open the PDF file in read-binary mode\nwith open(pdf_file_path, \"rb\") as pdf_file:\n    # Create a PDF reader object\n    pdf_reader = pypdf.PdfReader(pdf_file)\n\n    # Initialize a list to store page dimensions and orientation\n    page_info = []\n\n    # Iterate through the pages and extract page dimensions and orientation\n    for page in pdf_reader.pages:\n        page_width = page.MediaBox[2] - page.MediaBox[0]\n        page_height = page.MediaBox[3] - page.MediaBox[1]\n        orientation = \"Portrait\" if page_width <= page_height else \"Landscape\"\n        page_info.append(f\"Page {page.page_number}: {page_width} x {page_height} ({orientation})\")\n\n    # Display the extracted page information\n    for info in page_info:\n        print(info)\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n The annotations module\n\n\nPDF specifies several annotation types which pypdf makes available here.\n\n\nThe names of the annotations and their attributes do not reflect the names in\nthe specification in all cases. For example, the PDF standard defines a\n‘Square’ annotation that does not actually need to be square. For this reason,\npypdf calls it ‘Rectangle’.\n\n\nAt their core, all annotation types are DictionaryObjects. That means if pypdf\ndoes not implement a feature, users can easily extend the given functionality.\n\n\n*class* pypdf.annotations.AnnotationDictionary[source]\nBases: `DictionaryObject`, `ABC`\n\n\n*property* flags*: pypdf.constants.AnnotationFlag*\n\n\n*class* pypdf.annotations.MarkupAnnotation(*\\**, *title\\_bar: Optional[str] = None*)[source]\nBases: `AnnotationDictionary`, `ABC`\n\n\nBase class for all markup annotations.\n\nParameters\n**title\\_bar** – Text to be displayed in the title bar of the annotation;\nby convention this is the name of the author\n\n*class* pypdf.annotations.Ellipse(*rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *\\**, *interiour\\_color: Optional[str] = None*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.FreeText(*\\**, *text: str*, *rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *font: str = 'Helvetica'*, *bold: bool = False*, *italic: bool = False*, *font\\_size: str = '14pt'*, *font\\_color: str = '000000'*, *border\\_color: Optional[str] = '000000'*, *background\\_color: Optional[str] = 'ffffff'*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n\nA FreeText annotation\n\n*class* pypdf.annotations.Highlight(*\\**, *rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *quad\\_points: ArrayObject*, *highlight\\_color: str = 'ff0000'*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.Line(*p1: Tuple[float, float]*, *p2: Tuple[float, float]*, *rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *text: str = ''*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.Link(*\\*, rect: ~typing.Union[~pypdf.generic.\\_rectangle.RectangleObject, ~typing.Tuple[float, float, float, float]], border: ~typing.Optional[~pypdf.generic.\\_data\\_structures.ArrayObject] = None, url: ~typing.Optional[str] = None, target\\_page\\_index: ~typing.Optional[int] = None, fit: ~pypdf.generic.\\_fit.Fit = <pypdf.generic.\\_fit.Fit object>, \\*\\*kwargs: ~typing.Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.Polygon(*vertices: List[Tuple[float, float]]*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.PolyLine(*vertices: List[Tuple[float, float]]*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.Rectangle(*rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *\\**, *interiour\\_color: Optional[str] = None*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.Text(*\\*, rect: ~typing.Union[~pypdf.generic.\\_rectangle.RectangleObject, ~typing.Tuple[float, float, float, float]], text: str, open: bool = False, flags: int = AnnotationFlag.None, \\*\\*kwargs: ~typing.Any*)[source]\nBases: `MarkupAnnotation`\n\n\nA text annotation.\n\nParameters\n* **rect** – array of four integers `[xLL, yLL, xUR, yUR]`\nspecifying the clickable rectangular area\n* **text** – The text that is added to the document\n* **open** –\n* **flags** –\n\n*class* pypdf.annotations.Popup(*\\**, *rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *parent: Optional[DictionaryObject] = None*, *open: bool = False*, *\\*\\*kwargs: Any*)[source]\nBases: `AnnotationDictionary`\n\n# The Fit Class\n\n\n*class* pypdf.generic.Fit(*fit\\_type: str*, *fit\\_args: Tuple[Union[None, float, Any], ...] = ()*)[source]\nBases: `object`\n\n\n*classmethod* xyz(*left: Optional[float] = None*, *top: Optional[float] = None*, *zoom: Optional[float] = None*) → Fit[source]\nDisplay the page designated by page, with the coordinates (left , top)\npositioned at\n\n==================\n Document 1 \n----------------\nThe Fit Class\n\n\n*class* pypdf.generic.Fit(*fit\\_type: str*, *fit\\_args: Tuple[Union[None, float, Any], ...] = ()*)[source]\nBases: `object`\n\n\n*classmethod* xyz(*left: Optional[float] = None*, *top: Optional[float] = None*, *zoom: Optional[float] = None*) → Fit[source]\nDisplay the page designated by page, with the coordinates (left , top)\npositioned at the upper-left corner of the window and the contents\nof the page magnified by the factor zoom.\n\n\nA null value for any of the parameters left, top, or zoom specifies\nthat the current value of that parameter is to be retained unchanged.\n\n\nA zoom value of 0 has the same meaning as a null value.\n\nParameters\n* **left** –\n* **top** –\n* **zoom** –\n\nReturns\nThe created fit object.\n\n*classmethod* fit() → Fit[source]\nDisplay the page designated by page, with its contents magnified just\nenough to fit the entire page within the window both horizontally and\nvertically.\n\n\nIf the required horizontal and vertical magnification factors are\ndifferent, use the smaller of the two, centering the page within the\nwindow in the other dimension.\n\n*classmethod* fit\\_horizontally(*top: Optional[float] = None*) → Fit[source]\nDisplay the page designated by page , with the vertical coordinate top\npositioned at the top edge of the window and the contents of the page\nmagnified just enough to fit the entire width of the page within the\nwindow.\n\n\nA null value for `top` specifies that the current value of that\nparameter is to be retained unchanged.\n\nParameters\n**top** – \n\n*classmethod* fit\\_vertically(*left: Optional[float] = None*) → Fit[source]\n\n*classmethod* fit\\_rectangle(*left: Optional[float] = None*, *bottom: Optional[float] = None*, *right: Optional[float] = None*, *top: Optional[float] = None*) → Fit[source]\nDisplay the page designated by page , with its contents magnified\njust enough to fit the rectangle specified by the coordinates\nleft, bottom, right, and top entirely within the window\nboth horizontally and vertically.\n\n\nIf the required horizontal and vertical magnification factors are\ndifferent, use the smaller of the two, centering the rectangle within\nthe window in the other dimension.\n\n\nA null value for any of the parameters may result in unpredictable\nbehavior.\n\nParameters\n* **left** –\n* **bottom** –\n* **right** –\n* **top** –\n\n*classmethod* fit\\_box() → Fit[source]\nDisplay the page designated by page , with its contents magnified just\nenough to fit its bounding box entirely within the window both\nhorizontally and vertically.\n\n\nIf the required horizontal and vertical magnification factors are\ndifferent, use the smaller of the two, centering the bounding box\nwithin the window in the other dimension.\n\n*classmethod* fit\\_box\\_horizontally(*top: Optional[float] = None*) → Fit[source]\nDisplay the page designated by page , with the vertical coordinate top\npositioned at the top edge of the window and the contents of the page\nmagnified just enough to fit the entire width of its bounding box\nwithin the window.\n\n\nA null value for top specifies that the current value of that parameter\nis to be retained unchanged.\n\n*classmethod* fit\\_box\\_vertically(*left: Optional[float] = None*) → Fit[source]\nDisplay the page designated by page, with the horizontal coordinate\nleft positioned at the left edge of the window and the contents of the\npage magnified just enough to fit the entire height of its bounding box\nwithin the window.\n\n\nA null value for left specifies that the current value of that\nparameter is to be retained unchanged.\n\nParameters\n**left** – \n\n# The PaperSize Class\n\n\n*class* pypdf.PaperSize[source]\nBases: `object`\n\n\n(width, height) of the paper in portrait mode in pixels at 72 ppi.\n\n\nA0 *= Dimensions(width=2384, height=3370)*\n\nA1 *= Dimensions(width=1684, height=2384)*\n\nA2 *= Dimensions(width=1191, height=1684)*\n\nA3 *= Dimensions(width=842, height=1191)*\n\nA4 *= Dimensions(width=595, height=842)*\n\nA5 *= Dimensions(width=420, height=595)*\n\nA6 *= Dimensions(width=298, height=420)*\n\nA7 *= Dimensions(width=210, height=298)*\n\nA8 *= Dimensions(width=147, height=210)*\n\nC4 *= Dimensions(width=649, height=918)*\n\n\n## Add blank page with PaperSize\n\n```\n1from pypdf import PaperSize, PdfReader, PdfWriter\n2\n3pdf\\_reader = PdfReader(\"sample.pdf\")\n4pdf\\_writer = PdfWriter()\n5pdf\\_writer.append\\_pages\\_from\\_reader(pdf\\_reader)\n6pdf\\_writer.add\\_blank\\_page(PaperSize.A8.width, PaperSize.A8.height)\n7with open(\"output.pdf\", \"wb\") as output\\_stream:\n8    pdf\\_writer.write(output\\_stream)\n\n\n\n## Insert blank page with PaperSize\n\n```\n1from pypdf import PaperSize, PdfReader, PdfWriter\n2\n3pdf\\_reader = PdfReader(\"sample.pdf\")\n4pdf\\_writer = PdfWriter()\n5pdf\\_writer.append\\_pages\\_from\\_reader(pdf\\_reader)\n6pdf\\_writer.insert\\_blank\\_page(PaperSize.A8.width, PaperSize.A8.height, 1)\n7with open(\"output.pdf\", \"wb\") as output\\_stream:\n8    pdf\\_writer.write(output\\_stream)\n\n\n\n# The PdfWriter Class\n\n\n# The PdfReader Class\n\n\n\n# The PdfMerger Class\n\n\n\n# The PageObject Class\n\n\n\n# The Transformation Class\n\n\n\n# The DocumentInformation Class\n\n\n\n# The XmpInformation Class\n\n\n\n# The Destination Class\n\n\n\n# The RectangleObject Class\n\n\n\n# The Field Class\n\n\n# The PageRange Class\n\n\n\n# The annotations module\n\n\n# The Fit Class\n\n\n\n# The PaperSize Class\n\n==================\n Document 2 \n----------------\n Example: This will launch the print window when the PDF is opened.\n\n```\n\naddJS(*javascript: str*) → None[source]\nUse `add\\_js()` instead.\n\n\nadd\\_attachment(*filename: str*, *data: Union[str, bytes]*) → None[source]\nEmbed a file inside the PDF.\n\n\nReference:\nhttps://www.adobe.com/content/dam/Adobe/en/devnet/acrobat/pdfs/PDF32000\\_2008.pdf\nSection 7.11.3\n\nParameters\n* **filename** – The filename to display.\n* **data** – The data in the file.\n\naddAttachment(*fname: str*, *fdata: Union[str, bytes]*) → None[source]\nUse `add\\_attachment()` instead.\n\n\nappend\\_pages\\_from\\_reader(*reader: PdfReader*, *after\\_page\\_append: Optional[Callable[[PageObject], None]] = None*) → None[source]\nCopy pages from reader to writer. Includes an optional callback\nparameter which is invoked after pages are appended to the writer.\n\n\n`append` should be prefered.\n\nParameters\n* **reader** – a PdfReader object from which to copy page\nannotations to this writer object. The writer’s annots\nwill then be updated\n* **after\\_page\\_append** – Callback function that is invoked after each page is appended to\nthe writer. Signature includes a reference to the appended page\n(delegates to append\\_pages\\_from\\_reader). The single parameter of\nthe callback is a reference to the page just appended to the\ndocument.\n\nappendPagesFromReader(*reader: PdfReader*, *after\\_page\\_append: Optional[Callable[[PageObject], None]] = None*) → None[source]\nUse `append\\_pages\\_from\\_reader()` instead.\n\n\nupdate\\_page\\_form\\_field\\_values(*page: ~pypdf.\\_page.PageObject, fields: ~typing.Dict[str, ~typing.Any], flags: ~pypdf.constants.FieldFlag = FieldFlag.None, auto\\_regenerate: ~typing.Optional[bool] = True*) → None[source]\nUpdate the form field values for a given page from a fields dictionary.\n\n\nCopy field texts and values from fields to page.\nIf the field links to a parent object, add the information to the parent.\n\nParameters\n* **page** – Page reference from PDF writer where the\nannotations and field data will be updated.\n* **fields** – a Python dictionary of field names (/T) and text\nvalues (/V)\n* **flags** – An integer (0 to 7). The first bit sets ReadOnly, the\nsecond bit sets Required, the third bit sets NoExport. See\nPDF Reference Table 8.70 for details.\n* **auto\\_regenerate** – set/unset the need\\_appearances flag ;\nthe flag is unchanged if auto\\_regenerate is None\n\nupdatePageFormFieldValues(*page: ~pypdf.\\_page.PageObject, fields: ~typing.Dict[str, ~typing.Any], flags: ~pypdf.constants.FieldFlag = FieldFlag.None*) → None[source]\nUse `update\\_page\\_form\\_field\\_values()` instead.\n\n\nclone\\_reader\\_document\\_root(*reader: PdfReader*) → None[source]\nCopy the reader document root to the writer and all sub elements,\nincluding pages, threads, outlines,… For partial insertion, `append`\nshould be considered.\n\nParameters\n**reader** – PdfReader from the document root should be copied.\n\ncloneReaderDocumentRoot(*reader: PdfReader*) → None[source]\nUse `clone\\_reader\\_document\\_root()` instead.\n\n\nclone\\_document\\_from\\_reader(*reader: PdfReader*, *after\\_page\\_append: Optional[Callable[[PageObject], None]] = None*) → None[source]\nCreate a copy (clone) of a document from a PDF file reader cloning\nsection ‘/Root’ and ‘/Info’ and ‘/ID’ of the pdf.\n\nParameters\n* **reader** – PDF file reader instance from which the clone\nshould be created.\n* **after\\_page\\_append** – Callback function that is invoked after each page is appended to\nthe writer. Signature includes a reference to the appended page\n(delegates to append\\_pages\\_from\\_reader). The single parameter of\nthe callback is a reference to the page just appended to the\ndocument.\n\ncloneDocumentFromReader(*reader: PdfReader*, *after\\_page\\_append: Optional[Callable[[PageObject], None]] = None*) → None[source]\nUse `clone\\_document\\_from\\_reader()` instead.\n\n\ngenerate\\_file\\_identifiers() → None[source]\nGenerate an identifier for the PDF that will be written.\n\n\nThe only point of this is ensuring uniqueness. Reproducibility is not\nrequired; see 14.4 “File Identifiers”.\n\nencrypt(*user\\_password: Optional[str] = None*, *owner\\_password: Optional[str] = None*, *use\\_128bit: bool = True*, *permissions\\_flag: UserAccessPermissions = UserAccessPermissions.PRINT | MODIFY | EXTRACT | ADD\\_OR\\_MODIFY | R7 | R8 | FILL\\_FORM\\_FIELDS | EXTRACT\\_TEXT\\_AND\\_GRAPHICS | ASSEMBLE\\_DOC | PRINT\\_TO\\_REPRESENTATION | R13 | R14 | R15 | R16 | R17 | R18 | R19 | R20 | R21 | R22 | R23 | R24 | R25 | R26 | R27 | R28 | R29 | R30 | R31*, *user\\_pwd: Optional[str] = None*, *owner\\_pwd: Optional[str] = None*, *\\**, *algorithm: Optional[str] = None*) → None[source]\nEncrypt this PDF file with the PDF Standard encryption handler.\n\nParameters\n* **user\\_password** – The password which allows for opening\nand reading the PDF file with the restrictions provided.\n* **owner\\_password** – The password which allows for\nopening the PDF files without any restrictions. By default,\nthe owner password is the same as the user password.\n* **use\\_128bit** – flag as to whether to use 128bit\nencryption. When false, 40bit encryption will be used.\nBy default, this flag is on.\n* **permissions\\_flag** – permissions as described in\nTABLE 3.20 of the PDF 1.7 specification. A bit value of 1 means\nthe permission is grantend.\nHence an integer value of -1 will set all flags.\nBit position 3 is for printing, 4 is for modifying content,\n5 and 6 control annotations, 9 for form fields,\n10 for extraction of text and graphics.\n* **algorithm** – encrypt algorithm. Values maybe one of “RC4-40”, “RC4-128”,\n“AES-128”, “AES-256-R5”, “AES-256”. If it’s valid,\nuse\\_128bit will be ignored.\n\nwrite\\_stream(*stream: IO*) → None[source]\n\nwrite(*stream: Union[Path, str, IO]*) → Tuple[bool, IO][source]\nWrite the collection of pages added to this object out as a PDF file.\n\nParameters\n**stream** – An object to write the file to. The object can support\nthe write method and the tell method, similar to a file object, or\nbe a file path, just like the fileobj, just named it stream to keep\nexisting workflow.\n\nReturns\nA tuple (bool, IO)\n\nadd\\_metadata(*infos: Dict[str, Any]*) → None[source]\nAdd custom metadata to the output.\n\nParameters\n**infos** – a Python dictionary where each key is a field\nand each value is your new metadata.\n\naddMetadata(*infos: Dict[str, Any]*) → None[source]\nUse `add\\_metadata()` instead.\n\n\nget\\_reference(*obj: PdfObject*) → IndirectObject[source]\n\ngetReference(*obj: PdfObject*) → IndirectObject[source]\nUse `get\\_reference()` instead.\n\n\nget\\_outline\\_root() → TreeObject[source]\n\nget\\_threads\\_root() → ArrayObject[source]\nThe list of threads.\n\nReturns\nAn array (possibly empty) of Dictionaries with `/F` and\n`/I` properties.\n\n*property* threads*: pypdf.generic.\\_data\\_structures.ArrayObject*\nRead-only property for the list of threads.\n\n\nEach element is a dictionaries with `/F` and `/I` keys.\n\ngetOutlineRoot() → TreeObject[source]\nUse `get\\_outline\\_root()` instead.\n\n\nget\\_named\\_dest\\_root() → ArrayObject[source]\n\ngetNamedDestRoot() → ArrayObject[source]\nUse `get\\_named\\_dest\\_root()` instead.\n\n\nadd\\_outline\\_item\\_destination(*page\\_destination: Union[None, PageObject, TreeObject] = None*, *parent: Union[None, TreeObject, IndirectObject] = None*, *before: Union[None, TreeObject, IndirectObject] = None*, *is\\_open: bool = True*, *dest: Union[None, PageObject, TreeObject] = None*) → IndirectObject[source]\n\nadd\\_bookmark\\_destination(*dest: Union[PageObject, TreeObject]*, *parent: Union[None, TreeObject, IndirectObject] = None*) → IndirectObject[source]\nUse `add\\_outline\\_item\\_destination()` instead.\n\n\naddBookmarkDestination(*dest: PageObject*, *parent: Optional[TreeObject] = None*) → IndirectObject[source]\nUse `add\\_outline\\_item\\_destination()` instead.\n\n\nadd\\_outline\\_item\\_dict(*outline\\_item: Union[OutlineItem, Destination]*, *parent: Union[None, TreeObject, IndirectObject] = None*, *before: Union[None, TreeObject, IndirectObject] = None*, *is\\_open: bool = True*) → IndirectObject[source]\n\nadd\\_bookmark\\_dict(*outline\\_item: Union[OutlineItem, Destination]*, *parent: Optional[TreeObject] = None*) → IndirectObject[source]\nUse `add\\_outline\\_item\\_dict()` instead.\n\n\naddBookmarkDict(*outline\\_item: Union[OutlineItem, Destination]*, *parent: Optional[TreeObject] = None*) → IndirectObject[source]\nUse `add\\_outline\\_item\\_dict()` instead.\n\n\nadd\\_outline\\_item(*title: str, page\\_number: ~typing.Union[None, ~pypdf.\\_page.PageObject, ~pypdf.generic.\\_base.IndirectObject, int], parent: ~typing.Union[None, ~pypdf.generic.\\_data\\_structures.TreeObject, ~pypdf.generic.\\_base.IndirectObject] = None, before: ~typing.Union[None, ~pypdf.generic.\\_data\\_structures.TreeObject, ~pypdf.generic.\\_base.IndirectObject] = None, color: ~typing.Optional[~typing.Union[~typing.Tuple[float, float, float], str]] = None, bold: bool = False, italic: bool = False, fit: ~pypdf.generic.\\_fit.Fit = <pypdf.generic.\\_fit.Fit object>, is\\_open: bool = True, pagenum: ~typing.Optional[int] = None*) → IndirectObject[source]\nAdd an outline item (commonly referred to as a “Bookmark”) to the PDF file.\n\nParameters\n* **title** – Title to use for this outline item.\n* **page\\_number** – Page number this outline item will point to.\n* **parent** – A reference to a parent outline item to create nested\noutline items.\n* **before** –\n* **color** – Color of the outline item’s font as a red, green, blue tuple\nfrom 0.0 to 1.0 or as a Hex String (#RRGGBB)\n* **bold** – Outline item font is bold\n* **italic** – Outline item font is italic\n* **fit** – The fit of the destination page.\n\nReturns\nThe added outline item as an indirect object.\n\nadd\\_bookmark(*title: str*, *pagenum: int*, *parent: Union[None, TreeObject, IndirectObject] = None*, *color: Optional[Tuple[float, float, float]] = None*, *bold: bool = False*, *italic: bool = False*, *fit: Literal['/Fit', '/XYZ', '/FitH', '/FitV', '/FitR', '/FitB', '/FitBH', '/FitBV'] = '/Fit'*, *\\*args: Union[NumberObject, NullObject, float]*) → IndirectObject[source]\nUse `add\\_outline\\_item()` instead.\n\n\naddBookmark(*title: str*, *pagenum: int*, *parent: Union[None, TreeObject, IndirectObject] = None*, *color: Optional[Tuple[float, float, float]] = None*, *bold: bool = False*, *italic: bool = False*, *fit: Literal['/Fit', '/XYZ', '/FitH', '/FitV', '/FitR', '/FitB', '/FitBH', '/FitBV'] = '/Fit'*, *\\*args: Union[NumberObject, NullObject, float]*) → IndirectObject[source]\nUse `add\\_outline\\_item()` instead.\n\n\nadd\\_outline() → None[source]\n\nadd\\_named\\_destination\\_array(*title: TextStringObject*, *destination: Union[IndirectObject, ArrayObject]*) → None[source]\n\nadd\\_named\\_destination\\_object(*page\\_destination: Optional[PdfObject] = None*, *dest: Optional[PdfObject] = None*) → IndirectObject[source]\n\naddNamedDestinationObject(*dest: Destination*) → IndirectObject[source]\nUse `add\\_named\\_destination\\_object()` instead.\n\n\nadd\\_named\\_destination(*title: str*, *page\\_number: Optional[int] = None*, *pagenum: Optional[int] = None*) → IndirectObject[source]\n\naddNamedDestination(*title: str*, *pagenum: int*) → IndirectObject[source]\nUse `add\\_named\\_destination()` instead.\n\n\nremove\\_links() → None[source]\nRemove links and annotations from this output.\n\nremoveLinks() → None[source]\nUse `remove\\_links()` instead.\n\n\nremove\\_annotations(*subtypes: Optional[Union[Literal['/Text', '/Link', '/FreeText', '/Line', '/Square', '/Circle', '/Polygon', '/PolyLine', '/Highlight', '/Unterline', '/Squiggly', '/StrikeOut', '/Stamp', '/Caret', '/Ink', '/Popup', '/FileAttachment', '/Sound', '/Movie', '/Widget', '/Screen', '/PrinterMark', '/TrapNet', '/Watermark', '/3D', '/Redact'], Iterable[Literal['/Text', '/Link', '/FreeText', '/Line', '/Square', '/Circle', '/Polygon', '/PolyLine', '/Highlight', '/Unterline', '/Squiggly', '/StrikeOut', '/Stamp', '/Caret', '/Ink', '/Popup', '/FileAttachment', '/Sound', '/Movie', '/Widget', '/Screen', '/PrinterMark', '/TrapNet', '/Watermark', '/3D', '/Redact']]]]*) → None[source]\nRemove annotations by annotation subtype.\n\nParameters\n**subtypes** – SubType or list of SubTypes to be removed.\nExamples are: “/Link”, “/FileAttachment”, “/Sound”,\n“/Movie”, “/Screen”, …\nIf you want to remove all annotations, use subtypes=None.\n\nremove\\_objects\\_from\\_page(*page: Union[PageObject, DictionaryObject]*, *to\\_delete: Union[ObjectDeletionFlag, Iterable[ObjectDeletionFlag]]*) → None[source]\nRemove objects specified by `to\\_delete` from the given page.\n\nParameters\n* **page** – Page object to clean up.\n* **to\\_delete** – Objects to be deleted; can be a `ObjectDeletionFlag`\nor a list of ObjectDeletionFlag\n\nremove\\_images(*ignore\\_byte\\_string\\_object: Optional[bool] = None*) → None[source]\nRemove images from this output.\n\nParameters\n**ignore\\_byte\\_string\\_object** – deprecated\n\nremoveImages(*ignoreByteStringObject: bool = False*) → None[source]\nUse `remove\\_images()` instead.\n\n\nremove\\_text(*ignore\\_byte\\_string\\_object: Optional[bool] = None*) → None[source]\nRemove text from this output.\n\nremoveText(*ignoreByteStringObject: bool = False*) → None[source]\nUse `remove\\_text()` instead.\n\n\nadd\\_uri(*page\\_number: int*, *uri: str*, *rect: RectangleObject*, *border: Optional[ArrayObject] = None*, *pagenum: Optional[int] = None*) → None[source]\nAdd an URI from a rectangular area to the specified page.\n\n\nThis uses the basic structure of `add\\_link()`\n\nParameters\n* **page\\_number** – index of the page on which to place the URI action.\n* **uri** – URI of resource to link to.\n* **rect** – `RectangleObject` or\narray of four integers specifying the clickable rectangular area\n`[xLL, yLL, xUR, yUR]`, or string in the form\n`\"[ xLL yLL xUR yUR ]\"`.\n* **border** – if provided, an array describing border-drawing\nproperties. See the PDF spec for details. No border will be\ndrawn if this argument is omitted.\n\naddURI(*pagenum: int*, *uri: str*, *rect: RectangleObject*, *border: Optional[ArrayObject] = None*) → None[source]\nUse `add\\_uri()` instead.\n\n\nadd\\_link(*pagenum: int*, *page\\_destination: int*, *rect: RectangleObject*, *border: Optional[ArrayObject] = None*, *fit: Literal['/Fit', '/XYZ', '/FitH', '/FitV', '/FitR', '/FitB', '/FitBH', '/FitBV'] = '/Fit'*, *\\*args: Union[NumberObject, NullObject, float]*) → DictionaryObject[source]\n\naddLink(*pagenum: int*, *page\\_destination: int*, *rect: RectangleObject*, *border: Optional[ArrayObject] = None*, *fit: Literal['/Fit', '/XYZ', '/FitH', '/FitV', '/FitR', '/FitB', '/FitBH', '/FitBV'] = '/Fit'*, *\\*args: Union[NumberObject, NullObject, float]*) → None[source]\nUse `add\\_link()` instead.\n\n\ngetPageLayout() → Optional[Literal['/NoLayout', '/SinglePage', '/OneColumn', '/TwoColumnLeft', '/TwoColumnRight', '/TwoPageLeft', '/TwoPageRight']][source]\nUse `page\\_layout` instead.\n\n\nset\\_page\\_layout(*layout: Literal['/NoLayout', '/SinglePage', '/OneColumn', '/TwoColumnLeft', '/TwoColumnRight', '/TwoPageLeft', '/TwoPageRight']*) → None[source]\nSet the page layout.\n\nParameters\n**layout** – The page layout to be used\n\n\nValid `layout` arguments\n\nsetPageLayout(*layout: Literal['/NoLayout', '/SinglePage', '/OneColumn', '/TwoColumnLeft', '/TwoColumnRight', '/TwoPageLeft', '/TwoPageRight']*) → None[source]\nUse `page\\_layout` instead.\n\n\n*property* page\\_layout*: Optional[Literal['/NoLayout', '/SinglePage', '/OneColumn', '/TwoColumnLeft', '/TwoColumnRight', '/TwoPageLeft', '/TwoPageRight']]*\nPage layout property.\n\n*property* pageLayout*: Optional[Literal['/NoLayout', '/SinglePage', '/OneColumn', '/TwoColumnLeft', '/TwoColumnRight', '/TwoPageLeft', '/TwoPageRight']]*\nUse `page\\_layout` instead.\n\n\ngetPageMode() → Optional[Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']][source]\nUse `page\\_mode` instead.\n\n\nset\\_page\\_mode(*mode: Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']*) → None[source]\nUse `page\\_mode` instead.\n\n\nsetPageMode(*mode: Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']*) → None[source]\nUse `page\\_mode` instead.\n\n\n*property* page\\_mode*: Optional[Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']]*\nPage mode property.\n\n*property* pageMode*: Optional[Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']]*\nUse `page\\_mode` instead.\n\n\nadd\\_annotation(*page\\_number: Union[int, PageObject]*, *annotation: Dict[str, Any]*) → DictionaryObject[source]\nAdd a single annotation to the page.\nThe added annotation must be a new annotation.\nIt can not be recycled.\n\nParameters\n* **page\\_number** – PageObject or page index.\n* **annotation** – Annotation to be added (created with annotation).\n\nReturns\nThe inserted object\nThis can be used for pop-up creation, for example\n\nclean\\_page(*page: Union[PageObject, IndirectObject]*) → PageObject[source]\nPerform some clean up in the page.\nCurrently: convert NameObject nameddestination to TextStringObject\n(required for names/dests list)\n\nParameters\n**page** – \n\nReturns\nThe cleaned PageObject\n\nappend(*fileobj: Union[str, IO, PdfReader, Path]*, *outline\\_item: Union[str, None, PageRange, Tuple[int, int], Tuple[int, int, int], List[int]] = None*, *pages: Union[None, PageRange, Tuple[int, int], Tuple[int, int, int], List[int], List[PageObject]] = None*, *import\\_outline: bool = True*, *excluded\\_fields: Optional[Union[List[str], Tuple[str, ...]]] = None*) → None[source]\nIdentical to the `merge()` method, but assumes you want to\nconcatenate all pages onto the end of the file instead of specifying a\nposition.\n\nParameters\n* **fileobj** – A File Object or an object that supports the standard\nread and seek methods similar to a File Object. Could also be a\nstring representing a path to a PDF file.\n* **outline\\_item** – Optionally, you may specify a string to build an\noutline (aka ‘bookmark’) to identify the beginning of the\nincluded file.\n* **pages** – Can be a `PageRange`\nor a `(start, stop[, step])` tuple\nor a list of pages to be processed\nto merge only the specified range of pages from the source\ndocument into the output document.\n* **import\\_outline** – You may prevent the source document’s\noutline (collection of outline items, previously referred to as\n‘bookmarks’) from being imported by specifying this as `False`.\n* **excluded\\_fields** – Provide the list of fields/keys to be ignored\nif `/Annots` is part of the list, the annotation will be ignored\nif `/B` is part of the list, the articles will be ignored\n\nmerge(*position: Optional[int]*, *fileobj: Union[Path, str, IO, PdfReader]*, *outline\\_item: Optional[str] = None*, *pages: Optional[Union[str, PageRange, Tuple[int, int], Tuple[int, int, int], List[int], List[PageObject]]] = None*, *import\\_outline: bool = True*, *excluded\\_fields: Optional[Union[List[str], Tuple[str, ...]]] = ()*) → None[source]\nMerge the pages from the given file into the output file at the\nspecified page number.\n\nParameters\n* **position** – The *page number* to insert this file. File will\nbe inserted after the given number.\n* **fileobj** – A File Object or an object that supports the standard\nread and seek methods similar to a File Object. Could also be a\nstring representing a path to a PDF file.\n* **outline\\_item** – Optionally, you may specify a string to build an outline\n(aka ‘bookmark’) to identify the\nbeginning of the included file.\n* **pages** – can be a `PageRange`\nor a `(start, stop[, step])` tuple\nor a list of pages to be processed\nto merge only the specified range of pages from the source\ndocument into the output document.\n* **import\\_outline** – You may prevent the source document’s\noutline (collection of outline items, previously referred to as\n‘bookmarks’) from being imported by specifying this as `False`.\n* **excluded\\_fields** – provide the list of fields/keys to be ignored\nif `/Annots` is part of the list, the annotation will be ignored\nif `/B` is part of the list, the articles will be ignored\n\nRaises\n**TypeError** – The pages attribute is not configured properly\n\nadd\\_filtered\\_articles(*fltr: Union[Pattern, str]*, *pages: Dict[int, PageObject]*, *reader: PdfReader*) → None[source]\nAdd articles matching the defined criteria.\n\nParameters\n* **fltr** –\n* **pages** –\n* **reader** –\n\nclose() → None[source]\nTo match the functions from Merger.\n\nfind\\_outline\\_item(*outline\\_item: Dict[str, Any]*, *root: Optional[List[Union[Destination, List[Union[Destination, List[Destination]]]]]] = None*) → Optional[List[int]][source]\n\nfind\\_bookmark(*outline\\_item: Dict[str, Any]*, *root: Optional[List[Union[Destination, List[Union[Destination, List[Destination]]]]]] = None*) → Optional[List[int]][source]\n\nDeprecated since version 2.9.0: Use `find\\_outline\\_item()` instead.\n\n\nreset\\_translation(*reader: Union[None, PdfReader, IndirectObject] = None*) → None[source]\nReset the translation table between reader and the writer object.\n\n\nLate cloning will create new independent objects.\n\nParameters\n**reader** – PdfReader or IndirectObject refering a PdfReader object.\nif set to None or omitted, all tables will be reset.\n\nset\\_page\\_label(*page\\_index\\_from: int*, *page\\_index\\_to: int*, *style: Optional[PageLabelStyle] = None*, *prefix: Optional[str] = None*, *start: Optional[int] = 0*) → None[source]\nSet a page label to a range of pages.\n\n\nPage indexes must be given starting from 0.\nLabels must have a style, a prefix or both.\nIf to a range is not assigned any page label a decimal label starting from 1 is applied.\n\nParameters\n* **page\\_index\\_from** – page index of the beginning of the range starting from 0\n* **page\\_index\\_to** – page index of the beginning of the range starting from 0\n* **style** – The numbering style to be used for the numeric portion of each page label:\n‘/D’ Decimal arabic numerals\n‘/R’ Uppercase roman numerals\n‘/r’ Lowercase roman numerals\n‘/A’ Uppercase letters (A to Z for the first 26 pages,\n\n> \n> AA to ZZ for the next 26, and so on)\n> \n> \n> \n\n’/a’ Lowercase letters (a to z for the first 26 pages,aa to zz for the next 26, and so on)\n* **prefix** – The label prefix for page labels in this range.\n* **start** – The value of the numeric portion for the first page label\nin the range.\nSubsequent pages are numbered sequentially from this value,\nwhich must be greater than or equal to 1.\nDefault value: 1.\n# The PdfMerger Class\n\n\n*class* pypdf.PdfMerger(*strict: bool = False*, *fileobj: Union[Path, str, IO] = ''*)[source]\nBases: `object`\n\n\nUse `PdfWriter` instead.\n\nDeprecated since version 5.0.0.\n\nmerge(*page\\_number: Optional[int] = None*, *fileobj: Union[None, Path, str, IO, PdfReader] = None*, *outline\\_item: Optional[str] = None*, *pages: Optional[Union[str, PageRange, Tuple[int, int],\n\n==================\n Document 3 \n----------------\n The RectangleObject Class\n\n\n*class* pypdf.generic.RectangleObject(*arr: Union[RectangleObject, Tuple[float, float, float, float]]*)[source]\nBases: `ArrayObject`\n\n\nThis class is used to represent *page boxes* in pypdf.\n\n\nThese boxes include:\n\n\n* `artbox`\n* `bleedbox`\n* `cropbox`\n* `mediabox`\n* `trimbox`\n\n\nscale(*sx: float*, *sy: float*) → RectangleObject[source]\n\nensureIsNumber(*value: Any*) → Union[FloatObject, NumberObject][source]\n\n*property* left*: pypdf.generic.\\_base.FloatObject*\n\n*property* bottom*: pypdf.generic.\\_base.FloatObject*\n\n*property* right*: pypdf.generic.\\_base.FloatObject*\n\n*property* top*: pypdf.generic.\\_base.FloatObject*\n\ngetLowerLeft\\_x() → FloatObject[source]\n\ngetLowerLeft\\_y() → FloatObject[source]\n\ngetUpperRight\\_x() → FloatObject[source]\n\ngetUpperRight\\_y() → FloatObject[source]\n\ngetUpperLeft\\_x() → FloatObject[source]\n\ngetUpperLeft\\_y() → FloatObject[source]\n\ngetLowerRight\\_x() → FloatObject[source]\n\ngetLowerRight\\_y() → FloatObject[source]\n\n*property* lower\\_left*: Tuple[float, float]*\nProperty to read and modify the lower left coordinate of this box\nin (x,y) form.\n\n*property* lower\\_right*: Tuple[float, float]*\nProperty to read and modify the lower right coordinate of this box\nin (x,y) form.\n\n*property* upper\\_left*: Tuple[float, float]*\nProperty to read and modify the upper left coordinate of this box\nin (x,y) form.\n\n*property* upper\\_right*: Tuple[float, float]*\nProperty to read and modify the upper right coordinate of this box\nin (x,y) form.\n\ngetLowerLeft() → Tuple[float, float][source]\n\ngetLowerRight() → Tuple[float, float][source]\n\ngetUpperLeft() → Tuple[float, float][source]\n\ngetUpperRight() → Tuple[float, float][source]\n\nsetLowerLeft(*value: Tuple[float, float]*) → None[source]\n\nsetLowerRight(*value: Tuple[float, float]*) → None[source]\n\nsetUpperLeft(*value: Tuple[float, float]*) → None[source]\n\nsetUpperRight(*value: Tuple[float, float]*) → None[source]\n\n*property* width*: float*\n\ngetWidth() → float[source]\n\n*property* height*: float*\n\ngetHeight() → float[source]\n\n*property* lowerLeft*: Tuple[float, float]*\n\n*property* lowerRight*: Tuple[float, float]*\n\n*property* upperLeft*: Tuple[float, float]*\n\n*property* upperRight*: Tuple[float, float]*\n# The Field Class\n\n\n*class* pypdf.generic.Field(*data: DictionaryObject*)[source]\nBases: `TreeObject`\n\n\nA class representing a field dictionary.\n\n\nThis class is accessed through\n`get\\_fields()`\n\n\n*property* field\\_type*: Optional[pypdf.generic.\\_base.NameObject]*\nRead-only property accessing the type of this field.\n\n*property* fieldType*: Optional[pypdf.generic.\\_base.NameObject]*\nUse `field\\_type` instead.\n\n\n*property* parent*: Optional[pypdf.generic.\\_data\\_structures.DictionaryObject]*\nRead-only property accessing the parent of this field.\n\n*property* kids*: Optional[pypdf.generic.\\_data\\_structures.ArrayObject]*\nRead-only"
    },
    {
        "instruction": "Create a Python program using the 'pypdf' API to extract and display the content of a specific section (e.g., chapter) from a PDF file based on a keyword or title.",
        "api": "pypdf",
        "output": "#!pip install pypdf\nimport pypdf\n\n# Specify the path of the PDF file\npdf_file_path = \"document.pdf\"\n\n# Specify the keyword or title to search for\nsearch_keyword = \"Chapter 3\"\n\n# Open the PDF file in read-binary mode\nwith open(pdf_file_path, \"rb\") as pdf_file:\n    # Create a PDF reader object\n    pdf_reader = pypdf.PdfReader(pdf_file)\n\n    # Initialize variables to track the start and end of the section\n    section_start = None\n    section_end = None\n\n    # Iterate through the pages and find the section based on the keyword\n    for page_num, page in enumerate(pdf_reader.pages, start=1):\n        page_text = page.extract_text()\n        if search_keyword in page_text:\n            if section_start is None:\n                section_start = page_num\n            section_end = page_num\n\n    # Check if the section is found\n    if section_start is not None:\n        # Extract and display the content of the section\n        section_text = \"\"\n        for page_num in range(section_start, section_end + 1):\n            page = pdf_reader.pages[page_num - 1]\n            section_text += page.extract_text()\n        print(section_text)\n    else:\n        print(f\"Section '{search_keyword}' not found.\")\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n The annotations module\n\n\nPDF specifies several annotation types which pypdf makes available here.\n\n\nThe names of the annotations and their attributes do not reflect the names in\nthe specification in all cases. For example, the PDF standard defines a\n‘Square’ annotation that does not actually need to be square. For this reason,\npypdf calls it ‘Rectangle’.\n\n\nAt their core, all annotation types are DictionaryObjects. That means if pypdf\ndoes not implement a feature, users can easily extend the given functionality.\n\n\n*class* pypdf.annotations.AnnotationDictionary[source]\nBases: `DictionaryObject`, `ABC`\n\n\n*property* flags*: pypdf.constants.AnnotationFlag*\n\n\n*class* pypdf.annotations.MarkupAnnotation(*\\**, *title\\_bar: Optional[str] = None*)[source]\nBases: `AnnotationDictionary`, `ABC`\n\n\nBase class for all markup annotations.\n\nParameters\n**title\\_bar** – Text to be displayed in the title bar of the annotation;\nby convention this is the name of the author\n\n*class* pypdf.annotations.Ellipse(*rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *\\**, *interiour\\_color: Optional[str] = None*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.FreeText(*\\**, *text: str*, *rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *font: str = 'Helvetica'*, *bold: bool = False*, *italic: bool = False*, *font\\_size: str = '14pt'*, *font\\_color: str = '000000'*, *border\\_color: Optional[str] = '000000'*, *background\\_color: Optional[str] = 'ffffff'*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n\nA FreeText annotation\n\n*class* pypdf.annotations.Highlight(*\\**, *rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *quad\\_points: ArrayObject*, *highlight\\_color: str = 'ff0000'*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.Line(*p1: Tuple[float, float]*, *p2: Tuple[float, float]*, *rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *text: str = ''*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.Link(*\\*, rect: ~typing.Union[~pypdf.generic.\\_rectangle.RectangleObject, ~typing.Tuple[float, float, float, float]], border: ~typing.Optional[~pypdf.generic.\\_data\\_structures.ArrayObject] = None, url: ~typing.Optional[str] = None, target\\_page\\_index: ~typing.Optional[int] = None, fit: ~pypdf.generic.\\_fit.Fit = <pypdf.generic.\\_fit.Fit object>, \\*\\*kwargs: ~typing.Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.Polygon(*vertices: List[Tuple[float, float]]*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.PolyLine(*vertices: List[Tuple[float, float]]*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.Rectangle(*rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *\\**, *interiour\\_color: Optional[str] = None*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.Text(*\\*, rect: ~typing.Union[~pypdf.generic.\\_rectangle.RectangleObject, ~typing.Tuple[float, float, float, float]], text: str, open: bool = False, flags: int = AnnotationFlag.None, \\*\\*kwargs: ~typing.Any*)[source]\nBases: `MarkupAnnotation`\n\n\nA text annotation.\n\nParameters\n* **rect** – array of four integers `[xLL, yLL, xUR, yUR]`\nspecifying the clickable rectangular area\n* **text** – The text that is added to the document\n* **open** –\n* **flags** –\n\n*class* pypdf.annotations.Popup(*\\**, *rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *parent: Optional[DictionaryObject] = None*, *open: bool = False*, *\\*\\*kwargs: Any*)[source]\nBases: `AnnotationDictionary`\n\n# The Fit Class\n\n\n*class* pypdf.generic.Fit(*fit\\_type: str*, *fit\\_args: Tuple[Union[None, float, Any], ...] = ()*)[source]\nBases: `object`\n\n\n*classmethod* xyz(*left: Optional[float] = None*, *top: Optional[float] = None*, *zoom: Optional[float] = None*) → Fit[source]\nDisplay the page designated by page, with the coordinates (left , top)\npositioned at\n\n==================\n Document 1 \n----------------\n Example: This will launch the print window when the PDF is opened.\n\n```\n\naddJS(*javascript: str*) → None[source]\nUse `add\\_js()` instead.\n\n\nadd\\_attachment(*filename: str*, *data: Union[str, bytes]*) → None[source]\nEmbed a file inside the PDF.\n\n\nReference:\nhttps://www.adobe.com/content/dam/Adobe/en/devnet/acrobat/pdfs/PDF32000\\_2008.pdf\nSection 7.11.3\n\nParameters\n* **filename** – The filename to display.\n* **data** – The data in the file.\n\naddAttachment(*fname: str*, *fdata: Union[str, bytes]*) → None[source]\nUse `add\\_attachment()` instead.\n\n\nappend\\_pages\\_from\\_reader(*reader: PdfReader*, *after\\_page\\_append: Optional[Callable[[PageObject], None]] = None*) → None[source]\nCopy pages from reader to writer. Includes an optional callback\nparameter which is invoked after pages are appended to the writer.\n\n\n`append` should be prefered.\n\nParameters\n* **reader** – a PdfReader object from which to copy page\nannotations to this writer object. The writer’s annots\nwill then be updated\n* **after\\_page\\_append** – Callback function that is invoked after each page is appended to\nthe writer. Signature includes a reference to the appended page\n(delegates to append\\_pages\\_from\\_reader). The single parameter of\nthe callback is a reference to the page just appended to the\ndocument.\n\nappendPagesFromReader(*reader: PdfReader*, *after\\_page\\_append: Optional[Callable[[PageObject], None]] = None*) → None[source]\nUse `append\\_pages\\_from\\_reader()` instead.\n\n\nupdate\\_page\\_form\\_field\\_values(*page: ~pypdf.\\_page.PageObject, fields: ~typing.Dict[str, ~typing.Any], flags: ~pypdf.constants.FieldFlag = FieldFlag.None, auto\\_regenerate: ~typing.Optional[bool] = True*) → None[source]\nUpdate the form field values for a given page from a fields dictionary.\n\n\nCopy field texts and values from fields to page.\nIf the field links to a parent object, add the information to the parent.\n\nParameters\n* **page** – Page reference from PDF writer where the\nannotations and field data will be updated.\n* **fields** – a Python dictionary of field names (/T) and text\nvalues (/V)\n* **flags** – An integer (0 to 7). The first bit sets ReadOnly, the\nsecond bit sets Required, the third bit sets NoExport. See\nPDF Reference Table 8.70 for details.\n* **auto\\_regenerate** – set/unset the need\\_appearances flag ;\nthe flag is unchanged if auto\\_regenerate is None\n\nupdatePageFormFieldValues(*page: ~pypdf.\\_page.PageObject, fields: ~typing.Dict[str, ~typing.Any], flags: ~pypdf.constants.FieldFlag = FieldFlag.None*) → None[source]\nUse `update\\_page\\_form\\_field\\_values()` instead.\n\n\nclone\\_reader\\_document\\_root(*reader: PdfReader*) → None[source]\nCopy the reader document root to the writer and all sub elements,\nincluding pages, threads, outlines,… For partial insertion, `append`\nshould be considered.\n\nParameters\n**reader** – PdfReader from the document root should be copied.\n\ncloneReaderDocumentRoot(*reader: PdfReader*) → None[source]\nUse `clone\\_reader\\_document\\_root()` instead.\n\n\nclone\\_document\\_from\\_reader(*reader: PdfReader*, *after\\_page\\_append: Optional[Callable[[PageObject], None]] = None*) → None[source]\nCreate a copy (clone) of a document from a PDF file reader cloning\nsection ‘/Root’ and ‘/Info’ and ‘/ID’ of the pdf.\n\nParameters\n* **reader** – PDF file reader instance from which the clone\nshould be created.\n* **after\\_page\\_append** – Callback function that is invoked after each page is appended to\nthe writer. Signature includes a reference to the appended page\n(delegates to append\\_pages\\_from\\_reader). The single parameter of\nthe callback is a reference to the page just appended to the\ndocument.\n\ncloneDocumentFromReader(*reader: PdfReader*, *after\\_page\\_append: Optional[Callable[[PageObject], None]] = None*) → None[source]\nUse `clone\\_document\\_from\\_reader()` instead.\n\n\ngenerate\\_file\\_identifiers() → None[source]\nGenerate an identifier for the PDF that will be written.\n\n\nThe only point of this is ensuring uniqueness. Reproducibility is not\nrequired; see 14.4 “File Identifiers”.\n\nencrypt(*user\\_password: Optional[str] = None*, *owner\\_password: Optional[str] = None*, *use\\_128bit: bool = True*, *permissions\\_flag: UserAccessPermissions = UserAccessPermissions.PRINT | MODIFY | EXTRACT | ADD\\_OR\\_MODIFY | R7 | R8 | FILL\\_FORM\\_FIELDS | EXTRACT\\_TEXT\\_AND\\_GRAPHICS | ASSEMBLE\\_DOC | PRINT\\_TO\\_REPRESENTATION | R13 | R14 | R15 | R16 | R17 | R18 | R19 | R20 | R21 | R22 | R23 | R24 | R25 | R26 | R27 | R28 | R29 | R30 | R31*, *user\\_pwd: Optional[str] = None*, *owner\\_pwd: Optional[str] = None*, *\\**, *algorithm: Optional[str] = None*) → None[source]\nEncrypt this PDF file with the PDF Standard encryption handler.\n\nParameters\n* **user\\_password** – The password which allows for opening\nand reading the PDF file with the restrictions provided.\n* **owner\\_password** – The password which allows for\nopening the PDF files without any restrictions. By default,\nthe owner password is the same as the user password.\n* **use\\_128bit** – flag as to whether to use 128bit\nencryption. When false, 40bit encryption will be used.\nBy default, this flag is on.\n* **permissions\\_flag** – permissions as described in\nTABLE 3.20 of the PDF 1.7 specification. A bit value of 1 means\nthe permission is grantend.\nHence an integer value of -1 will set all flags.\nBit position 3 is for printing, 4 is for modifying content,\n5 and 6 control annotations, 9 for form fields,\n10 for extraction of text and graphics.\n* **algorithm** – encrypt algorithm. Values maybe one of “RC4-40”, “RC4-128”,\n“AES-128”, “AES-256-R5”, “AES-256”. If it’s valid,\nuse\\_128bit will be ignored.\n\nwrite\\_stream(*stream: IO*) → None[source]\n\nwrite(*stream: Union[Path, str, IO]*) → Tuple[bool, IO][source]\nWrite the collection of pages added to this object out as a PDF file.\n\nParameters\n**stream** – An object to write the file to. The object can support\nthe write method and the tell method, similar to a file object, or\nbe a file path, just like the fileobj, just named it stream to keep\nexisting workflow.\n\nReturns\nA tuple (bool, IO)\n\nadd\\_metadata(*infos: Dict[str, Any]*) → None[source]\nAdd custom metadata to the output.\n\nParameters\n**infos** – a Python dictionary where each key is a field\nand each value is your new metadata.\n\naddMetadata(*infos: Dict[str, Any]*) → None[source]\nUse `add\\_metadata()` instead.\n\n\nget\\_reference(*obj: PdfObject*) → IndirectObject[source]\n\ngetReference(*obj: PdfObject*) → IndirectObject[source]\nUse `get\\_reference()` instead.\n\n\nget\\_outline\\_root() → TreeObject[source]\n\nget\\_threads\\_root() → ArrayObject[source]\nThe list of threads.\n\nReturns\nAn array (possibly empty) of Dictionaries with `/F` and\n`/I` properties.\n\n*property* threads*: pypdf.generic.\\_data\\_structures.ArrayObject*\nRead-only property for the list of threads.\n\n\nEach element is a dictionaries with `/F` and `/I` keys.\n\ngetOutlineRoot() → TreeObject[source]\nUse `get\\_outline\\_root()` instead.\n\n\nget\\_named\\_dest\\_root() → ArrayObject[source]\n\ngetNamedDestRoot() → ArrayObject[source]\nUse `get\\_named\\_dest\\_root()` instead.\n\n\nadd\\_outline\\_item\\_destination(*page\\_destination: Union[None, PageObject, TreeObject] = None*, *parent: Union[None, TreeObject, IndirectObject] = None*, *before: Union[None, TreeObject, IndirectObject] = None*, *is\\_open: bool = True*, *dest: Union[None, PageObject, TreeObject] = None*) → IndirectObject[source]\n\nadd\\_bookmark\\_destination(*dest: Union[PageObject, TreeObject]*, *parent: Union[None, TreeObject, IndirectObject] = None*) → IndirectObject[source]\nUse `add\\_outline\\_item\\_destination()` instead.\n\n\naddBookmarkDestination(*dest: PageObject*, *parent: Optional[TreeObject] = None*) → IndirectObject[source]\nUse `add\\_outline\\_item\\_destination()` instead.\n\n\nadd\\_outline\\_item\\_dict(*outline\\_item: Union[OutlineItem, Destination]*, *parent: Union[None, TreeObject, IndirectObject] = None*, *before: Union[None, TreeObject, IndirectObject] = None*, *is\\_open: bool = True*) → IndirectObject[source]\n\nadd\\_bookmark\\_dict(*outline\\_item: Union[OutlineItem, Destination]*, *parent: Optional[TreeObject] = None*) → IndirectObject[source]\nUse `add\\_outline\\_item\\_dict()` instead.\n\n\naddBookmarkDict(*outline\\_item: Union[OutlineItem, Destination]*, *parent: Optional[TreeObject] = None*) → IndirectObject[source]\nUse `add\\_outline\\_item\\_dict()` instead.\n\n\nadd\\_outline\\_item(*title: str, page\\_number: ~typing.Union[None, ~pypdf.\\_page.PageObject, ~pypdf.generic.\\_base.IndirectObject, int], parent: ~typing.Union[None, ~pypdf.generic.\\_data\\_structures.TreeObject, ~pypdf.generic.\\_base.IndirectObject] = None, before: ~typing.Union[None, ~pypdf.generic.\\_data\\_structures.TreeObject, ~pypdf.generic.\\_base.IndirectObject] = None, color: ~typing.Optional[~typing.Union[~typing.Tuple[float, float, float], str]] = None, bold: bool = False, italic: bool = False, fit: ~pypdf.generic.\\_fit.Fit = <pypdf.generic.\\_fit.Fit object>, is\\_open: bool = True, pagenum: ~typing.Optional[int] = None*) → IndirectObject[source]\nAdd an outline item (commonly referred to as a “Bookmark”) to the PDF file.\n\nParameters\n* **title** – Title to use for this outline item.\n* **page\\_number** – Page number this outline item will point to.\n* **parent** – A reference to a parent outline item to create nested\noutline items.\n* **before** –\n* **color** – Color of the outline item’s font as a red, green, blue tuple\nfrom 0.0 to 1.0 or as a Hex String (#RRGGBB)\n* **bold** – Outline item font is bold\n* **italic** – Outline item font is italic\n* **fit** – The fit of the destination page.\n\nReturns\nThe added outline item as an indirect object.\n\nadd\\_bookmark(*title: str*, *pagenum: int*, *parent: Union[None, TreeObject, IndirectObject] = None*, *color: Optional[Tuple[float, float, float]] = None*, *bold: bool = False*, *italic: bool = False*, *fit: Literal['/Fit', '/XYZ', '/FitH', '/FitV', '/FitR', '/FitB', '/FitBH', '/FitBV'] = '/Fit'*, *\\*args: Union[NumberObject, NullObject, float]*) → IndirectObject[source]\nUse `add\\_outline\\_item()` instead.\n\n\naddBookmark(*title: str*, *pagenum: int*, *parent: Union[None, TreeObject, IndirectObject] = None*, *color: Optional[Tuple[float, float, float]] = None*, *bold: bool = False*, *italic: bool = False*, *fit: Literal['/Fit', '/XYZ', '/FitH', '/FitV', '/FitR', '/FitB', '/FitBH', '/FitBV'] = '/Fit'*, *\\*args: Union[NumberObject, NullObject, float]*) → IndirectObject[source]\nUse `add\\_outline\\_item()` instead.\n\n\nadd\\_outline() → None[source]\n\nadd\\_named\\_destination\\_array(*title: TextStringObject*, *destination: Union[IndirectObject, ArrayObject]*) → None[source]\n\nadd\\_named\\_destination\\_object(*page\\_destination: Optional[PdfObject] = None*, *dest: Optional[PdfObject] = None*) → IndirectObject[source]\n\naddNamedDestinationObject(*dest: Destination*) → IndirectObject[source]\nUse `add\\_named\\_destination\\_object()` instead.\n\n\nadd\\_named\\_destination(*title: str*, *page\\_number: Optional[int] = None*, *pagenum: Optional[int] = None*) → IndirectObject[source]\n\naddNamedDestination(*title: str*, *pagenum: int*) → IndirectObject[source]\nUse `add\\_named\\_destination()` instead.\n\n\nremove\\_links() → None[source]\nRemove links and annotations from this output.\n\nremoveLinks() → None[source]\nUse `remove\\_links()` instead.\n\n\nremove\\_annotations(*subtypes: Optional[Union[Literal['/Text', '/Link', '/FreeText', '/Line', '/Square', '/Circle', '/Polygon', '/PolyLine', '/Highlight', '/Unterline', '/Squiggly', '/StrikeOut', '/Stamp', '/Caret', '/Ink', '/Popup', '/FileAttachment', '/Sound', '/Movie', '/Widget', '/Screen', '/PrinterMark', '/TrapNet', '/Watermark', '/3D', '/Redact'], Iterable[Literal['/Text', '/Link', '/FreeText', '/Line', '/Square', '/Circle', '/Polygon', '/PolyLine', '/Highlight', '/Unterline', '/Squiggly', '/StrikeOut', '/Stamp', '/Caret', '/Ink', '/Popup', '/FileAttachment', '/Sound', '/Movie', '/Widget', '/Screen', '/PrinterMark', '/TrapNet', '/Watermark', '/3D', '/Redact']]]]*) → None[source]\nRemove annotations by annotation subtype.\n\nParameters\n**subtypes** – SubType or list of SubTypes to be removed.\nExamples are: “/Link”, “/FileAttachment”, “/Sound”,\n“/Movie”, “/Screen”, …\nIf you want to remove all annotations, use subtypes=None.\n\nremove\\_objects\\_from\\_page(*page: Union[PageObject, DictionaryObject]*, *to\\_delete: Union[ObjectDeletionFlag, Iterable[ObjectDeletionFlag]]*) → None[source]\nRemove objects specified by `to\\_delete` from the given page.\n\nParameters\n* **page** – Page object to clean up.\n* **to\\_delete** – Objects to be deleted; can be a `ObjectDeletionFlag`\nor a list of ObjectDeletionFlag\n\nremove\\_images(*ignore\\_byte\\_string\\_object: Optional[bool] = None*) → None[source]\nRemove images from this output.\n\nParameters\n**ignore\\_byte\\_string\\_object** – deprecated\n\nremoveImages(*ignoreByteStringObject: bool = False*) → None[source]\nUse `remove\\_images()` instead.\n\n\nremove\\_text(*ignore\\_byte\\_string\\_object: Optional[bool] = None*) → None[source]\nRemove text from this output.\n\nremoveText(*ignoreByteStringObject: bool = False*) → None[source]\nUse `remove\\_text()` instead.\n\n\nadd\\_uri(*page\\_number: int*, *uri: str*, *rect: RectangleObject*, *border: Optional[ArrayObject] = None*, *pagenum: Optional[int] = None*) → None[source]\nAdd an URI from a rectangular area to the specified page.\n\n\nThis uses the basic structure of `add\\_link()`\n\nParameters\n* **page\\_number** – index of the page on which to place the URI action.\n* **uri** – URI of resource to link to.\n* **rect** – `RectangleObject` or\narray of four integers specifying the clickable rectangular area\n`[xLL, yLL, xUR, yUR]`, or string in the form\n`\"[ xLL yLL xUR yUR ]\"`.\n* **border** – if provided, an array describing border-drawing\nproperties. See the PDF spec for details. No border will be\ndrawn if this argument is omitted.\n\naddURI(*pagenum: int*, *uri: str*, *rect: RectangleObject*, *border: Optional[ArrayObject] = None*) → None[source]\nUse `add\\_uri()` instead.\n\n\nadd\\_link(*pagenum: int*, *page\\_destination: int*, *rect: RectangleObject*, *border: Optional[ArrayObject] = None*, *fit: Literal['/Fit', '/XYZ', '/FitH', '/FitV', '/FitR', '/FitB', '/FitBH', '/FitBV'] = '/Fit'*, *\\*args: Union[NumberObject, NullObject, float]*) → DictionaryObject[source]\n\naddLink(*pagenum: int*, *page\\_destination: int*, *rect: RectangleObject*, *border: Optional[ArrayObject] = None*, *fit: Literal['/Fit', '/XYZ', '/FitH', '/FitV', '/FitR', '/FitB', '/FitBH', '/FitBV'] = '/Fit'*, *\\*args: Union[NumberObject, NullObject, float]*) → None[source]\nUse `add\\_link()` instead.\n\n\ngetPageLayout() → Optional[Literal['/NoLayout', '/SinglePage', '/OneColumn', '/TwoColumnLeft', '/TwoColumnRight', '/TwoPageLeft', '/TwoPageRight']][source]\nUse `page\\_layout` instead.\n\n\nset\\_page\\_layout(*layout: Literal['/NoLayout', '/SinglePage', '/OneColumn', '/TwoColumnLeft', '/TwoColumnRight', '/TwoPageLeft', '/TwoPageRight']*) → None[source]\nSet the page layout.\n\nParameters\n**layout** – The page layout to be used\n\n\nValid `layout` arguments\n\nsetPageLayout(*layout: Literal['/NoLayout', '/SinglePage', '/OneColumn', '/TwoColumnLeft', '/TwoColumnRight', '/TwoPageLeft', '/TwoPageRight']*) → None[source]\nUse `page\\_layout` instead.\n\n\n*property* page\\_layout*: Optional[Literal['/NoLayout', '/SinglePage', '/OneColumn', '/TwoColumnLeft', '/TwoColumnRight', '/TwoPageLeft', '/TwoPageRight']]*\nPage layout property.\n\n*property* pageLayout*: Optional[Literal['/NoLayout', '/SinglePage', '/OneColumn', '/TwoColumnLeft', '/TwoColumnRight', '/TwoPageLeft', '/TwoPageRight']]*\nUse `page\\_layout` instead.\n\n\ngetPageMode() → Optional[Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']][source]\nUse `page\\_mode` instead.\n\n\nset\\_page\\_mode(*mode: Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']*) → None[source]\nUse `page\\_mode` instead.\n\n\nsetPageMode(*mode: Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']*) → None[source]\nUse `page\\_mode` instead.\n\n\n*property* page\\_mode*: Optional[Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']]*\nPage mode property.\n\n*property* pageMode*: Optional[Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']]*\nUse `page\\_mode` instead.\n\n\nadd\\_annotation(*page\\_number: Union[int, PageObject]*, *annotation: Dict[str, Any]*) → DictionaryObject[source]\nAdd a single annotation to the page.\nThe added annotation must be a new annotation.\nIt can not be recycled.\n\nParameters\n* **page\\_number** – PageObject or page index.\n* **annotation** – Annotation to be added (created with annotation).\n\nReturns\nThe inserted object\nThis can be used for pop-up creation, for example\n\nclean\\_page(*page: Union[PageObject, IndirectObject]*) → PageObject[source]\nPerform some clean up in the page.\nCurrently: convert NameObject nameddestination to TextStringObject\n(required for names/dests list)\n\nParameters\n**page** – \n\nReturns\nThe cleaned PageObject\n\nappend(*fileobj: Union[str, IO, PdfReader, Path]*, *outline\\_item: Union[str, None, PageRange, Tuple[int, int], Tuple[int, int, int], List[int]] = None*, *pages: Union[None, PageRange, Tuple[int, int], Tuple[int, int, int], List[int], List[PageObject]] = None*, *import\\_outline: bool = True*, *excluded\\_fields: Optional[Union[List[str], Tuple[str, ...]]] = None*) → None[source]\nIdentical to the `merge()` method, but assumes you want to\nconcatenate all pages onto the end of the file instead of specifying a\nposition.\n\nParameters\n* **fileobj** – A File Object or an object that supports the standard\nread and seek methods similar to a File Object. Could also be a\nstring representing a path to a PDF file.\n* **outline\\_item** – Optionally, you may specify a string to build an\noutline (aka ‘bookmark’) to identify the beginning of the\nincluded file.\n* **pages** – Can be a `PageRange`\nor a `(start, stop[, step])` tuple\nor a list of pages to be processed\nto merge only the specified range of pages from the source\ndocument into the output document.\n* **import\\_outline** – You may prevent the source document’s\noutline (collection of outline items, previously referred to as\n‘bookmarks’) from being imported by specifying this as `False`.\n* **excluded\\_fields** – Provide the list of fields/keys to be ignored\nif `/Annots` is part of the list, the annotation will be ignored\nif `/B` is part of the list, the articles will be ignored\n\nmerge(*position: Optional[int]*, *fileobj: Union[Path, str, IO, PdfReader]*, *outline\\_item: Optional[str] = None*, *pages: Optional[Union[str, PageRange, Tuple[int, int], Tuple[int, int, int], List[int], List[PageObject]]] = None*, *import\\_outline: bool = True*, *excluded\\_fields: Optional[Union[List[str], Tuple[str, ...]]] = ()*) → None[source]\nMerge the pages from the given file into the output file at the\nspecified page number.\n\nParameters\n* **position** – The *page number* to insert this file. File will\nbe inserted after the given number.\n* **fileobj** – A File Object or an object that supports the standard\nread and seek methods similar to a File Object. Could also be a\nstring representing a path to a PDF file.\n* **outline\\_item** – Optionally, you may specify a string to build an outline\n(aka ‘bookmark’) to identify the\nbeginning of the included file.\n* **pages** – can be a `PageRange`\nor a `(start, stop[, step])` tuple\nor a list of pages to be processed\nto merge only the specified range of pages from the source\ndocument into the output document.\n* **import\\_outline** – You may prevent the source document’s\noutline (collection of outline items, previously referred to as\n‘bookmarks’) from being imported by specifying this as `False`.\n* **excluded\\_fields** – provide the list of fields/keys to be ignored\nif `/Annots` is part of the list, the annotation will be ignored\nif `/B` is part of the list, the articles will be ignored\n\nRaises\n**TypeError** – The pages attribute is not configured properly\n\nadd\\_filtered\\_articles(*fltr: Union[Pattern, str]*, *pages: Dict[int, PageObject]*, *reader: PdfReader*) → None[source]\nAdd articles matching the defined criteria.\n\nParameters\n* **fltr** –\n* **pages** –\n* **reader** –\n\nclose() → None[source]\nTo match the functions from Merger.\n\nfind\\_outline\\_item(*outline\\_item: Dict[str, Any]*, *root: Optional[List[Union[Destination, List[Union[Destination, List[Destination]]]]]] = None*) → Optional[List[int]][source]\n\nfind\\_bookmark(*outline\\_item: Dict[str, Any]*, *root: Optional[List[Union[Destination, List[Union[Destination, List[Destination]]]]]] = None*) → Optional[List[int]][source]\n\nDeprecated since version 2.9.0: Use `find\\_outline\\_item()` instead.\n\n\nreset\\_translation(*reader: Union[None, PdfReader, IndirectObject] = None*) → None[source]\nReset the translation table between reader and the writer object.\n\n\nLate cloning will create new independent objects.\n\nParameters\n**reader** – PdfReader or IndirectObject refering a PdfReader object.\nif set to None or omitted, all tables will be reset.\n\nset\\_page\\_label(*page\\_index\\_from: int*, *page\\_index\\_to: int*, *style: Optional[PageLabelStyle] = None*, *prefix: Optional[str] = None*, *start: Optional[int] = 0*) → None[source]\nSet a page label to a range of pages.\n\n\nPage indexes must be given starting from 0.\nLabels must have a style, a prefix or both.\nIf to a range is not assigned any page label a decimal label starting from 1 is applied.\n\nParameters\n* **page\\_index\\_from** – page index of the beginning of the range starting from 0\n* **page\\_index\\_to** – page index of the beginning of the range starting from 0\n* **style** – The numbering style to be used for the numeric portion of each page label:\n‘/D’ Decimal arabic numerals\n‘/R’ Uppercase roman numerals\n‘/r’ Lowercase roman numerals\n‘/A’ Uppercase letters (A to Z for the first 26 pages,\n\n> \n> AA to ZZ for the next 26, and so on)\n> \n> \n> \n\n’/a’ Lowercase letters (a to z for the first 26 pages,aa to zz for the next 26, and so on)\n* **prefix** – The label prefix for page labels in this range.\n* **start** – The value of the numeric portion for the first page label\nin the range.\nSubsequent pages are numbered sequentially from this value,\nwhich must be greater than or equal to 1.\nDefault value: 1.\n# The PdfMerger Class\n\n\n*class* pypdf.PdfMerger(*strict: bool = False*, *fileobj: Union[Path, str, IO] = ''*)[source]\nBases: `object`\n\n\nUse `PdfWriter` instead.\n\nDeprecated since version 5.0.0.\n\nmerge(*page\\_number: Optional[int] = None*, *fileobj: Union[None, Path, str, IO, PdfReader] = None*, *outline\\_item: Optional[str] = None*, *pages: Optional[Union[str, PageRange, Tuple[int, int],\n\n==================\n Document 2 \n----------------\n The PdfMerger Class\n\n\n*class* pypdf.PdfMerger(*strict: bool = False*, *fileobj: Union[Path, str, IO] = ''*)[source]\nBases: `object`\n\n\nUse `PdfWriter` instead.\n\nDeprecated since version 5.0.0.\n\nmerge(*page\\_number: Optional[int] = None*, *fileobj: Union[None, Path, str, IO, PdfReader] = None*, *outline\\_item: Optional[str] = None*, *pages: Optional[Union[str, PageRange, Tuple[int, int], Tuple[int, int, int], List[int]]] = None*, *import\\_outline: bool = True*, *position: Optional[int] = None*) → None[source]\nMerge the pages from the given file into the output file at the\nspecified page number.\n\nParameters\n* **page\\_number** – The *page number* to insert this file. File will\nbe inserted after the given number.\n* **fileobj** – A File Object or an object that supports the standard\nread and seek methods similar to a File Object. Could also be a\nstring representing a path to a PDF file.\nNone as an argument is deprecated.\n* **outline\\_item** – Optionally, you may specify an outline item\n(previously referred to as a ‘bookmark’) to be applied at the\nbeginning of the included file by supplying the text of the outline item.\n* **pages** – \ncan be a `PageRange`or a `(start, stop[, step])` tuple\nto merge only the specified range of pages from the source\ndocument into the output document.\nCan also be a list of pages to merge.\n\nimport\\_outline: You may prevent the source document’soutline (collection of outline items, previously referred to as\n‘bookmarks’) from being imported by specifying this as `False`.\n\nappend(*fileobj: Union[str, IO, PdfReader, Path]*, *outline\\_item: Optional[str] = None*, *pages: Union[None, PageRange, Tuple[int, int], Tuple[int, int, int], List[int]] = None*, *import\\_outline: bool = True*) → None[source]\nIdentical to the `merge()` method, but assumes you want to\nconcatenate all pages onto the end of the file instead of specifying a\nposition.\n\nParameters\n* **fileobj** – A File Object or an object that supports the standard\nread and seek methods similar to a File Object. Could also be a\nstring representing a path to a PDF file.\n* **outline\\_item** – Optionally, you may specify an outline item\n(previously referred to as a ‘bookmark’) to be applied at the\nbeginning of the included file by supplying the text of the outline item.\n* **pages** – can be a `PageRange`\nor a `(start, stop[, step])` tuple\nto merge only the specified range of pages from the source\ndocument into the output document.\nCan also be a list of pages to append.\n* **import\\_outline** – You may prevent the source document’s\noutline (collection of outline items, previously referred to as\n‘bookmarks’) from being imported by specifying this as `False`.\n\nwrite(*fileobj: Union[Path, str, IO]*) → None[source]\nWrite all data that has been merged to the given output file.\n\nParameters\n**fileobj** – Output file. Can be a filename or any kind of\nfile-like object.\n\nclose() → None[source]\nShut all file descriptors (input and output) and clear all memory usage.\n\nParameters\n**infos** – a Python dictionary where each key is a field\nand each value is your new metadata.\nAn example is `{'/Title': 'My title'}`\n\n\nsetPageLayout(*layout: Literal['/NoLayout', '/SinglePage', '/OneColumn', '/TwoColumnLeft', '/TwoColumnRight', '/TwoPageLeft', '/TwoPageRight']*) → None[source]\nUse `set\\_page\\_layout()` instead.\n\nsetPageMode(*mode: Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']*) → None[source]\nUse `set\\_page\\_mode()` instead.\n\n\nset\\_page\\_mode(*mode: Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']*) → None[source]\nSet the page mode.\n\nParameters\n**mode** – The page mode to use.\n\n\nValid `mode` arguments\n\n\nadd\\_outline\\_item(*title: str*, *page\\_number: ~typing.Optional[int] = None*, *parent: ~typing.Union[None*, *~pypdf.generic.\\_data\\_structures.TreeObject*, *~pypdf.generic.\\_base.IndirectObject] = None*, *color: ~typing.Optional[~typing.Tuple[float*, *float*, *float]] = None*, *bold: bool = False*, *italic: bool = False*, *fit: ~pypdf.generic.\\_fit.Fit = <pypdf.generic.\\_fit.Fit object>*, *pagenum: ~typing.Optional[int] = None*) → IndirectObject[source]\nAdd an outline item (commonly referred to as a “Bookmark”) to this PDF file.\n\nParameters\n* **title** – Title to use for this outline item.\n* **page\\_number** – Page number this outline item will point to.\n* **parent** – A reference to a parent outline item to create nested\noutline items.\n* **color** – Color of the outline item’s font as a red, green, blue tuple\nfrom 0.0 to 1.0\n* **bold** – Outline item font is bold\n* **italic** – Outline item font is italic\n* **fit** – The fit of the destination page.\n\naddBookmark(*title: str*, *pagenum: int*, *parent: Union[None, TreeObject, IndirectObject] = None*, *color: Optional[Tuple[float, float, float]] = None*, *bold: bool = False*, *italic: bool = False*, *fit: Literal['/Fit', '/XYZ', '/FitH', '/FitV', '/FitR', '/FitB', '/FitBH', '/FitBV'] = '/Fit'*, *\\*args: Union[NumberObject, NullObject, float]*) → IndirectObject[source]\n\nDeprecated since version 1.28.0: Use `add\\_outline\\_item()` instead.\n\n\nadd\\_bookmark(*title: str*, *pagenum: int*, *parent: Union[None, TreeObject, IndirectObject] = None*, *color: Optional[Tuple[float, float, float]] = None*, *bold: bool = False*, *italic: bool = False*, *fit: Literal['/Fit', '/XYZ', '/FitH', '/FitV', '/FitR', '/FitB', '/FitBH', '/FitBV'] = '/Fit'*, *\\*args: Union[NumberObject, NullObject, float]*) → IndirectObject[source]\n\nDeprecated since version 2.9.0: Use `add\\_outline\\_item()` instead.\n\n\naddNamedDestination(*title: str*, *pagenum: int*) → None[source]\n\nDeprecated since version 1.28.0: Use `add\\_named\\_destination()` instead.\n\n\nadd\\_named\\_destination(*title: str*, *page\\_number: Optional[int] = None*, *pagenum: Optional[int] = None*) → None[source]\nAdd a destination to the output.\n\nParameters\n* **title** – Title to use\n* **page\\_number** – Page number this destination points at.\n# The PageObject Class\n\n\n*class* pypdf.\\_page.PageObject(*pdf: Union[None, PdfReaderProtocol, PdfWriterProtocol] = None*, *indirect\\_reference: Optional[IndirectObject] = None*, *indirect\\_ref: Optional[IndirectObject] = None*)[source]\nBases: `DictionaryObject`\n\n\nPageObject represents a single page within a PDF file.\n\n\nTypically this object will be created by accessing the\n`get\\_page()` method of the\n`PdfReader` class, but"
    },
    {
        "instruction": "Create a Python program using the 'pypdf' API to extract and display the content of a specific page (e.g., page 3) from a PDF file.",
        "api": "pypdf",
        "output": "#!pip install pypdf\nimport pypdf\n\n# Specify the path of the PDF file\npdf_file_path = \"document.pdf\"\n\n# Specify the page number to extract (e.g., page 3)\npage_number_to_extract = 3\n\n# Open the PDF file in read-binary mode\nwith open(pdf_file_path, \"rb\") as pdf_file:\n    # Create a PDF reader object\n    pdf_reader = pypdf.PdfReader(pdf_file)\n\n    # Check if the specified page number is within the valid range\n    if 1 <= page_number_to_extract <= len(pdf_reader.pages):\n        # Extract and display the content of the specified page\n        page = pdf_reader.pages[page_number_to_extract - 1]\n        page_text = page.extract_text()\n        print(page_text)\n    else:\n        print(\"Page number is out of range.\")\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n Example: This will launch the print window when the PDF is opened.\n\n```\n\naddJS(*javascript: str*) → None[source]\nUse `add\\_js()` instead.\n\n\nadd\\_attachment(*filename: str*, *data: Union[str, bytes]*) → None[source]\nEmbed a file inside the PDF.\n\n\nReference:\nhttps://www.adobe.com/content/dam/Adobe/en/devnet/acrobat/pdfs/PDF32000\\_2008.pdf\nSection 7.11.3\n\nParameters\n* **filename** – The filename to display.\n* **data** – The data in the file.\n\naddAttachment(*fname: str*, *fdata: Union[str, bytes]*) → None[source]\nUse `add\\_attachment()` instead.\n\n\nappend\\_pages\\_from\\_reader(*reader: PdfReader*, *after\\_page\\_append: Optional[Callable[[PageObject], None]] = None*) → None[source]\nCopy pages from reader to writer. Includes an optional callback\nparameter which is invoked after pages are appended to the writer.\n\n\n`append` should be prefered.\n\nParameters\n* **reader** – a PdfReader object from which to copy page\nannotations to this writer object. The writer’s annots\nwill then be updated\n* **after\\_page\\_append** – Callback function that is invoked after each page is appended to\nthe writer. Signature includes a reference to the appended page\n(delegates to append\\_pages\\_from\\_reader). The single parameter of\nthe callback is a reference to the page just appended to the\ndocument.\n\nappendPagesFromReader(*reader: PdfReader*, *after\\_page\\_append: Optional[Callable[[PageObject], None]] = None*) → None[source]\nUse `append\\_pages\\_from\\_reader()` instead.\n\n\nupdate\\_page\\_form\\_field\\_values(*page: ~pypdf.\\_page.PageObject, fields: ~typing.Dict[str, ~typing.Any], flags: ~pypdf.constants.FieldFlag = FieldFlag.None, auto\\_regenerate: ~typing.Optional[bool] = True*) → None[source]\nUpdate the form field values for a given page from a fields dictionary.\n\n\nCopy field texts and values from fields to page.\nIf the field links to a parent object, add the information to the parent.\n\nParameters\n* **page** – Page reference from PDF writer where the\nannotations and field data will be updated.\n* **fields** – a Python dictionary of field names (/T) and text\nvalues (/V)\n* **flags** – An integer (0 to 7). The first bit sets ReadOnly, the\nsecond bit sets Required, the third bit sets NoExport. See\nPDF Reference Table 8.70 for details.\n* **auto\\_regenerate** – set/unset the need\\_appearances flag ;\nthe flag is unchanged if auto\\_regenerate is None\n\nupdatePageFormFieldValues(*page: ~pypdf.\\_page.PageObject, fields: ~typing.Dict[str, ~typing.Any], flags: ~pypdf.constants.FieldFlag = FieldFlag.None*) → None[source]\nUse `update\\_page\\_form\\_field\\_values()` instead.\n\n\nclone\\_reader\\_document\\_root(*reader: PdfReader*) → None[source]\nCopy the reader document root to the writer and all sub elements,\nincluding pages, threads, outlines,… For partial insertion, `append`\nshould be considered.\n\nParameters\n**reader** – PdfReader from the document root should be copied.\n\ncloneReaderDocumentRoot(*reader: PdfReader*) → None[source]\nUse `clone\\_reader\\_document\\_root()` instead.\n\n\nclone\\_document\\_from\\_reader(*reader: PdfReader*, *after\\_page\\_append: Optional[Callable[[PageObject], None]] = None*) → None[source]\nCreate a copy (clone) of a document from a PDF file reader cloning\nsection ‘/Root’ and ‘/Info’ and ‘/ID’ of the pdf.\n\nParameters\n* **reader** – PDF file reader instance from which the clone\nshould be created.\n* **after\\_page\\_append** – Callback function that is invoked after each page is appended to\nthe writer. Signature includes a reference to the appended page\n(delegates to append\\_pages\\_from\\_reader). The single parameter of\nthe callback is a reference to the page just appended to the\ndocument.\n\ncloneDocumentFromReader(*reader: PdfReader*, *after\\_page\\_append: Optional[Callable[[PageObject], None]] = None*) → None[source]\nUse `clone\\_document\\_from\\_reader()` instead.\n\n\ngenerate\\_file\\_identifiers() → None[source]\nGenerate an identifier for the PDF that will be written.\n\n\nThe only point of this is ensuring uniqueness. Reproducibility is not\nrequired; see 14.4 “File Identifiers”.\n\nencrypt(*user\\_password: Optional[str] = None*, *owner\\_password: Optional[str] = None*, *use\\_128bit: bool = True*, *permissions\\_flag: UserAccessPermissions = UserAccessPermissions.PRINT | MODIFY | EXTRACT | ADD\\_OR\\_MODIFY | R7 | R8 | FILL\\_FORM\\_FIELDS | EXTRACT\\_TEXT\\_AND\\_GRAPHICS | ASSEMBLE\\_DOC | PRINT\\_TO\\_REPRESENTATION | R13 | R14 | R15 | R16 | R17 | R18 | R19 | R20 | R21 | R22 | R23 | R24 | R25 | R26 | R27 | R28 | R29 | R30 | R31*, *user\\_pwd: Optional[str] = None*, *owner\\_pwd: Optional[str] = None*, *\\**, *algorithm: Optional[str] = None*) → None[source]\nEncrypt this PDF file with the PDF Standard encryption handler.\n\nParameters\n* **user\\_password** – The password which allows for opening\nand reading the PDF file with the restrictions provided.\n* **owner\\_password** – The password which allows for\nopening the PDF files without any restrictions. By default,\nthe owner password is the same as the user password.\n* **use\\_128bit** – flag as to whether to use 128bit\nencryption. When false, 40bit encryption will be used.\nBy default, this flag is on.\n* **permissions\\_flag** – permissions as described in\nTABLE 3.20 of the PDF 1.7 specification. A bit value of 1 means\nthe permission is grantend.\nHence an integer value of -1 will set all flags.\nBit position 3 is for printing, 4 is for modifying content,\n5 and 6 control annotations, 9 for form fields,\n10 for extraction of text and graphics.\n* **algorithm** – encrypt algorithm. Values maybe one of “RC4-40”, “RC4-128”,\n“AES-128”, “AES-256-R5”, “AES-256”. If it’s valid,\nuse\\_128bit will be ignored.\n\nwrite\\_stream(*stream: IO*) → None[source]\n\nwrite(*stream: Union[Path, str, IO]*) → Tuple[bool, IO][source]\nWrite the collection of pages added to this object out as a PDF file.\n\nParameters\n**stream** – An object to write the file to. The object can support\nthe write method and the tell method, similar to a file object, or\nbe a file path, just like the fileobj, just named it stream to keep\nexisting workflow.\n\nReturns\nA tuple (bool, IO)\n\nadd\\_metadata(*infos: Dict[str, Any]*) → None[source]\nAdd custom metadata to the output.\n\nParameters\n**infos** – a Python dictionary where each key is a field\nand each value is your new metadata.\n\naddMetadata(*infos: Dict[str, Any]*) → None[source]\nUse `add\\_metadata()` instead.\n\n\nget\\_reference(*obj: PdfObject*) → IndirectObject[source]\n\ngetReference(*obj: PdfObject*) → IndirectObject[source]\nUse `get\\_reference()` instead.\n\n\nget\\_outline\\_root() → TreeObject[source]\n\nget\\_threads\\_root() → ArrayObject[source]\nThe list of threads.\n\nReturns\nAn array (possibly empty) of Dictionaries with `/F` and\n`/I` properties.\n\n*property* threads*: pypdf.generic.\\_data\\_structures.ArrayObject*\nRead-only property for the list of threads.\n\n\nEach element is a dictionaries with `/F` and `/I` keys.\n\ngetOutlineRoot() → TreeObject[source]\nUse `get\\_outline\\_root()` instead.\n\n\nget\\_named\\_dest\\_root() → ArrayObject[source]\n\ngetNamedDestRoot() → ArrayObject[source]\nUse `get\\_named\\_dest\\_root()` instead.\n\n\nadd\\_outline\\_item\\_destination(*page\\_destination: Union[None, PageObject, TreeObject] = None*, *parent: Union[None, TreeObject, IndirectObject] = None*, *before: Union[None, TreeObject, IndirectObject] = None*, *is\\_open: bool = True*, *dest: Union[None, PageObject, TreeObject] = None*) → IndirectObject[source]\n\nadd\\_bookmark\\_destination(*dest: Union[PageObject, TreeObject]*, *parent: Union[None, TreeObject, IndirectObject] = None*) → IndirectObject[source]\nUse `add\\_outline\\_item\\_destination()` instead.\n\n\naddBookmarkDestination(*dest: PageObject*, *parent: Optional[TreeObject] = None*) → IndirectObject[source]\nUse `add\\_outline\\_item\\_destination()` instead.\n\n\nadd\\_outline\\_item\\_dict(*outline\\_item: Union[OutlineItem, Destination]*, *parent: Union[None, TreeObject, IndirectObject] = None*, *before: Union[None, TreeObject, IndirectObject] = None*, *is\\_open: bool = True*) → IndirectObject[source]\n\nadd\\_bookmark\\_dict(*outline\\_item: Union[OutlineItem, Destination]*, *parent: Optional[TreeObject] = None*) → IndirectObject[source]\nUse `add\\_outline\\_item\\_dict()` instead.\n\n\naddBookmarkDict(*outline\\_item: Union[OutlineItem, Destination]*, *parent: Optional[TreeObject] = None*) → IndirectObject[source]\nUse `add\\_outline\\_item\\_dict()` instead.\n\n\nadd\\_outline\\_item(*title: str, page\\_number: ~typing.Union[None, ~pypdf.\\_page.PageObject, ~pypdf.generic.\\_base.IndirectObject, int], parent: ~typing.Union[None, ~pypdf.generic.\\_data\\_structures.TreeObject, ~pypdf.generic.\\_base.IndirectObject] = None, before: ~typing.Union[None, ~pypdf.generic.\\_data\\_structures.TreeObject, ~pypdf.generic.\\_base.IndirectObject] = None, color: ~typing.Optional[~typing.Union[~typing.Tuple[float, float, float], str]] = None, bold: bool = False, italic: bool = False, fit: ~pypdf.generic.\\_fit.Fit = <pypdf.generic.\\_fit.Fit object>, is\\_open: bool = True, pagenum: ~typing.Optional[int] = None*) → IndirectObject[source]\nAdd an outline item (commonly referred to as a “Bookmark”) to the PDF file.\n\nParameters\n* **title** – Title to use for this outline item.\n* **page\\_number** – Page number this outline item will point to.\n* **parent** – A reference to a parent outline item to create nested\noutline items.\n* **before** –\n* **color** – Color of the outline item’s font as a red, green, blue tuple\nfrom 0.0 to 1.0 or as a Hex String (#RRGGBB)\n* **bold** – Outline item font is bold\n* **italic** – Outline item font is italic\n* **fit** – The fit of the destination page.\n\nReturns\nThe added outline item as an indirect object.\n\nadd\\_bookmark(*title: str*, *pagenum: int*, *parent: Union[None, TreeObject, IndirectObject] = None*, *color: Optional[Tuple[float, float, float]] = None*, *bold: bool = False*, *italic: bool = False*, *fit: Literal['/Fit', '/XYZ', '/FitH', '/FitV', '/FitR', '/FitB', '/FitBH', '/FitBV'] = '/Fit'*, *\\*args: Union[NumberObject, NullObject, float]*) → IndirectObject[source]\nUse `add\\_outline\\_item()` instead.\n\n\naddBookmark(*title: str*, *pagenum: int*, *parent: Union[None, TreeObject, IndirectObject] = None*, *color: Optional[Tuple[float, float, float]] = None*, *bold: bool = False*, *italic: bool = False*, *fit: Literal['/Fit', '/XYZ', '/FitH', '/FitV', '/FitR', '/FitB', '/FitBH', '/FitBV'] = '/Fit'*, *\\*args: Union[NumberObject, NullObject, float]*) → IndirectObject[source]\nUse `add\\_outline\\_item()` instead.\n\n\nadd\\_outline() → None[source]\n\nadd\\_named\\_destination\\_array(*title: TextStringObject*, *destination: Union[IndirectObject, ArrayObject]*) → None[source]\n\nadd\\_named\\_destination\\_object(*page\\_destination: Optional[PdfObject] = None*, *dest: Optional[PdfObject] = None*) → IndirectObject[source]\n\naddNamedDestinationObject(*dest: Destination*) → IndirectObject[source]\nUse `add\\_named\\_destination\\_object()` instead.\n\n\nadd\\_named\\_destination(*title: str*, *page\\_number: Optional[int] = None*, *pagenum: Optional[int] = None*) → IndirectObject[source]\n\naddNamedDestination(*title: str*, *pagenum: int*) → IndirectObject[source]\nUse `add\\_named\\_destination()` instead.\n\n\nremove\\_links() → None[source]\nRemove links and annotations from this output.\n\nremoveLinks() → None[source]\nUse `remove\\_links()` instead.\n\n\nremove\\_annotations(*subtypes: Optional[Union[Literal['/Text', '/Link', '/FreeText', '/Line', '/Square', '/Circle', '/Polygon', '/PolyLine', '/Highlight', '/Unterline', '/Squiggly', '/StrikeOut', '/Stamp', '/Caret', '/Ink', '/Popup', '/FileAttachment', '/Sound', '/Movie', '/Widget', '/Screen', '/PrinterMark', '/TrapNet', '/Watermark', '/3D', '/Redact'], Iterable[Literal['/Text', '/Link', '/FreeText', '/Line', '/Square', '/Circle', '/Polygon', '/PolyLine', '/Highlight', '/Unterline', '/Squiggly', '/StrikeOut', '/Stamp', '/Caret', '/Ink', '/Popup', '/FileAttachment', '/Sound', '/Movie', '/Widget', '/Screen', '/PrinterMark', '/TrapNet', '/Watermark', '/3D', '/Redact']]]]*) → None[source]\nRemove annotations by annotation subtype.\n\nParameters\n**subtypes** – SubType or list of SubTypes to be removed.\nExamples are: “/Link”, “/FileAttachment”, “/Sound”,\n“/Movie”, “/Screen”, …\nIf you want to remove all annotations, use subtypes=None.\n\nremove\\_objects\\_from\\_page(*page: Union[PageObject, DictionaryObject]*, *to\\_delete: Union[ObjectDeletionFlag, Iterable[ObjectDeletionFlag]]*) → None[source]\nRemove objects specified by `to\\_delete` from the given page.\n\nParameters\n* **page** – Page object to clean up.\n* **to\\_delete** – Objects to be deleted; can be a `ObjectDeletionFlag`\nor a list of ObjectDeletionFlag\n\nremove\\_images(*ignore\\_byte\\_string\\_object: Optional[bool] = None*) → None[source]\nRemove images from this output.\n\nParameters\n**ignore\\_byte\\_string\\_object** – deprecated\n\nremoveImages(*ignoreByteStringObject: bool = False*) → None[source]\nUse `remove\\_images()` instead.\n\n\nremove\\_text(*ignore\\_byte\\_string\\_object: Optional[bool] = None*) → None[source]\nRemove text from this output.\n\nremoveText(*ignoreByteStringObject: bool = False*) → None[source]\nUse `remove\\_text()` instead.\n\n\nadd\\_uri(*page\\_number: int*, *uri: str*, *rect: RectangleObject*, *border: Optional[ArrayObject] = None*, *pagenum: Optional[int] = None*) → None[source]\nAdd an URI from a rectangular area to the specified page.\n\n\nThis uses the basic structure of `add\\_link()`\n\nParameters\n* **page\\_number** – index of the page on which to place the URI action.\n* **uri** – URI of resource to link to.\n* **rect** – `RectangleObject` or\narray of four integers specifying the clickable rectangular area\n`[xLL, yLL, xUR, yUR]`, or string in the form\n`\"[ xLL yLL xUR yUR ]\"`.\n* **border** – if provided, an array describing border-drawing\nproperties. See the PDF spec for details. No border will be\ndrawn if this argument is omitted.\n\naddURI(*pagenum: int*, *uri: str*, *rect: RectangleObject*, *border: Optional[ArrayObject] = None*) → None[source]\nUse `add\\_uri()` instead.\n\n\nadd\\_link(*pagenum: int*, *page\\_destination: int*, *rect: RectangleObject*, *border: Optional[ArrayObject] = None*, *fit: Literal['/Fit', '/XYZ', '/FitH', '/FitV', '/FitR', '/FitB', '/FitBH', '/FitBV'] = '/Fit'*, *\\*args: Union[NumberObject, NullObject, float]*) → DictionaryObject[source]\n\naddLink(*pagenum: int*, *page\\_destination: int*, *rect: RectangleObject*, *border: Optional[ArrayObject] = None*, *fit: Literal['/Fit', '/XYZ', '/FitH', '/FitV', '/FitR', '/FitB', '/FitBH', '/FitBV'] = '/Fit'*, *\\*args: Union[NumberObject, NullObject, float]*) → None[source]\nUse `add\\_link()` instead.\n\n\ngetPageLayout() → Optional[Literal['/NoLayout', '/SinglePage', '/OneColumn', '/TwoColumnLeft', '/TwoColumnRight', '/TwoPageLeft', '/TwoPageRight']][source]\nUse `page\\_layout` instead.\n\n\nset\\_page\\_layout(*layout: Literal['/NoLayout', '/SinglePage', '/OneColumn', '/TwoColumnLeft', '/TwoColumnRight', '/TwoPageLeft', '/TwoPageRight']*) → None[source]\nSet the page layout.\n\nParameters\n**layout** – The page layout to be used\n\n\nValid `layout` arguments\n\nsetPageLayout(*layout: Literal['/NoLayout', '/SinglePage', '/OneColumn', '/TwoColumnLeft', '/TwoColumnRight', '/TwoPageLeft', '/TwoPageRight']*) → None[source]\nUse `page\\_layout` instead.\n\n\n*property* page\\_layout*: Optional[Literal['/NoLayout', '/SinglePage', '/OneColumn', '/TwoColumnLeft', '/TwoColumnRight', '/TwoPageLeft', '/TwoPageRight']]*\nPage layout property.\n\n*property* pageLayout*: Optional[Literal['/NoLayout', '/SinglePage', '/OneColumn', '/TwoColumnLeft', '/TwoColumnRight', '/TwoPageLeft', '/TwoPageRight']]*\nUse `page\\_layout` instead.\n\n\ngetPageMode() → Optional[Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']][source]\nUse `page\\_mode` instead.\n\n\nset\\_page\\_mode(*mode: Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']*) → None[source]\nUse `page\\_mode` instead.\n\n\nsetPageMode(*mode: Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']*) → None[source]\nUse `page\\_mode` instead.\n\n\n*property* page\\_mode*: Optional[Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']]*\nPage mode property.\n\n*property* pageMode*: Optional[Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']]*\nUse `page\\_mode` instead.\n\n\nadd\\_annotation(*page\\_number: Union[int, PageObject]*, *annotation: Dict[str, Any]*) → DictionaryObject[source]\nAdd a single annotation to the page.\nThe added annotation must be a new annotation.\nIt can not be recycled.\n\nParameters\n* **page\\_number** – PageObject or page index.\n* **annotation** – Annotation to be added (created with annotation).\n\nReturns\nThe inserted object\nThis can be used for pop-up creation, for example\n\nclean\\_page(*page: Union[PageObject, IndirectObject]*) → PageObject[source]\nPerform some clean up in the page.\nCurrently: convert NameObject nameddestination to TextStringObject\n(required for names/dests list)\n\nParameters\n**page** – \n\nReturns\nThe cleaned PageObject\n\nappend(*fileobj: Union[str, IO, PdfReader, Path]*, *outline\\_item: Union[str, None, PageRange, Tuple[int, int], Tuple[int, int, int], List[int]] = None*, *pages: Union[None, PageRange, Tuple[int, int], Tuple[int, int, int], List[int], List[PageObject]] = None*, *import\\_outline: bool = True*, *excluded\\_fields: Optional[Union[List[str], Tuple[str, ...]]] = None*) → None[source]\nIdentical to the `merge()` method, but assumes you want to\nconcatenate all pages onto the end of the file instead of specifying a\nposition.\n\nParameters\n* **fileobj** – A File Object or an object that supports the standard\nread and seek methods similar to a File Object. Could also be a\nstring representing a path to a PDF file.\n* **outline\\_item** – Optionally, you may specify a string to build an\noutline (aka ‘bookmark’) to identify the beginning of the\nincluded file.\n* **pages** – Can be a `PageRange`\nor a `(start, stop[, step])` tuple\nor a list of pages to be processed\nto merge only the specified range of pages from the source\ndocument into the output document.\n* **import\\_outline** – You may prevent the source document’s\noutline (collection of outline items, previously referred to as\n‘bookmarks’) from being imported by specifying this as `False`.\n* **excluded\\_fields** – Provide the list of fields/keys to be ignored\nif `/Annots` is part of the list, the annotation will be ignored\nif `/B` is part of the list, the articles will be ignored\n\nmerge(*position: Optional[int]*, *fileobj: Union[Path, str, IO, PdfReader]*, *outline\\_item: Optional[str] = None*, *pages: Optional[Union[str, PageRange, Tuple[int, int], Tuple[int, int, int], List[int], List[PageObject]]] = None*, *import\\_outline: bool = True*, *excluded\\_fields: Optional[Union[List[str], Tuple[str, ...]]] = ()*) → None[source]\nMerge the pages from the given file into the output file at the\nspecified page number.\n\nParameters\n* **position** – The *page number* to insert this file. File will\nbe inserted after the given number.\n* **fileobj** – A File Object or an object that supports the standard\nread and seek methods similar to a File Object. Could also be a\nstring representing a path to a PDF file.\n* **outline\\_item** – Optionally, you may specify a string to build an outline\n(aka ‘bookmark’) to identify the\nbeginning of the included file.\n* **pages** – can be a `PageRange`\nor a `(start, stop[, step])` tuple\nor a list of pages to be processed\nto merge only the specified range of pages from the source\ndocument into the output document.\n* **import\\_outline** – You may prevent the source document’s\noutline (collection of outline items, previously referred to as\n‘bookmarks’) from being imported by specifying this as `False`.\n* **excluded\\_fields** – provide the list of fields/keys to be ignored\nif `/Annots` is part of the list, the annotation will be ignored\nif `/B` is part of the list, the articles will be ignored\n\nRaises\n**TypeError** – The pages attribute is not configured properly\n\nadd\\_filtered\\_articles(*fltr: Union[Pattern, str]*, *pages: Dict[int, PageObject]*, *reader: PdfReader*) → None[source]\nAdd articles matching the defined criteria.\n\nParameters\n* **fltr** –\n* **pages** –\n* **reader** –\n\nclose() → None[source]\nTo match the functions from Merger.\n\nfind\\_outline\\_item(*outline\\_item: Dict[str, Any]*, *root: Optional[List[Union[Destination, List[Union[Destination, List[Destination]]]]]] = None*) → Optional[List[int]][source]\n\nfind\\_bookmark(*outline\\_item: Dict[str, Any]*, *root: Optional[List[Union[Destination, List[Union[Destination, List[Destination]]]]]] = None*) → Optional[List[int]][source]\n\nDeprecated since version 2.9.0: Use `find\\_outline\\_item()` instead.\n\n\nreset\\_translation(*reader: Union[None, PdfReader, IndirectObject] = None*) → None[source]\nReset the translation table between reader and the writer object.\n\n\nLate cloning will create new independent objects.\n\nParameters\n**reader** – PdfReader or IndirectObject refering a PdfReader object.\nif set to None or omitted, all tables will be reset.\n\nset\\_page\\_label(*page\\_index\\_from: int*, *page\\_index\\_to: int*, *style: Optional[PageLabelStyle] = None*, *prefix: Optional[str] = None*, *start: Optional[int] = 0*) → None[source]\nSet a page label to a range of pages.\n\n\nPage indexes must be given starting from 0.\nLabels must have a style, a prefix or both.\nIf to a range is not assigned any page label a decimal label starting from 1 is applied.\n\nParameters\n* **page\\_index\\_from** – page index of the beginning of the range starting from 0\n* **page\\_index\\_to** – page index of the beginning of the range starting from 0\n* **style** – The numbering style to be used for the numeric portion of each page label:\n‘/D’ Decimal arabic numerals\n‘/R’ Uppercase roman numerals\n‘/r’ Lowercase roman numerals\n‘/A’ Uppercase letters (A to Z for the first 26 pages,\n\n> \n> AA to ZZ for the next 26, and so on)\n> \n> \n> \n\n’/a’ Lowercase letters (a to z for the first 26 pages,aa to zz for the next 26, and so on)\n* **prefix** – The label prefix for page labels in this range.\n* **start** – The value of the numeric portion for the first page label\nin the range.\nSubsequent pages are numbered sequentially from this value,\nwhich must be greater than or equal to 1.\nDefault value: 1.\n# The PdfMerger Class\n\n\n*class* pypdf.PdfMerger(*strict: bool = False*, *fileobj: Union[Path, str, IO] = ''*)[source]\nBases: `object`\n\n\nUse `PdfWriter` instead.\n\nDeprecated since version 5.0.0.\n\nmerge(*page\\_number: Optional[int] = None*, *fileobj: Union[None, Path, str, IO, PdfReader] = None*, *outline\\_item: Optional[str] = None*, *pages: Optional[Union[str, PageRange, Tuple[int, int],\n\n==================\n Document 1 \n----------------\n The PdfMerger Class\n\n\n*class* pypdf.PdfMerger(*strict: bool = False*, *fileobj: Union[Path, str, IO] = ''*)[source]\nBases: `object`\n\n\nUse `PdfWriter` instead.\n\nDeprecated since version 5.0.0.\n\nmerge(*page\\_number: Optional[int] = None*, *fileobj: Union[None, Path, str, IO, PdfReader] = None*, *outline\\_item: Optional[str] = None*, *pages: Optional[Union[str, PageRange, Tuple[int, int], Tuple[int, int, int], List[int]]] = None*, *import\\_outline: bool = True*, *position: Optional[int] = None*) → None[source]\nMerge the pages from the given file into the output file at the\nspecified page number.\n\nParameters\n* **page\\_number** – The *page number* to insert this file. File will\nbe inserted after the given number.\n* **fileobj** – A File Object or an object that supports the standard\nread and seek methods similar to a File Object. Could also be a\nstring representing a path to a PDF file.\nNone as an argument is deprecated.\n* **outline\\_item** – Optionally, you may specify an outline item\n(previously referred to as a ‘bookmark’) to be applied at the\nbeginning of the included file by supplying the text of the outline item.\n* **pages** – \ncan be a `PageRange`or a `(start, stop[, step])` tuple\nto merge only the specified range of pages from the source\ndocument into the output document.\nCan also be a list of pages to merge.\n\nimport\\_outline: You may prevent the source document’soutline (collection of outline items, previously referred to as\n‘bookmarks’) from being imported by specifying this as `False`.\n\nappend(*fileobj: Union[str, IO, PdfReader, Path]*, *outline\\_item: Optional[str] = None*, *pages: Union[None, PageRange, Tuple[int, int], Tuple[int, int, int], List[int]] = None*, *import\\_outline: bool = True*) → None[source]\nIdentical to the `merge()` method, but assumes you want to\nconcatenate all pages onto the end of the file instead of specifying a\nposition.\n\nParameters\n* **fileobj** – A File Object or an object that supports the standard\nread and seek methods similar to a File Object. Could also be a\nstring representing a path to a PDF file.\n* **outline\\_item** – Optionally, you may specify an outline item\n(previously referred to as a ‘bookmark’) to be applied at the\nbeginning of the included file by supplying the text of the outline item.\n* **pages** – can be a `PageRange`\nor a `(start, stop[, step])` tuple\nto merge only the specified range of pages from the source\ndocument into the output document.\nCan also be a list of pages to append.\n* **import\\_outline** – You may prevent the source document’s\noutline (collection of outline items, previously referred to as\n‘bookmarks’) from being imported by specifying this as `False`.\n\nwrite(*fileobj: Union[Path, str, IO]*) → None[source]\nWrite all data that has been merged to the given output file.\n\nParameters\n**fileobj** – Output file. Can be a filename or any kind of\nfile-like object.\n\nclose() → None[source]\nShut all file descriptors (input and output) and clear all memory usage.\n\nParameters\n**infos** – a Python dictionary where each key is a field\nand each value is your new metadata.\nAn example is `{'/Title': 'My title'}`\n\n\nsetPageLayout(*layout: Literal['/NoLayout', '/SinglePage', '/OneColumn', '/TwoColumnLeft', '/TwoColumnRight', '/TwoPageLeft', '/TwoPageRight']*) → None[source]\nUse `set\\_page\\_layout()` instead.\n\nsetPageMode(*mode: Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']*) → None[source]\nUse `set\\_page\\_mode()` instead.\n\n\nset\\_page\\_mode(*mode: Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']*) → None[source]\nSet the page mode.\n\nParameters\n**mode** – The page mode to use.\n\n\nValid `mode` arguments\n\n\nadd\\_outline\\_item(*title: str*, *page\\_number: ~typing.Optional[int] = None*, *parent: ~typing.Union[None*, *~pypdf.generic.\\_data\\_structures.TreeObject*, *~pypdf.generic.\\_base.IndirectObject] = None*, *color: ~typing.Optional[~typing.Tuple[float*, *float*, *float]] = None*, *bold: bool = False*, *italic: bool = False*, *fit: ~pypdf.generic.\\_fit.Fit = <pypdf.generic.\\_fit.Fit object>*, *pagenum: ~typing.Optional[int] = None*) → IndirectObject[source]\nAdd an outline item (commonly referred to as a “Bookmark”) to this PDF file.\n\nParameters\n* **title** – Title to use for this outline item.\n* **page\\_number** – Page number this outline item will point to.\n* **parent** – A reference to a parent outline item to create nested\noutline items.\n* **color** – Color of the outline item’s font as a red, green, blue tuple\nfrom 0.0 to 1.0\n* **bold** – Outline item font is bold\n* **italic** – Outline item font is italic\n* **fit** – The fit of the destination page.\n\naddBookmark(*title: str*, *pagenum: int*, *parent: Union[None, TreeObject, IndirectObject] = None*, *color: Optional[Tuple[float, float, float]] = None*, *bold: bool = False*, *italic: bool = False*, *fit: Literal['/Fit', '/XYZ', '/FitH', '/FitV', '/FitR', '/FitB', '/FitBH', '/FitBV'] = '/Fit'*, *\\*args: Union[NumberObject, NullObject, float]*) → IndirectObject[source]\n\nDeprecated since version 1.28.0: Use `add\\_outline\\_item()` instead.\n\n\nadd\\_bookmark(*title: str*, *pagenum: int*, *parent: Union[None, TreeObject, IndirectObject] = None*, *color: Optional[Tuple[float, float, float]] = None*, *bold: bool = False*, *italic: bool = False*, *fit: Literal['/Fit', '/XYZ', '/FitH', '/FitV', '/FitR', '/FitB', '/FitBH', '/FitBV'] = '/Fit'*, *\\*args: Union[NumberObject, NullObject, float]*) → IndirectObject[source]\n\nDeprecated since version 2.9.0: Use `add\\_outline\\_item()` instead.\n\n\naddNamedDestination(*title: str*, *pagenum: int*) → None[source]\n\nDeprecated since version 1.28.0: Use `add\\_named\\_destination()` instead.\n\n\nadd\\_named\\_destination(*title: str*, *page\\_number: Optional[int] = None*, *pagenum: Optional[int] = None*) → None[source]\nAdd a destination to the output.\n\nParameters\n* **title** – Title to use\n* **page\\_number** – Page number this destination points at.\n# The PageObject Class\n\n\n*class* pypdf.\\_page.PageObject(*pdf: Union[None, PdfReaderProtocol, PdfWriterProtocol] = None*, *indirect\\_reference: Optional[IndirectObject] = None*, *indirect\\_ref: Optional[IndirectObject] = None*)[source]\nBases: `DictionaryObject`\n\n\nPageObject represents a single page within a PDF file.\n\n\nTypically this object will be created by accessing the\n`get\\_page()` method of the\n`PdfReader` class, but\n\n==================\n Document 2 \n----------------\n The PdfWriter Class\n\n\n*class* pypdf.PdfWriter(*fileobj: Union[str, IO] = ''*, *clone\\_from: Union[None, PdfReader, str, IO, Path] = None*)[source]\nBases: `object`\n\n\nWrite a PDF file out, given pages produced by another class.\n\n\nTypically data is added from a `PdfReader`.\n\n\n*property* pdf\\_header*: bytes*\nHeader of the PDF document that is written.\n\n\nThis should be something like `b'%PDF-1.5'`. It is recommended to set\nthe lowest version that supports all features which are used within the\nPDF file.\n\nget\\_object(*indirect\\_reference: Union[None, int, IndirectObject] = None*, *ido: Optional[IndirectObject] = None*) → PdfObject[source]\n\ngetObject(*ido: Union[int, IndirectObject]*) → PdfObject[source]\nUse `get\\_object()` instead.\n\n\nset\\_need\\_appearances\\_writer(*state: bool = True*) → None[source]\nSets the “NeedAppearances” flag in the PDF writer.\n\n\nThe “NeedAppearances” flag indicates whether the appearance dictionary\nfor form fields should be automatically generated by the PDF viewer or\nif the embedded appearence should be used.\n\nParameters\n**state** – The actual value of the NeedAppearances flag.\n\nReturns\nNone\n\n*property* viewer\\_preferences*: Optional[pypdf.generic.\\_viewerpref.ViewerPreferences]*\nReturns the existing ViewerPreferences as an overloaded dictionary.\n\ncreate\\_viewer\\_preferences() → ViewerPreferences[source]\n\nadd\\_page(*page: PageObject*, *excluded\\_keys: Iterable[str] = ()*) → PageObject[source]\nAdd a page to this PDF file.\n\n\nRecommended for advanced usage including the adequate excluded\\_keys.\n\n\nThe page is usually acquired from a `PdfReader`\ninstance.\n\nParameters\n* **page** – The page to add to the document. Should be\nan instance of `PageObject`\n* **excluded\\_keys** –\n\nReturns\nThe added PageObject.\n\naddPage(*page: PageObject*, *excluded\\_keys: Iterable[str] = ()*) → PageObject[source]\nUse `add\\_page()` instead.\n\n\ninsert\\_page(*page: PageObject*, *index: int = 0*, *excluded\\_keys: Iterable[str] = ()*) → PageObject[source]\nInsert a page in this PDF file. The page is usually acquired from a\n`PdfReader` instance.\n\nParameters\n* **page** – The page to add to the document.\n* **index** – Position at which the page will be inserted.\n* **excluded\\_keys** –\n\ninsertPage(*page: PageObject*, *index: int = 0*, *excluded\\_keys: Iterable[str] = ()*) → PageObject[source]\nUse `insert\\_page()` instead.\n\n\nget\\_page(*page\\_number: Optional[int] = None*, *pageNumber: Optional[int] = None*) → PageObject[source]\nRetrieve a page by number from this PDF file.\n\nParameters\n**page\\_number** – The page number to retrieve\n(pages begin at zero)\n\nReturns\nThe page at the index given by *page\\_number*\n\ngetPage(*pageNumber: int*) → PageObject[source]\nUse `writer.pages[page\\_number]` instead.\n\n\ngetNumPages() → int[source]\nUse `len(writer.pages)` instead.\n\n\n*property* pages*: List[PageObject]*\nProperty that emulates a list of `PageObject`.\nthis property allows to get a page or a range of pages.\n\n\nIt provides also capability to remove a page/range of page from the list\n(through del operator)\nNote: only the page entry is removed. As the objects beneath can be used\nsomewhere else.\na solution to completely remove them - if they are not used somewhere -\nis to write to a buffer/temporary and to then load it into a new PdfWriter\nobject.\n\nadd\\_blank\\_page(*width: Optional[float] = None*, *height: Optional[float] = None*) → PageObject[source]\nAppend a blank page to this PDF file and returns it.\n\n\nIf no page size is specified, use the size of the last page.\n\nParameters\n* **width** – The width of the new page expressed in default user\nspace units.\n* **height** – The height of the new page expressed in default\nuser space units.\n\nReturns\nThe newly appended page\n\nRaises\n**PageSizeNotDefinedError** – if width and height are not defined\n and previous page does not exist.\n\naddBlankPage(*width: Optional[float] = None*, *height: Optional[float] = None*) → PageObject[source]\nUse `add\\_blank\\_page()` instead.\n\n\ninsert\\_blank\\_page(*width: Optional[Union[float, Decimal]] = None*, *height: Optional[Union[float, Decimal]] = None*, *index: int = 0*) → PageObject[source]\nInsert a blank page to this PDF file and returns it.\n\nParameters\n* **width** – The width of the new page expressed in default user\nspace units.\n* **height** – The height of the new page expressed in default\nuser space units.\n* **index** – Position to add the page.\n\ninsertBlankPage(*width: Optional[Union[float, Decimal]] = None*, *height: Optional[Union[float, Decimal]] = None*, *index: int = 0*) → PageObject[source]\nUse `insertBlankPage()` instead.\n\n\n*property* open\\_destination*: Union[None, Destination, pypdf.generic.\\_base.TextStringObject, pypdf.generic.\\_base.ByteStringObject]*\nProperty to access the opening destination (`/OpenAction` entry in\nthe PDF catalog). It returns `None` if the entry does not exist is not\nset.\n\nRaises\n**Exception** – If a destination is invalid.\n\nadd\\_js(*javascript: str*) → None[source]\nAdd Javascript which will launch upon opening this PDF.\n\nParameters\n**javascript** – Your Javascript.\n\n```\n>>> output.add\\_js(\"this.print({bUI:true,bSilent:false,bShrinkToFit:true});\")# Example: This will launch the print window when the PDF is opened.\n\n```\n\naddJS(*javascript: str*) → None[source]\nUse `add\\_js()` instead.\n\n\nadd\\_attachment(*filename: str*, *data: Union[str, bytes]*) → None[source]\nEmbed a file inside the PDF.\n\n\nReference:\nhttps://www.adobe.com/content/dam/Adobe/en/devnet/acrobat/pdfs/PDF32000\\_2008.pdf\nSection 7.11.3\n\nParameters\n* **filename** – The filename to display.\n* **data** – The data\n\n==================\n Document 3 \n----------------\n The annotations module\n\n\nPDF specifies several annotation types which pypdf makes available here.\n\n\nThe names of the annotations and their attributes do not reflect the names in\nthe specification in all cases. For example, the PDF standard defines a\n‘Square’ annotation that does not actually need to be square. For this reason,\npypdf calls it ‘Rectangle’.\n\n\nAt their core, all annotation types are DictionaryObjects. That means if pypdf\ndoes not implement a feature, users can easily extend the given functionality.\n\n\n*class* pypdf.annotations.AnnotationDictionary[source]\nBases: `DictionaryObject`, `ABC`\n\n\n*property* flags*: pypdf.constants.AnnotationFlag*\n\n\n*class* pypdf.annotations.MarkupAnnotation(*\\**, *title\\_bar: Optional[str] = None*)[source]\nBases: `AnnotationDictionary`, `ABC`\n\n\nBase class for all markup annotations.\n\nParameters\n**title\\_bar** – Text to be displayed in the title bar of the annotation;\nby convention this is the name of the author\n\n*class* pypdf.annotations.Ellipse(*rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *\\**, *interiour\\_color: Optional[str] = None*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.FreeText(*\\**, *text: str*, *rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *font: str = 'Helvetica'*, *bold: bool = False*, *italic: bool = False*, *font\\_size: str = '14pt'*, *font\\_color: str = '000000'*, *border\\_color: Optional[str] = '000000'*, *background\\_color: Optional[str] = 'ffffff'*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n\nA FreeText annotation\n\n*class* pypdf.annotations.Highlight(*\\**, *rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *quad\\_points: ArrayObject*, *highlight\\_color: str = 'ff0000'*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.Line(*p1: Tuple[float, float]*, *p2: Tuple[float, float]*, *rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *text: str = ''*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.Link(*\\*, rect: ~typing.Union[~pypdf.generic.\\_rectangle.RectangleObject, ~typing.Tuple[float, float, float, float]], border: ~typing.Optional[~pypdf.generic.\\_data\\_structures.ArrayObject] = None, url: ~typing.Optional[str] = None, target\\_page\\_index: ~typing.Optional[int] = None, fit: ~pypdf.generic.\\_fit.Fit = <pypdf.generic.\\_fit.Fit object>, \\*\\*kwargs: ~typing.Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.Polygon(*vertices: List[Tuple[float, float]]*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.PolyLine(*vertices: List[Tuple[float, float]]*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.Rectangle(*rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *\\**, *interiour\\_color: Optional[str] = None*, *\\*\\*kwargs: Any*)[source]\nBases: `MarkupAnnotation`\n\n*class* pypdf.annotations.Text(*\\*, rect: ~typing.Union[~pypdf.generic.\\_rectangle.RectangleObject, ~typing.Tuple[float, float, float, float]], text: str, open: bool = False, flags: int = AnnotationFlag.None, \\*\\*kwargs: ~typing.Any*)[source]\nBases: `MarkupAnnotation`\n\n\nA text annotation.\n\nParameters\n* **rect** – array of four integers `[xLL, yLL, xUR, yUR]`\nspecifying the clickable rectangular area\n* **text** – The text that is added to the document\n* **open** –\n* **flags** –\n\n*class* pypdf.annotations.Popup(*\\**, *rect: Union[RectangleObject, Tuple[float, float, float, float]]*, *parent: Optional[DictionaryObject] = None*, *open: bool = False*, *\\*\\*kwargs: Any*)[source]\nBases: `AnnotationDictionary`\n\n# The Fit Class\n\n\n*class* pypdf.generic.Fit(*fit\\_type: str*, *fit\\_args: Tuple[Union[None, float, Any], ...] = ()*)[source]\nBases: `object`\n\n\n*classmethod* xyz(*left: Optional[float] = None*, *top: Optional[float] = None*, *zoom: Optional[float] = None*) → Fit[source]\nDisplay the page designated by page, with the coordinates (left , top)\npositioned at"
    },
    {
        "instruction": "Create a Python program using the 'pytorch-lightning' API to define a 'DataModule' class that prepares and tokenizes data for a natural language processing task. The 'DataModule' should load a dataset using the 'datasets' and 'transformers' libraries, tokenize the data with a specific model tokenizer, and set up data loaders. The program should demonstrate the use of 'pytorch-lightning' to create a 'DataModule' instance, prepare data, set it up, and print the shape of a batch from the data loader.",
        "api": "pytorch-lightning",
        "output": "#!pip install pytorch-lightning\nimport torch\nimport pytorch_lightning as pl\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\n\nclass DataModule(pl.LightningDataModule):\n    def __init__(\n        self,\n        model_name=\"google/bert_uncased_L-2_H-128_A-2\",\n        batch_size=64,\n        max_length=128,\n    ):\n        super().__init__()\n\n        self.batch_size = batch_size\n        self.max_length = max_length\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    def prepare_data(self):\n        cola_dataset = load_dataset(\"glue\", \"cola\")\n        self.train_data = cola_dataset[\"train\"]\n        self.val_data = cola_dataset[\"validation\"]\n\n    def tokenize_data(self, example):\n        return self.tokenizer(\n            example[\"sentence\"],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_length,\n        )\n\n    def setup(self, stage=None):\n        if stage == \"fit\" or stage is None:\n            self.train_data = self.train_data.map(self.tokenize_data, batched=True)\n            self.train_data.set_format(\n                type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n            )\n\n            self.val_data = self.val_data.map(self.tokenize_data, batched=True)\n            self.val_data.set_format(\n                type=\"torch\",\n                columns=[\"input_ids\", \"attention_mask\", \"label\"],\n                output_all_columns=True,\n            )\n\n    def train_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.train_data, batch_size=self.batch_size, shuffle=True\n        )\n\n    def val_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.val_data, batch_size=self.batch_size, shuffle=False\n        )\n\n\nif __name__ == \"__main__\":\n    data_model = DataModule()\n    data_model.prepare_data()\n    data_model.setup()\n    print(next(iter(data_model.train_dataloader()))[\"input_ids\"].shape)",
        "documentation": "\n\n==================\n Document 0 \n----------------\nstrategies¶\n\n\n|  |  |\n| --- | --- |\n| `DDPStrategy` | Strategy for multi-process single-device training on one or multiple nodes. |\n| `DeepSpeedStrategy` | Provides capabilities to run training using the DeepSpeed library, with training optimizations for large billion parameter models. |\n| `FSDPStrategy` | Strategy for Fully Sharded Data Parallel provided by torch.distributed. |\n| `ParallelStrategy` | Strategy for training with multiple processes in parallel. |\n| `SingleDeviceStrategy` | Strategy that handles communication on a single device. |\n| `SingleDeviceXLAStrategy` | Strategy for training on a single XLA device. |\n| `Strategy` | Base class for all strategies that change the behaviour of the training, validation and test- loop. |\n| `XLAStrategy` | Strategy for training multiple TPU devices using the `torch\\_xla.distributed.xla\\_multiprocessing.spawn()` method. |\n\n\n# tuner¶\n\n\n|  |  |\n| --- | --- |\n| `Tuner` | Tuner class to tune your model. |\n\n\n\n# utilities¶\n\n\n|  |  |\n| --- | --- |\n| `combined\\_loader` |  |\n| `data` |  |\n| `deepspeed` | Utilities that can be used with Deepspeed. |\n| `memory` | Utilities related to memory. |\n| `model\\_summary` |  |\n| `parsing` | Utilities used for parameter parsing. |\n| `rank\\_zero` | Utilities that can be used for calling functions on a particular rank. |\n| `seed` | Utilities to help with reproducibility of models. |\n| `warnings` | Warning-related utilities. |\n\n==================\n Document 1 \n----------------\n\n\n# accelerators¶\n\n\n|  |  |\n| --- | --- |\n| `Accelerator` | The Accelerator base class for Lightning PyTorch. |\n| `CPUAccelerator` | Accelerator for CPU devices. |\n| `CUDAAccelerator` | Accelerator for NVIDIA CUDA devices. |\n| `XLAAccelerator` | Accelerator for XLA devices, normally TPUs. |\n\n\n# callbacks¶\n\n\n|  |  |\n| --- | --- |\n| `BackboneFinetuning` | Finetune a backbone model based on a learning rate user-defined scheduling. |\n| `BaseFinetuning` | This class implements the base logic for writing your own Finetuning Callback. |\n| `BasePredictionWriter`\n\n==================\n Document 2 \n----------------\n callbacks¶\n\n\n|  |  |\n| --- | --- |\n| `BackboneFinetuning` | Finetune a backbone model based on a learning rate user-defined scheduling. |\n| `BaseFinetuning` | This class implements the base logic for writing your own Finetuning Callback. |\n| `BasePredictionWriter` | Base class to implement how the predictions should be stored. |\n| `BatchSizeFinder` | The `BatchSizeFinder` callback tries to find the largest batch size for a given model that does not give an out of memory (OOM) error. |\n| `Callback` | Abstract base class used to build new callbacks. |\n| `DeviceStatsMonitor` | Automatically monitors and logs device stats during training, validation and testing stage. |\n| `EarlyStopping` | Monitor a metric and stop training when it stops improving. |\n| `GradientAccumulationScheduler` | Change gradient accumulation factor according to scheduling. |\n| `LambdaCallback` | Create a simple callback on the fly using lambda functions. |\n| `LearningRateFinder` | The `LearningRateFinder` callback enables the user to do a range test of good initial learning rates, to reduce the amount of guesswork in picking a good starting learning rate. |\n| `LearningRateMonitor` | Automatically monitor and logs learning rate for learning rate schedulers during training. |\n| `ModelCheckpoint` | Save the model periodically by monitoring a quantity. |\n| `ModelPruning` | Model pruning Callback, using PyTorch's prune utilities. |\n| `ModelSummary` | Generates a summary of all layers in a `LightningModule`. |\n| `OnExceptionCheckpoint` | Used to save a checkpoint on exception. |\n| `ProgressBar` | The base class for progress bars in Lightning. |\n| `RichModelSummary` | Generates a summary of all layers in a `LightningModule` with rich text formatting. |\n| `RichProgressBar` | Create a progress bar with rich text formatting.\n |\n| `StochasticWeightAveraging` | Implements the Stochastic Weight Averaging (SWA) Callback to average a model. |\n| `Timer` | The Timer callback tracks the time spent in the training, validation, and test loops and interrupts the Trainer if the given time limit for the training loop is reached. |\n| `TQDMProgressBar` | This is the default progress bar used by Lightning. |\n\n\n# cli¶\n\n\n|  |  |\n| --- | --- |\n| `LightningCLI` | Implementation of a configurable command line tool for pytorch-lightning. |\n| `LightningArgumentParser` | Extension of jsonargparse's ArgumentParser for pytorch-lightning. |\n| `SaveConfigCallback` | Saves a LightningCLI config to the log\\_dir when training starts. |\n\n\n\n# core¶\n\n\n|  |  |\n| --- | --- |\n| `CheckpointHooks` | Hooks to be used with Checkpointing. |\n| `DataHooks` | Hooks to be used for data related stuff. |\n| `ModelHooks` | Hooks to be used in LightningModule. |\n| `LightningDataModule` | A DataModule standardizes the training, val, test splits, data preparation and transforms. |\n| `LightningModule` |  |\n| `HyperparametersMixin` |  |\n| `LightningOptimizer` | This class is used to wrap the user optimizers and handle properly the backward and optimizer\\_step logic across accelerators, AMP, accumulate\\_grad\\_batches. |\n\n\n\n# loggers¶\n\n\n|  |  |\n| --- | --- |\n| `logger` | Abstract base class used to build new loggers. |\n| `comet` | Comet Logger |\n| `csv\\_logs` | CSV logger |\n| `mlflow` | MLflow Logger |\n| `neptune` | Neptune Logger |\n| `tensorboard` | TensorBoard Logger |\n| `wandb` | Weights and Biases Logger |\n\n\n## plugins¶\n\n### precision¶\n\n\n|  |  |\n| --- | --- |\n| `DeepSpeedPrecisionPlugin` | Precision plugin for DeepSpeed integration. |\n| `DoublePrecisionPlugin` | Plugin for training with double (`torch.float64`) precision. |\n| `HalfPrecisionPlugin` | Plugin for training with half precision. |\n| `FSDPPrecisionPlugin` | Precision"
    },
    {
        "instruction": "Create a Python program using the 'pytorch-lightning' API to define a custom callback that logs the training loss and validation loss during training. The program should demonstrate the use of this custom callback to monitor and log losses during model training.",
        "api": "pytorch-lightning",
        "output": "#!pip install pytorch-lightning\nimport pytorch_lightning as pl\nimport torch\n\n# Define a custom callback for logging losses\nclass LossLoggingCallback(pl.Callback):\n    def on_epoch_end(self, trainer, pl_module):\n        train_loss = trainer.callback_metrics[\"train_loss_step\"]\n        val_loss = trainer.callback_metrics[\"val_loss_step\"]\n        print(f\"Epoch {trainer.current_epoch}: Train Loss - {train_loss:.4f}, Val Loss - {val_loss:.4f}\")\n\n# Create a LightningModule for demonstration\nclass ExampleModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(10, 1)\n\n    def forward(self, x):\n        return self.l1(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = torch.nn.functional.mse_loss(y_hat, y)\n        self.log(\"train_loss_step\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = torch.nn.functional.mse_loss(y_hat, y)\n        self.log(\"val_loss_step\", loss)\n\nif __name__ == \"__main__\":\n    model = ExampleModel()\n\n    trainer = pl.Trainer(\n        gpus=0,\n        max_epochs=5,\n        callbacks=[LossLoggingCallback()],\n    )\n\n    trainer.fit(model)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n callbacks¶\n\n\n|  |  |\n| --- | --- |\n| `BackboneFinetuning` | Finetune a backbone model based on a learning rate user-defined scheduling. |\n| `BaseFinetuning` | This class implements the base logic for writing your own Finetuning Callback. |\n| `BasePredictionWriter` | Base class to implement how the predictions should be stored. |\n| `BatchSizeFinder` | The `BatchSizeFinder` callback tries to find the largest batch size for a given model that does not give an out of memory (OOM) error. |\n| `Callback` | Abstract base class used to build new callbacks. |\n| `DeviceStatsMonitor` | Automatically monitors and logs device stats during training, validation and testing stage. |\n| `EarlyStopping` | Monitor a metric and stop training when it stops improving. |\n| `GradientAccumulationScheduler` | Change gradient accumulation factor according to scheduling. |\n| `LambdaCallback` | Create a simple callback on the fly using lambda functions. |\n| `LearningRateFinder` | The `LearningRateFinder` callback enables the user to do a range test of good initial learning rates, to reduce the amount of guesswork in picking a good starting learning rate. |\n| `LearningRateMonitor` | Automatically monitor and logs learning rate for learning rate schedulers during training. |\n| `ModelCheckpoint` | Save the model periodically by monitoring a quantity. |\n| `ModelPruning` | Model pruning Callback, using PyTorch's prune utilities. |\n| `ModelSummary` | Generates a summary of all layers in a `LightningModule`. |\n| `OnExceptionCheckpoint` | Used to save a checkpoint on exception. |\n| `ProgressBar` | The base class for progress bars in Lightning. |\n| `RichModelSummary` | Generates a summary of all layers in a `LightningModule` with rich text formatting. |\n| `RichProgressBar` | Create a progress bar with rich text formatting.\n |\n| `StochasticWeightAveraging` | Implements the Stochastic Weight Averaging (SWA) Callback to average a model. |\n| `Timer` | The Timer callback tracks the time spent in the training, validation, and test loops and interrupts the Trainer if the given time limit for the training loop is reached. |\n| `TQDMProgressBar` | This is the default progress bar used by Lightning. |\n\n\n# cli¶\n\n\n|  |  |\n| --- | --- |\n| `LightningCLI` | Implementation of a configurable command line tool for pytorch-lightning. |\n| `LightningArgumentParser` | Extension of jsonargparse's ArgumentParser for pytorch-lightning. |\n| `SaveConfigCallback` | Saves a LightningCLI config to the log\\_dir when training starts. |\n\n\n\n# core¶\n\n\n|  |  |\n| --- | --- |\n| `CheckpointHooks` | Hooks to be used with Checkpointing. |\n| `DataHooks` | Hooks to be used for data related stuff. |\n| `ModelHooks` | Hooks to be used in LightningModule. |\n| `LightningDataModule` | A DataModule standardizes the training, val, test splits, data preparation and transforms. |\n| `LightningModule` |  |\n| `HyperparametersMixin` |  |\n| `LightningOptimizer` | This class is used to wrap the user optimizers and handle properly the backward and optimizer\\_step logic across accelerators, AMP, accumulate\\_grad\\_batches. |\n\n\n\n# loggers¶\n\n\n|  |  |\n| --- | --- |\n| `logger` | Abstract base class used to build new loggers. |\n| `comet` | Comet Logger |\n| `csv\\_logs` | CSV logger |\n| `mlflow` | MLflow Logger |\n| `neptune` | Neptune Logger |\n| `tensorboard` | TensorBoard Logger |\n| `wandb` | Weights and Biases Logger |\n\n\n## plugins¶\n\n### precision¶\n\n\n|  |  |\n| --- | --- |\n| `DeepSpeedPrecisionPlugin` | Precision plugin for DeepSpeed integration. |\n| `DoublePrecisionPlugin` | Plugin for training with double (`torch.float64`) precision. |\n| `HalfPrecisionPlugin` | Plugin for training with half precision. |\n| `FSDPPrecisionPlugin` | Precision\n\n==================\n Document 1 \n----------------\n\n\n# accelerators¶\n\n\n|  |  |\n| --- | --- |\n| `Accelerator` | The Accelerator base class for Lightning PyTorch. |\n| `CPUAccelerator` | Accelerator for CPU devices. |\n| `CUDAAccelerator` | Accelerator for NVIDIA CUDA devices. |\n| `XLAAccelerator` | Accelerator for XLA devices, normally TPUs. |\n\n\n# callbacks¶\n\n\n|  |  |\n| --- | --- |\n| `BackboneFinetuning` | Finetune a backbone model based on a learning rate user-defined scheduling. |\n| `BaseFinetuning` | This class implements the base logic for writing your own Finetuning Callback. |\n| `BasePredictionWriter`\n\n==================\n Document 2 \n----------------\n profiler¶\n\n\n|  |  |\n| --- | --- |\n| `AdvancedProfiler` | This profiler uses Python's cProfiler to record more detailed information about time spent in each function call recorded during a given action. |\n| `PassThroughProfiler` | This class should be used when you don't want the (small) overhead of profiling. |\n| `Profiler` | If you wish to write a custom profiler, you should inherit from this class. |\n| `PyTorchProfiler` | This profiler uses PyTorch's Autograd Profiler and lets you inspect the cost of. |\n| `SimpleProfiler` | This profiler simply records the duration of actions (in seconds) and reports the mean duration of each action and the total time spent over the entire training run. |\n| `XLAProfiler` | XLA Profiler will help you debug and optimize training workload performance for your models using Cloud TPU performance tools. |\n\n\n# trainer¶\n\n\n|  |  |\n| --- | --- |\n| `Trainer` | Customize every aspect of training via flags. |\n\n\n# strategies¶\n\n\n|  |  |\n| --- | --- |\n| `DDPStrategy` | Strategy for multi-process single-device training on one or multiple nodes. |\n| `DeepSpeedStrategy` | Provides capabilities to run training using the DeepSpeed library, with training optimizations for large billion"
    },
    {
        "instruction": "Create a Python program using the 'rapidfuzz' API to demonstrate fuzzy string matching. The program should take a query string and compare it to a list of target strings using the 'fuzz.WRatio' scorer from 'rapidfuzz'. Print the query string and the similarity score for each target string. Additionally, the program should calculate and print the token ratio similarity score for each target string. The goal is to find similar strings in the list based on both the weighted ratio and token ratio similarity scores.",
        "api": "rapidfuzz",
        "output": "#!pip install rapidfuzz\nfrom rapidfuzz import fuzz, process\n\ntarget_strings = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\", \"fig\", \"grape\"]\n\nquery_string = \"cranberry\"\n\nprint(\"Query: \", query_string)\n\nresults = process.extract(query_string, target_strings, scorer=fuzz.WRatio, limit=len(target_strings))\n\nfor result in results:\n    target, score, _ = result\n    token_ratio_score = fuzz.token_ratio(query_string, target)\n    print(f\"{target}: Weighted Ratio Score {score}, Token Ratio Score {token_ratio_score}\")",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n\nBack to top\n\n\nToggle Light / Dark / Auto color theme\n\n\nToggle table of contents sidebar\n\n# header-image#\n\n\nRapidFuzz performs fuzzy string matching between two or more sequences using various string metrics.\n\n\n## Fundamental algorithms#\n\n\nRapidFuzz provides a wide range of fundamental algorithms for string matching.\nIn addition it provides an API compatible implementation of all algorithms in `thefuzz` / `fuzzywuzzy`.\n\n\n\n## Performant#\n\n\nRapidfuzz wraps highly-optimized implementations written in C++.\nEnjoy the flexibility of Python with the speed of compiled code.\n\n\n\n## Easy to use#\n\n\nRapidFuzz’s high level syntax makes it accessible and productive for programmers\nfrom any background or experience level.\n\n\n\n## Compatible#\n\n\nRapidFuzz provides a pure python fallback for every algorithm. This ensures that the library works on platforms\nwhere compiling a C extension is not possible.\n\n\n\n## Open source#\n\n\nDistributed under a MIT License, RapidFuzz is developed and maintained publicly on GitHub.\n\n\n# Installation#\n\n\nWhile there are several ways of install RapidFuzz, the recommended methods\nare either by using `pip` (the Python package manager) or\n`conda` (an open-source, cross-platform, package manager)\n\n\n## using pip#\n\n\nRapidFuzz can be installed with `pip`:\n\n```\npip install rapidfuzz\n\n```\n\n\nThere are pre-built binaries (wheels) of RapidFuzz for MacOS (10.9 and later), Linux x86\\_64 and Windows.\n\nfailure \"ImportError: DLL load failed\"\n\n If you run into this error on Windows the reason is most likely, that the\n Visual C++ 2019 redistributable is not installed, which is required to\n find C++ Libraries (The C++ 2019 version includes the 2015, 2017 and 2019 version).\n\n\n\n## using conda#\n\n\nRapidFuzz can be installed with `conda`:\n\n```\nconda install -c conda-forge rapidfuzz\n\n\n\n## from git#\n\n\nRapidFuzz can be directly used from GitHub by cloning the\nrepository. This requires a C++17 capable compiler.\n\n```\ngit clone --recursive https://github.com/maxbachmann/rapidfuzz.git\ncd rapidfuzz\npip install .\n\n# Usage#\n\n* rapidfuzz.process\n\t+ cdist\n\t\t- `cdist()`\n\t+ extract\n\t\t- `extract()`\n\t+ extract\\_iter\n\t\t- `extract\\_iter()`\n\t+ extractOne\n\t\t- `extractOne()`\n* distance\n\t+ Editop\n\t\t- `Editop`\n\t+ Editops\n\t\t- `Editops`\n\t+ Opcode\n\t\t- `Opcode`\n\t+ Opcodes\n\t\t- `Opcodes`\n\t\t- Damerau Levenshtein\n\t\t- Hamming\n\t\t- Indel\n\t\t- Jaro\n\t\t- JaroWinkler\n\t\t- Levenshtein\n\t\t- Optimal String Alignment (OSA)\n\t\t- Prefix\n\t\t- Postfix\n* rapidfuzz.fuzz\n\t+ ratio\n\t\t- `ratio()`\n\t+ partial\\_ratio\n\t\t- `partial\\_ratio()`\n\t+ partial\\_ratio\\_alignment\n\t\t- `partial\\_ratio\\_alignment()`\n\t+\n\n==================\n Document 1 \n----------------\n Usage#\n\n* rapidfuzz.process\n\t+ cdist\n\t\t- `cdist()`\n\t+ extract\n\t\t- `extract()`\n\t+ extract\\_iter\n\t\t- `extract\\_iter()`\n\t+ extractOne\n\t\t- `extractOne()`\n* distance\n\t+ Editop\n\t\t- `Editop`\n\t+ Editops\n\t\t- `Editops`\n\t+ Opcode\n\t\t- `Opcode`\n\t+ Opcodes\n\t\t- `Opcodes`\n\t\t- Damerau Levenshtein\n\t\t- Hamming\n\t\t- Indel\n\t\t- Jaro\n\t\t- JaroWinkler\n\t\t- Levenshtein\n\t\t- Optimal String Alignment (OSA)\n\t\t- Prefix\n\t\t- Postfix\n* rapidfuzz.fuzz\n\t+ ratio\n\t\t- `ratio()`\n\t+ partial\\_ratio\n\t\t- `partial\\_ratio()`\n\t+ partial\\_ratio\\_alignment\n\t\t- `partial\\_ratio\\_alignment()`\n\t+ token\\_set\\_ratio\n\t\t- `token\\_set\\_ratio()`\n\t+ partial\\_token\\_set\\_ratio\n\t\t- `partial\\_token\\_set\\_ratio()`\n\t+ token\\_sort\\_ratio\n\t\t- `token\\_sort\\_ratio()`\n\t+ partial\\_token\\_sort\\_ratio\n\t\t- `partial\\_token\\_sort\\_ratio()`\n\t+ token\\_ratio\n\t\t- `token\\_ratio()`\n\t+ partial\\_token\\_ratio\n\t\t- `partial\\_token\\_ratio()`\n\t+ WRatio\n\t\t- `WRatio()`\n\t+ QRatio\n\t\t- `QRatio()`\n* rapidfuzz.utils\n\t+ default\\_process\n\t\t- `default\\_process()`\n\n# rapidfuzz.process#\n\n## cdist#\n\n\nrapidfuzz.process.cdist(*queries: ~typing.Collection[~typing.Sequence[~typing.Hashable] | None], choices: ~typing.Collection[~typing.Sequence[~typing.Hashable] | None], \\*, scorer: ~typing.Callable[[...], int | float] = <function ratio>, processor: ~typing.Callable[[...], ~typing.Sequence[~typing.Hashable]] | None = None, score\\_cutoff: int | float | None = None, score\\_hint: int | float | None =\n\n==================\n Document 2 \n----------------\n# [1.0.0] - 2021-02-12#\n\n\n* all normalized string\\_metrics can now be used as scorer for process.extract/extractOne\n* Implementation of the C++ Wrapper completely refactored to make it easier to add more scorers, processors and string matching algorithms in the future.\n* increased test coverage, that already helped to fix some bugs and help to prevent regressions in the future\n* improved docstrings of functions\n\n\n* Added bit-parallel implementation of the Levenshtein distance for the weights (1,1,1) and (1,1,2).\n* Added specialized implementation of the Levenshtein distance for cases with a small maximum edit distance, that is even faster, than the bit-parallel implementation.\n* Improved performance of `fuzz.partial\\_ratio`\n-> Since `fuzz.ratio` and `fuzz.partial\\_ratio` are used in most scorers, this improves the overall performance.\n* Improved performance of `process.extract` and `process.extractOne`\n\n\n* the `rapidfuzz.levenshtein` module is now deprecated and will be removed in v2.0.0\nThese functions are now placed in `rapidfuzz.string\\_metric`. `distance`, `normalized\\_distance`, `weighted\\_distance` and `weighted\\_normalized\\_distance` are combined into `levenshtein` and `normalized\\_levenshtein`.\n\n\n* added normalized version of the hamming distance in `string\\_metric.normalized\\_hamming`\n* process.extract\\_iter as a generator, that yields the similarity of all elements, that have a similarity >= score\\_cutoff\n\n\n* multiple bugs in extractOne when used with a scorer, that’s not from RapidFuzz\n* fixed bug in `token\\_ratio`\n* fixed bug in result normalization causing zero division\n\n## [0.14.2] - 2020-12-31#\n\n\n* utf8 usage in the copyright header caused problems with python2.7 on some platforms (see #70)\n\n\n## [0.14.1] - 2020-12-13#\n\n\n* when a custom processor like `lambda s: s` was used with any of the methods inside fuzz.\\* it always returned a score of 100. This release fixes this and adds a better test coverage to prevent this bug in the future.\n\n\n## [0.14.0] - 2020-12-09#\n\n\n* added hamming distance metric in the levenshtein module\n\n\n* improved performance of default\\_process by using lookup table\n\n\n## [0.13.4] - 2020-11-30#\n\n\n* Add missing virtual destructor that caused a segmentation fault on Mac Os\n\n\n## [0.13.3] - 2020-11-21#\n\n\n* C++11 Support\n* manylinux wheels\n\n\n## [0.13.2] - 2020-11-21#\n\n\n* Levenshtein was not imported from \\_\\_init\\_\\_\n* The reference count of a Python Object inside process.extractOne was decremented to early\n\n\n## [0.13.1] - 2020-11-17#\n\n\n* process.extractOne exits early when a score of 100 is found. This way the other strings do not have to be preprocessed anymore.\n\n\n## [0.13.0] - 2020-11-16#\n\n\n* string objects passed to scorers had to be strings even before preprocessing them. This was changed, so they only have to be strings after preprocessing similar to process.extract/process.extractOne\n\n\n* process.extractOne is now implemented in C++ making it a lot faster\n* When token\\_sort\\_ratio or partial\\_token\\_sort ratio is used inprocess.extractOne the words in the query are only sorted once to improve the runtime\n\n\n* process.extractOne/process.extract do now return the index of the match, when the choices are a list.\n\n\n* process.extractIndices got removed, since the indices are now already returned by process.extractOne/process.extract\n\n\n## [0.12.5] - 2020-10-26#\n\n\n* fix documentation of process.extractOne (see #48)\n\n\n## [0.12.4] - 2020-10-22#\n\n\n* Added wheels for\n\n\n\t+ CPython 2.7 on windows 64 bit\n\t+ CPython 2.7 on windows 32 bit\n\t+ PyPy 2.7 on windows 32 bit\n\n\n## [0.12.3] - 2020-10-09#\n\n\n* fix bug in partial\\_ratio (see #43)\n\n\n## [0.12.2] - 2020-10-01#\n\n\n* fix inconsistency with fuzzywuzzy in partial\\_ratio when using strings of equal length\n\n\n## [0.12.1] - 2020-09-30#\n\n\n* MSVC has a bug and therefore crashed on some of the templates used. This Release simplifies the templates so compiling on msvc works again\n\n\n## [0.12.0] - 2020-09-30#\n\n\n* partial\\_ratio is using the Levenshtein distance now, which is a lot faster. Since many of the other algorithms use partial\\_ratio, this helps to improve the overall performance\n\n\n## [0.11.3] - 2020-09-22#\n\n\n* fix partial\\_token\\_set\\_ratio returning 100 all the time\n\n\n## [0.11.2] - 2020-09-12#\n\n\n* added rapidfuzz.\\_\\_author\\_\\_, rapidfuzz.\\_\\_license\\_\\_ and rapidfuzz.\\_\\_version\\_\\_\n\n\n## [0.11.1] - 2020-09-01#\n\n\n* do not use auto junk when searching the optimal alignment for partial\\_ratio\n\n\n## [0.11.0] - 2020-08-22#\n\n\n* support for python 2.7 added #40\n* add wheels for python2.7 (both pypy and cpython) on MacOS and Linux\n\n\n## [0.10.0] - 2020-08-17#\n\n\n* added wheels for Python3.9\n\n\n* tuple scores in process.extractOne are now supported #39\n\n\n# License#\n\n\nRapidFuzz is free and open-source software licensed under the MIT license.\n\n## MIT License#\n\n```\nCopyright © 2020-present Max Bachmann\nCopyright © 2011 Adam Cohen\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the"
    },
    {
        "instruction": "Create a Python program using the 'rapidfuzz' API to demonstrate fuzzy string matching. The program should take a query string and compare it to a list of target strings using the 'fuzz.WRatio' scorer from 'rapidfuzz'. Print the query string and the similarity score for each target string. Additionally, the program should calculate and print the token set ratio similarity score for each target string. The goal is to find similar strings in the list based on both the weighted ratio and token set ratio similarity scores.",
        "api": "rapidfuzz",
        "output": "#!pip install rapidfuzz\nfrom rapidfuzz import fuzz, process\n\ntarget_strings = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\", \"fig\", \"grape\"]\n\nquery_string = \"cranberry\"\n\nprint(\"Query: \", query_string)\n\nresults = process.extract(query_string, target_strings, scorer=fuzz.WRatio, limit=len(target_strings))\n\nfor result in results:\n    target, score, _ = result\n    token_set_ratio_score = fuzz.token_set_ratio(query_string, target)\n    print(f\"{target}: Weighted Ratio Score {score}, Token Set Ratio Score {token_set_ratio_score}\")",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n\nBack to top\n\n\nToggle Light / Dark / Auto color theme\n\n\nToggle table of contents sidebar\n\n# header-image#\n\n\nRapidFuzz performs fuzzy string matching between two or more sequences using various string metrics.\n\n\n## Fundamental algorithms#\n\n\nRapidFuzz provides a wide range of fundamental algorithms for string matching.\nIn addition it provides an API compatible implementation of all algorithms in `thefuzz` / `fuzzywuzzy`.\n\n\n\n## Performant#\n\n\nRapidfuzz wraps highly-optimized implementations written in C++.\nEnjoy the flexibility of Python with the speed of compiled code.\n\n\n\n## Easy to use#\n\n\nRapidFuzz’s high level syntax makes it accessible and productive for programmers\nfrom any background or experience level.\n\n\n\n## Compatible#\n\n\nRapidFuzz provides a pure python fallback for every algorithm. This ensures that the library works on platforms\nwhere compiling a C extension is not possible.\n\n\n\n## Open source#\n\n\nDistributed under a MIT License, RapidFuzz is developed and maintained publicly on GitHub.\n\n\n# Installation#\n\n\nWhile there are several ways of install RapidFuzz, the recommended methods\nare either by using `pip` (the Python package manager) or\n`conda` (an open-source, cross-platform, package manager)\n\n\n## using pip#\n\n\nRapidFuzz can be installed with `pip`:\n\n```\npip install rapidfuzz\n\n```\n\n\nThere are pre-built binaries (wheels) of RapidFuzz for MacOS (10.9 and later), Linux x86\\_64 and Windows.\n\nfailure \"ImportError: DLL load failed\"\n\n If you run into this error on Windows the reason is most likely, that the\n Visual C++ 2019 redistributable is not installed, which is required to\n find C++ Libraries (The C++ 2019 version includes the 2015, 2017 and 2019 version).\n\n\n\n## using conda#\n\n\nRapidFuzz can be installed with `conda`:\n\n```\nconda install -c conda-forge rapidfuzz\n\n\n\n## from git#\n\n\nRapidFuzz can be directly used from GitHub by cloning the\nrepository. This requires a C++17 capable compiler.\n\n```\ngit clone --recursive https://github.com/maxbachmann/rapidfuzz.git\ncd rapidfuzz\npip install .\n\n# Usage#\n\n* rapidfuzz.process\n\t+ cdist\n\t\t- `cdist()`\n\t+ extract\n\t\t- `extract()`\n\t+ extract\\_iter\n\t\t- `extract\\_iter()`\n\t+ extractOne\n\t\t- `extractOne()`\n* distance\n\t+ Editop\n\t\t- `Editop`\n\t+ Editops\n\t\t- `Editops`\n\t+ Opcode\n\t\t- `Opcode`\n\t+ Opcodes\n\t\t- `Opcodes`\n\t\t- Damerau Levenshtein\n\t\t- Hamming\n\t\t- Indel\n\t\t- Jaro\n\t\t- JaroWinkler\n\t\t- Levenshtein\n\t\t- Optimal String Alignment (OSA)\n\t\t- Prefix\n\t\t- Postfix\n* rapidfuzz.fuzz\n\t+ ratio\n\t\t- `ratio()`\n\t+ partial\\_ratio\n\t\t- `partial\\_ratio()`\n\t+ partial\\_ratio\\_alignment\n\t\t- `partial\\_ratio\\_alignment()`\n\t+\n\n==================\n Document 1 \n----------------\n# [1.0.0] - 2021-02-12#\n\n\n* all normalized string\\_metrics can now be used as scorer for process.extract/extractOne\n* Implementation of the C++ Wrapper completely refactored to make it easier to add more scorers, processors and string matching algorithms in the future.\n* increased test coverage, that already helped to fix some bugs and help to prevent regressions in the future\n* improved docstrings of functions\n\n\n* Added bit-parallel implementation of the Levenshtein distance for the weights (1,1,1) and (1,1,2).\n* Added specialized implementation of the Levenshtein distance for cases with a small maximum edit distance, that is even faster, than the bit-parallel implementation.\n* Improved performance of `fuzz.partial\\_ratio`\n-> Since `fuzz.ratio` and `fuzz.partial\\_ratio` are used in most scorers, this improves the overall performance.\n* Improved performance of `process.extract` and `process.extractOne`\n\n\n* the `rapidfuzz.levenshtein` module is now deprecated and will be removed in v2.0.0\nThese functions are now placed in `rapidfuzz.string\\_metric`. `distance`, `normalized\\_distance`, `weighted\\_distance` and `weighted\\_normalized\\_distance` are combined into `levenshtein` and `normalized\\_levenshtein`.\n\n\n* added normalized version of the hamming distance in `string\\_metric.normalized\\_hamming`\n* process.extract\\_iter as a generator, that yields the similarity of all elements, that have a similarity >= score\\_cutoff\n\n\n* multiple bugs in extractOne when used with a scorer, that’s not from RapidFuzz\n* fixed bug in `token\\_ratio`\n* fixed bug in result normalization causing zero division\n\n## [0.14.2] - 2020-12-31#\n\n\n* utf8 usage in the copyright header caused problems with python2.7 on some platforms (see #70)\n\n\n## [0.14.1] - 2020-12-13#\n\n\n* when a custom processor like `lambda s: s` was used with any of the methods inside fuzz.\\* it always returned a score of 100. This release fixes this and adds a better test coverage to prevent this bug in the future.\n\n\n## [0.14.0] - 2020-12-09#\n\n\n* added hamming distance metric in the levenshtein module\n\n\n* improved performance of default\\_process by using lookup table\n\n\n## [0.13.4] - 2020-11-30#\n\n\n* Add missing virtual destructor that caused a segmentation fault on Mac Os\n\n\n## [0.13.3] - 2020-11-21#\n\n\n* C++11 Support\n* manylinux wheels\n\n\n## [0.13.2] - 2020-11-21#\n\n\n* Levenshtein was not imported from \\_\\_init\\_\\_\n* The reference count of a Python Object inside process.extractOne was decremented to early\n\n\n## [0.13.1] - 2020-11-17#\n\n\n* process.extractOne exits early when a score of 100 is found. This way the other strings do not have to be preprocessed anymore.\n\n\n## [0.13.0] - 2020-11-16#\n\n\n* string objects passed to scorers had to be strings even before preprocessing them. This was changed, so they only have to be strings after preprocessing similar to process.extract/process.extractOne\n\n\n* process.extractOne is now implemented in C++ making it a lot faster\n* When token\\_sort\\_ratio or partial\\_token\\_sort ratio is used inprocess.extractOne the words in the query are only sorted once to improve the runtime\n\n\n* process.extractOne/process.extract do now return the index of the match, when the choices are a list.\n\n\n* process.extractIndices got removed, since the indices are now already returned by process.extractOne/process.extract\n\n\n## [0.12.5] - 2020-10-26#\n\n\n* fix documentation of process.extractOne (see #48)\n\n\n## [0.12.4] - 2020-10-22#\n\n\n* Added wheels for\n\n\n\t+ CPython 2.7 on windows 64 bit\n\t+ CPython 2.7 on windows 32 bit\n\t+ PyPy 2.7 on windows 32 bit\n\n\n## [0.12.3] - 2020-10-09#\n\n\n* fix bug in partial\\_ratio (see #43)\n\n\n## [0.12.2] - 2020-10-01#\n\n\n* fix inconsistency with fuzzywuzzy in partial\\_ratio when using strings of equal length\n\n\n## [0.12.1] - 2020-09-30#\n\n\n* MSVC has a bug and therefore crashed on some of the templates used. This Release simplifies the templates so compiling on msvc works again\n\n\n## [0.12.0] - 2020-09-30#\n\n\n* partial\\_ratio is using the Levenshtein distance now, which is a lot faster. Since many of the other algorithms use partial\\_ratio, this helps to improve the overall performance\n\n\n## [0.11.3] - 2020-09-22#\n\n\n* fix partial\\_token\\_set\\_ratio returning 100 all the time\n\n\n## [0.11.2] - 2020-09-12#\n\n\n* added rapidfuzz.\\_\\_author\\_\\_, rapidfuzz.\\_\\_license\\_\\_ and rapidfuzz.\\_\\_version\\_\\_\n\n\n## [0.11.1] - 2020-09-01#\n\n\n* do not use auto junk when searching the optimal alignment for partial\\_ratio\n\n\n## [0.11.0] - 2020-08-22#\n\n\n* support for python 2.7 added #40\n* add wheels for python2.7 (both pypy and cpython) on MacOS and Linux\n\n\n## [0.10.0] - 2020-08-17#\n\n\n* added wheels for Python3.9\n\n\n* tuple scores in process.extractOne are now supported #39\n\n\n# License#\n\n\nRapidFuzz is free and open-source software licensed under the MIT license.\n\n## MIT License#\n\n```\nCopyright © 2020-present Max Bachmann\nCopyright © 2011 Adam Cohen\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the\n\n==================\n Document 2 \n----------------\n Usage#\n\n* rapidfuzz.process\n\t+ cdist\n\t\t- `cdist()`\n\t+ extract\n\t\t- `extract()`\n\t+ extract\\_iter\n\t\t- `extract\\_iter()`\n\t+ extractOne\n\t\t- `extractOne()`\n* distance\n\t+ Editop\n\t\t- `Editop`\n\t+ Editops\n\t\t- `Editops`\n\t+ Opcode\n\t\t- `Opcode`\n\t+ Opcodes\n\t\t- `Opcodes`\n\t\t- Damerau Levenshtein\n\t\t- Hamming\n\t\t- Indel\n\t\t- Jaro\n\t\t- JaroWinkler\n\t\t- Levenshtein\n\t\t- Optimal String Alignment (OSA)\n\t\t- Prefix\n\t\t- Postfix\n* rapidfuzz.fuzz\n\t+ ratio\n\t\t- `ratio()`\n\t+ partial\\_ratio\n\t\t- `partial\\_ratio()`\n\t+ partial\\_ratio\\_alignment\n\t\t- `partial\\_ratio\\_alignment()`\n\t+ token\\_set\\_ratio\n\t\t- `token\\_set\\_ratio()`\n\t+ partial\\_token\\_set\\_ratio\n\t\t- `partial\\_token\\_set\\_ratio()`\n\t+ token\\_sort\\_ratio\n\t\t- `token\\_sort\\_ratio()`\n\t+ partial\\_token\\_sort\\_ratio\n\t\t- `partial\\_token\\_sort\\_ratio()`\n\t+ token\\_ratio\n\t\t- `token\\_ratio()`\n\t+ partial\\_token\\_ratio\n\t\t- `partial\\_token\\_ratio()`\n\t+ WRatio\n\t\t- `WRatio()`\n\t+ QRatio\n\t\t- `QRatio()`\n* rapidfuzz.utils\n\t+ default\\_process\n\t\t- `default\\_process()`\n\n# rapidfuzz.process#\n\n## cdist#\n\n\nrapidfuzz.process.cdist(*queries: ~typing.Collection[~typing.Sequence[~typing.Hashable] | None], choices: ~typing.Collection[~typing.Sequence[~typing.Hashable] | None], \\*, scorer: ~typing.Callable[[...], int | float] = <function ratio>, processor: ~typing.Callable[[...], ~typing.Sequence[~typing.Hashable]] | None = None, score\\_cutoff: int | float | None = None, score\\_hint: int | float | None ="
    },
    {
        "instruction": "Create a Python program using the 'rich' API to display a colorful and styled list of famous historical quotes. The program should include the quote text, the author, and the source of the quote.",
        "api": "rich",
        "output": "# Install rich\n#!pip install rich\nfrom rich.console import Console\nfrom rich.table import Table\n\n# Define a list of famous historical quotes\nquotes = [\n    {\n        \"text\": \"Those who cannot remember the past are condemned to repeat it.\",\n        \"author\": \"George Santayana\",\n        \"source\": \"The Life of Reason\",\n    },\n    {\n        \"text\": \"In the end, we will remember not the words of our enemies, but the silence of our friends.\",\n        \"author\": \"Martin Luther King Jr.\",\n        \"source\": \"Strength to Love\",\n    },\n    {\n        \"text\": \"Never interrupt your enemy when he is making a mistake.\",\n        \"author\": \"Napoleon Bonaparte\",\n        \"source\": \"Letter to Charles-Maurice Talleyrand-Perigord\",\n    },\n    {\n        \"text\": \"We must, indeed, all hang together or, most assuredly, we shall all hang separately.\",\n        \"author\": \"Benjamin Franklin\",\n        \"source\": \"In the Continental Congress just before signing the Declaration of Independence\",\n    },\n]\n\n# Create a rich table for famous historical quotes\nconsole = Console()\ntable = Table(title=\"Famous Historical Quotes\", style=\"italic\")\ntable.add_column(\"Quote\", style=\"bold\")\ntable.add_column(\"Author\", style=\"bold\")\ntable.add_column(\"Source\", style=\"bold\")\n\n# Populate the table with quote data\nfor quote in quotes:\n    table.add_row(quote[\"text\"], quote[\"author\"], quote[\"source\"])\n\n# Display the famous historical quotes\nconsole.print(table)\n\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n rich¶\n\n\nRich text and beautiful formatting in the terminal.\n\n\nrich.get\\_console()[source]¶\nGet a global `Console` instance. This function is used when Rich requires a Console,\nand hasn’t been explicitly given one.\n\nReturns\nA console instance.\n\nReturn type\nConsole\n\nrich.inspect(*obj*, *\\**, *console=None*, *title=None*, *help=False*, *methods=False*, *docs=True*, *private=False*, *dunder=False*, *sort=True*, *all=False*, *value=True*)[source]¶\nInspect any Python object.\n\n\n* inspect(<OBJECT>) to see summarized info.\n* inspect(<OBJECT>, methods=True) to see methods.\n* inspect(<OBJECT>, help=True) to see full (non-abbreviated) help.\n* inspect(<OBJECT>, private=True) to see private attributes (single underscore).\n* inspect(<OBJECT>, dunder=True) to see attributes beginning with double underscore.\n* inspect(<OBJECT>, all=True) to see all attributes.\n\nParameters\n* **obj** (*Any*) – An object to inspect.\n* **title** (*str**,* *optional*) – Title to display over inspect result, or None use type. Defaults to None.\n* **help** (*bool**,* *optional*) – Show full help text rather than just first paragraph. Defaults to False.\n* **methods** (*bool**,* *optional*) – Enable inspection of callables. Defaults to False.\n* **docs** (*bool**,* *optional*) – Also render doc strings. Defaults to True.\n* **private** (*bool**,* *optional*) – Show private attributes (beginning with underscore). Defaults to False.\n* **dunder** (*bool**,* *optional*) – Show attributes starting with double underscore. Defaults to False.\n* **sort** (*bool**,* *optional*) – Sort attributes alphabetically. Defaults to True.\n* **all** (*bool**,* *optional*) – Show all attributes. Defaults to False.\n* **value** (*bool**,* *optional*) – Pretty print value. Defaults to True.\n* **console** (*Optional**[**Console**]*) –\n\nrich.print(*\\*objects*, *sep=' '*, *end='\\n'*, *file=None*, *flush=False*)[source]¶\nPrint object(s) supplied via positional arguments.\nThis function has an identical signature to the built-in print.\nFor more advanced features, see the `Console` class.\n\nParameters\n* **sep** (*str**,* *optional*) – Separator between printed objects. Defaults to ” “.\n* **end** (*str**,* *optional*) – Character to write at end of output. Defaults to “\\n”.\n* **file** (*IO**[**str**]**,* *optional*) – File to write to, or None for stdout. Defaults to None.\n* **flush** (*bool**,* *optional*) – Has no effect as Rich always flushes output. Defaults to False.\n* **objects** (*Any*) –\n\nrich.print\\_json(*json=None*, *\\**, *data=None*, *indent=2*, *highlight=True*, *skip\\_keys=False*, *ensure\\_ascii=False*, *check\\_circular=True*, *allow\\_nan=True*, *default=None*, *sort\\_keys=False*)[source]¶\nPretty prints JSON. Output will be valid JSON.\n\nParameters\n* **json** (*str*) – A string containing JSON.\n* **data** (*Any*) – If json is not supplied, then encode this data.\n* **indent** (*int**,* *optional*) – Number of spaces to indent. Defaults to 2.\n* **highlight** (*bool**,* *optional*) – Enable highlighting of output: Defaults to True.\n* **skip\\_keys** (*bool**,* *optional*) – Skip keys not of a basic type. Defaults to False.\n* **ensure\\_ascii** (*bool**,* *optional*) – Escape all non-ascii characters. Defaults to False.\n* **check\\_circular** (*bool**,* *optional*) – Check for circular references. Defaults to True.\n* **allow\\_nan** (*bool**,* *optional*) – Allow NaN and Infinity values. Defaults to True.\n* **default** (*Callable**,* *optional*) – A callable that converts values that can not be encoded\nin to something that can be JSON encoded. Defaults to None.\n* **sort\\_keys** (*bool**,* *optional*) – Sort dictionary keys. Defaults to False.\n\nrich.reconfigure(*\\*args*, *\\*\\*kwargs*)[source]¶\nReconfigures the global console by replacing it with another.\n\nParameters\n* **\\*args** (*Any*) – Positional arguments for the replacement `Console`.\n* **\\*\\*kwargs** (*Any*) – Keyword arguments for the replacement `Console`.\n\n# rich.json¶\n\n\n*class* rich.json.JSON(*json*, *indent=2*, *highlight=True*, *skip\\_keys=False*, *ensure\\_ascii=False*, *check\\_circular=True*, *allow\\_nan=True*, *default=None*, *sort\\_keys=False*)[source]¶\nA renderable which pretty prints JSON.\n\nParameters\n* **json** (*str*) – JSON encoded data.\n* **indent** (*Union**[**None**,* *int**,* *str**]**,* *optional*) – Number of characters to indent by. Defaults to 2.\n* **highlight** (*bool**,* *optional*)\n\n==================\n Document 1 \n----------------\n rich.console¶\n\n\n*class* rich.console.Capture(*console*)[source]¶\nContext manager to capture the result of printing to the console.\nSee `capture()` for how to use.\n\nParameters\n**console** (*Console*) – A console instance to capture output.\n\n\nget()[source]¶\nGet the result of the capture.\n\n\n*exception* rich.console.CaptureError[source]¶\nAn error in the Capture context manager.\n\n*class* rich.console.Console(*\\**, *color\\_system='auto'*, *force\\_terminal=None*, *force\\_jupyter=None*, *force\\_interactive=None*, *soft\\_wrap=False*, *theme=None*, *stderr=False*, *file=None*, *quiet=False*, *width=None*, *height=None*, *style=None*, *no\\_color=None*, *tab\\_size=8*, *record=False*, *markup=True*, *emoji=True*, *emoji\\_variant=None*, *highlight=True*, *log\\_time=True*, *log\\_path=True*, *log\\_time\\_format='[%X]'*, *highlighter=<rich.highlighter.ReprHighlighter object>*, *legacy\\_windows=None*, *safe\\_box=True*, *get\\_datetime=None*, *get\\_time=None*, *\\_environ=None*)[source]¶\nA high level console interface.\n\nParameters\n* **color\\_system** (*str**,* *optional*) – The color system supported by your terminal,\neither `\"standard\"`, `\"256\"` or `\"truecolor\"`. Leave as `\"auto\"` to autodetect.\n* **force\\_terminal** (*Optional**[**bool**]**,* *optional*) – Enable/disable terminal control codes, or None to auto-detect terminal. Defaults to None.\n* **force\\_jupyter** (*Optional**[**bool**]**,* *optional*) – Enable/disable Jupyter rendering, or None to auto-detect Jupyter. Defaults to None.\n* **force\\_interactive** (*Optional**[**bool**]**,* *optional*) – Enable/disable interactive mode, or None to auto detect. Defaults to None.\n* **soft\\_wrap** (*Optional**[**bool**]**,* *optional*) – Set soft wrap default on print method. Defaults to False.\n* **theme** (*Theme**,* *optional*) – An optional style theme object, or `None` for default theme.\n* **stderr** (*bool**,* *optional*) – Use stderr rather than stdout if `file` is not specified. Defaults to False.\n* **file** (*IO**,* *optional*) – A file object where the console should write to. Defaults to stdout.\n* **quiet** (*bool**,* *Optional*) – Boolean to suppress all output. Defaults to False.\n* **width** (*int**,* *optional*) – The width of the terminal. Leave as default to auto-detect width.\n* **height** (*int**,* *optional*) – The height of the terminal. Leave as default to auto-detect height.\n* **style** (*StyleType**,* *optional*) – Style to apply to all output, or None for no style. Defaults to None.\n* **no\\_color** (*Optional**[**bool**]**,* *optional*) – Enabled no color mode, or None to auto detect. Defaults to None.\n* **tab\\_size** (*int**,* *optional*) – Number of spaces used to replace a tab character. Defaults to 8.\n* **record** (*bool**,* *optional*) – Boolean to enable recording of terminal output,\nrequired to call `export\\_html()`, `export\\_svg()`, and `export\\_text()`. Defaults to False.\n* **markup** (*bool**,* *optional*) – Boolean to enable Console Markup. Defaults to True.\n* **emoji** (*bool**,* *optional*) – Enable emoji code. Defaults to True.\n* **emoji\\_variant** (*str**,* *optional*) – Optional emoji variant, either “text” or “emoji”. Defaults to None.\n* **highlight** (*bool**,* *optional*) – Enable automatic highlighting. Defaults to True.\n* **log\\_time** (*bool**,* *optional*) – Boolean to enable logging of time by `log()` methods. Defaults to True.\n* **log\\_path** (*bool**,* *optional*) – Boolean to enable the logging of the caller by `log()`. Defaults to True.\n* **log\\_time\\_format** (*Union**[**str**,* *TimeFormatterCallable**]**,* *optional*) – If `log\\_time` is enabled, either string for strftime or callable that formats the time. Defaults to “[%X] “.\n* **highlighter** (*HighlighterType**,* *optional*) – Default highlighter.\n* **legacy\\_windows** (*bool**,* *optional*) – Enable legacy Windows mode, or `None` to auto detect. Defaults to `None`.\n* **safe\\_box** (*bool**,* *optional*) – Restrict box options that don’t render on legacy Windows.\n* **get\\_datetime** (*Callable**[**[**]**,* *datetime**]**,* *optional*) – Callable that gets the current time as a datetime.datetime object (used by Console.log),\nor None for datetime.now.\n* **get\\_time** (*Callable**[**[**]**,* *time**]**,* *optional*) – Callable that gets the current time in seconds, default uses time.monotonic.\n* **\\_environ** (*Mapping**[**str**,* *str**]*) –\n\n\nbegin\\_capture()[source]¶\nBegin capturing console output. Call `end\\_capture()` to exit capture mode and return output.\n\nbell()[source]¶\nPlay a ‘bell’ sound (if supported by the terminal).\n\ncapture()[source]¶\nA context manager to *capture* the result of print() or log() in a string,\nrather than writing it to the console.\n\n\nExample\n\n```\n>>> from rich.console import Console\n>>> console = Console()\n>>> with console.capture() as capture:\n...     console.print(\"[bold magenta]Hello World[/]\")\n>>> print(capture.get())\n\nReturns\nContext manager with disables writing to the terminal.\n\nReturn type\nCapture\n\nclear(*home=True*)[source]¶\nClear the screen.\n\nParameters\n**home** (*bool**,* *optional*) – Also move the cursor to ‘home’ position. Defaults to True.\n\nclear\\_live()[source]¶\nClear the Live instance.\n\n*property* color\\_system*: Optional[str]*¶\nGet color system string.\n\nReturns\n“standard”, “256” or “truecolor”.\n\nReturn type\nOptional[str]\n\ncontrol(*\\*control*)[source]¶\nInsert non-printing control codes.\n\nParameters\n* **control\\_codes** (*str*) – Control codes, such as those that may move the cursor.\n* **control** (*Control*) –\n\n*property* encoding*: str*¶\nGet the encoding of the console file, e.g. `\"utf-8\"`.\n\nReturns\nA standard encoding string.\n\nend\\_capture()[source]¶\nEnd capture mode and return captured string.\n\nReturns\nConsole output.\n\nexport\\_html(*\\**, *theme=None*, *clear=True*, *code\\_format=None*, *inline\\_styles=False*)[source]¶\nGenerate HTML from console contents (requires record=True argument in constructor).\n\nParameters\n* **theme** (*TerminalTheme**,* *optional*) – TerminalTheme object containing console colors.\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`.\n* **code\\_format** (*str**,* *optional*) – Format string to render HTML. In addition to ‘{foreground}’,\n‘{background}’, and ‘{code}’, should contain ‘{stylesheet}’ if inline\\_styles is `False`.\n* **inline\\_styles** (*bool**,* *optional*) – If `True` styles will be inlined in to spans, which makes files\nlarger but easier to cut and paste markup. If `False`, styles will be embedded in a style tag.\nDefaults to False.\n\nReturns\nString containing console contents as HTML.\n\nexport\\_svg(*\\**, *title='Rich'*, *theme=None*, *clear=True*, *code\\_format='<svg class=\"rich-terminal\" viewBox=\"0 0 {width} {height}\" xmlns=\"http://www.w3.org/2000/svg\">\\n    <!-- Generated with Rich https://www.textualize.io -->\\n    <style>\\n\\n    @font-face {{\\n        font-family: \"Fira Code\";\\n        src: local(\"FiraCode-Regular\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Regular.woff2\") format(\"woff2\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Regular.woff\") format(\"woff\");\\n        font-style: normal;\\n        font-weight: 400;\\n    }}\\n    @font-face {{\\n        font-family: \"Fira Code\";\\n        src: local(\"FiraCode-Bold\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Bold.woff2\") format(\"woff2\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Bold.woff\") format(\"woff\");\\n        font-style: bold;\\n        font-weight: 700;\\n    }}\\n\\n    .{unique\\_id}-matrix {{\\n        font-family: Fira Code, monospace;\\n        font-size: {char\\_height}px;\\n        line-height: {line\\_height}px;\\n        font-variant-east-asian: full-width;\\n    }}\\n\\n    .{unique\\_id}-title {{\\n        font-size: 18px;\\n        font-weight: bold;\\n        font-family: arial;\\n    }}\\n\\n    {styles}\\n    </style>\\n\\n    <defs>\\n    <clipPath id=\"{unique\\_id}-clip-terminal\">\\n      <rect x=\"0\" y=\"0\" width=\"{terminal\\_width}\" height=\"{terminal\\_height}\" />\\n    </clipPath>\\n    {lines}\\n    </defs>\\n\\n    {chrome}\\n    <g transform=\"translate({terminal\\_x}, {terminal\\_y})\" clip-path=\"url(#{unique\\_id}-clip-terminal)\">\\n    {backgrounds}\\n    <g class=\"{unique\\_id}-matrix\">\\n    {matrix}\\n    </g>\\n    </g>\\n</svg>\\n'*, *font\\_aspect\\_ratio=0.61*, *unique\\_id=None*)[source]¶\nGenerate an SVG from the console contents (requires record=True in Console constructor).\n\nParameters\n* **title** (*str**,* *optional*) – The title of the tab in the output image\n* **theme** (*TerminalTheme**,* *optional*) – The `TerminalTheme` object to use to style the terminal\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`\n* **code\\_format** (*str**,* *optional*) – Format string used to generate the SVG. Rich will inject a number of variables\ninto the string in order to form the final SVG output. The default template used and the variables\ninjected by Rich can be found by inspecting the `console.CONSOLE\\_SVG\\_FORMAT` variable.\n* **font\\_aspect\\_ratio** (*float**,* *optional*) – The width to height ratio of the font used in the `code\\_format`\nstring. Defaults to 0.61, which is the width to height ratio of Fira Code (the default font).\nIf you aren’t specifying a different font inside `code\\_format`, you probably don’t need this.\n* **unique\\_id** (*str**,* *optional*) – unique id that is used as the prefix for various elements (CSS styles, node\nids). If not set, this defaults to a computed value based on the recorded content.\n\nexport\\_text(*\\**, *clear=True*, *styles=False*)[source]¶\nGenerate text from console contents (requires record=True argument in constructor).\n\nParameters\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`.\n* **styles** (*bool**,* *optional*) – If `True`, ansi escape codes will be included. `False` for plain text.\nDefaults to `False`.\n\nReturns\nString containing console contents.\n\n*property* file*: IO[str]*¶\nGet the file object to write to.\n\nget\\_style(*name*, *\\**, *default=None*)[source]¶\nGet a Style instance by its theme name or parse a definition.\n\nParameters\n* **name** (*str*) – The name of a style or a style definition.\n* **default** (*Optional**[**Union**[**str**,* *Style**]**]*) –\n\nReturns\nA Style object.\n\nReturn type\nStyle\n\nRaises\n**MissingStyle** – If no style could be parsed from name.\n\n*property* height*: int*¶\nGet the height of the console.\n\nReturns\nThe height (in lines) of the console.\n\ninput(*prompt=''*, *\\**, *markup=True*, *emoji=True*, *password=False*, *stream=None*)[source]¶\nDisplays a prompt and waits for input from the user. The prompt may contain color / style.\n\n\nIt works in the same way as Python’s builtin `input()` function and provides elaborate line editing and history features if Python’s builtin `readline` module is previously loaded.\n\nParameters\n* **prompt** (*Union**[**str**,* *Text**]*) – Text to render in the prompt.\n* **markup** (*bool**,* *optional*) – Enable console markup (requires a str prompt). Defaults to True.\n* **emoji** (*bool**,* *optional*) – Enable emoji (requires a str prompt). Defaults to True.\n* **password** (*bool*) – (bool, optional): Hide typed text. Defaults to False.\n* **stream** (*Optional**[**TextIO**]*) – (TextIO, optional): Optional file to read input from (rather than stdin). Defaults to None.\n\nReturns\nText read from stdin.\n\n*property* is\\_alt\\_screen*: bool*¶\nCheck if the alt screen was enabled.\n\nReturns\nTrue if the alt screen was enabled, otherwise False.\n\nReturn type\nbool\n\n*property* is\\_dumb\\_terminal*: bool*¶\nDetect dumb terminal.\n\nReturns\nTrue if writing to a dumb terminal, otherwise False.\n\n*property* is\\_terminal*: bool*¶\nCheck if the console is writing to a terminal.\n\nReturns\nTrue if the console writing to a device capable of\nunderstanding terminal codes, otherwise False.\n\nline(*count=1*)[source]¶\nWrite new line(s).\n\nParameters\n**count** (*int**,* *optional*) – Number of new lines. Defaults to 1.\n\nlog(*\\*objects*, *sep=' '*, *end='\\n'*, *style=None*, *justify=None*, *emoji=None*, *markup=None*, *highlight=None*, *log\\_locals=False*, *\\_stack\\_offset=1*)[source]¶\nLog rich content to the terminal.\n\nParameters\n* **objects** (*positional args*) – Objects to log to the terminal.\n* **sep** (*str**,* *optional*) – String to write between print data. Defaults to ” “.\n* **end** (*str**,* *optional*) – String to write at end of print data. Defaults to “\\n”.\n* **style** (*Union**[**str**,* *Style**]**,* *optional*) – A style to apply to output. Defaults to None.\n* **justify** (*str**,* *optional*) – One of “left”, “right”, “center”, or “full”. Defaults to `None`.\n* **emoji** (*Optional**[**bool**]**,* *optional*) – Enable emoji code, or `None` to use console default. Defaults to None.\n* **markup** (*Optional**[**bool**]**,* *optional*) – Enable markup, or `None` to use console default. Defaults to None.\n* **highlight** (*Optional**[**bool**]**,* *optional*) – Enable automatic highlighting, or `None` to use console default. Defaults to None.\n* **log\\_locals** (*bool**,* *optional*) – Boolean to enable logging of locals where `log()`\nwas called. Defaults to False.\n* **\\_stack\\_offset** (*int**,* *optional*) – Offset of caller from end of call stack. Defaults to 1.\n\nmeasure(*renderable*, *\\**, *options=None*)[source]¶\nMeasure a renderable. Returns a `Measurement` object which contains\ninformation regarding the number of characters required to print the renderable.\n\nParameters\n* **renderable** (*RenderableType*) – Any renderable or string.\n* **options** (*Optional**[**ConsoleOptions**]**,* *optional*) – Options to use when measuring, or None\nto use default options. Defaults to None.\n\nReturns\nA measurement of the renderable.\n\n*property* options*: ConsoleOptions*¶\nGet default console options.\n\nout(*\\*objects*, *sep=' '*, *end='\\n'*, *style=None*, *highlight=None*)[source]¶\nOutput to the terminal. This is a low-level way of writing to the terminal which unlike\n`print()` won’t pretty print, wrap text, or apply markup, but will\noptionally apply highlighting and a basic style.\n\nParameters\n* **sep** (*str**,* *optional*) – String to write between print data. Defaults to ” “.\n* **end** (*str**,* *optional*) – String to write at end of print data. Defaults to “\\n”.\n* **style** (*Union**[**str**,* *Style**]**,* *optional*) – A style to apply to output. Defaults to None.\n* **highlight** (*Optional**[**bool**]**,* *optional*) – Enable automatic highlighting, or `None` to use\nconsole default. Defaults to `None`.\n* **objects** (*Any*) –\n\npager(*pager=None*, *styles=False*, *links=False*)[source]¶\nA context manager to display anything printed within a “pager”. The pager application\nis defined by the system and will typically support at least pressing a key to scroll.\n\nParameters\n* **pager** (*Pager**,* *optional*) – A pager object, or None to use `SystemPager`. Defaults to None.\n* **styles** (*bool**,* *optional*) – Show styles in pager. Defaults to False.\n* **links** (*bool**,* *optional*) – Show links in pager. Defaults to False.\n\nReturn type\n*PagerContext*\n\n```\n>>> from rich.console import Console\n>>> from rich.\\_\\_main\\_\\_ import make\\_test\\_card\n>>> console = Console()\n>>> with console.pager():\n console.print(make\\_test\\_card())\n\nReturns\nA context manager.\n\nReturn type\nPagerContext\n\nParameters\n* **pager** (*Optional**[**Pager**]*) –\n* **styles** (*bool*) –\n* **links** (*bool*) –\n\npop\\_render\\_hook()[source]¶\nPop the last renderhook from the stack.\n\npop\\_theme()[source]¶\nRemove theme from top of stack, restoring previous theme.\n\nprint(*\\*objects*, *sep=' '*, *end='\\n'*, *style=None*, *justify=None*, *overflow=None*, *no\\_wrap=None*, *emoji=None*, *markup=None*, *highlight=None*, *width=None*, *height=None*, *crop=True*, *soft\\_wrap=None*, *new\\_line\\_start=False*)[source]¶\nPrint to the console.\n\nParameters\n* **objects** (*positional args*) – Objects to log to the terminal.\n* **sep** (*str**,* *optional*) – String to write between print data. Defaults to ” “.\n* **end** (*str**,* *optional*) – String to write at end of print data. Defaults to “\\n”.\n* **style** (*Union**[**str**,* *Style**]**,* *optional*) – A style to apply to output. Defaults to None.\n* **justify** (*str**,* *optional*) – Justify method: “default”, “left”, “right”, “center”, or “full”. Defaults to `None`.\n* **overflow** (*str**,* *optional*) – Overflow method: “ignore”, “crop”, “fold”, or “ellipsis”. Defaults to None.\n* **no\\_wrap** (*Optional**[**bool**]**,* *optional*) – Disable word wrapping. Defaults to None.\n* **emoji** (*Optional**[**bool**]**,* *optional*) – Enable emoji code, or `None` to use console default. Defaults to `None`.\n* **markup** (*Optional**[**bool**]**,* *optional*) – Enable markup, or `None` to use console default. Defaults to `None`.\n* **highlight** (*Optional**[**bool**]**,* *optional*) – Enable automatic highlighting, or `None` to use console default. Defaults to `None`.\n* **width** (*Optional**[**int**]**,* *optional*) – Width of output, or `None` to auto-detect. Defaults to `None`.\n* **crop** (*Optional**[**bool**]**,* *optional*) – Crop output to width of terminal. Defaults to True.\n* **soft\\_wrap** (*bool**,* *optional*) – Enable soft wrap mode which disables word wrapping and cropping of text or `None` for\nConsole default. Defaults to `None`.\n* **new\\_line\\_start** (*bool**,* *False*) – Insert a new line at the start if the output contains more than one line. Defaults to `False`.\n* **height** (*Optional**[**int**]*) –\n\nprint\\_exception(*\\**, *width=100*, *extra\\_lines=3*, *theme=None*, *word\\_wrap=False*, *show\\_locals=False*, *suppress=()*, *max\\_frames=100*)[source]¶\nPrints a rich render of the last exception and traceback.\n\nParameters\n* **width** (*Optional**[**int**]**,* *optional*) – Number of characters used to render code. Defaults to 100.\n* **extra\\_lines** (*int**,* *optional*) – Additional lines of code to render. Defaults to 3.\n* **theme** (*str**,* *optional*) – Override pygments theme used in traceback\n* **word\\_wrap** (*bool**,* *optional*) – Enable word wrapping of long lines. Defaults to False.\n* **show\\_locals** (*bool**,* *optional*) – Enable display of local variables. Defaults to False.\n* **suppress** (*Iterable**[**Union**[**str**,* *ModuleType**]**]*) – Optional sequence of modules or paths to exclude from traceback.\n* **max\\_frames** (*int*) – Maximum number of frames to show in a traceback, 0 for no maximum. Defaults to 100.\n\nprint\\_json(*json=None*, *\\**, *data=None*, *indent=2*, *highlight=True*, *skip\\_keys=False*, *ensure\\_ascii=False*, *check\\_circular=True*, *allow\\_nan=True*, *default=None*, *sort\\_keys=False*)[source]¶\nPretty prints JSON. Output will be valid JSON.\n\nParameters\n* **json** (*Optional**[**str**]*) – A string containing JSON.\n* **data** (*Any*) – If json is not supplied, then encode this data.\n* **indent** (*Union**[**None**,* *int**,* *str**]**,* *optional*) – Number of spaces to indent. Defaults to 2.\n* **highlight** (*bool**,* *optional*) – Enable highlighting of output: Defaults to True.\n* **skip\\_keys** (*bool**,* *optional*) – Skip keys not of a basic type. Defaults to False.\n* **ensure\\_ascii** (*bool**,* *optional*) – Escape all non-ascii characters. Defaults to False.\n* **check\\_circular** (*bool**,* *optional*) – Check for circular references. Defaults to True.\n* **allow\\_nan** (*bool**,* *optional*) – Allow NaN and Infinity values. Defaults to True.\n* **default** (*Callable**,* *optional*) – A callable that converts values that can not be encoded\nin to something that can be JSON encoded. Defaults to None.\n* **sort\\_keys** (*bool**,* *optional*) – Sort dictionary keys. Defaults to False.\n\npush\\_render\\_hook(*hook*)[source]¶\nAdd a new render hook to the stack.\n\nParameters\n**hook** (*RenderHook*) – Render hook instance.\n\npush\\_theme(*theme*, *\\**, *inherit=True*)[source]¶\nPush a new theme on to the top of the stack, replacing the styles from the previous theme.\nGenerally speaking, you should call `use\\_theme()` to get a context manager, rather\nthan calling this method directly.\n\nParameters\n* **theme** (*Theme*) – A theme instance.\n* **inherit** (*bool**,* *optional*) – Inherit existing styles. Defaults to True.\n\nrender(*renderable*, *options=None*)[source]¶\nRender an object in to an iterable of Segment instances.\n\n\nThis method contains the logic for rendering objects with the console protocol.\nYou are unlikely to need to use it directly, unless you are extending the library.\n\nParameters\n* **renderable** (*RenderableType*) – An object supporting the console protocol, or\nan object that may be converted to a string.\n* **options** (*ConsoleOptions**,* *optional*) – An options object, or None to use self.options. Defaults to None.\n\nReturns\nAn iterable of segments that may be rendered.\n\nrender\\_lines(*renderable*, *options=None*, *\\**, *style=None*, *pad=True*, *new\\_lines=False*)[source]¶\nRender objects in to a list of lines.\n\n> \n> The output of render\\_lines is useful when further formatting of rendered console text\n> is required, such as the Panel class which draws a border around any renderable object.\n> \n> \n> \n> Args:renderable (RenderableType): Any object renderable in the console.\n> options (Optional[ConsoleOptions], optional): Console options, or None to use self.options. Default to `None`.\n> style (Style, optional): Optional style to apply to renderables. Defaults to `None`.\n> pad (bool, optional): Pad lines shorter than render width. Defaults to `True`.\n> new\\_lines (bool, optional): Include “\n> \n> \n> \n> \n> \n\n\n” characters at end of lines.\n\n> \n> \n> Returns:List[List[Segment]]: A list of lines, where a line is a list of Segment objects.\n> \n> \n> \n> \n> \n\nParameters\n* **renderable** (*Union**[**ConsoleRenderable**,* *RichCast**,* *str**]*) –\n* **options** (*Optional**[**ConsoleOptions**]*) –\n* **style** (*Optional**[**Style**]*) –\n* **pad** (*bool*) –\n* **new\\_lines** (*bool*) –\n\nrender\\_str(*text*, *\\**, *style=''*, *justify=None*, *overflow=None*, *emoji=None*, *markup=None*, *highlight=None*, *highlighter=None*)[source]¶\nConvert a string to a Text instance. This is called automatically if\nyou print or log a string.\n\nParameters\n* **text** (*str*) – Text to render.\n* **style** (*Union**[**str**,* *Style**]**,* *optional*) – Style to apply to rendered text.\n* **justify** (*str**,* *optional*) – Justify method: “default”, “left”, “center”, “full”, or “right”. Defaults to `None`.\n* **overflow** (*str**,* *optional*) – Overflow method: “crop”, “fold”, or “ellipsis”. Defaults to `None`.\n* **emoji** (*Optional**[**bool**]**,* *optional*) – Enable emoji, or `None` to use Console default.\n* **markup** (*Optional**[**bool**]**,* *optional*) – Enable markup, or `None` to use Console default.\n* **highlight** (*Optional**[**bool**]**,* *optional*) – Enable highlighting, or `None` to use Console default.\n* **highlighter** (*HighlighterType**,* *optional*) – Optional highlighter to apply.\n\nReturns\nRenderable object.\n\nReturn type\nConsoleRenderable\n\nrule(*title=''*, *\\**, *characters='─'*, *style='rule.line'*, *align='center'*)[source]¶\nDraw a line with optional centered title.\n\nParameters\n* **title** (*str**,* *optional*) – Text to render over the rule. Defaults to “”.\n* **characters** (*str**,* *optional*) – Character(s) to form the line. Defaults to “─”.\n* **style** (*str**,* *optional*) – Style of line. Defaults to “rule.line”.\n* **align** (*str**,* *optional*) – How to align the title, one of “left”, “center”, or “right”. Defaults to “center”.\n\nsave\\_html(*path*, *\\**, *theme=None*, *clear=True*, *code\\_format='<!DOCTYPE html>\\n<html>\\n<head>\\n<meta charset=\"UTF-8\">\\n<style>\\n{stylesheet}\\nbody {{\\n    color: {foreground};\\n    background-color: {background};\\n}}\\n</style>\\n</head>\\n<body>\\n    <pre style=\"font-family:Menlo,\\'DejaVu Sans Mono\\',consolas,\\'Courier New\\',monospace\"><code>{code}</code></pre>\\n</body>\\n</html>\\n'*, *inline\\_styles=False*)[source]¶\nGenerate HTML from console contents and write to a file (requires record=True argument in constructor).\n\nParameters\n* **path** (*str*) – Path to write html file.\n* **theme** (*TerminalTheme**,* *optional*) – TerminalTheme object containing console colors.\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`.\n* **code\\_format** (*str**,* *optional*) – Format string to render HTML. In addition to ‘{foreground}’,\n‘{background}’, and ‘{code}’, should contain ‘{stylesheet}’ if inline\\_styles is `False`.\n* **inline\\_styles** (*bool**,* *optional*) – If `True` styles will be inlined in to spans, which makes files\nlarger but easier to cut and paste markup. If `False`, styles will be embedded in a style tag.\nDefaults to False.\n\nsave\\_svg(*path*, *\\**, *title='Rich'*, *theme=None*, *clear=True*, *code\\_format='<svg class=\"rich-terminal\" viewBox=\"0 0 {width} {height}\" xmlns=\"http://www.w3.org/2000/svg\">\\n    <!-- Generated with Rich https://www.textualize.io -->\\n    <style>\\n\\n    @font-face {{\\n        font-family: \"Fira Code\";\\n        src: local(\"FiraCode-Regular\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Regular.woff2\") format(\"woff2\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Regular.woff\") format(\"woff\");\\n        font-style: normal;\\n        font-weight: 400;\\n    }}\\n    @font-face {{\\n        font-family: \"Fira Code\";\\n        src: local(\"FiraCode-Bold\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Bold.woff2\") format(\"woff2\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Bold.woff\") format(\"woff\");\\n        font-style: bold;\\n        font-weight: 700;\\n    }}\\n\\n    .{unique\\_id}-matrix {{\\n        font-family: Fira Code, monospace;\\n        font-size: {char\\_height}px;\\n        line-height: {line\\_height}px;\\n        font-variant-east-asian: full-width;\\n    }}\\n\\n    .{unique\\_id}-title {{\\n        font-size: 18px;\\n        font-weight: bold;\\n        font-family: arial;\\n    }}\\n\\n    {styles}\\n    </style>\\n\\n    <defs>\\n    <clipPath id=\"{unique\\_id}-clip-terminal\">\\n      <rect x=\"0\" y=\"0\" width=\"{terminal\\_width}\" height=\"{terminal\\_height}\" />\\n    </clipPath>\\n    {lines}\\n    </defs>\\n\\n    {chrome}\\n    <g transform=\"translate({terminal\\_x}, {terminal\\_y})\" clip-path=\"url(#{unique\\_id}-clip-terminal)\">\\n    {backgrounds}\\n    <g class=\"{unique\\_id}-matrix\">\\n    {matrix}\\n    </g>\\n    </g>\\n</svg>\\n'*, *font\\_aspect\\_ratio=0.61*, *unique\\_id=None*)[source]¶\nGenerate an SVG file from the console contents (requires record=True in Console constructor).\n\nParameters\n* **path** (*str*) – The path to write the SVG to.\n* **title** (*str**,* *optional*) – The title of the tab in the output image\n* **theme** (*TerminalTheme**,* *optional*) – The `TerminalTheme` object to use to style the terminal\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`\n* **code\\_format** (*str**,* *optional*) – Format string used to generate the SVG. Rich will inject a number of variables\ninto the string in order to form the final SVG output. The default template used and the variables\ninjected by Rich can be found by inspecting the `console.CONSOLE\\_SVG\\_FORMAT` variable.\n* **font\\_aspect\\_ratio** (*float**,* *optional*) – The width to height ratio of the font used in the `code\\_format`\nstring. Defaults to 0.61, which is the width to height ratio of Fira Code (the default font).\nIf you aren’t specifying a different font inside `code\\_format`, you probably don’t need this.\n* **unique\\_id** (*str**,* *optional*) – unique id that is used as the prefix for various elements (CSS styles, node\nids). If not set, this defaults to a computed value based on the recorded content.\n\nsave\\_text(*path*, *\\**, *clear=True*, *styles=False*)[source]¶\nGenerate text from console and save to a given location (requires record=True argument in constructor).\n\nParameters\n* **path** (*str*) – Path to write text files.\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`.\n* **styles** (*bool**,* *optional*) – If `True`, ansi style codes will be included. `False` for plain text.\nDefaults to `False`.\n\nscreen(*hide\\_cursor=True*, *style=None*)[source]¶\nContext manager to enable and disable ‘alternative screen’ mode.\n\nParameters\n* **hide\\_cursor** (*bool**,* *optional*) – Also hide the cursor. Defaults to False.\n* **style** (*Style**,* *optional*) – Optional style for screen. Defaults to None.\n\nReturns\nContext which enables alternate screen on enter, and disables it on exit.\n\nReturn type\n~ScreenContext\n\nset\\_alt\\_screen(*enable=True*)[source]¶\nEnables alternative screen mode.\n\n\nNote, if you enable this mode, you should ensure that is disabled before\nthe application exits. See `screen()` for a context manager\nthat handles this for you.\n\nParameters\n**enable** (*bool**,* *optional*) – Enable (True) or disable (False) alternate screen. Defaults to True.\n\nReturns\nTrue if the control codes were written.\n\nset\\_live(*live*)[source]¶\nSet Live instance. Used by Live context manager.\n\nParameters\n**live** (*Live*) – Live instance using this Console.\n\nRaises\n**errors.LiveError** – If this Console has a Live context currently active.\n\nset\\_window\\_title(*title*)[source]¶\nSet the title of the console terminal window.\n\n\nWarning: There is no means within Rich of “resetting” the window title to its\nprevious value, meaning the title you set will persist even after your application\nexits.\n\n\n`fish` shell resets the window title before and after each command by default,\nnegating this issue. Windows Terminal and command prompt will also reset the title for you.\nMost other shells and terminals, however, do not do this.\n\n\nSome terminals may require configuration changes before you can set the title.\nSome terminals may not support setting the title at all.\n\n\nOther software (including the terminal itself, the shell, custom prompts, plugins, etc.)\nmay also set the terminal window title. This could result in whatever value you write\nusing this method being overwritten.\n\nParameters\n**title** (*str*) – The new title of the terminal window.\n\nTrue if the control code to change the terminal title waswritten, otherwise False. Note that a return value of True\ndoes not guarantee that the window title has actually changed,\nsince the feature may be unsupported/disabled in some terminals.\n\n\nReturn type\nbool\n\nshow\\_cursor(*show=True*)[source]¶\nShow or hide the cursor.\n\nParameters\n**show** (*bool**,* *optional*) – Set visibility of the cursor.\n\n*property* size*: ConsoleDimensions*¶\nGet the size of the console.\n\nReturns\nA named tuple containing the dimensions.\n\nReturn type\nConsoleDimensions\n\nstatus(*status*, *\\**, *spinner='dots'*, *spinner\\_style='status.spinner'*, *speed=1.0*, *refresh\\_per\\_second=12.5*)[source]¶\nDisplay a status and spinner.\n\nParameters\n* **status** (*RenderableType*) – A status renderable (str or Text typically).\n* **spinner** (*str**,* *optional*) – Name of spinner animation (see python -m rich.spinner). Defaults to “dots”.\n* **spinner\\_style** (*StyleType**,* *optional*) – Style of spinner. Defaults to “status.spinner”.\n* **speed** (*float**,* *optional*) – Speed factor for spinner animation. Defaults to 1.0.\n* **refresh\\_per\\_second** (*float**,* *optional*) – Number of refreshes per second. Defaults to 12.5.\n\nReturns\nA Status object that may be used as a context manager.\n\nReturn type\nStatus\n\nupdate\\_screen(*renderable*, *\\**, *region=None*, *options=None*)[source]¶\nUpdate the screen at a given offset.\n\nParameters\n* **renderable** (*RenderableType*) – A Rich renderable.\n* **region** (*Region**,* *optional*) – Region of screen to update, or None for entire screen. Defaults to None.\n* **x** (*int**,* *optional*) – x offset. Defaults to 0.\n* **y** (*int**,* *optional*) – y offset. Defaults to 0.\n* **options** (*Optional**[**ConsoleOptions**]*) –\n\nRaises\n**errors.NoAltScreen** – If the Console isn’t in alt screen mode.\n\nupdate\\_screen\\_lines(*lines*, *x=0*, *y=0*)[source]¶\nUpdate lines of the screen at a given offset.\n\nParameters\n* **lines** (*List**[**List**[**Segment**]**]*) – Rendered lines (as produced by `render\\_lines()`).\n* **x** (*int**,* *optional*) – x offset (column no). Defaults to 0.\n* **y** (*int**,* *optional*) – y offset (column no). Defaults to 0.\n\nuse\\_theme(*theme*, *\\**, *inherit=True*)[source]¶\nUse a different theme for the duration of the context manager.\n\nParameters\n* **theme** (*Theme*) – Theme instance to user.\n* **inherit** (*bool**,* *optional*) – Inherit existing console styles. Defaults to True.\n\nReturns\n[description]\n\nReturn type\nThemeContext\n\n*property* width*: int*¶\nGet the width of the console.\n\nReturns\nThe width (in characters) of the console.\n\n\n*class* rich.console.ConsoleDimensions(*width*, *height*)[source]¶\nSize of the terminal.\n\nParameters\n* **width** (*int*) –\n* **height** (*int*) –\n\n\n*property* height¶\nThe height of the console in lines.\n\n*property* width¶\nThe width of the console in ‘cells’.\n\n\n*class* rich.console.ConsoleOptions(*size*, *legacy\\_windows*, *min\\_width*, *max\\_width*, *is\\_terminal*, *encoding*, *max\\_height*, *justify=None*, *overflow=None*, *no\\_wrap=False*, *highlight=None*, *markup=None*, *height=None*)[source]¶\nOptions for \\_\\_rich\\_console\\_\\_ method.\n\nParameters\n* **size** (*ConsoleDimensions*) –\n* **legacy\\_windows** (*bool*) –\n* **min\\_width** (*int*) –\n* **max\\_width** (*int*) –\n* **is\\_terminal** (*bool*) –\n* **encoding** (*str*) –\n* **max\\_height** (*int*) –\n* **justify** (*Optional**[**typing\\_extensions.Literal**[**default**,* *left**,* *center**,* *right**,* *full**]**]*) –\n* **overflow** (*Optional**[**typing\\_extensions.Literal**[**fold**,* *crop**,* *ellipsis**,* *ignore**]**]*) –\n* **no\\_wrap** (*Optional**[**bool**]*) –\n* **highlight** (*Optional**[**bool**]*) –\n* **markup** (*Optional**[**bool**]*) –\n* **height** (*Optional**[**int**]*) –\n\n\n*property* ascii\\_only*: bool*¶\nCheck if renderables should use ascii only.\n\ncopy()[source]¶\nReturn a copy of the options.\n\nReturns\na copy of self.\n\nReturn type\nConsoleOptions\n\nencoding*: str*¶\nEncoding of terminal.\n\nhighlight*: Optional[bool]* *= None*¶\nHighlight override for render\\_str.\n\nis\\_terminal*: bool*¶\nTrue if the target is a terminal, otherwise False.\n\njustify*: Optional[typing\\_extensions.Literal[default, left, center, right, full]]* *= None*¶\nJustify value override for renderable.\n\nlegacy\\_windows*: bool*¶\nflag for legacy windows.\n\nType\nlegacy\\_windows\n\nmarkup*: Optional[bool]* *= None*¶\nEnable markup when rendering strings.\n\nmax\\_height*: int*¶\nHeight of container (starts as terminal)\n\nmax\\_width*: int*¶\nMaximum width of renderable.\n\nmin\\_width*: int*¶\nMinimum width of renderable.\n\nno\\_wrap*: Optional[bool]* *= False*¶\nDisable wrapping for text.\n\noverflow*: Optional[typing\\_extensions.Literal[fold, crop, ellipsis, ignore]]* *= None*¶\nOverflow value override for renderable.\n\nreset\\_height()[source]¶\nReturn a copy of the options with height set to `None`.\n\nReturns\nNew console options instance.\n\nReturn type\n~ConsoleOptions\n\nsize*: ConsoleDimensions*¶\nSize of console.\n\nupdate(*\\**, *width=<rich.console.NoChange object>*, *min\\_width=<rich.console.NoChange object>*, *max\\_width=<rich.console.NoChange object>*, *justify=<rich.console.NoChange object>*, *overflow=<rich.console.NoChange object>*, *no\\_wrap=<rich.console.NoChange object>*, *highlight=<rich.console.NoChange object>*, *markup=<rich.console.NoChange object>*, *height=<rich.console.NoChange object>*)[source]¶\nUpdate values, return a copy.\n\nParameters\n* **width** (*Union**[**int**,* *NoChange**]*) –\n* **min\\_width** (*Union**[**int**,* *NoChange**]*) –\n* **max\\_width** (*Union**[**int**,* *NoChange**]*) –\n* **justify** (*Union**[**typing\\_extensions.Literal**[**default**,* *left**,* *center**,* *right**,* *full**]**,* *None**,* *NoChange**]*) –\n* **overflow** (*Union**[**typing\\_extensions.Literal**[**fold**,* *crop**,* *ellipsis**,* *ignore**]**,* *None**,* *NoChange**]*) –\n* **no\\_wrap** (*Union**[**bool**,* *None**,* *NoChange**]*) –\n* **highlight** (*Union**[**bool**,* *None**,* *NoChange**]*) –\n* **markup** (*Union**[**bool**,* *None**,* *NoChange**]*) –\n* **height** (*Union**[**int**,* *None**,* *NoChange**]*) –\n\nReturn type\n*ConsoleOptions*\n\nupdate\\_dimensions(*width*, *height*)[source]¶\nUpdate the width and height, and return a copy.\n\nParameters\n* **width** (*int*) – New width (sets both min\\_width and max\\_width).\n* **height** (*int*) – New height.\n\nupdate\\_height(*height*)[source]¶\nUpdate the height, and return a copy.\n\nParameters\n**height** (*int*) – New height\n\nReturns\nNew Console options instance.\n\nupdate\\_width(*width*)[source]¶\nUpdate just the width, return a copy.\n\nParameters\n**width** (*int*) – New width (sets both min\\_width and max\\_width)\n\n\n*class* rich.console.ConsoleRenderable(*\\*args*, *\\*\\*kwds*)[source]¶\nAn object that supports the console protocol.\n\n*class* rich.console.ConsoleThreadLocals(*theme\\_stack*, *buffer=<factory>*, *buffer\\_index=0*)[source]¶\nThread local values for Console context.\n\nParameters\n* **theme\\_stack** (*ThemeStack*) –\n* **buffer** (*List**[**Segment**]*) –\n* **buffer\\_index** (*int*) –\n\n*class* rich.console.Group(*\\*renderables*, *fit=True*)[source]¶\nTakes a group of renderables and returns a renderable object that renders the group.\n\nParameters\n* **renderables** (*Iterable**[**RenderableType**]*) – An iterable of renderable objects.\n* **fit** (*bool**,* *optional*) – Fit dimension of group to contents, or fill available space. Defaults to True.\n\n*class* rich.console.NewLine(*count=1*)[source]¶\nA renderable to generate new line(s)\n\nParameters\n**count** (*int*) – \n\n*class* rich.console.PagerContext(*console*, *pager=None*, *styles=False*, *links=False*)[source]¶\nA context manager that ‘pages’ content. See `pager()` for usage.\n\nParameters\n* **console** (*Console*) –\n* **pager** (*Optional**[**Pager**]*) –\n* **styles** (*bool*) –\n* **links** (*bool*) –\n\n*class* rich.console.RenderHook[source]¶\nProvides hooks in to the render process.\n\n\n*abstract* process\\_renderables(*renderables*)[source]¶\nCalled with a list of objects to render.\n\n\nThis method can return a new list of renderables, or modify and return the same list.\n\nParameters\n**renderables** (*List**[**ConsoleRenderable**]*) – A number of renderable objects.\n\nReturns\nA replacement list of renderables.\n\nReturn type\nList[ConsoleRenderable]\n\n\nrich.console.RenderableType¶\nA string or any object that may be rendered by Rich.\n\n\nalias of `Union`[`ConsoleRenderable`, `RichCast`, `str`]\n\n*class* rich.console.RichCast(*\\*args*, *\\*\\*kwds*)[source]¶\nAn object that may be ‘cast’ to a console renderable.\n\n*class* rich.console.ScreenContext(*console*, *hide\\_cursor*, *style=''*)[source]¶\nA context manager that enables an alternative screen. See `screen()` for usage.\n\nParameters\n* **console** (*Console*) –\n* **hide\\_cursor** (*bool*) –\n* **style** (*Union**[**str**,* *Style**]*) –\n\n\nupdate(*\\*renderables*, *style=None*)[source]¶\nUpdate the screen.\n\nParameters\n* **renderable** (*RenderableType**,* *optional*) – Optional renderable to replace current renderable,\nor None for no change. Defaults to None.\n* **style** (*Optional**[**Union**[**str**,* *Style**]**]*) – (Style, optional): Replacement style, or None for no change. Defaults to None.\n* **renderables** (*Union**[**ConsoleRenderable**,* *RichCast**,* *str**]*) –\n\n\n*class* rich.console.ScreenUpdate(*lines*, *x*, *y*)[source]¶\nRender a list of lines at a given offset.\n\nParameters\n* **lines** (*List**[**List**[**Segment**]**]*) –\n* **x** (*int*) –\n* **y** (*int*) –\n\n*class* rich.console.ThemeContext(*console*, *theme*, *inherit=True*)[source]¶\nA context manager to use a temporary theme. See `use\\_theme()` for usage.\n\nParameters\n* **console** (*Console*) –\n* **theme** (*Theme*) –\n* **inherit** (*bool*) –\n\nrich.console.detect\\_legacy\\_windows()[source]¶\nDetect legacy Windows.\n\nrich.console.group(*fit=True*)[source]¶\nA decorator that turns an iterable of renderables in to a group.\n\nParameters\n**fit** (*bool**,* *optional*) – Fit dimension of group to contents, or fill available space. Defaults to True.\n\nReturn type\n*Callable*[[…], *Callable*[[…], *Group*]]\n\n\n# rich.highlighter¶\n\n\n# rich¶\n\n\n\n# rich.measure¶\n\n\n# rich.layout¶\n\n\n*class* rich.layout.ColumnSplitter[source]¶\nSplit a layout region in to columns.\n\n\ndivide(*children*, *region*)[source]¶\nDivide a region amongst several child layouts.\n\nParameters\n* **children** (*Sequence**(**Layout**)*) – A number of child layouts.\n* **region** (*Region*) – A rectangular region to divide.\n\nReturn type\n*Iterable*[*Tuple*[*Layout*, *Region*]]\n\nget\\_tree\\_icon()[source]¶\nGet the icon (emoji) used in layout.tree\n\n\n*class*\n\n==================\n Document 2 \n----------------\n rich.syntax¶\n\n\n*class* rich.syntax.Syntax(*code*, *lexer*, *\\**, *theme='monokai'*, *dedent=False*, *line\\_numbers=False*, *start\\_line=1*, *line\\_range=None*, *highlight\\_lines=None*, *code\\_width=None*, *tab\\_size=4*, *word\\_wrap=False*, *background\\_color=None*, *indent\\_guides=False*, *padding=0*)[source]¶\nConstruct a Syntax object to render syntax highlighted code.\n\nParameters\n* **code** (*str*) – Code to highlight.\n* **lexer** (*Lexer* *|* *str*) – Lexer to use (see https://pygments.org/docs/lexers/)\n* **theme** (*str**,* *optional*) – Color theme, aka Pygments style (see https://pygments.org/docs/styles/#getting-a-list-of-available-styles). Defaults to “monokai”.\n* **dedent** (*bool**,* *optional*) – Enable stripping of initial whitespace. Defaults to False.\n* **line\\_numbers** (*bool**,* *optional*) – Enable rendering of line numbers. Defaults to False.\n* **start\\_line** (*int**,* *optional*) – Starting number for line numbers. Defaults to 1.\n* **line\\_range** (*Tuple**[**int* *|* *None**,* *int* *|* *None**]**,* *optional*) – If given should be a tuple of the start and end line to render.\nA value of None in the tuple indicates the range is open in that direction.\n* **highlight\\_lines** (*Set**[**int**]*) – A set of line numbers to highlight.\n* **code\\_width** (*Optional**[**int**]*) – Width of code to render (not including line numbers), or `None` to use all available width.\n* **tab\\_size** (*int**,* *optional*) – Size of tabs. Defaults to 4.\n* **word\\_wrap** (*bool**,* *optional*) – Enable word wrapping.\n* **background\\_color** (*str**,* *optional*) – Optional background color, or None to use theme color. Defaults to None.\n* **indent\\_guides** (*bool**,* *optional*) – Show indent guides. Defaults to False.\n* **padding** (*PaddingDimensions*) – Padding to apply around the syntax. Defaults to 0 (no padding).\n\n\n*property* default\\_lexer*: Lexer*¶\nA Pygments Lexer to use if one is not specified or invalid.\n\n*classmethod* from\\_path(*path*, *encoding='utf-8'*, *lexer=None*, *theme='monokai'*, *dedent=False*, *line\\_numbers=False*, *line\\_range=None*, *start\\_line=1*, *highlight\\_lines=None*, *code\\_width=None*, *tab\\_size=4*, *word\\_wrap=False*, *background\\_color=None*, *indent\\_guides=False*, *padding=0*)[source]¶\nConstruct a Syntax object from a file.\n\nParameters\n* **path** (*str*) – Path to file to highlight.\n* **encoding** (*str*) – Encoding of file.\n* **lexer** (*str* *|* *Lexer**,* *optional*) – Lexer to use. If None, lexer will be auto-detected from path/file content.\n* **theme** (*str**,* *optional*) – Color theme, aka Pygments style (see https://pygments.org/docs/styles/#getting-a-list-of-available-styles). Defaults to “emacs”.\n* **dedent** (*bool**,* *optional*) – Enable stripping of initial whitespace. Defaults to True.\n* **line\\_numbers** (*bool**,* *optional*) – Enable rendering of line numbers. Defaults to False.\n* **start\\_line** (*int**,* *optional*) – Starting number for line numbers. Defaults to 1.\n* **line\\_range** (*Tuple**[**int**,* *int**]**,* *optional*) – If given should be a tuple of the start and end line to render.\n* **highlight\\_lines** (*Set**[**int**]*) – A set of line numbers to highlight.\n* **code\\_width** (*Optional**[**int**]*) – Width of code to render (not including line numbers), or `None` to use all available width.\n* **tab\\_size** (*int**,* *optional*) – Size of tabs. Defaults to 4.\n* **word\\_wrap** (*bool**,* *optional*) – Enable word wrapping of code.\n* **background\\_color** (*str**,* *optional*) – Optional background color, or None to use theme color. Defaults to None.\n* **indent\\_guides** (*bool**,* *optional*) – Show indent guides. Defaults to False.\n* **padding** (*PaddingDimensions*) – Padding to apply around the syntax. Defaults to 0 (no padding).\n\nReturns\nA Syntax object that may be printed to the console\n\nReturn type\n[Syntax]\n\n*classmethod* get\\_theme(*name*)[source]¶\nGet a syntax theme instance.\n\nParameters\n**name** (*Union**[**str**,* *SyntaxTheme**]*) – \n\nReturn type\n*SyntaxTheme*\n\n*classmethod* guess\\_lexer(*path*, *code=None*)[source]¶\nGuess the alias of the Pygments lexer to use based on a path and an optional string of code.\nIf code is supplied, it will use a combination of the code and the filename to determine the\nbest lexer to use. For example, if the file is `index.html` and the file contains Django\ntemplating syntax, then “html+django” will be returned. If the file is `index.html`, and no\ntemplating language is used, the “html” lexer will be used. If no string of code\nis supplied, the lexer will be chosen based on the file extension..\n\nParameters\n* **path** (*AnyStr*) – The path to the file containing the code you wish to know the lexer for.\n* **code** (*str**,* *optional*) – Optional string of code that will be used as a fallback if no lexer\nis found for the supplied path.\n\nReturns\nThe name of the Pygments lexer that best matches the supplied path/code.\n\nhighlight(*code*, *line\\_range=None*)[source]¶\nHighlight code and return a Text instance.\n\nParameters\n* **code** (*str*) – Code to highlight.\n* **line\\_range** (*Tuple**[**int**,* *int**]**,* *optional*) – Optional line range to highlight.\n\nReturns\nA text instance containing highlighted syntax.\n\n*property* lexer*: Optional[Lexer]*¶\nThe lexer for this syntax, or None if no lexer was found.\n\n\nTries to find the lexer by name if a string was passed to the constructor.\n\nstylize\\_range(*style*, *start*, *end*)[source]¶\nAdds a custom style on a part of the code, that will be applied to the syntax display when it’s rendered.\nLine numbers are 1-based, while column indexes are 0-based.\n\nParameters\n* **style** (*StyleType*) – The style to apply.\n* **start** (*Tuple**[**int**,* *int**]*) – The start of the range, in the form [line number, column index].\n* **end** (*Tuple**[**int**,* *int**]*) – The end of the range, in the form [line number, column index].\n\n\n# rich.theme¶\n\n\n# rich.text¶\n\n\n*class* rich.text.Text(*text=''*, *style=''*, *\\**, *justify=None*, *overflow=None*, *no\\_wrap=None*, *end='\\n'*, *tab\\_size=None*, *spans=None*)[source]¶\nText with color / style.\n\nParameters\n* **text** (*str**,* *optional*) – Default unstyled text. Defaults to “”.\n* **style** (*Union**[**str**,* *Style**]**,* *optional*) – Base style for text. Defaults to “”.\n* **justify** (*str**,* *optional*)\n\n==================\n Document 3 \n----------------\n rich.theme¶\n\n\n*class* rich.theme.Theme(*styles=None*, *inherit=True*)[source]¶\nA container for style information, used by `Console`.\n\nParameters\n* **styles** (*Dict**[**str**,* *Style**]**,* *optional*) – A mapping of style names on to styles. Defaults to None for a theme with no styles.\n* **inherit** (*bool**,* *optional*) – Inherit default styles. Defaults to True.\n\n\n*property* config*: str*¶\nGet contents of a config file for this theme.\n\n*classmethod* from\\_file(*config\\_file*, *source=None*, *inherit=True*)[source]¶\nLoad a theme from a text mode file.\n\nParameters\n* **config\\_file** (*IO**[**str**]*) – An open conf file.\n* **source** (*str**,* *optional*) – The filename of the open file. Defaults to None.\n* **inherit** (*bool**,* *optional*) – Inherit default styles. Defaults to True.\n\nReturns\nA New theme instance.\n\nReturn type\nTheme\n\n*classmethod* read(*path*, *inherit=True*, *encoding=None*)[source]¶\nRead a theme from a path.\n\nParameters\n* **path** (*str*) – Path to a config file readable by Python configparser module.\n* **inherit** (*bool**,* *optional*) – Inherit default styles. Defaults to True.\n* **encoding** (*str**,* *optional*) – Encoding of the config file. Defaults to None.\n\nReturns\nA new theme instance.\n\n# rich.abc¶\n\n\n*class* rich.abc.RichRenderable[source]¶\nAn abstract base class for Rich renderables.\n\n\nNote that there is no need to extend this class, the intended use is to check if an\nobject supports the Rich renderable protocol. For example:\n\n```\nif isinstance(my\\_object, RichRenderable):\n    console.print(my\\_object)\n\n```\n\n\n# rich.columns¶\n\n\n*class* rich.columns.Columns(*renderables=None*, *padding=(0, 1)*, *\\**, *width=None*, *expand=False*, *equal=False*, *column\\_first=False*, *right\\_to\\_left=False*, *align=None*, *title=None*)[source]¶\nDisplay renderables in neat columns.\n\nParameters\n* **renderables** (*Iterable**[**RenderableType**]*) – Any number of Rich renderables (including str).\n* **width** (*int**,* *optional*) – The desired width of the columns, or None to"
    },
    {
        "instruction": "Generate a Python program using the 'rich' API to display a stylish and colored digital clock that shows the current time. The program should update the time every second.",
        "api": "rich",
        "output": "# Install rich\n#!pip install rich\nimport time\nfrom rich.console import Console\n\n# Create a real-time digital clock\nconsole = Console()\nwhile True:\n    console.clear()\n    current_time = time.strftime(\"%H:%M:%S\")\n    console.print(current_time, style=\"bold yellow on green\")\n    time.sleep(1)\n\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n rich.console¶\n\n\n*class* rich.console.Capture(*console*)[source]¶\nContext manager to capture the result of printing to the console.\nSee `capture()` for how to use.\n\nParameters\n**console** (*Console*) – A console instance to capture output.\n\n\nget()[source]¶\nGet the result of the capture.\n\n\n*exception* rich.console.CaptureError[source]¶\nAn error in the Capture context manager.\n\n*class* rich.console.Console(*\\**, *color\\_system='auto'*, *force\\_terminal=None*, *force\\_jupyter=None*, *force\\_interactive=None*, *soft\\_wrap=False*, *theme=None*, *stderr=False*, *file=None*, *quiet=False*, *width=None*, *height=None*, *style=None*, *no\\_color=None*, *tab\\_size=8*, *record=False*, *markup=True*, *emoji=True*, *emoji\\_variant=None*, *highlight=True*, *log\\_time=True*, *log\\_path=True*, *log\\_time\\_format='[%X]'*, *highlighter=<rich.highlighter.ReprHighlighter object>*, *legacy\\_windows=None*, *safe\\_box=True*, *get\\_datetime=None*, *get\\_time=None*, *\\_environ=None*)[source]¶\nA high level console interface.\n\nParameters\n* **color\\_system** (*str**,* *optional*) – The color system supported by your terminal,\neither `\"standard\"`, `\"256\"` or `\"truecolor\"`. Leave as `\"auto\"` to autodetect.\n* **force\\_terminal** (*Optional**[**bool**]**,* *optional*) – Enable/disable terminal control codes, or None to auto-detect terminal. Defaults to None.\n* **force\\_jupyter** (*Optional**[**bool**]**,* *optional*) – Enable/disable Jupyter rendering, or None to auto-detect Jupyter. Defaults to None.\n* **force\\_interactive** (*Optional**[**bool**]**,* *optional*) – Enable/disable interactive mode, or None to auto detect. Defaults to None.\n* **soft\\_wrap** (*Optional**[**bool**]**,* *optional*) – Set soft wrap default on print method. Defaults to False.\n* **theme** (*Theme**,* *optional*) – An optional style theme object, or `None` for default theme.\n* **stderr** (*bool**,* *optional*) – Use stderr rather than stdout if `file` is not specified. Defaults to False.\n* **file** (*IO**,* *optional*) – A file object where the console should write to. Defaults to stdout.\n* **quiet** (*bool**,* *Optional*) – Boolean to suppress all output. Defaults to False.\n* **width** (*int**,* *optional*) – The width of the terminal. Leave as default to auto-detect width.\n* **height** (*int**,* *optional*) – The height of the terminal. Leave as default to auto-detect height.\n* **style** (*StyleType**,* *optional*) – Style to apply to all output, or None for no style. Defaults to None.\n* **no\\_color** (*Optional**[**bool**]**,* *optional*) – Enabled no color mode, or None to auto detect. Defaults to None.\n* **tab\\_size** (*int**,* *optional*) – Number of spaces used to replace a tab character. Defaults to 8.\n* **record** (*bool**,* *optional*) – Boolean to enable recording of terminal output,\nrequired to call `export\\_html()`, `export\\_svg()`, and `export\\_text()`. Defaults to False.\n* **markup** (*bool**,* *optional*) – Boolean to enable Console Markup. Defaults to True.\n* **emoji** (*bool**,* *optional*) – Enable emoji code. Defaults to True.\n* **emoji\\_variant** (*str**,* *optional*) – Optional emoji variant, either “text” or “emoji”. Defaults to None.\n* **highlight** (*bool**,* *optional*) – Enable automatic highlighting. Defaults to True.\n* **log\\_time** (*bool**,* *optional*) – Boolean to enable logging of time by `log()` methods. Defaults to True.\n* **log\\_path** (*bool**,* *optional*) – Boolean to enable the logging of the caller by `log()`. Defaults to True.\n* **log\\_time\\_format** (*Union**[**str**,* *TimeFormatterCallable**]**,* *optional*) – If `log\\_time` is enabled, either string for strftime or callable that formats the time. Defaults to “[%X] “.\n* **highlighter** (*HighlighterType**,* *optional*) – Default highlighter.\n* **legacy\\_windows** (*bool**,* *optional*) – Enable legacy Windows mode, or `None` to auto detect. Defaults to `None`.\n* **safe\\_box** (*bool**,* *optional*) – Restrict box options that don’t render on legacy Windows.\n* **get\\_datetime** (*Callable**[**[**]**,* *datetime**]**,* *optional*) – Callable that gets the current time as a datetime.datetime object (used by Console.log),\nor None for datetime.now.\n* **get\\_time** (*Callable**[**[**]**,* *time**]**,* *optional*) – Callable that gets the current time in seconds, default uses time.monotonic.\n* **\\_environ** (*Mapping**[**str**,* *str**]*) –\n\n\nbegin\\_capture()[source]¶\nBegin capturing console output. Call `end\\_capture()` to exit capture mode and return output.\n\nbell()[source]¶\nPlay a ‘bell’ sound (if supported by the terminal).\n\ncapture()[source]¶\nA context manager to *capture* the result of print() or log() in a string,\nrather than writing it to the console.\n\n\nExample\n\n```\n>>> from rich.console import Console\n>>> console = Console()\n>>> with console.capture() as capture:\n...     console.print(\"[bold magenta]Hello World[/]\")\n>>> print(capture.get())\n\nReturns\nContext manager with disables writing to the terminal.\n\nReturn type\nCapture\n\nclear(*home=True*)[source]¶\nClear the screen.\n\nParameters\n**home** (*bool**,* *optional*) – Also move the cursor to ‘home’ position. Defaults to True.\n\nclear\\_live()[source]¶\nClear the Live instance.\n\n*property* color\\_system*: Optional[str]*¶\nGet color system string.\n\nReturns\n“standard”, “256” or “truecolor”.\n\nReturn type\nOptional[str]\n\ncontrol(*\\*control*)[source]¶\nInsert non-printing control codes.\n\nParameters\n* **control\\_codes** (*str*) – Control codes, such as those that may move the cursor.\n* **control** (*Control*) –\n\n*property* encoding*: str*¶\nGet the encoding of the console file, e.g. `\"utf-8\"`.\n\nReturns\nA standard encoding string.\n\nend\\_capture()[source]¶\nEnd capture mode and return captured string.\n\nReturns\nConsole output.\n\nexport\\_html(*\\**, *theme=None*, *clear=True*, *code\\_format=None*, *inline\\_styles=False*)[source]¶\nGenerate HTML from console contents (requires record=True argument in constructor).\n\nParameters\n* **theme** (*TerminalTheme**,* *optional*) – TerminalTheme object containing console colors.\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`.\n* **code\\_format** (*str**,* *optional*) – Format string to render HTML. In addition to ‘{foreground}’,\n‘{background}’, and ‘{code}’, should contain ‘{stylesheet}’ if inline\\_styles is `False`.\n* **inline\\_styles** (*bool**,* *optional*) – If `True` styles will be inlined in to spans, which makes files\nlarger but easier to cut and paste markup. If `False`, styles will be embedded in a style tag.\nDefaults to False.\n\nReturns\nString containing console contents as HTML.\n\nexport\\_svg(*\\**, *title='Rich'*, *theme=None*, *clear=True*, *code\\_format='<svg class=\"rich-terminal\" viewBox=\"0 0 {width} {height}\" xmlns=\"http://www.w3.org/2000/svg\">\\n    <!-- Generated with Rich https://www.textualize.io -->\\n    <style>\\n\\n    @font-face {{\\n        font-family: \"Fira Code\";\\n        src: local(\"FiraCode-Regular\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Regular.woff2\") format(\"woff2\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Regular.woff\") format(\"woff\");\\n        font-style: normal;\\n        font-weight: 400;\\n    }}\\n    @font-face {{\\n        font-family: \"Fira Code\";\\n        src: local(\"FiraCode-Bold\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Bold.woff2\") format(\"woff2\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Bold.woff\") format(\"woff\");\\n        font-style: bold;\\n        font-weight: 700;\\n    }}\\n\\n    .{unique\\_id}-matrix {{\\n        font-family: Fira Code, monospace;\\n        font-size: {char\\_height}px;\\n        line-height: {line\\_height}px;\\n        font-variant-east-asian: full-width;\\n    }}\\n\\n    .{unique\\_id}-title {{\\n        font-size: 18px;\\n        font-weight: bold;\\n        font-family: arial;\\n    }}\\n\\n    {styles}\\n    </style>\\n\\n    <defs>\\n    <clipPath id=\"{unique\\_id}-clip-terminal\">\\n      <rect x=\"0\" y=\"0\" width=\"{terminal\\_width}\" height=\"{terminal\\_height}\" />\\n    </clipPath>\\n    {lines}\\n    </defs>\\n\\n    {chrome}\\n    <g transform=\"translate({terminal\\_x}, {terminal\\_y})\" clip-path=\"url(#{unique\\_id}-clip-terminal)\">\\n    {backgrounds}\\n    <g class=\"{unique\\_id}-matrix\">\\n    {matrix}\\n    </g>\\n    </g>\\n</svg>\\n'*, *font\\_aspect\\_ratio=0.61*, *unique\\_id=None*)[source]¶\nGenerate an SVG from the console contents (requires record=True in Console constructor).\n\nParameters\n* **title** (*str**,* *optional*) – The title of the tab in the output image\n* **theme** (*TerminalTheme**,* *optional*) – The `TerminalTheme` object to use to style the terminal\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`\n* **code\\_format** (*str**,* *optional*) – Format string used to generate the SVG. Rich will inject a number of variables\ninto the string in order to form the final SVG output. The default template used and the variables\ninjected by Rich can be found by inspecting the `console.CONSOLE\\_SVG\\_FORMAT` variable.\n* **font\\_aspect\\_ratio** (*float**,* *optional*) – The width to height ratio of the font used in the `code\\_format`\nstring. Defaults to 0.61, which is the width to height ratio of Fira Code (the default font).\nIf you aren’t specifying a different font inside `code\\_format`, you probably don’t need this.\n* **unique\\_id** (*str**,* *optional*) – unique id that is used as the prefix for various elements (CSS styles, node\nids). If not set, this defaults to a computed value based on the recorded content.\n\nexport\\_text(*\\**, *clear=True*, *styles=False*)[source]¶\nGenerate text from console contents (requires record=True argument in constructor).\n\nParameters\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`.\n* **styles** (*bool**,* *optional*) – If `True`, ansi escape codes will be included. `False` for plain text.\nDefaults to `False`.\n\nReturns\nString containing console contents.\n\n*property* file*: IO[str]*¶\nGet the file object to write to.\n\nget\\_style(*name*, *\\**, *default=None*)[source]¶\nGet a Style instance by its theme name or parse a definition.\n\nParameters\n* **name** (*str*) – The name of a style or a style definition.\n* **default** (*Optional**[**Union**[**str**,* *Style**]**]*) –\n\nReturns\nA Style object.\n\nReturn type\nStyle\n\nRaises\n**MissingStyle** – If no style could be parsed from name.\n\n*property* height*: int*¶\nGet the height of the console.\n\nReturns\nThe height (in lines) of the console.\n\ninput(*prompt=''*, *\\**, *markup=True*, *emoji=True*, *password=False*, *stream=None*)[source]¶\nDisplays a prompt and waits for input from the user. The prompt may contain color / style.\n\n\nIt works in the same way as Python’s builtin `input()` function and provides elaborate line editing and history features if Python’s builtin `readline` module is previously loaded.\n\nParameters\n* **prompt** (*Union**[**str**,* *Text**]*) – Text to render in the prompt.\n* **markup** (*bool**,* *optional*) – Enable console markup (requires a str prompt). Defaults to True.\n* **emoji** (*bool**,* *optional*) – Enable emoji (requires a str prompt). Defaults to True.\n* **password** (*bool*) – (bool, optional): Hide typed text. Defaults to False.\n* **stream** (*Optional**[**TextIO**]*) – (TextIO, optional): Optional file to read input from (rather than stdin). Defaults to None.\n\nReturns\nText read from stdin.\n\n*property* is\\_alt\\_screen*: bool*¶\nCheck if the alt screen was enabled.\n\nReturns\nTrue if the alt screen was enabled, otherwise False.\n\nReturn type\nbool\n\n*property* is\\_dumb\\_terminal*: bool*¶\nDetect dumb terminal.\n\nReturns\nTrue if writing to a dumb terminal, otherwise False.\n\n*property* is\\_terminal*: bool*¶\nCheck if the console is writing to a terminal.\n\nReturns\nTrue if the console writing to a device capable of\nunderstanding terminal codes, otherwise False.\n\nline(*count=1*)[source]¶\nWrite new line(s).\n\nParameters\n**count** (*int**,* *optional*) – Number of new lines. Defaults to 1.\n\nlog(*\\*objects*, *sep=' '*, *end='\\n'*, *style=None*, *justify=None*, *emoji=None*, *markup=None*, *highlight=None*, *log\\_locals=False*, *\\_stack\\_offset=1*)[source]¶\nLog rich content to the terminal.\n\nParameters\n* **objects** (*positional args*) – Objects to log to the terminal.\n* **sep** (*str**,* *optional*) – String to write between print data. Defaults to ” “.\n* **end** (*str**,* *optional*) – String to write at end of print data. Defaults to “\\n”.\n* **style** (*Union**[**str**,* *Style**]**,* *optional*) – A style to apply to output. Defaults to None.\n* **justify** (*str**,* *optional*) – One of “left”, “right”, “center”, or “full”. Defaults to `None`.\n* **emoji** (*Optional**[**bool**]**,* *optional*) – Enable emoji code, or `None` to use console default. Defaults to None.\n* **markup** (*Optional**[**bool**]**,* *optional*) – Enable markup, or `None` to use console default. Defaults to None.\n* **highlight** (*Optional**[**bool**]**,* *optional*) – Enable automatic highlighting, or `None` to use console default. Defaults to None.\n* **log\\_locals** (*bool**,* *optional*) – Boolean to enable logging of locals where `log()`\nwas called. Defaults to False.\n* **\\_stack\\_offset** (*int**,* *optional*) – Offset of caller from end of call stack. Defaults to 1.\n\nmeasure(*renderable*, *\\**, *options=None*)[source]¶\nMeasure a renderable. Returns a `Measurement` object which contains\ninformation regarding the number of characters required to print the renderable.\n\nParameters\n* **renderable** (*RenderableType*) – Any renderable or string.\n* **options** (*Optional**[**ConsoleOptions**]**,* *optional*) – Options to use when measuring, or None\nto use default options. Defaults to None.\n\nReturns\nA measurement of the renderable.\n\n*property* options*: ConsoleOptions*¶\nGet default console options.\n\nout(*\\*objects*, *sep=' '*, *end='\\n'*, *style=None*, *highlight=None*)[source]¶\nOutput to the terminal. This is a low-level way of writing to the terminal which unlike\n`print()` won’t pretty print, wrap text, or apply markup, but will\noptionally apply highlighting and a basic style.\n\nParameters\n* **sep** (*str**,* *optional*) – String to write between print data. Defaults to ” “.\n* **end** (*str**,* *optional*) – String to write at end of print data. Defaults to “\\n”.\n* **style** (*Union**[**str**,* *Style**]**,* *optional*) – A style to apply to output. Defaults to None.\n* **highlight** (*Optional**[**bool**]**,* *optional*) – Enable automatic highlighting, or `None` to use\nconsole default. Defaults to `None`.\n* **objects** (*Any*) –\n\npager(*pager=None*, *styles=False*, *links=False*)[source]¶\nA context manager to display anything printed within a “pager”. The pager application\nis defined by the system and will typically support at least pressing a key to scroll.\n\nParameters\n* **pager** (*Pager**,* *optional*) – A pager object, or None to use `SystemPager`. Defaults to None.\n* **styles** (*bool**,* *optional*) – Show styles in pager. Defaults to False.\n* **links** (*bool**,* *optional*) – Show links in pager. Defaults to False.\n\nReturn type\n*PagerContext*\n\n```\n>>> from rich.console import Console\n>>> from rich.\\_\\_main\\_\\_ import make\\_test\\_card\n>>> console = Console()\n>>> with console.pager():\n console.print(make\\_test\\_card())\n\nReturns\nA context manager.\n\nReturn type\nPagerContext\n\nParameters\n* **pager** (*Optional**[**Pager**]*) –\n* **styles** (*bool*) –\n* **links** (*bool*) –\n\npop\\_render\\_hook()[source]¶\nPop the last renderhook from the stack.\n\npop\\_theme()[source]¶\nRemove theme from top of stack, restoring previous theme.\n\nprint(*\\*objects*, *sep=' '*, *end='\\n'*, *style=None*, *justify=None*, *overflow=None*, *no\\_wrap=None*, *emoji=None*, *markup=None*, *highlight=None*, *width=None*, *height=None*, *crop=True*, *soft\\_wrap=None*, *new\\_line\\_start=False*)[source]¶\nPrint to the console.\n\nParameters\n* **objects** (*positional args*) – Objects to log to the terminal.\n* **sep** (*str**,* *optional*) – String to write between print data. Defaults to ” “.\n* **end** (*str**,* *optional*) – String to write at end of print data. Defaults to “\\n”.\n* **style** (*Union**[**str**,* *Style**]**,* *optional*) – A style to apply to output. Defaults to None.\n* **justify** (*str**,* *optional*) – Justify method: “default”, “left”, “right”, “center”, or “full”. Defaults to `None`.\n* **overflow** (*str**,* *optional*) – Overflow method: “ignore”, “crop”, “fold”, or “ellipsis”. Defaults to None.\n* **no\\_wrap** (*Optional**[**bool**]**,* *optional*) – Disable word wrapping. Defaults to None.\n* **emoji** (*Optional**[**bool**]**,* *optional*) – Enable emoji code, or `None` to use console default. Defaults to `None`.\n* **markup** (*Optional**[**bool**]**,* *optional*) – Enable markup, or `None` to use console default. Defaults to `None`.\n* **highlight** (*Optional**[**bool**]**,* *optional*) – Enable automatic highlighting, or `None` to use console default. Defaults to `None`.\n* **width** (*Optional**[**int**]**,* *optional*) – Width of output, or `None` to auto-detect. Defaults to `None`.\n* **crop** (*Optional**[**bool**]**,* *optional*) – Crop output to width of terminal. Defaults to True.\n* **soft\\_wrap** (*bool**,* *optional*) – Enable soft wrap mode which disables word wrapping and cropping of text or `None` for\nConsole default. Defaults to `None`.\n* **new\\_line\\_start** (*bool**,* *False*) – Insert a new line at the start if the output contains more than one line. Defaults to `False`.\n* **height** (*Optional**[**int**]*) –\n\nprint\\_exception(*\\**, *width=100*, *extra\\_lines=3*, *theme=None*, *word\\_wrap=False*, *show\\_locals=False*, *suppress=()*, *max\\_frames=100*)[source]¶\nPrints a rich render of the last exception and traceback.\n\nParameters\n* **width** (*Optional**[**int**]**,* *optional*) – Number of characters used to render code. Defaults to 100.\n* **extra\\_lines** (*int**,* *optional*) – Additional lines of code to render. Defaults to 3.\n* **theme** (*str**,* *optional*) – Override pygments theme used in traceback\n* **word\\_wrap** (*bool**,* *optional*) – Enable word wrapping of long lines. Defaults to False.\n* **show\\_locals** (*bool**,* *optional*) – Enable display of local variables. Defaults to False.\n* **suppress** (*Iterable**[**Union**[**str**,* *ModuleType**]**]*) – Optional sequence of modules or paths to exclude from traceback.\n* **max\\_frames** (*int*) – Maximum number of frames to show in a traceback, 0 for no maximum. Defaults to 100.\n\nprint\\_json(*json=None*, *\\**, *data=None*, *indent=2*, *highlight=True*, *skip\\_keys=False*, *ensure\\_ascii=False*, *check\\_circular=True*, *allow\\_nan=True*, *default=None*, *sort\\_keys=False*)[source]¶\nPretty prints JSON. Output will be valid JSON.\n\nParameters\n* **json** (*Optional**[**str**]*) – A string containing JSON.\n* **data** (*Any*) – If json is not supplied, then encode this data.\n* **indent** (*Union**[**None**,* *int**,* *str**]**,* *optional*) – Number of spaces to indent. Defaults to 2.\n* **highlight** (*bool**,* *optional*) – Enable highlighting of output: Defaults to True.\n* **skip\\_keys** (*bool**,* *optional*) – Skip keys not of a basic type. Defaults to False.\n* **ensure\\_ascii** (*bool**,* *optional*) – Escape all non-ascii characters. Defaults to False.\n* **check\\_circular** (*bool**,* *optional*) – Check for circular references. Defaults to True.\n* **allow\\_nan** (*bool**,* *optional*) – Allow NaN and Infinity values. Defaults to True.\n* **default** (*Callable**,* *optional*) – A callable that converts values that can not be encoded\nin to something that can be JSON encoded. Defaults to None.\n* **sort\\_keys** (*bool**,* *optional*) – Sort dictionary keys. Defaults to False.\n\npush\\_render\\_hook(*hook*)[source]¶\nAdd a new render hook to the stack.\n\nParameters\n**hook** (*RenderHook*) – Render hook instance.\n\npush\\_theme(*theme*, *\\**, *inherit=True*)[source]¶\nPush a new theme on to the top of the stack, replacing the styles from the previous theme.\nGenerally speaking, you should call `use\\_theme()` to get a context manager, rather\nthan calling this method directly.\n\nParameters\n* **theme** (*Theme*) – A theme instance.\n* **inherit** (*bool**,* *optional*) – Inherit existing styles. Defaults to True.\n\nrender(*renderable*, *options=None*)[source]¶\nRender an object in to an iterable of Segment instances.\n\n\nThis method contains the logic for rendering objects with the console protocol.\nYou are unlikely to need to use it directly, unless you are extending the library.\n\nParameters\n* **renderable** (*RenderableType*) – An object supporting the console protocol, or\nan object that may be converted to a string.\n* **options** (*ConsoleOptions**,* *optional*) – An options object, or None to use self.options. Defaults to None.\n\nReturns\nAn iterable of segments that may be rendered.\n\nrender\\_lines(*renderable*, *options=None*, *\\**, *style=None*, *pad=True*, *new\\_lines=False*)[source]¶\nRender objects in to a list of lines.\n\n> \n> The output of render\\_lines is useful when further formatting of rendered console text\n> is required, such as the Panel class which draws a border around any renderable object.\n> \n> \n> \n> Args:renderable (RenderableType): Any object renderable in the console.\n> options (Optional[ConsoleOptions], optional): Console options, or None to use self.options. Default to `None`.\n> style (Style, optional): Optional style to apply to renderables. Defaults to `None`.\n> pad (bool, optional): Pad lines shorter than render width. Defaults to `True`.\n> new\\_lines (bool, optional): Include “\n> \n> \n> \n> \n> \n\n\n” characters at end of lines.\n\n> \n> \n> Returns:List[List[Segment]]: A list of lines, where a line is a list of Segment objects.\n> \n> \n> \n> \n> \n\nParameters\n* **renderable** (*Union**[**ConsoleRenderable**,* *RichCast**,* *str**]*) –\n* **options** (*Optional**[**ConsoleOptions**]*) –\n* **style** (*Optional**[**Style**]*) –\n* **pad** (*bool*) –\n* **new\\_lines** (*bool*) –\n\nrender\\_str(*text*, *\\**, *style=''*, *justify=None*, *overflow=None*, *emoji=None*, *markup=None*, *highlight=None*, *highlighter=None*)[source]¶\nConvert a string to a Text instance. This is called automatically if\nyou print or log a string.\n\nParameters\n* **text** (*str*) – Text to render.\n* **style** (*Union**[**str**,* *Style**]**,* *optional*) – Style to apply to rendered text.\n* **justify** (*str**,* *optional*) – Justify method: “default”, “left”, “center”, “full”, or “right”. Defaults to `None`.\n* **overflow** (*str**,* *optional*) – Overflow method: “crop”, “fold”, or “ellipsis”. Defaults to `None`.\n* **emoji** (*Optional**[**bool**]**,* *optional*) – Enable emoji, or `None` to use Console default.\n* **markup** (*Optional**[**bool**]**,* *optional*) – Enable markup, or `None` to use Console default.\n* **highlight** (*Optional**[**bool**]**,* *optional*) – Enable highlighting, or `None` to use Console default.\n* **highlighter** (*HighlighterType**,* *optional*) – Optional highlighter to apply.\n\nReturns\nRenderable object.\n\nReturn type\nConsoleRenderable\n\nrule(*title=''*, *\\**, *characters='─'*, *style='rule.line'*, *align='center'*)[source]¶\nDraw a line with optional centered title.\n\nParameters\n* **title** (*str**,* *optional*) – Text to render over the rule. Defaults to “”.\n* **characters** (*str**,* *optional*) – Character(s) to form the line. Defaults to “─”.\n* **style** (*str**,* *optional*) – Style of line. Defaults to “rule.line”.\n* **align** (*str**,* *optional*) – How to align the title, one of “left”, “center”, or “right”. Defaults to “center”.\n\nsave\\_html(*path*, *\\**, *theme=None*, *clear=True*, *code\\_format='<!DOCTYPE html>\\n<html>\\n<head>\\n<meta charset=\"UTF-8\">\\n<style>\\n{stylesheet}\\nbody {{\\n    color: {foreground};\\n    background-color: {background};\\n}}\\n</style>\\n</head>\\n<body>\\n    <pre style=\"font-family:Menlo,\\'DejaVu Sans Mono\\',consolas,\\'Courier New\\',monospace\"><code>{code}</code></pre>\\n</body>\\n</html>\\n'*, *inline\\_styles=False*)[source]¶\nGenerate HTML from console contents and write to a file (requires record=True argument in constructor).\n\nParameters\n* **path** (*str*) – Path to write html file.\n* **theme** (*TerminalTheme**,* *optional*) – TerminalTheme object containing console colors.\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`.\n* **code\\_format** (*str**,* *optional*) – Format string to render HTML. In addition to ‘{foreground}’,\n‘{background}’, and ‘{code}’, should contain ‘{stylesheet}’ if inline\\_styles is `False`.\n* **inline\\_styles** (*bool**,* *optional*) – If `True` styles will be inlined in to spans, which makes files\nlarger but easier to cut and paste markup. If `False`, styles will be embedded in a style tag.\nDefaults to False.\n\nsave\\_svg(*path*, *\\**, *title='Rich'*, *theme=None*, *clear=True*, *code\\_format='<svg class=\"rich-terminal\" viewBox=\"0 0 {width} {height}\" xmlns=\"http://www.w3.org/2000/svg\">\\n    <!-- Generated with Rich https://www.textualize.io -->\\n    <style>\\n\\n    @font-face {{\\n        font-family: \"Fira Code\";\\n        src: local(\"FiraCode-Regular\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Regular.woff2\") format(\"woff2\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Regular.woff\") format(\"woff\");\\n        font-style: normal;\\n        font-weight: 400;\\n    }}\\n    @font-face {{\\n        font-family: \"Fira Code\";\\n        src: local(\"FiraCode-Bold\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Bold.woff2\") format(\"woff2\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Bold.woff\") format(\"woff\");\\n        font-style: bold;\\n        font-weight: 700;\\n    }}\\n\\n    .{unique\\_id}-matrix {{\\n        font-family: Fira Code, monospace;\\n        font-size: {char\\_height}px;\\n        line-height: {line\\_height}px;\\n        font-variant-east-asian: full-width;\\n    }}\\n\\n    .{unique\\_id}-title {{\\n        font-size: 18px;\\n        font-weight: bold;\\n        font-family: arial;\\n    }}\\n\\n    {styles}\\n    </style>\\n\\n    <defs>\\n    <clipPath id=\"{unique\\_id}-clip-terminal\">\\n      <rect x=\"0\" y=\"0\" width=\"{terminal\\_width}\" height=\"{terminal\\_height}\" />\\n    </clipPath>\\n    {lines}\\n    </defs>\\n\\n    {chrome}\\n    <g transform=\"translate({terminal\\_x}, {terminal\\_y})\" clip-path=\"url(#{unique\\_id}-clip-terminal)\">\\n    {backgrounds}\\n    <g class=\"{unique\\_id}-matrix\">\\n    {matrix}\\n    </g>\\n    </g>\\n</svg>\\n'*, *font\\_aspect\\_ratio=0.61*, *unique\\_id=None*)[source]¶\nGenerate an SVG file from the console contents (requires record=True in Console constructor).\n\nParameters\n* **path** (*str*) – The path to write the SVG to.\n* **title** (*str**,* *optional*) – The title of the tab in the output image\n* **theme** (*TerminalTheme**,* *optional*) – The `TerminalTheme` object to use to style the terminal\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`\n* **code\\_format** (*str**,* *optional*) – Format string used to generate the SVG. Rich will inject a number of variables\ninto the string in order to form the final SVG output. The default template used and the variables\ninjected by Rich can be found by inspecting the `console.CONSOLE\\_SVG\\_FORMAT` variable.\n* **font\\_aspect\\_ratio** (*float**,* *optional*) – The width to height ratio of the font used in the `code\\_format`\nstring. Defaults to 0.61, which is the width to height ratio of Fira Code (the default font).\nIf you aren’t specifying a different font inside `code\\_format`, you probably don’t need this.\n* **unique\\_id** (*str**,* *optional*) – unique id that is used as the prefix for various elements (CSS styles, node\nids). If not set, this defaults to a computed value based on the recorded content.\n\nsave\\_text(*path*, *\\**, *clear=True*, *styles=False*)[source]¶\nGenerate text from console and save to a given location (requires record=True argument in constructor).\n\nParameters\n* **path** (*str*) – Path to write text files.\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`.\n* **styles** (*bool**,* *optional*) – If `True`, ansi style codes will be included. `False` for plain text.\nDefaults to `False`.\n\nscreen(*hide\\_cursor=True*, *style=None*)[source]¶\nContext manager to enable and disable ‘alternative screen’ mode.\n\nParameters\n* **hide\\_cursor** (*bool**,* *optional*) – Also hide the cursor. Defaults to False.\n* **style** (*Style**,* *optional*) – Optional style for screen. Defaults to None.\n\nReturns\nContext which enables alternate screen on enter, and disables it on exit.\n\nReturn type\n~ScreenContext\n\nset\\_alt\\_screen(*enable=True*)[source]¶\nEnables alternative screen mode.\n\n\nNote, if you enable this mode, you should ensure that is disabled before\nthe application exits. See `screen()` for a context manager\nthat handles this for you.\n\nParameters\n**enable** (*bool**,* *optional*) – Enable (True) or disable (False) alternate screen. Defaults to True.\n\nReturns\nTrue if the control codes were written.\n\nset\\_live(*live*)[source]¶\nSet Live instance. Used by Live context manager.\n\nParameters\n**live** (*Live*) – Live instance using this Console.\n\nRaises\n**errors.LiveError** – If this Console has a Live context currently active.\n\nset\\_window\\_title(*title*)[source]¶\nSet the title of the console terminal window.\n\n\nWarning: There is no means within Rich of “resetting” the window title to its\nprevious value, meaning the title you set will persist even after your application\nexits.\n\n\n`fish` shell resets the window title before and after each command by default,\nnegating this issue. Windows Terminal and command prompt will also reset the title for you.\nMost other shells and terminals, however, do not do this.\n\n\nSome terminals may require configuration changes before you can set the title.\nSome terminals may not support setting the title at all.\n\n\nOther software (including the terminal itself, the shell, custom prompts, plugins, etc.)\nmay also set the terminal window title. This could result in whatever value you write\nusing this method being overwritten.\n\nParameters\n**title** (*str*) – The new title of the terminal window.\n\nTrue if the control code to change the terminal title waswritten, otherwise False. Note that a return value of True\ndoes not guarantee that the window title has actually changed,\nsince the feature may be unsupported/disabled in some terminals.\n\n\nReturn type\nbool\n\nshow\\_cursor(*show=True*)[source]¶\nShow or hide the cursor.\n\nParameters\n**show** (*bool**,* *optional*) – Set visibility of the cursor.\n\n*property* size*: ConsoleDimensions*¶\nGet the size of the console.\n\nReturns\nA named tuple containing the dimensions.\n\nReturn type\nConsoleDimensions\n\nstatus(*status*, *\\**, *spinner='dots'*, *spinner\\_style='status.spinner'*, *speed=1.0*, *refresh\\_per\\_second=12.5*)[source]¶\nDisplay a status and spinner.\n\nParameters\n* **status** (*RenderableType*) – A status renderable (str or Text typically).\n* **spinner** (*str**,* *optional*) – Name of spinner animation (see python -m rich.spinner). Defaults to “dots”.\n* **spinner\\_style** (*StyleType**,* *optional*) – Style of spinner. Defaults to “status.spinner”.\n* **speed** (*float**,* *optional*) – Speed factor for spinner animation. Defaults to 1.0.\n* **refresh\\_per\\_second** (*float**,* *optional*) – Number of refreshes per second. Defaults to 12.5.\n\nReturns\nA Status object that may be used as a context manager.\n\nReturn type\nStatus\n\nupdate\\_screen(*renderable*, *\\**, *region=None*, *options=None*)[source]¶\nUpdate the screen at a given offset.\n\nParameters\n* **renderable** (*RenderableType*) – A Rich renderable.\n* **region** (*Region**,* *optional*) – Region of screen to update, or None for entire screen. Defaults to None.\n* **x** (*int**,* *optional*) – x offset. Defaults to 0.\n* **y** (*int**,* *optional*) – y offset. Defaults to 0.\n* **options** (*Optional**[**ConsoleOptions**]*) –\n\nRaises\n**errors.NoAltScreen** – If the Console isn’t in alt screen mode.\n\nupdate\\_screen\\_lines(*lines*, *x=0*, *y=0*)[source]¶\nUpdate lines of the screen at a given offset.\n\nParameters\n* **lines** (*List**[**List**[**Segment**]**]*) – Rendered lines (as produced by `render\\_lines()`).\n* **x** (*int**,* *optional*) – x offset (column no). Defaults to 0.\n* **y** (*int**,* *optional*) – y offset (column no). Defaults to 0.\n\nuse\\_theme(*theme*, *\\**, *inherit=True*)[source]¶\nUse a different theme for the duration of the context manager.\n\nParameters\n* **theme** (*Theme*) – Theme instance to user.\n* **inherit** (*bool**,* *optional*) – Inherit existing console styles. Defaults to True.\n\nReturns\n[description]\n\nReturn type\nThemeContext\n\n*property* width*: int*¶\nGet the width of the console.\n\nReturns\nThe width (in characters) of the console.\n\n\n*class* rich.console.ConsoleDimensions(*width*, *height*)[source]¶\nSize of the terminal.\n\nParameters\n* **width** (*int*) –\n* **height** (*int*) –\n\n\n*property* height¶\nThe height of the console in lines.\n\n*property* width¶\nThe width of the console in ‘cells’.\n\n\n*class* rich.console.ConsoleOptions(*size*, *legacy\\_windows*, *min\\_width*, *max\\_width*, *is\\_terminal*, *encoding*, *max\\_height*, *justify=None*, *overflow=None*, *no\\_wrap=False*, *highlight=None*, *markup=None*, *height=None*)[source]¶\nOptions for \\_\\_rich\\_console\\_\\_ method.\n\nParameters\n* **size** (*ConsoleDimensions*) –\n* **legacy\\_windows** (*bool*) –\n* **min\\_width** (*int*) –\n* **max\\_width** (*int*) –\n* **is\\_terminal** (*bool*) –\n* **encoding** (*str*) –\n* **max\\_height** (*int*) –\n* **justify** (*Optional**[**typing\\_extensions.Literal**[**default**,* *left**,* *center**,* *right**,* *full**]**]*) –\n* **overflow** (*Optional**[**typing\\_extensions.Literal**[**fold**,* *crop**,* *ellipsis**,* *ignore**]**]*) –\n* **no\\_wrap** (*Optional**[**bool**]*) –\n* **highlight** (*Optional**[**bool**]*) –\n* **markup** (*Optional**[**bool**]*) –\n* **height** (*Optional**[**int**]*) –\n\n\n*property* ascii\\_only*: bool*¶\nCheck if renderables should use ascii only.\n\ncopy()[source]¶\nReturn a copy of the options.\n\nReturns\na copy of self.\n\nReturn type\nConsoleOptions\n\nencoding*: str*¶\nEncoding of terminal.\n\nhighlight*: Optional[bool]* *= None*¶\nHighlight override for render\\_str.\n\nis\\_terminal*: bool*¶\nTrue if the target is a terminal, otherwise False.\n\njustify*: Optional[typing\\_extensions.Literal[default, left, center, right, full]]* *= None*¶\nJustify value override for renderable.\n\nlegacy\\_windows*: bool*¶\nflag for legacy windows.\n\nType\nlegacy\\_windows\n\nmarkup*: Optional[bool]* *= None*¶\nEnable markup when rendering strings.\n\nmax\\_height*: int*¶\nHeight of container (starts as terminal)\n\nmax\\_width*: int*¶\nMaximum width of renderable.\n\nmin\\_width*: int*¶\nMinimum width of renderable.\n\nno\\_wrap*: Optional[bool]* *= False*¶\nDisable wrapping for text.\n\noverflow*: Optional[typing\\_extensions.Literal[fold, crop, ellipsis, ignore]]* *= None*¶\nOverflow value override for renderable.\n\nreset\\_height()[source]¶\nReturn a copy of the options with height set to `None`.\n\nReturns\nNew console options instance.\n\nReturn type\n~ConsoleOptions\n\nsize*: ConsoleDimensions*¶\nSize of console.\n\nupdate(*\\**, *width=<rich.console.NoChange object>*, *min\\_width=<rich.console.NoChange object>*, *max\\_width=<rich.console.NoChange object>*, *justify=<rich.console.NoChange object>*, *overflow=<rich.console.NoChange object>*, *no\\_wrap=<rich.console.NoChange object>*, *highlight=<rich.console.NoChange object>*, *markup=<rich.console.NoChange object>*, *height=<rich.console.NoChange object>*)[source]¶\nUpdate values, return a copy.\n\nParameters\n* **width** (*Union**[**int**,* *NoChange**]*) –\n* **min\\_width** (*Union**[**int**,* *NoChange**]*) –\n* **max\\_width** (*Union**[**int**,* *NoChange**]*) –\n* **justify** (*Union**[**typing\\_extensions.Literal**[**default**,* *left**,* *center**,* *right**,* *full**]**,* *None**,* *NoChange**]*) –\n* **overflow** (*Union**[**typing\\_extensions.Literal**[**fold**,* *crop**,* *ellipsis**,* *ignore**]**,* *None**,* *NoChange**]*) –\n* **no\\_wrap** (*Union**[**bool**,* *None**,* *NoChange**]*) –\n* **highlight** (*Union**[**bool**,* *None**,* *NoChange**]*) –\n* **markup** (*Union**[**bool**,* *None**,* *NoChange**]*) –\n* **height** (*Union**[**int**,* *None**,* *NoChange**]*) –\n\nReturn type\n*ConsoleOptions*\n\nupdate\\_dimensions(*width*, *height*)[source]¶\nUpdate the width and height, and return a copy.\n\nParameters\n* **width** (*int*) – New width (sets both min\\_width and max\\_width).\n* **height** (*int*) – New height.\n\nupdate\\_height(*height*)[source]¶\nUpdate the height, and return a copy.\n\nParameters\n**height** (*int*) – New height\n\nReturns\nNew Console options instance.\n\nupdate\\_width(*width*)[source]¶\nUpdate just the width, return a copy.\n\nParameters\n**width** (*int*) – New width (sets both min\\_width and max\\_width)\n\n\n*class* rich.console.ConsoleRenderable(*\\*args*, *\\*\\*kwds*)[source]¶\nAn object that supports the console protocol.\n\n*class* rich.console.ConsoleThreadLocals(*theme\\_stack*, *buffer=<factory>*, *buffer\\_index=0*)[source]¶\nThread local values for Console context.\n\nParameters\n* **theme\\_stack** (*ThemeStack*) –\n* **buffer** (*List**[**Segment**]*) –\n* **buffer\\_index** (*int*) –\n\n*class* rich.console.Group(*\\*renderables*, *fit=True*)[source]¶\nTakes a group of renderables and returns a renderable object that renders the group.\n\nParameters\n* **renderables** (*Iterable**[**RenderableType**]*) – An iterable of renderable objects.\n* **fit** (*bool**,* *optional*) – Fit dimension of group to contents, or fill available space. Defaults to True.\n\n*class* rich.console.NewLine(*count=1*)[source]¶\nA renderable to generate new line(s)\n\nParameters\n**count** (*int*) – \n\n*class* rich.console.PagerContext(*console*, *pager=None*, *styles=False*, *links=False*)[source]¶\nA context manager that ‘pages’ content. See `pager()` for usage.\n\nParameters\n* **console** (*Console*) –\n* **pager** (*Optional**[**Pager**]*) –\n* **styles** (*bool*) –\n* **links** (*bool*) –\n\n*class* rich.console.RenderHook[source]¶\nProvides hooks in to the render process.\n\n\n*abstract* process\\_renderables(*renderables*)[source]¶\nCalled with a list of objects to render.\n\n\nThis method can return a new list of renderables, or modify and return the same list.\n\nParameters\n**renderables** (*List**[**ConsoleRenderable**]*) – A number of renderable objects.\n\nReturns\nA replacement list of renderables.\n\nReturn type\nList[ConsoleRenderable]\n\n\nrich.console.RenderableType¶\nA string or any object that may be rendered by Rich.\n\n\nalias of `Union`[`ConsoleRenderable`, `RichCast`, `str`]\n\n*class* rich.console.RichCast(*\\*args*, *\\*\\*kwds*)[source]¶\nAn object that may be ‘cast’ to a console renderable.\n\n*class* rich.console.ScreenContext(*console*, *hide\\_cursor*, *style=''*)[source]¶\nA context manager that enables an alternative screen. See `screen()` for usage.\n\nParameters\n* **console** (*Console*) –\n* **hide\\_cursor** (*bool*) –\n* **style** (*Union**[**str**,* *Style**]*) –\n\n\nupdate(*\\*renderables*, *style=None*)[source]¶\nUpdate the screen.\n\nParameters\n* **renderable** (*RenderableType**,* *optional*) – Optional renderable to replace current renderable,\nor None for no change. Defaults to None.\n* **style** (*Optional**[**Union**[**str**,* *Style**]**]*) – (Style, optional): Replacement style, or None for no change. Defaults to None.\n* **renderables** (*Union**[**ConsoleRenderable**,* *RichCast**,* *str**]*) –\n\n\n*class* rich.console.ScreenUpdate(*lines*, *x*, *y*)[source]¶\nRender a list of lines at a given offset.\n\nParameters\n* **lines** (*List**[**List**[**Segment**]**]*) –\n* **x** (*int*) –\n* **y** (*int*) –\n\n*class* rich.console.ThemeContext(*console*, *theme*, *inherit=True*)[source]¶\nA context manager to use a temporary theme. See `use\\_theme()` for usage.\n\nParameters\n* **console** (*Console*) –\n* **theme** (*Theme*) –\n* **inherit** (*bool*) –\n\nrich.console.detect\\_legacy\\_windows()[source]¶\nDetect legacy Windows.\n\nrich.console.group(*fit=True*)[source]¶\nA decorator that turns an iterable of renderables in to a group.\n\nParameters\n**fit** (*bool**,* *optional*) – Fit dimension of group to contents, or fill available space. Defaults to True.\n\nReturn type\n*Callable*[[…], *Callable*[[…], *Group*]]\n\n\n# rich.highlighter¶\n\n\n# rich¶\n\n\n\n# rich.measure¶\n\n\n# rich.layout¶\n\n\n*class* rich.layout.ColumnSplitter[source]¶\nSplit a layout region in to columns.\n\n\ndivide(*children*, *region*)[source]¶\nDivide a region amongst several child layouts.\n\nParameters\n* **children** (*Sequence**(**Layout**)*) – A number of child layouts.\n* **region** (*Region*) – A rectangular region to divide.\n\nReturn type\n*Iterable*[*Tuple*[*Layout*, *Region*]]\n\nget\\_tree\\_icon()[source]¶\nGet the icon (emoji) used in layout.tree\n\n\n*class*\n\n==================\n Document 1 \n----------------\n rich¶\n\n\nRich text and beautiful formatting in the terminal.\n\n\nrich.get\\_console()[source]¶\nGet a global `Console` instance. This function is used when Rich requires a Console,\nand hasn’t been explicitly given one.\n\nReturns\nA console instance.\n\nReturn type\nConsole\n\nrich.inspect(*obj*, *\\**, *console=None*, *title=None*, *help=False*, *methods=False*, *docs=True*, *private=False*, *dunder=False*, *sort=True*, *all=False*, *value=True*)[source]¶\nInspect any Python object.\n\n\n* inspect(<OBJECT>) to see summarized info.\n* inspect(<OBJECT>, methods=True) to see methods.\n* inspect(<OBJECT>, help=True) to see full (non-abbreviated) help.\n* inspect(<OBJECT>, private=True) to see private attributes (single underscore).\n* inspect(<OBJECT>, dunder=True) to see attributes beginning with double underscore.\n* inspect(<OBJECT>, all=True) to see all attributes.\n\nParameters\n* **obj** (*Any*) – An object to inspect.\n* **title** (*str**,* *optional*) – Title to display over inspect result, or None use type. Defaults to None.\n* **help** (*bool**,* *optional*) – Show full help text rather than just first paragraph. Defaults to False.\n* **methods** (*bool**,* *optional*) – Enable inspection of callables. Defaults to False.\n* **docs** (*bool**,* *optional*) – Also render doc strings. Defaults to True.\n* **private** (*bool**,* *optional*) – Show private attributes (beginning with underscore). Defaults to False.\n* **dunder** (*bool**,* *optional*) – Show attributes starting with double underscore. Defaults to False.\n* **sort** (*bool**,* *optional*) – Sort attributes alphabetically. Defaults to True.\n* **all** (*bool**,* *optional*) – Show all attributes. Defaults to False.\n* **value** (*bool**,* *optional*) – Pretty print value. Defaults to True.\n* **console** (*Optional**[**Console**]*) –\n\nrich.print(*\\*objects*, *sep=' '*, *end='\\n'*, *file=None*, *flush=False*)[source]¶\nPrint object(s) supplied via positional arguments.\nThis function has an identical signature to the built-in print.\nFor more advanced features, see the `Console` class.\n\nParameters\n* **sep** (*str**,* *optional*) – Separator between printed objects. Defaults to ” “.\n* **end** (*str**,* *optional*) – Character to write at end of output. Defaults to “\\n”.\n* **file** (*IO**[**str**]**,* *optional*) – File to write to, or None for stdout. Defaults to None.\n* **flush** (*bool**,* *optional*) – Has no effect as Rich always flushes output. Defaults to False.\n* **objects** (*Any*) –\n\nrich.print\\_json(*json=None*, *\\**, *data=None*, *indent=2*, *highlight=True*, *skip\\_keys=False*, *ensure\\_ascii=False*, *check\\_circular=True*, *allow\\_nan=True*, *default=None*, *sort\\_keys=False*)[source]¶\nPretty prints JSON. Output will be valid JSON.\n\nParameters\n* **json** (*str*) – A string containing JSON.\n* **data** (*Any*) – If json is not supplied, then encode this data.\n* **indent** (*int**,* *optional*) – Number of spaces to indent. Defaults to 2.\n* **highlight** (*bool**,* *optional*) – Enable highlighting of output: Defaults to True.\n* **skip\\_keys** (*bool**,* *optional*) – Skip keys not of a basic type. Defaults to False.\n* **ensure\\_ascii** (*bool**,* *optional*) – Escape all non-ascii characters. Defaults to False.\n* **check\\_circular** (*bool**,* *optional*) – Check for circular references. Defaults to True.\n* **allow\\_nan** (*bool**,* *optional*) – Allow NaN and Infinity values. Defaults to True.\n* **default** (*Callable**,* *optional*) – A callable that converts values that can not be encoded\nin to something that can be JSON encoded. Defaults to None.\n* **sort\\_keys** (*bool**,* *optional*) – Sort dictionary keys. Defaults to False.\n\nrich.reconfigure(*\\*args*, *\\*\\*kwargs*)[source]¶\nReconfigures the global console by replacing it with another.\n\nParameters\n* **\\*args** (*Any*) – Positional arguments for the replacement `Console`.\n* **\\*\\*kwargs** (*Any*) – Keyword arguments for the replacement `Console`.\n\n# rich.json¶\n\n\n*class* rich.json.JSON(*json*, *indent=2*, *highlight=True*, *skip\\_keys=False*, *ensure\\_ascii=False*, *check\\_circular=True*, *allow\\_nan=True*, *default=None*, *sort\\_keys=False*)[source]¶\nA renderable which pretty prints JSON.\n\nParameters\n* **json** (*str*) – JSON encoded data.\n* **indent** (*Union**[**None**,* *int**,* *str**]**,* *optional*) – Number of characters to indent by. Defaults to 2.\n* **highlight** (*bool**,* *optional*)\n\n==================\n Document 2 \n----------------\n rich.progress¶\n\n\n*class* rich.progress.BarColumn(*bar\\_width=40*, *style='bar.back'*, *complete\\_style='bar.complete'*, *finished\\_style='bar.finished'*, *pulse\\_style='bar.pulse'*, *table\\_column=None*)[source]¶\nRenders a visual progress bar.\n\nParameters\n* **bar\\_width** (*Optional**[**int**]**,* *optional*) – Width of bar or None for full width. Defaults to 40.\n* **style** (*StyleType**,* *optional*) – Style for the bar background. Defaults to “bar.back”.\n* **complete\\_style** (*StyleType**,* *optional*) – Style for the completed bar. Defaults to “bar.complete”.\n* **finished\\_style** (*StyleType**,* *optional*) – Style for a finished bar. Defaults to “bar.finished”.\n* **pulse\\_style** (*StyleType**,* *optional*) – Style for pulsing bars. Defaults to “bar.pulse”.\n* **table\\_column** (*Optional**[**Column**]*) –\n\n\nrender(*task*)[source]¶\nGets a progress bar widget for a task.\n\nParameters\n**task** (*Task*) – \n\nReturn type\n*ProgressBar*\n\n\n*class* rich.progress.DownloadColumn(*binary\\_units=False*, *table\\_column=None*)[source]¶\nRenders file size downloaded and total, e.g. ‘0.5/2.3 GB’.\n\nParameters\n* **binary\\_units** (*bool**,* *optional*) – Use binary units, KiB, MiB etc. Defaults to False.\n* **table\\_column** (*Optional**[**Column**]*) –\n\n\nrender(*task*)[source]¶\nCalculate common unit for completed and total.\n\nReturn type\n*Text*\n\n\n*class* rich.progress.FileSizeColumn(*table\\_column=None*)[source]¶\nRenders completed filesize.\n\nParameters\n**table\\_column** (*Optional**[**Column**]*) – \n\n\nrender(*task*)[source]¶\nShow data completed.\n\n\n*class* rich.progress.MofNCompleteColumn(*separator='/'*, *table\\_column=None*)[source]¶\nRenders completed count/total, e.g. ‘ 10/1000’.\n\n\nBest for bounded tasks with int quantities.\n\n\nSpace pads the completed count so that progress length does not change as task progresses\npast powers of 10.\n\nParameters\n* **separator** (*str**,* *optional*) – Text to separate completed and total values. Defaults to “/”.\n* **table\\_column** (*Optional**[**Column**]*) –\n\n\nrender(*task*)[source]¶\nShow completed/total.\n\n\n*class* rich.progress.Progress(*\\*columns*, *console=None*, *auto\\_refresh=True*, *refresh\\_per\\_second=10*, *speed\\_estimate\\_period=30.0*, *transient=False*, *redirect\\_stdout=True*, *redirect\\_stderr=True*, *get\\_time=None*, *disable=False*, *expand=False*)[source]¶\nRenders an auto-updating progress bar(s).\n\nParameters\n* **console** (*Console**,* *optional*) – Optional Console instance. Default will an internal Console instance writing to stdout.\n* **auto\\_refresh** (*bool**,* *optional*) – Enable auto refresh. If disabled, you will need to call refresh().\n* **refresh\\_per\\_second** (*Optional**[**float**]**,* *optional*) – Number of times per second to refresh the progress information or None to use default (10). Defaults to None.\n* **speed\\_estimate\\_period** (*float*) – (float, optional): Period (in seconds) used to calculate the speed estimate. Defaults to 30.\n* **transient** (*bool*) – (bool, optional): Clear the progress on exit. Defaults to False.\n* **redirect\\_stdout** (*bool*) – (bool, optional): Enable redirection of stdout, so `print` may be used. Defaults to True.\n* **redirect\\_stderr** (*bool*) – (bool, optional): Enable redirection of stderr. Defaults to True.\n* **get\\_time** (*Optional**[**Callable**[**[**]**,* *float**]**]*) – (Callable, optional): A callable that gets the current time, or None to use Console.get\\_time. Defaults to None.\n* **disable** (*bool**,* *optional*) – Disable progress display. Defaults to False\n* **expand** (*bool**,* *optional*) – Expand tasks table to fit width. Defaults to False.\n* **columns** (*Union**[**str**,* *ProgressColumn**]*) –\n\n\nadd\\_task(*description*, *start=True*, *total=100.0*, *completed=0*, *visible=True*, *\\*\\*fields*)[source]¶\nAdd a new ‘task’ to the Progress display.\n\nParameters\n* **description** (*str*) – A description of the task.\n* **start** (*bool**,* *optional*) – Start the task immediately (to calculate elapsed time). If set to False,\nyou will need to call start manually. Defaults to True.\n* **total** (*float**,* *optional*) – Number of total steps in the progress if known.\nSet to None to render a pulsing animation. Defaults to 100.\n* **completed** (*int**,* *optional*) – Number of steps completed so far. Defaults to 0.\n* **visible** (*bool**,* *optional*) – Enable display of the task. Defaults to True.\n* **\\*\\*fields** (*str*) – Additional data fields required for rendering.\n\nReturns\nAn ID you can use when calling update.\n\nReturn type\nTaskID\n\nadvance(*task\\_id*, *advance=1*)[source]¶\nAdvance task by a number of steps.\n\nParameters\n* **task\\_id** (*TaskID*) – ID of task.\n* **advance** (*float*) – Number of steps to advance. Default is 1.\n\n*property* finished*: bool*¶\nCheck if all tasks have been completed.\n\n*classmethod* get\\_default\\_columns()[source]¶\n\nGet the default columns used for a new Progress instance:* a text column for the description (TextColumn)\n* the bar itself (BarColumn)\n* a text column showing completion percentage (TextColumn)\n* an estimated-time-remaining column (TimeRemainingColumn)\n\n\nIf the Progress instance is created without passing a columns argument,\nthe default columns defined here will be used.\n\n\nYou can also create a Progress instance using custom columns before\nand/or after the defaults, as in this example:\n\n> \n> \n> progress = Progress(SpinnerColumn(),\n> \\*Progress.default\\_columns(),\n> “Elapsed:”,\n> TimeElapsedColumn(),\n> \n> \n> \n> \n> )\n> \n> \n> \n\n\nThis code shows the creation of a Progress display, containing\na spinner to the left, the default columns, and a labeled elapsed\ntime column.\n\nReturn type\n*Tuple*[*ProgressColumn*, …]\n\nget\\_renderable()[source]¶\nGet a renderable for the progress display.\n\nReturn type\n*Union*[*ConsoleRenderable*, *RichCast*, str]\n\nget\\_renderables()[source]¶\nGet a number of renderables for the progress display.\n\nReturn type\n*Iterable*[*Union*[*ConsoleRenderable*, *RichCast*, str]]\n\nmake\\_tasks\\_table(*tasks*)[source]¶\nGet a table to render the Progress display.\n\nParameters\n**tasks** (*Iterable**[**Task**]*) – An iterable of Task instances, one per row of the table.\n\nopen(*file: Union[str, PathLike[str], bytes]*, *mode: typing\\_extensions.Literal[rb]*, *buffering: int = -1*, *encoding: Optional[str] = None*, *errors: Optional[str] = None*, *newline: Optional[str] = None*, *\\**, *total: Optional[int] = None*, *task\\_id: Optional[TaskID] = None*, *description: str = 'Reading...'*) → BinaryIO[source]¶\n\nopen(*file: Union[str, PathLike[str], bytes]*, *mode: Union[typing\\_extensions.Literal[r], typing\\_extensions.Literal[rt]]*, *buffering: int = -1*, *encoding: Optional[str] = None*, *errors: Optional[str] = None*, *newline: Optional[str] = None*, *\\**, *total: Optional[int] = None*, *task\\_id: Optional[TaskID] = None*, *description: str = 'Reading...'*) → TextIO\nTrack progress while reading from a binary file.\n\nParameters\n* **path** (*Union**[**str**,* *PathLike**[**str**]**]*) – The path to the file to read.\n* **mode** (*str*) – The mode to use to open the file. Only supports “r”, “rb” or “rt”.\n* **buffering** (*int*) – The buffering strategy to use, see `io.open()`.\n* **encoding** (*str**,* *optional*) – The encoding to use when reading in text mode, see `io.open()`.\n* **errors** (*str**,* *optional*) – The error handling strategy for decoding errors, see `io.open()`.\n* **newline** (*str**,* *optional*) – The strategy for handling newlines in text mode, see `io.open()`.\n* **total** (*int**,* *optional*) – Total number of bytes to read. If none given, os.stat(path).st\\_size is used.\n* **task\\_id** (*TaskID*) – Task to track. Default is new task.\n* **description** (*str**,* *optional*) – Description of task, if new task is created.\n\nReturns\nA readable file-like object in binary mode.\n\nReturn type\nBinaryIO\n\nRaises\n**ValueError** – When an invalid mode is given.\n\nrefresh()[source]¶\nRefresh (render) the progress information.\n\nremove\\_task(*task\\_id*)[source]¶\nDelete a task if it exists.\n\nParameters\n**task\\_id** (*TaskID*) – A task ID.\n\nreset(*task\\_id*, *\\**, *start=True*, *total=None*, *completed=0*, *visible=None*, *description=None*, *\\*\\*fields*)[source]¶\nReset a task so completed is 0 and the clock is reset.\n\nParameters\n* **task\\_id** (*TaskID*) – ID of task.\n* **start** (*bool**,* *optional*) – Start the task after reset. Defaults to True.\n* **total** (*float**,* *optional*) – New total steps in task, or None to use current total. Defaults to None.\n* **completed** (*int**,* *optional*) – Number of steps completed. Defaults to 0.\n* **visible** (*bool**,* *optional*) – Enable display of the task. Defaults to True.\n* **description** (*str**,* *optional*) – Change task description if not None. Defaults to None.\n* **\\*\\*fields** (*str*) – Additional data fields required for rendering.\n\nstart()[source]¶\nStart the progress display.\n\nstart\\_task(*task\\_id*)[source]¶\nStart a task.\n\n\nStarts a task (used when calculating elapsed time). You may need to call this manually,\nif you called `add\\_task` with `start=False`.\n\nParameters\n**task\\_id** (*TaskID*) – ID of task.\n\nstop()[source]¶\nStop the progress display.\n\nstop\\_task(*task\\_id*)[source]¶\nStop a task.\n\n\nThis will freeze the elapsed time on the task.\n\n*property* task\\_ids*: List[TaskID]*¶\nA list of task IDs.\n\n*property* tasks*: List[Task]*¶\nGet a list of Task instances.\n\ntrack(*sequence*, *total=None*, *task\\_id=None*, *description='Working...'*, *update\\_period=0.1*)[source]¶\nTrack progress by iterating over a sequence.\n\nParameters\n* **sequence** (*Sequence**[**ProgressType**]*) – A sequence of values you want to iterate over and track progress.\n* **total** (*Optional**[**float**]*) – (float, optional): Total number of steps. Default is len(sequence).\n* **task\\_id** (*Optional**[**TaskID**]*) – (TaskID): Task to track. Default is new task.\n* **description** (*str*) – (str, optional): Description of task, if new task is created.\n* **update\\_period** (*float**,* *optional*) – Minimum time (in seconds) between calls to update(). Defaults to 0.1.\n\nReturns\nAn iterable of values taken from the provided sequence.\n\nReturn type\nIterable[ProgressType]\n\nupdate(*task\\_id*, *\\**, *total=None*, *completed=None*, *advance=None*, *description=None*, *visible=None*, *refresh=False*, *\\*\\*fields*)[source]¶\nUpdate information associated with a task.\n\nParameters\n* **task\\_id** (*TaskID*) – Task id (returned by add\\_task).\n* **total** (*float**,* *optional*) – Updates task.total if not None.\n* **completed** (*float**,* *optional*) – Updates task.completed if not None.\n* **advance** (*float**,* *optional*) – Add a value to task.completed if not None.\n* **description** (*str**,* *optional*) – Change task description if not None.\n* **visible** (*bool**,* *optional*) – Set visible flag if not None.\n* **refresh** (*bool*) – Force a refresh of progress information. Default is False.\n* **\\*\\*fields** (*Any*) – Additional data fields required for rendering.\n\nwrap\\_file(*file*, *total=None*, *\\**, *task\\_id=None*, *description='Reading...'*)[source]¶\nTrack progress file reading from a binary file.\n\nParameters\n* **file** (*BinaryIO*) – A file-like object opened in binary mode.\n* **total** (*int**,* *optional*) – Total number of bytes to read. This must be provided unless a task with a total is also given.\n* **task\\_id** (*TaskID*) – Task to track. Default is new task.\n* **description** (*str**,* *optional*) – Description of task, if new task is created.\n\nRaises\n**ValueError** – When no total value can be extracted from the arguments or the task.\n\n\n*class* rich.progress.ProgressColumn(*table\\_column=None*)[source]¶\nBase class for a widget to use in progress display.\n\n\nget\\_table\\_column()[source]¶\nGet a table column, used to build tasks table.\n\n*abstract* render(*task*)[source]¶\nShould return a renderable object.\n\n\n*class* rich.progress.ProgressSample(*timestamp*, *completed*)[source]¶\nSample of progress for a given time.\n\nParameters\n* **timestamp** (*float*) –\n* **completed** (*float*) –\n\n\n*property* completed¶\nNumber of steps completed.\n\n*property* timestamp¶\nTimestamp of sample.\n\n\n*class* rich.progress.RenderableColumn(*renderable=''*, *\\**, *table\\_column=None*)[source]¶\nA column to insert an arbitrary column.\n\nParameters\n* **renderable** (*RenderableType**,* *optional*) – Any renderable. Defaults to empty string.\n* **table\\_column** (*Optional**[**Column**]*) –\n\n\nrender(*task*)[source]¶\nShould return a renderable object.\n\n\n*class* rich.progress.SpinnerColumn(*spinner\\_name='dots'*, *style='progress.spinner'*, *speed=1.0*, *finished\\_text=' '*, *table\\_column=None*)[source]¶\nA column with a ‘spinner’ animation.\n\nParameters\n* **spinner\\_name** (*str**,* *optional*) – Name of spinner animation. Defaults to “dots”.\n* **style** (*StyleType**,* *optional*) – Style of spinner. Defaults to “progress.spinner”.\n* **speed** (*float**,* *optional*) – Speed factor of spinner. Defaults to 1.0.\n* **finished\\_text** (*TextType**,* *optional*) – Text used when task is finished. Defaults to ” “.\n* **table\\_column** (*Optional**[**Column**]*) –\n\nset\\_spinner(*spinner\\_name*, *spinner\\_style='progress.spinner'*, *speed=1.0*)[source]¶\nSet a new spinner.\n\nParameters\n* **spinner\\_name** (*str*) – Spinner name, see python -m rich.spinner.\n* **spinner\\_style** (*Optional**[**StyleType**]**,* *optional*) – Spinner style. Defaults to “progress.spinner”.\n* **speed** (*float**,* *optional*) – Speed factor of spinner. Defaults to 1.0.\n\n\n*class* rich.progress.Task(*id*, *description*, *total*, *completed*, *\\_get\\_time*, *finished\\_time=None*, *visible=True*, *fields=<factory>*, *finished\\_speed=None*, *\\_lock=<factory>*)[source]¶\nInformation regarding a progress task.\n\n\nThis object should be considered read-only outside of the `Progress` class.\n\nParameters\n* **id** (*TaskID*) –\n* **description** (*str*) –\n* **total** (*Optional**[**float**]*) –\n* **completed** (*float*) –\n* **\\_get\\_time** (*Callable**[**[**]**,* *float**]*) –\n* **finished\\_time** (*Optional**[**float**]*) –\n* **visible** (*bool*) –\n* **fields** (*Dict**[**str**,* *Any**]*) –\n* **finished\\_speed** (*Optional**[**float**]*) –\n* **\\_lock** (*RLock*) –\n\n\ncompleted*: float*¶\nNumber of steps completed\n\nType\nfloat\n\ndescription*: str*¶\nDescription of the task.\n\n*property* elapsed*: Optional[float]*¶\nTime elapsed since task was started, or `None` if the task hasn’t started.\n\nType\nOptional[float]\n\nfields*: Dict[str, Any]*¶\nArbitrary fields passed in via Progress.update.\n\nType\ndict\n\n*property* finished*: bool*¶\nCheck if the task has finished.\n\nfinished\\_speed*: Optional[float]* *= None*¶\nThe last speed for a finished task.\n\nfinished\\_time*: Optional[float]* *= None*¶\nTime task was finished.\n\nget\\_time()[source]¶\nfloat: Get the current time, in seconds.\n\nReturn type\nfloat\n\nid*: TaskID*¶\nTask ID associated with this task (used in Progress methods).\n\n*property* percentage*: float*¶\nGet progress of task as a percentage. If a None total was set, returns 0\n\n*property* remaining*: Optional[float]*¶\nGet the number of steps remaining, if a non-None total was set.\n\n*property* speed*: Optional[float]*¶\nGet the estimated speed in steps per second.\n\nstart\\_time*: Optional[float]* *= None*¶\nTime this task was started, or None if not started.\n\n*property* started*: bool*¶\nCheck if the task as started.\n\nstop\\_time*: Optional[float]* *= None*¶\nTime this task was stopped, or None if not stopped.\n\n*property* time\\_remaining*: Optional[float]*¶\nGet estimated time to completion, or `None` if no data.\n\ntotal*: Optional[float]*¶\nTotal number of steps in this task.\n\nvisible*: bool* *= True*¶\nIndicates if this task is visible in the progress display.\n\n\n*class* rich.progress.TaskProgressColumn(*text\\_format='[progress.percentage]{task.percentage:>3.0f}%'*, *text\\_format\\_no\\_percentage=''*, *style='none'*, *justify='left'*, *markup=True*, *highlighter=None*, *table\\_column=None*, *show\\_speed=False*)[source]¶\nShow task progress as a percentage.\n\nParameters\n* **text\\_format** (*str**,* *optional*) – Format for percentage display. Defaults to “[progress.percentage]{task.percentage:>3.0f}%”.\n* **text\\_format\\_no\\_percentage** (*str**,* *optional*) – Format if percentage is unknown. Defaults to “”.\n* **style** (*StyleType**,* *optional*) – Style of output. Defaults to “none”.\n* **justify** (*JustifyMethod**,* *optional*) – Text justification. Defaults to “left”.\n* **markup** (*bool**,* *optional*) – Enable markup. Defaults to True.\n* **highlighter** (*Optional**[**Highlighter**]**,* *optional*) – Highlighter to apply to output. Defaults to None.\n* **table\\_column** (*Optional**[**Column**]**,* *optional*) – Table Column to use. Defaults to None.\n* **show\\_speed** (*bool**,* *optional*) – Show speed if total is unknown. Defaults to False.\n\n*classmethod* render\\_speed(*speed*)[source]¶\nRender the speed in iterations per second.\n\nParameters\n* **task** (*Task*) – A Task object.\n* **speed** (*Optional**[**float**]*) –\n\nReturns\nText object containing the task speed.\n\n\n*class* rich.progress.TextColumn(*text\\_format*, *style='none'*, *justify='left'*, *markup=True*, *highlighter=None*, *table\\_column=None*)[source]¶\nA column containing text.\n\nParameters\n* **text\\_format** (*str*) –\n* **style** (*Union**[**str**,* *Style**]*) –\n* **justify** (*typing\\_extensions.Literal**[**default**,* *left**,* *center**,* *right**,* *full**]*) –\n* **markup** (*bool*) –\n* **highlighter** (*Optional**[**Highlighter**]*) –\n* **table\\_column** (*Optional**[**Column**]*) –\n\n\n*class* rich.progress.TimeElapsedColumn(*table\\_column=None*)[source]¶\nRenders time elapsed.\n\n\nrender(*task*)[source]¶\nShow time elapsed.\n\n\n*class* rich.progress.TimeRemainingColumn(*compact=False*, *elapsed\\_when\\_finished=False*, *table\\_column=None*)[source]¶\nRenders estimated time remaining.\n\nParameters\n* **compact** (*bool**,* *optional*) – Render MM:SS when time remaining is less than an hour. Defaults to False.\n* **elapsed\\_when\\_finished** (*bool**,* *optional*) – Render time elapsed when the task is finished. Defaults to False.\n* **table\\_column** (*Optional**[**Column**]*) –\n\n\nrender(*task*)[source]¶\nShow time remaining.\n\n\n*class* rich.progress.TotalFileSizeColumn(*table\\_column=None*)[source]¶\nRenders total filesize.\n\n\n*class* rich.progress.TransferSpeedColumn(*table\\_column=None*)[source]¶\nRenders human readable transfer speed.\n\n\nrender(*task*)[source]¶\nShow data transfer speed.\n\n\nrich.progress.open(*file: Union[str, PathLike[str], bytes]*, *mode: Union[typing\\_extensions.Literal[rt], typing\\_extensions.Literal[r]]*, *buffering: int = -1*, *encoding: Optional[str] = None*, *errors: Optional[str] = None*, *newline: Optional[str] = None*, *\\**, *total: Optional[int] = None*, *description: str = 'Reading...'*, *auto\\_refresh: bool = True*, *console: Optional[Console] = None*, *transient: bool = False*, *get\\_time: Optional[Callable[[], float]] = None*, *refresh\\_per\\_second: float = 10*, *style: Union[str, Style] = 'bar.back'*, *complete\\_style: Union[str, Style] = 'bar.complete'*, *finished\\_style: Union[str, Style] = 'bar.finished'*, *pulse\\_style: Union[str, Style] = 'bar.pulse'*, *disable: bool = False*) → AbstractContextManager[TextIO][source]¶\n\nrich.progress.open(*file: Union[str, PathLike[str], bytes]*, *mode: typing\\_extensions.Literal[rb]*, *buffering: int = -1*, *encoding: Optional[str] = None*, *errors: Optional[str] = None*, *newline: Optional[str] = None*, *\\**, *total: Optional[int] = None*, *description: str = 'Reading...'*, *auto\\_refresh: bool = True*, *console: Optional[Console] = None*, *transient: bool = False*, *get\\_time: Optional[Callable[[], float]] = None*, *refresh\\_per\\_second: float = 10*, *style: Union[str, Style] = 'bar.back'*, *complete\\_style: Union[str, Style] = 'bar.complete'*, *finished\\_style: Union[str, Style] = 'bar.finished'*, *pulse\\_style: Union[str, Style] = 'bar.pulse'*, *disable: bool = False*) → AbstractContextManager[BinaryIO]\nRead bytes from a file while tracking progress.\n\nParameters\n* **path** (*Union**[**str**,* *PathLike**[**str**]**,* *BinaryIO**]*) – The path to the file to read, or a file-like object in binary mode.\n* **mode** (*str*) – The mode to use to open the file. Only supports “r”, “rb” or “rt”.\n* **buffering** (*int*) – The buffering strategy to use, see `io.open()`.\n* **encoding** (*str**,* *optional*) – The encoding to use when reading in text mode, see `io.open()`.\n* **errors** (*str**,* *optional*) – The error handling strategy for decoding errors, see `io.open()`.\n* **newline** (*str**,* *optional*) – The strategy for handling newlines in text mode, see `io.open()`\n* **total** – (int, optional): Total number of bytes to read. Must be provided if reading from a file handle. Default for a path is os.stat(file).st\\_size.\n* **description** (*str**,* *optional*) – Description of task show next to progress bar. Defaults to “Reading”.\n* **auto\\_refresh** (*bool**,* *optional*) – Automatic refresh, disable to force a refresh after each iteration. Default is True.\n* **transient** – (bool, optional): Clear the progress on exit. Defaults to False.\n* **console** (*Console**,* *optional*) – Console to write to. Default creates internal Console instance.\n* **refresh\\_per\\_second** (*float*) – Number of times per second to refresh the progress information. Defaults to 10.\n* **style** (*StyleType**,* *optional*) – Style for the bar background. Defaults to “bar.back”.\n* **complete\\_style** (*StyleType**,* *optional*) – Style for the completed bar. Defaults to “bar.complete”.\n* **finished\\_style** (*StyleType**,* *optional*) – Style for a finished bar. Defaults to “bar.finished”.\n* **pulse\\_style** (*StyleType**,* *optional*) – Style for pulsing bars. Defaults to “bar.pulse”.\n* **disable** (*bool**,* *optional*) – Disable display of progress.\n* **encoding** – The encoding to use when reading in text mode.\n\nReturns\nA context manager yielding a progress reader.\n\nReturn type\nContextManager[BinaryIO]\n\nrich.progress.track(*sequence*, *description='Working...'*, *total=None*, *auto\\_refresh=True*, *console=None*, *transient=False*, *get\\_time=None*, *refresh\\_per\\_second=10*, *style='bar.back'*, *complete\\_style='bar.complete'*, *finished\\_style='bar.finished'*, *pulse\\_style='bar.pulse'*, *update\\_period=0.1*, *disable=False*, *show\\_speed=True*)[source]¶\nTrack progress by iterating over a sequence.\n\nParameters\n* **sequence** (*Iterable**[**ProgressType**]*) – A sequence (must support “len”) you wish to iterate over.\n* **description** (*str**,* *optional*) – Description of task show next to progress bar. Defaults to “Working”.\n* **total** (*Optional**[**float**]*) – (float, optional): Total number of steps. Default is len(sequence).\n* **auto\\_refresh** (*bool**,* *optional*) – Automatic refresh, disable to force a refresh after each iteration. Default is True.\n* **transient** (*bool*) – (bool, optional): Clear the progress on exit. Defaults to False.\n* **console** (*Console**,* *optional*) – Console to write to. Default creates internal Console instance.\n* **refresh\\_per\\_second** (*float*) – Number of times per second to refresh the progress information. Defaults to 10.\n* **style** (*StyleType**,* *optional*) – Style for the bar background. Defaults to “bar.back”.\n* **complete\\_style** (*StyleType**,* *optional*) – Style for the completed bar. Defaults to “bar.complete”.\n* **finished\\_style** (*StyleType**,* *optional*) – Style for a finished bar. Defaults to “bar.finished”.\n* **pulse\\_style** (*StyleType**,* *optional*) – Style for pulsing bars. Defaults to “bar.pulse”.\n* **update\\_period** (*float**,* *optional*) – Minimum time (in seconds) between calls to update(). Defaults to 0.1.\n* **disable** (*bool**,* *optional*) – Disable display of progress.\n* **show\\_speed** (*bool**,* *optional*) – Show speed if total isn’t known. Defaults to True.\n* **get\\_time** (*Optional**[**Callable**[**[**]**,* *float**]**]*) –\n\nReturns\nAn iterable of the values in the sequence.\n\nrich.progress.wrap\\_file(*file*, *total*, *\\**, *description='Reading...'*, *auto\\_refresh=True*, *console=None*, *transient=False*, *get\\_time=None*, *refresh\\_per\\_second=10*, *style='bar.back'*, *complete\\_style='bar.complete'*, *finished\\_style='bar.finished'*, *pulse\\_style='bar.pulse'*, *disable=False*)[source]¶\nRead bytes from a file while tracking progress.\n\nParameters\n* **file** (*Union**[**str**,* *PathLike**[**str**]**,* *BinaryIO**]*) – The path to the file to read, or a file-like object in binary mode.\n* **total** (*int*) – Total number of bytes to read.\n* **description** (*str**,* *optional*) – Description of task show next to progress bar. Defaults to “Reading”.\n* **auto\\_refresh** (*bool**,* *optional*) – Automatic refresh, disable to force a refresh after each iteration. Default is True.\n* **transient** (*bool*) – (bool, optional): Clear the progress on exit. Defaults to False.\n* **console** (*Console**,* *optional*) – Console to write to. Default creates internal Console instance.\n* **refresh\\_per\\_second** (*float*) – Number of times per second to refresh the progress information. Defaults to 10.\n* **style** (*StyleType**,* *optional*) – Style for the bar background. Defaults to “bar.back”.\n* **complete\\_style** (*StyleType**,* *optional*) – Style for the completed bar. Defaults to “bar.complete”.\n* **finished\\_style** (*StyleType**,* *optional*) – Style for a finished bar. Defaults to “bar.finished”.\n* **pulse\\_style** (*StyleType**,* *optional*) – Style for pulsing bars. Defaults to “bar.pulse”.\n* **disable** (*bool**,* *optional*) – Disable display of progress.\n* **get\\_time** (*Optional**[**Callable**[**[**]**,* *float**]**]*) –\n\n# rich.prompt¶\n\n\n*class* rich.prompt.Confirm(*prompt=''*, *\\**, *console=None*, *password=False*, *choices=None*, *show\\_default=True*, *show\\_choices=True*)[source]¶\nA yes / no confirmation prompt.\n\n```\n>>> if Confirm.ask(\"Continue\"):\n run\\_job()\n\n\nprocess\\_response(*value*)[source]¶\nConvert choices to a bool.\n\nParameters\n**value** (*str*) – \n\nrender\\_default(*default*)[source]¶\nRender the default as (y) or (n) rather than True/False.\n\nParameters\n**default** (*DefaultType*) – \n\nresponse\\_type¶\nalias of `bool`\n\n\n*class* rich.prompt.FloatPrompt(*prompt=''*, *\\**,\n\n==================\n Document 3 \n----------------\n rich.spinner¶\n\n\n*class* rich.spinner.Spinner(*name*, *text=''*, *\\**, *style=None*, *speed=1.0*)[source]¶\nA spinner animation.\n\nParameters\n* **name** (*str*) – Name of spinner (run python -m rich.spinner).\n* **text** (*RenderableType**,* *optional*) – A renderable to display at the right of the spinner (str or Text typically). Defaults to “”.\n* **style** (*StyleType**,* *optional*) – Style for spinner animation. Defaults to None.\n* **speed** (*float**,* *optional*) – Speed factor for animation. Defaults to 1.0.\n\nRaises\n**KeyError** – If name isn’t one of the supported spinner animations.\n\n\nrender(*time*)[source]¶\nRender the spinner for a given time.\n\nParameters\n**time** (*float*) – Time in seconds.\n\nReturns\nA renderable containing animation frame.\n\nReturn type\nRenderableType\n\nupdate(*\\**, *text=''*, *style=None*, *speed=None*)[source]¶\nUpdates attributes of a spinner after it has been started.\n\nParameters\n* **text** (*RenderableType**,* *optional*) – A renderable to display at the right of the spinner (str or Text typically). Defaults to “”.\n* **style** (*StyleType**,* *optional*) – Style for spinner animation. Defaults to None.\n* **speed** (*float**,* *optional*) – Speed factor for animation. Defaults to None.\n# rich.status¶\n\n\n*class* rich.status.Status(*status*, *\\**, *console=None*, *spinner='dots'*, *spinner\\_style='status.spinner'*, *speed=1.0*, *refresh\\_per\\_second=12.5*)[source]¶\nDisplays a status indicator with a ‘spinner’ animation.\n\nParameters\n* **status** (*RenderableType*) – A status renderable (str or Text typically).\n* **console** (*Console**,* *optional*) – Console instance to use, or None for global console. Defaults\n\n==================\n Document 4 \n----------------\n rich.live¶\n\n\n*class* rich.live.Live(*renderable=None*, *\\**, *console=None*, *screen=False*, *auto\\_refresh=True*, *refresh\\_per\\_second=4*, *transient=False*, *redirect\\_stdout=True*, *redirect\\_stderr=True*, *vertical\\_overflow='ellipsis'*, *get\\_renderable=None*)[source]¶\nRenders an auto-updating live display of any given renderable.\n\nParameters\n* **renderable** (*RenderableType**,* *optional*) – The renderable to live display. Defaults to displaying nothing.\n* **console** (*Console**,* *optional*) – Optional Console instance. Default will an internal Console instance writing to stdout.\n* **screen** (*bool**,* *optional*) – Enable alternate screen mode. Defaults to False.\n* **auto\\_refresh** (*bool**,* *optional*) – Enable auto refresh. If disabled, you will need to call refresh() or update() with refresh flag. Defaults to True\n* **refresh\\_per\\_second** (*float**,* *optional*) – Number of times per second to refresh the live display. Defaults to 4.\n* **transient** (*bool**,* *optional*) – Clear the renderable on exit (has no effect when screen=True). Defaults to False.\n* **redirect\\_stdout** (*bool**,* *optional*) – Enable redirection of stdout, so `print` may be used. Defaults to True.\n* **redirect\\_stderr** (*bool**,* *optional*) – Enable redirection of stderr. Defaults to True.\n* **vertical\\_overflow** (*VerticalOverflowMethod**,* *optional*) – How to handle renderable when it is too tall for the console. Defaults to “ellipsis”.\n* **get\\_renderable** (*Callable**[**[**]**,* *RenderableType**]**,* *optional*) – Optional callable to get renderable. Defaults to None.\n\n\n*property* is\\_started*: bool*¶\nCheck if live display has been started.\n\nprocess\\_renderables(*renderables*)[source]¶\nProcess renderables to restore cursor and display progress.\n\nParameters\n**renderables** (*List**[**ConsoleRenderable**]*) – \n\nReturn type\n*List*[*ConsoleRenderable*]\n\nrefresh()[source]¶\nUpdate the display of the Live Render.\n\n*property* renderable*: Union[ConsoleRenderable, RichCast, str]*¶\nGet the renderable that is being displayed\n\nReturns\nDisplayed renderable.\n\nstart(*refresh=False*)[source]¶\nStart live rendering display.\n\nParameters\n**refresh** (*bool**,* *optional*) – Also refresh. Defaults to False.\n\nstop()[source]¶\nStop live rendering display.\n\nupdate(*renderable*, *\\**, *refresh=False*)[source]¶\nUpdate the renderable that is being displayed\n\nParameters\n* **renderable** (*RenderableType*) – New renderable to use.\n* **refresh** (*bool**,* *optional*) – Refresh the display. Defaults to False.\n\n# rich.logging¶\n\n\n*class* rich.logging.RichHandler(*level=0*, *console=None*, *\\**, *show\\_time=True*, *omit\\_repeated\\_times=True*, *show\\_level=True*, *show\\_path=True*, *enable\\_link\\_path=True*, *highlighter=None*, *markup=False*, *rich\\_tracebacks=False*, *tracebacks\\_width=None*, *tracebacks\\_extra\\_lines=3*, *tracebacks\\_theme=None*, *tracebacks\\_word\\_wrap=True*, *tracebacks\\_show\\_locals=False*, *tracebacks\\_suppress=()*, *locals\\_max\\_length=10*, *locals\\_max\\_string=80*, *log\\_time\\_format='[%x %X]'*, *keywords=None*)[source]¶\nA logging handler that renders output with Rich. The time / level / message and file are"
    },
    {
        "instruction": "Generate a Python program using the 'rich' API to display a styled and colorful list of inspirational quotes. The program should include the quote text, the author, and the source of the quote.",
        "api": "rich",
        "output": "# Install rich\n#!pip install rich\nfrom rich.console import Console\nfrom rich.table import Table\n\n# Define a list of inspirational quotes\nquotes = [\n    {\n        \"text\": \"The future belongs to those who believe in the beauty of their dreams.\",\n        \"author\": \"Eleanor Roosevelt\",\n        \"source\": \"You Learn by Living: Eleven Keys for a More Fulfilling Life\",\n    },\n    {\n        \"text\": \"The only limit to our realization of tomorrow will be our doubts of today.\",\n        \"author\": \"Franklin D. Roosevelt\",\n        \"source\": \"Great Speeches\",\n    },\n    {\n        \"text\": \"Believe you can and you're halfway there.\",\n        \"author\": \"Theodore Roosevelt\",\n        \"source\": \"Quoted in Half-Hearted Enemies: Nova Scotia Texts\",\n    },\n    {\n        \"text\": \"The greatest glory in living lies not in never falling, but in rising every time we fall.\",\n        \"author\": \"Nelson Mandela\",\n        \"source\": \"Long Walk to Freedom: The Autobiography of Nelson Mandela\",\n    },\n]\n\n# Create a rich table for inspirational quotes\nconsole = Console()\ntable = Table(title=\"Inspirational Quotes\", style=\"italic\")\ntable.add_column(\"Quote\", style=\"bold\")\ntable.add_column(\"Author\", style=\"bold\")\ntable.add_column(\"Source\", style=\"bold\")\n\n# Populate the table with quote data\nfor quote in quotes:\n    table.add_row(quote[\"text\"], quote[\"author\"], quote[\"source\"])\n\n# Display the inspirational quotes\nconsole.print(table)\n\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n rich¶\n\n\nRich text and beautiful formatting in the terminal.\n\n\nrich.get\\_console()[source]¶\nGet a global `Console` instance. This function is used when Rich requires a Console,\nand hasn’t been explicitly given one.\n\nReturns\nA console instance.\n\nReturn type\nConsole\n\nrich.inspect(*obj*, *\\**, *console=None*, *title=None*, *help=False*, *methods=False*, *docs=True*, *private=False*, *dunder=False*, *sort=True*, *all=False*, *value=True*)[source]¶\nInspect any Python object.\n\n\n* inspect(<OBJECT>) to see summarized info.\n* inspect(<OBJECT>, methods=True) to see methods.\n* inspect(<OBJECT>, help=True) to see full (non-abbreviated) help.\n* inspect(<OBJECT>, private=True) to see private attributes (single underscore).\n* inspect(<OBJECT>, dunder=True) to see attributes beginning with double underscore.\n* inspect(<OBJECT>, all=True) to see all attributes.\n\nParameters\n* **obj** (*Any*) – An object to inspect.\n* **title** (*str**,* *optional*) – Title to display over inspect result, or None use type. Defaults to None.\n* **help** (*bool**,* *optional*) – Show full help text rather than just first paragraph. Defaults to False.\n* **methods** (*bool**,* *optional*) – Enable inspection of callables. Defaults to False.\n* **docs** (*bool**,* *optional*) – Also render doc strings. Defaults to True.\n* **private** (*bool**,* *optional*) – Show private attributes (beginning with underscore). Defaults to False.\n* **dunder** (*bool**,* *optional*) – Show attributes starting with double underscore. Defaults to False.\n* **sort** (*bool**,* *optional*) – Sort attributes alphabetically. Defaults to True.\n* **all** (*bool**,* *optional*) – Show all attributes. Defaults to False.\n* **value** (*bool**,* *optional*) – Pretty print value. Defaults to True.\n* **console** (*Optional**[**Console**]*) –\n\nrich.print(*\\*objects*, *sep=' '*, *end='\\n'*, *file=None*, *flush=False*)[source]¶\nPrint object(s) supplied via positional arguments.\nThis function has an identical signature to the built-in print.\nFor more advanced features, see the `Console` class.\n\nParameters\n* **sep** (*str**,* *optional*) – Separator between printed objects. Defaults to ” “.\n* **end** (*str**,* *optional*) – Character to write at end of output. Defaults to “\\n”.\n* **file** (*IO**[**str**]**,* *optional*) – File to write to, or None for stdout. Defaults to None.\n* **flush** (*bool**,* *optional*) – Has no effect as Rich always flushes output. Defaults to False.\n* **objects** (*Any*) –\n\nrich.print\\_json(*json=None*, *\\**, *data=None*, *indent=2*, *highlight=True*, *skip\\_keys=False*, *ensure\\_ascii=False*, *check\\_circular=True*, *allow\\_nan=True*, *default=None*, *sort\\_keys=False*)[source]¶\nPretty prints JSON. Output will be valid JSON.\n\nParameters\n* **json** (*str*) – A string containing JSON.\n* **data** (*Any*) – If json is not supplied, then encode this data.\n* **indent** (*int**,* *optional*) – Number of spaces to indent. Defaults to 2.\n* **highlight** (*bool**,* *optional*) – Enable highlighting of output: Defaults to True.\n* **skip\\_keys** (*bool**,* *optional*) – Skip keys not of a basic type. Defaults to False.\n* **ensure\\_ascii** (*bool**,* *optional*) – Escape all non-ascii characters. Defaults to False.\n* **check\\_circular** (*bool**,* *optional*) – Check for circular references. Defaults to True.\n* **allow\\_nan** (*bool**,* *optional*) – Allow NaN and Infinity values. Defaults to True.\n* **default** (*Callable**,* *optional*) – A callable that converts values that can not be encoded\nin to something that can be JSON encoded. Defaults to None.\n* **sort\\_keys** (*bool**,* *optional*) – Sort dictionary keys. Defaults to False.\n\nrich.reconfigure(*\\*args*, *\\*\\*kwargs*)[source]¶\nReconfigures the global console by replacing it with another.\n\nParameters\n* **\\*args** (*Any*) – Positional arguments for the replacement `Console`.\n* **\\*\\*kwargs** (*Any*) – Keyword arguments for the replacement `Console`.\n\n# rich.json¶\n\n\n*class* rich.json.JSON(*json*, *indent=2*, *highlight=True*, *skip\\_keys=False*, *ensure\\_ascii=False*, *check\\_circular=True*, *allow\\_nan=True*, *default=None*, *sort\\_keys=False*)[source]¶\nA renderable which pretty prints JSON.\n\nParameters\n* **json** (*str*) – JSON encoded data.\n* **indent** (*Union**[**None**,* *int**,* *str**]**,* *optional*) – Number of characters to indent by. Defaults to 2.\n* **highlight** (*bool**,* *optional*)\n\n==================\n Document 1 \n----------------\n rich.console¶\n\n\n*class* rich.console.Capture(*console*)[source]¶\nContext manager to capture the result of printing to the console.\nSee `capture()` for how to use.\n\nParameters\n**console** (*Console*) – A console instance to capture output.\n\n\nget()[source]¶\nGet the result of the capture.\n\n\n*exception* rich.console.CaptureError[source]¶\nAn error in the Capture context manager.\n\n*class* rich.console.Console(*\\**, *color\\_system='auto'*, *force\\_terminal=None*, *force\\_jupyter=None*, *force\\_interactive=None*, *soft\\_wrap=False*, *theme=None*, *stderr=False*, *file=None*, *quiet=False*, *width=None*, *height=None*, *style=None*, *no\\_color=None*, *tab\\_size=8*, *record=False*, *markup=True*, *emoji=True*, *emoji\\_variant=None*, *highlight=True*, *log\\_time=True*, *log\\_path=True*, *log\\_time\\_format='[%X]'*, *highlighter=<rich.highlighter.ReprHighlighter object>*, *legacy\\_windows=None*, *safe\\_box=True*, *get\\_datetime=None*, *get\\_time=None*, *\\_environ=None*)[source]¶\nA high level console interface.\n\nParameters\n* **color\\_system** (*str**,* *optional*) – The color system supported by your terminal,\neither `\"standard\"`, `\"256\"` or `\"truecolor\"`. Leave as `\"auto\"` to autodetect.\n* **force\\_terminal** (*Optional**[**bool**]**,* *optional*) – Enable/disable terminal control codes, or None to auto-detect terminal. Defaults to None.\n* **force\\_jupyter** (*Optional**[**bool**]**,* *optional*) – Enable/disable Jupyter rendering, or None to auto-detect Jupyter. Defaults to None.\n* **force\\_interactive** (*Optional**[**bool**]**,* *optional*) – Enable/disable interactive mode, or None to auto detect. Defaults to None.\n* **soft\\_wrap** (*Optional**[**bool**]**,* *optional*) – Set soft wrap default on print method. Defaults to False.\n* **theme** (*Theme**,* *optional*) – An optional style theme object, or `None` for default theme.\n* **stderr** (*bool**,* *optional*) – Use stderr rather than stdout if `file` is not specified. Defaults to False.\n* **file** (*IO**,* *optional*) – A file object where the console should write to. Defaults to stdout.\n* **quiet** (*bool**,* *Optional*) – Boolean to suppress all output. Defaults to False.\n* **width** (*int**,* *optional*) – The width of the terminal. Leave as default to auto-detect width.\n* **height** (*int**,* *optional*) – The height of the terminal. Leave as default to auto-detect height.\n* **style** (*StyleType**,* *optional*) – Style to apply to all output, or None for no style. Defaults to None.\n* **no\\_color** (*Optional**[**bool**]**,* *optional*) – Enabled no color mode, or None to auto detect. Defaults to None.\n* **tab\\_size** (*int**,* *optional*) – Number of spaces used to replace a tab character. Defaults to 8.\n* **record** (*bool**,* *optional*) – Boolean to enable recording of terminal output,\nrequired to call `export\\_html()`, `export\\_svg()`, and `export\\_text()`. Defaults to False.\n* **markup** (*bool**,* *optional*) – Boolean to enable Console Markup. Defaults to True.\n* **emoji** (*bool**,* *optional*) – Enable emoji code. Defaults to True.\n* **emoji\\_variant** (*str**,* *optional*) – Optional emoji variant, either “text” or “emoji”. Defaults to None.\n* **highlight** (*bool**,* *optional*) – Enable automatic highlighting. Defaults to True.\n* **log\\_time** (*bool**,* *optional*) – Boolean to enable logging of time by `log()` methods. Defaults to True.\n* **log\\_path** (*bool**,* *optional*) – Boolean to enable the logging of the caller by `log()`. Defaults to True.\n* **log\\_time\\_format** (*Union**[**str**,* *TimeFormatterCallable**]**,* *optional*) – If `log\\_time` is enabled, either string for strftime or callable that formats the time. Defaults to “[%X] “.\n* **highlighter** (*HighlighterType**,* *optional*) – Default highlighter.\n* **legacy\\_windows** (*bool**,* *optional*) – Enable legacy Windows mode, or `None` to auto detect. Defaults to `None`.\n* **safe\\_box** (*bool**,* *optional*) – Restrict box options that don’t render on legacy Windows.\n* **get\\_datetime** (*Callable**[**[**]**,* *datetime**]**,* *optional*) – Callable that gets the current time as a datetime.datetime object (used by Console.log),\nor None for datetime.now.\n* **get\\_time** (*Callable**[**[**]**,* *time**]**,* *optional*) – Callable that gets the current time in seconds, default uses time.monotonic.\n* **\\_environ** (*Mapping**[**str**,* *str**]*) –\n\n\nbegin\\_capture()[source]¶\nBegin capturing console output. Call `end\\_capture()` to exit capture mode and return output.\n\nbell()[source]¶\nPlay a ‘bell’ sound (if supported by the terminal).\n\ncapture()[source]¶\nA context manager to *capture* the result of print() or log() in a string,\nrather than writing it to the console.\n\n\nExample\n\n```\n>>> from rich.console import Console\n>>> console = Console()\n>>> with console.capture() as capture:\n...     console.print(\"[bold magenta]Hello World[/]\")\n>>> print(capture.get())\n\nReturns\nContext manager with disables writing to the terminal.\n\nReturn type\nCapture\n\nclear(*home=True*)[source]¶\nClear the screen.\n\nParameters\n**home** (*bool**,* *optional*) – Also move the cursor to ‘home’ position. Defaults to True.\n\nclear\\_live()[source]¶\nClear the Live instance.\n\n*property* color\\_system*: Optional[str]*¶\nGet color system string.\n\nReturns\n“standard”, “256” or “truecolor”.\n\nReturn type\nOptional[str]\n\ncontrol(*\\*control*)[source]¶\nInsert non-printing control codes.\n\nParameters\n* **control\\_codes** (*str*) – Control codes, such as those that may move the cursor.\n* **control** (*Control*) –\n\n*property* encoding*: str*¶\nGet the encoding of the console file, e.g. `\"utf-8\"`.\n\nReturns\nA standard encoding string.\n\nend\\_capture()[source]¶\nEnd capture mode and return captured string.\n\nReturns\nConsole output.\n\nexport\\_html(*\\**, *theme=None*, *clear=True*, *code\\_format=None*, *inline\\_styles=False*)[source]¶\nGenerate HTML from console contents (requires record=True argument in constructor).\n\nParameters\n* **theme** (*TerminalTheme**,* *optional*) – TerminalTheme object containing console colors.\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`.\n* **code\\_format** (*str**,* *optional*) – Format string to render HTML. In addition to ‘{foreground}’,\n‘{background}’, and ‘{code}’, should contain ‘{stylesheet}’ if inline\\_styles is `False`.\n* **inline\\_styles** (*bool**,* *optional*) – If `True` styles will be inlined in to spans, which makes files\nlarger but easier to cut and paste markup. If `False`, styles will be embedded in a style tag.\nDefaults to False.\n\nReturns\nString containing console contents as HTML.\n\nexport\\_svg(*\\**, *title='Rich'*, *theme=None*, *clear=True*, *code\\_format='<svg class=\"rich-terminal\" viewBox=\"0 0 {width} {height}\" xmlns=\"http://www.w3.org/2000/svg\">\\n    <!-- Generated with Rich https://www.textualize.io -->\\n    <style>\\n\\n    @font-face {{\\n        font-family: \"Fira Code\";\\n        src: local(\"FiraCode-Regular\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Regular.woff2\") format(\"woff2\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Regular.woff\") format(\"woff\");\\n        font-style: normal;\\n        font-weight: 400;\\n    }}\\n    @font-face {{\\n        font-family: \"Fira Code\";\\n        src: local(\"FiraCode-Bold\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Bold.woff2\") format(\"woff2\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Bold.woff\") format(\"woff\");\\n        font-style: bold;\\n        font-weight: 700;\\n    }}\\n\\n    .{unique\\_id}-matrix {{\\n        font-family: Fira Code, monospace;\\n        font-size: {char\\_height}px;\\n        line-height: {line\\_height}px;\\n        font-variant-east-asian: full-width;\\n    }}\\n\\n    .{unique\\_id}-title {{\\n        font-size: 18px;\\n        font-weight: bold;\\n        font-family: arial;\\n    }}\\n\\n    {styles}\\n    </style>\\n\\n    <defs>\\n    <clipPath id=\"{unique\\_id}-clip-terminal\">\\n      <rect x=\"0\" y=\"0\" width=\"{terminal\\_width}\" height=\"{terminal\\_height}\" />\\n    </clipPath>\\n    {lines}\\n    </defs>\\n\\n    {chrome}\\n    <g transform=\"translate({terminal\\_x}, {terminal\\_y})\" clip-path=\"url(#{unique\\_id}-clip-terminal)\">\\n    {backgrounds}\\n    <g class=\"{unique\\_id}-matrix\">\\n    {matrix}\\n    </g>\\n    </g>\\n</svg>\\n'*, *font\\_aspect\\_ratio=0.61*, *unique\\_id=None*)[source]¶\nGenerate an SVG from the console contents (requires record=True in Console constructor).\n\nParameters\n* **title** (*str**,* *optional*) – The title of the tab in the output image\n* **theme** (*TerminalTheme**,* *optional*) – The `TerminalTheme` object to use to style the terminal\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`\n* **code\\_format** (*str**,* *optional*) – Format string used to generate the SVG. Rich will inject a number of variables\ninto the string in order to form the final SVG output. The default template used and the variables\ninjected by Rich can be found by inspecting the `console.CONSOLE\\_SVG\\_FORMAT` variable.\n* **font\\_aspect\\_ratio** (*float**,* *optional*) – The width to height ratio of the font used in the `code\\_format`\nstring. Defaults to 0.61, which is the width to height ratio of Fira Code (the default font).\nIf you aren’t specifying a different font inside `code\\_format`, you probably don’t need this.\n* **unique\\_id** (*str**,* *optional*) – unique id that is used as the prefix for various elements (CSS styles, node\nids). If not set, this defaults to a computed value based on the recorded content.\n\nexport\\_text(*\\**, *clear=True*, *styles=False*)[source]¶\nGenerate text from console contents (requires record=True argument in constructor).\n\nParameters\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`.\n* **styles** (*bool**,* *optional*) – If `True`, ansi escape codes will be included. `False` for plain text.\nDefaults to `False`.\n\nReturns\nString containing console contents.\n\n*property* file*: IO[str]*¶\nGet the file object to write to.\n\nget\\_style(*name*, *\\**, *default=None*)[source]¶\nGet a Style instance by its theme name or parse a definition.\n\nParameters\n* **name** (*str*) – The name of a style or a style definition.\n* **default** (*Optional**[**Union**[**str**,* *Style**]**]*) –\n\nReturns\nA Style object.\n\nReturn type\nStyle\n\nRaises\n**MissingStyle** – If no style could be parsed from name.\n\n*property* height*: int*¶\nGet the height of the console.\n\nReturns\nThe height (in lines) of the console.\n\ninput(*prompt=''*, *\\**, *markup=True*, *emoji=True*, *password=False*, *stream=None*)[source]¶\nDisplays a prompt and waits for input from the user. The prompt may contain color / style.\n\n\nIt works in the same way as Python’s builtin `input()` function and provides elaborate line editing and history features if Python’s builtin `readline` module is previously loaded.\n\nParameters\n* **prompt** (*Union**[**str**,* *Text**]*) – Text to render in the prompt.\n* **markup** (*bool**,* *optional*) – Enable console markup (requires a str prompt). Defaults to True.\n* **emoji** (*bool**,* *optional*) – Enable emoji (requires a str prompt). Defaults to True.\n* **password** (*bool*) – (bool, optional): Hide typed text. Defaults to False.\n* **stream** (*Optional**[**TextIO**]*) – (TextIO, optional): Optional file to read input from (rather than stdin). Defaults to None.\n\nReturns\nText read from stdin.\n\n*property* is\\_alt\\_screen*: bool*¶\nCheck if the alt screen was enabled.\n\nReturns\nTrue if the alt screen was enabled, otherwise False.\n\nReturn type\nbool\n\n*property* is\\_dumb\\_terminal*: bool*¶\nDetect dumb terminal.\n\nReturns\nTrue if writing to a dumb terminal, otherwise False.\n\n*property* is\\_terminal*: bool*¶\nCheck if the console is writing to a terminal.\n\nReturns\nTrue if the console writing to a device capable of\nunderstanding terminal codes, otherwise False.\n\nline(*count=1*)[source]¶\nWrite new line(s).\n\nParameters\n**count** (*int**,* *optional*) – Number of new lines. Defaults to 1.\n\nlog(*\\*objects*, *sep=' '*, *end='\\n'*, *style=None*, *justify=None*, *emoji=None*, *markup=None*, *highlight=None*, *log\\_locals=False*, *\\_stack\\_offset=1*)[source]¶\nLog rich content to the terminal.\n\nParameters\n* **objects** (*positional args*) – Objects to log to the terminal.\n* **sep** (*str**,* *optional*) – String to write between print data. Defaults to ” “.\n* **end** (*str**,* *optional*) – String to write at end of print data. Defaults to “\\n”.\n* **style** (*Union**[**str**,* *Style**]**,* *optional*) – A style to apply to output. Defaults to None.\n* **justify** (*str**,* *optional*) – One of “left”, “right”, “center”, or “full”. Defaults to `None`.\n* **emoji** (*Optional**[**bool**]**,* *optional*) – Enable emoji code, or `None` to use console default. Defaults to None.\n* **markup** (*Optional**[**bool**]**,* *optional*) – Enable markup, or `None` to use console default. Defaults to None.\n* **highlight** (*Optional**[**bool**]**,* *optional*) – Enable automatic highlighting, or `None` to use console default. Defaults to None.\n* **log\\_locals** (*bool**,* *optional*) – Boolean to enable logging of locals where `log()`\nwas called. Defaults to False.\n* **\\_stack\\_offset** (*int**,* *optional*) – Offset of caller from end of call stack. Defaults to 1.\n\nmeasure(*renderable*, *\\**, *options=None*)[source]¶\nMeasure a renderable. Returns a `Measurement` object which contains\ninformation regarding the number of characters required to print the renderable.\n\nParameters\n* **renderable** (*RenderableType*) – Any renderable or string.\n* **options** (*Optional**[**ConsoleOptions**]**,* *optional*) – Options to use when measuring, or None\nto use default options. Defaults to None.\n\nReturns\nA measurement of the renderable.\n\n*property* options*: ConsoleOptions*¶\nGet default console options.\n\nout(*\\*objects*, *sep=' '*, *end='\\n'*, *style=None*, *highlight=None*)[source]¶\nOutput to the terminal. This is a low-level way of writing to the terminal which unlike\n`print()` won’t pretty print, wrap text, or apply markup, but will\noptionally apply highlighting and a basic style.\n\nParameters\n* **sep** (*str**,* *optional*) – String to write between print data. Defaults to ” “.\n* **end** (*str**,* *optional*) – String to write at end of print data. Defaults to “\\n”.\n* **style** (*Union**[**str**,* *Style**]**,* *optional*) – A style to apply to output. Defaults to None.\n* **highlight** (*Optional**[**bool**]**,* *optional*) – Enable automatic highlighting, or `None` to use\nconsole default. Defaults to `None`.\n* **objects** (*Any*) –\n\npager(*pager=None*, *styles=False*, *links=False*)[source]¶\nA context manager to display anything printed within a “pager”. The pager application\nis defined by the system and will typically support at least pressing a key to scroll.\n\nParameters\n* **pager** (*Pager**,* *optional*) – A pager object, or None to use `SystemPager`. Defaults to None.\n* **styles** (*bool**,* *optional*) – Show styles in pager. Defaults to False.\n* **links** (*bool**,* *optional*) – Show links in pager. Defaults to False.\n\nReturn type\n*PagerContext*\n\n```\n>>> from rich.console import Console\n>>> from rich.\\_\\_main\\_\\_ import make\\_test\\_card\n>>> console = Console()\n>>> with console.pager():\n console.print(make\\_test\\_card())\n\nReturns\nA context manager.\n\nReturn type\nPagerContext\n\nParameters\n* **pager** (*Optional**[**Pager**]*) –\n* **styles** (*bool*) –\n* **links** (*bool*) –\n\npop\\_render\\_hook()[source]¶\nPop the last renderhook from the stack.\n\npop\\_theme()[source]¶\nRemove theme from top of stack, restoring previous theme.\n\nprint(*\\*objects*, *sep=' '*, *end='\\n'*, *style=None*, *justify=None*, *overflow=None*, *no\\_wrap=None*, *emoji=None*, *markup=None*, *highlight=None*, *width=None*, *height=None*, *crop=True*, *soft\\_wrap=None*, *new\\_line\\_start=False*)[source]¶\nPrint to the console.\n\nParameters\n* **objects** (*positional args*) – Objects to log to the terminal.\n* **sep** (*str**,* *optional*) – String to write between print data. Defaults to ” “.\n* **end** (*str**,* *optional*) – String to write at end of print data. Defaults to “\\n”.\n* **style** (*Union**[**str**,* *Style**]**,* *optional*) – A style to apply to output. Defaults to None.\n* **justify** (*str**,* *optional*) – Justify method: “default”, “left”, “right”, “center”, or “full”. Defaults to `None`.\n* **overflow** (*str**,* *optional*) – Overflow method: “ignore”, “crop”, “fold”, or “ellipsis”. Defaults to None.\n* **no\\_wrap** (*Optional**[**bool**]**,* *optional*) – Disable word wrapping. Defaults to None.\n* **emoji** (*Optional**[**bool**]**,* *optional*) – Enable emoji code, or `None` to use console default. Defaults to `None`.\n* **markup** (*Optional**[**bool**]**,* *optional*) – Enable markup, or `None` to use console default. Defaults to `None`.\n* **highlight** (*Optional**[**bool**]**,* *optional*) – Enable automatic highlighting, or `None` to use console default. Defaults to `None`.\n* **width** (*Optional**[**int**]**,* *optional*) – Width of output, or `None` to auto-detect. Defaults to `None`.\n* **crop** (*Optional**[**bool**]**,* *optional*) – Crop output to width of terminal. Defaults to True.\n* **soft\\_wrap** (*bool**,* *optional*) – Enable soft wrap mode which disables word wrapping and cropping of text or `None` for\nConsole default. Defaults to `None`.\n* **new\\_line\\_start** (*bool**,* *False*) – Insert a new line at the start if the output contains more than one line. Defaults to `False`.\n* **height** (*Optional**[**int**]*) –\n\nprint\\_exception(*\\**, *width=100*, *extra\\_lines=3*, *theme=None*, *word\\_wrap=False*, *show\\_locals=False*, *suppress=()*, *max\\_frames=100*)[source]¶\nPrints a rich render of the last exception and traceback.\n\nParameters\n* **width** (*Optional**[**int**]**,* *optional*) – Number of characters used to render code. Defaults to 100.\n* **extra\\_lines** (*int**,* *optional*) – Additional lines of code to render. Defaults to 3.\n* **theme** (*str**,* *optional*) – Override pygments theme used in traceback\n* **word\\_wrap** (*bool**,* *optional*) – Enable word wrapping of long lines. Defaults to False.\n* **show\\_locals** (*bool**,* *optional*) – Enable display of local variables. Defaults to False.\n* **suppress** (*Iterable**[**Union**[**str**,* *ModuleType**]**]*) – Optional sequence of modules or paths to exclude from traceback.\n* **max\\_frames** (*int*) – Maximum number of frames to show in a traceback, 0 for no maximum. Defaults to 100.\n\nprint\\_json(*json=None*, *\\**, *data=None*, *indent=2*, *highlight=True*, *skip\\_keys=False*, *ensure\\_ascii=False*, *check\\_circular=True*, *allow\\_nan=True*, *default=None*, *sort\\_keys=False*)[source]¶\nPretty prints JSON. Output will be valid JSON.\n\nParameters\n* **json** (*Optional**[**str**]*) – A string containing JSON.\n* **data** (*Any*) – If json is not supplied, then encode this data.\n* **indent** (*Union**[**None**,* *int**,* *str**]**,* *optional*) – Number of spaces to indent. Defaults to 2.\n* **highlight** (*bool**,* *optional*) – Enable highlighting of output: Defaults to True.\n* **skip\\_keys** (*bool**,* *optional*) – Skip keys not of a basic type. Defaults to False.\n* **ensure\\_ascii** (*bool**,* *optional*) – Escape all non-ascii characters. Defaults to False.\n* **check\\_circular** (*bool**,* *optional*) – Check for circular references. Defaults to True.\n* **allow\\_nan** (*bool**,* *optional*) – Allow NaN and Infinity values. Defaults to True.\n* **default** (*Callable**,* *optional*) – A callable that converts values that can not be encoded\nin to something that can be JSON encoded. Defaults to None.\n* **sort\\_keys** (*bool**,* *optional*) – Sort dictionary keys. Defaults to False.\n\npush\\_render\\_hook(*hook*)[source]¶\nAdd a new render hook to the stack.\n\nParameters\n**hook** (*RenderHook*) – Render hook instance.\n\npush\\_theme(*theme*, *\\**, *inherit=True*)[source]¶\nPush a new theme on to the top of the stack, replacing the styles from the previous theme.\nGenerally speaking, you should call `use\\_theme()` to get a context manager, rather\nthan calling this method directly.\n\nParameters\n* **theme** (*Theme*) – A theme instance.\n* **inherit** (*bool**,* *optional*) – Inherit existing styles. Defaults to True.\n\nrender(*renderable*, *options=None*)[source]¶\nRender an object in to an iterable of Segment instances.\n\n\nThis method contains the logic for rendering objects with the console protocol.\nYou are unlikely to need to use it directly, unless you are extending the library.\n\nParameters\n* **renderable** (*RenderableType*) – An object supporting the console protocol, or\nan object that may be converted to a string.\n* **options** (*ConsoleOptions**,* *optional*) – An options object, or None to use self.options. Defaults to None.\n\nReturns\nAn iterable of segments that may be rendered.\n\nrender\\_lines(*renderable*, *options=None*, *\\**, *style=None*, *pad=True*, *new\\_lines=False*)[source]¶\nRender objects in to a list of lines.\n\n> \n> The output of render\\_lines is useful when further formatting of rendered console text\n> is required, such as the Panel class which draws a border around any renderable object.\n> \n> \n> \n> Args:renderable (RenderableType): Any object renderable in the console.\n> options (Optional[ConsoleOptions], optional): Console options, or None to use self.options. Default to `None`.\n> style (Style, optional): Optional style to apply to renderables. Defaults to `None`.\n> pad (bool, optional): Pad lines shorter than render width. Defaults to `True`.\n> new\\_lines (bool, optional): Include “\n> \n> \n> \n> \n> \n\n\n” characters at end of lines.\n\n> \n> \n> Returns:List[List[Segment]]: A list of lines, where a line is a list of Segment objects.\n> \n> \n> \n> \n> \n\nParameters\n* **renderable** (*Union**[**ConsoleRenderable**,* *RichCast**,* *str**]*) –\n* **options** (*Optional**[**ConsoleOptions**]*) –\n* **style** (*Optional**[**Style**]*) –\n* **pad** (*bool*) –\n* **new\\_lines** (*bool*) –\n\nrender\\_str(*text*, *\\**, *style=''*, *justify=None*, *overflow=None*, *emoji=None*, *markup=None*, *highlight=None*, *highlighter=None*)[source]¶\nConvert a string to a Text instance. This is called automatically if\nyou print or log a string.\n\nParameters\n* **text** (*str*) – Text to render.\n* **style** (*Union**[**str**,* *Style**]**,* *optional*) – Style to apply to rendered text.\n* **justify** (*str**,* *optional*) – Justify method: “default”, “left”, “center”, “full”, or “right”. Defaults to `None`.\n* **overflow** (*str**,* *optional*) – Overflow method: “crop”, “fold”, or “ellipsis”. Defaults to `None`.\n* **emoji** (*Optional**[**bool**]**,* *optional*) – Enable emoji, or `None` to use Console default.\n* **markup** (*Optional**[**bool**]**,* *optional*) – Enable markup, or `None` to use Console default.\n* **highlight** (*Optional**[**bool**]**,* *optional*) – Enable highlighting, or `None` to use Console default.\n* **highlighter** (*HighlighterType**,* *optional*) – Optional highlighter to apply.\n\nReturns\nRenderable object.\n\nReturn type\nConsoleRenderable\n\nrule(*title=''*, *\\**, *characters='─'*, *style='rule.line'*, *align='center'*)[source]¶\nDraw a line with optional centered title.\n\nParameters\n* **title** (*str**,* *optional*) – Text to render over the rule. Defaults to “”.\n* **characters** (*str**,* *optional*) – Character(s) to form the line. Defaults to “─”.\n* **style** (*str**,* *optional*) – Style of line. Defaults to “rule.line”.\n* **align** (*str**,* *optional*) – How to align the title, one of “left”, “center”, or “right”. Defaults to “center”.\n\nsave\\_html(*path*, *\\**, *theme=None*, *clear=True*, *code\\_format='<!DOCTYPE html>\\n<html>\\n<head>\\n<meta charset=\"UTF-8\">\\n<style>\\n{stylesheet}\\nbody {{\\n    color: {foreground};\\n    background-color: {background};\\n}}\\n</style>\\n</head>\\n<body>\\n    <pre style=\"font-family:Menlo,\\'DejaVu Sans Mono\\',consolas,\\'Courier New\\',monospace\"><code>{code}</code></pre>\\n</body>\\n</html>\\n'*, *inline\\_styles=False*)[source]¶\nGenerate HTML from console contents and write to a file (requires record=True argument in constructor).\n\nParameters\n* **path** (*str*) – Path to write html file.\n* **theme** (*TerminalTheme**,* *optional*) – TerminalTheme object containing console colors.\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`.\n* **code\\_format** (*str**,* *optional*) – Format string to render HTML. In addition to ‘{foreground}’,\n‘{background}’, and ‘{code}’, should contain ‘{stylesheet}’ if inline\\_styles is `False`.\n* **inline\\_styles** (*bool**,* *optional*) – If `True` styles will be inlined in to spans, which makes files\nlarger but easier to cut and paste markup. If `False`, styles will be embedded in a style tag.\nDefaults to False.\n\nsave\\_svg(*path*, *\\**, *title='Rich'*, *theme=None*, *clear=True*, *code\\_format='<svg class=\"rich-terminal\" viewBox=\"0 0 {width} {height}\" xmlns=\"http://www.w3.org/2000/svg\">\\n    <!-- Generated with Rich https://www.textualize.io -->\\n    <style>\\n\\n    @font-face {{\\n        font-family: \"Fira Code\";\\n        src: local(\"FiraCode-Regular\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Regular.woff2\") format(\"woff2\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Regular.woff\") format(\"woff\");\\n        font-style: normal;\\n        font-weight: 400;\\n    }}\\n    @font-face {{\\n        font-family: \"Fira Code\";\\n        src: local(\"FiraCode-Bold\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Bold.woff2\") format(\"woff2\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Bold.woff\") format(\"woff\");\\n        font-style: bold;\\n        font-weight: 700;\\n    }}\\n\\n    .{unique\\_id}-matrix {{\\n        font-family: Fira Code, monospace;\\n        font-size: {char\\_height}px;\\n        line-height: {line\\_height}px;\\n        font-variant-east-asian: full-width;\\n    }}\\n\\n    .{unique\\_id}-title {{\\n        font-size: 18px;\\n        font-weight: bold;\\n        font-family: arial;\\n    }}\\n\\n    {styles}\\n    </style>\\n\\n    <defs>\\n    <clipPath id=\"{unique\\_id}-clip-terminal\">\\n      <rect x=\"0\" y=\"0\" width=\"{terminal\\_width}\" height=\"{terminal\\_height}\" />\\n    </clipPath>\\n    {lines}\\n    </defs>\\n\\n    {chrome}\\n    <g transform=\"translate({terminal\\_x}, {terminal\\_y})\" clip-path=\"url(#{unique\\_id}-clip-terminal)\">\\n    {backgrounds}\\n    <g class=\"{unique\\_id}-matrix\">\\n    {matrix}\\n    </g>\\n    </g>\\n</svg>\\n'*, *font\\_aspect\\_ratio=0.61*, *unique\\_id=None*)[source]¶\nGenerate an SVG file from the console contents (requires record=True in Console constructor).\n\nParameters\n* **path** (*str*) – The path to write the SVG to.\n* **title** (*str**,* *optional*) – The title of the tab in the output image\n* **theme** (*TerminalTheme**,* *optional*) – The `TerminalTheme` object to use to style the terminal\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`\n* **code\\_format** (*str**,* *optional*) – Format string used to generate the SVG. Rich will inject a number of variables\ninto the string in order to form the final SVG output. The default template used and the variables\ninjected by Rich can be found by inspecting the `console.CONSOLE\\_SVG\\_FORMAT` variable.\n* **font\\_aspect\\_ratio** (*float**,* *optional*) – The width to height ratio of the font used in the `code\\_format`\nstring. Defaults to 0.61, which is the width to height ratio of Fira Code (the default font).\nIf you aren’t specifying a different font inside `code\\_format`, you probably don’t need this.\n* **unique\\_id** (*str**,* *optional*) – unique id that is used as the prefix for various elements (CSS styles, node\nids). If not set, this defaults to a computed value based on the recorded content.\n\nsave\\_text(*path*, *\\**, *clear=True*, *styles=False*)[source]¶\nGenerate text from console and save to a given location (requires record=True argument in constructor).\n\nParameters\n* **path** (*str*) – Path to write text files.\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`.\n* **styles** (*bool**,* *optional*) – If `True`, ansi style codes will be included. `False` for plain text.\nDefaults to `False`.\n\nscreen(*hide\\_cursor=True*, *style=None*)[source]¶\nContext manager to enable and disable ‘alternative screen’ mode.\n\nParameters\n* **hide\\_cursor** (*bool**,* *optional*) – Also hide the cursor. Defaults to False.\n* **style** (*Style**,* *optional*) – Optional style for screen. Defaults to None.\n\nReturns\nContext which enables alternate screen on enter, and disables it on exit.\n\nReturn type\n~ScreenContext\n\nset\\_alt\\_screen(*enable=True*)[source]¶\nEnables alternative screen mode.\n\n\nNote, if you enable this mode, you should ensure that is disabled before\nthe application exits. See `screen()` for a context manager\nthat handles this for you.\n\nParameters\n**enable** (*bool**,* *optional*) – Enable (True) or disable (False) alternate screen. Defaults to True.\n\nReturns\nTrue if the control codes were written.\n\nset\\_live(*live*)[source]¶\nSet Live instance. Used by Live context manager.\n\nParameters\n**live** (*Live*) – Live instance using this Console.\n\nRaises\n**errors.LiveError** – If this Console has a Live context currently active.\n\nset\\_window\\_title(*title*)[source]¶\nSet the title of the console terminal window.\n\n\nWarning: There is no means within Rich of “resetting” the window title to its\nprevious value, meaning the title you set will persist even after your application\nexits.\n\n\n`fish` shell resets the window title before and after each command by default,\nnegating this issue. Windows Terminal and command prompt will also reset the title for you.\nMost other shells and terminals, however, do not do this.\n\n\nSome terminals may require configuration changes before you can set the title.\nSome terminals may not support setting the title at all.\n\n\nOther software (including the terminal itself, the shell, custom prompts, plugins, etc.)\nmay also set the terminal window title. This could result in whatever value you write\nusing this method being overwritten.\n\nParameters\n**title** (*str*) – The new title of the terminal window.\n\nTrue if the control code to change the terminal title waswritten, otherwise False. Note that a return value of True\ndoes not guarantee that the window title has actually changed,\nsince the feature may be unsupported/disabled in some terminals.\n\n\nReturn type\nbool\n\nshow\\_cursor(*show=True*)[source]¶\nShow or hide the cursor.\n\nParameters\n**show** (*bool**,* *optional*) – Set visibility of the cursor.\n\n*property* size*: ConsoleDimensions*¶\nGet the size of the console.\n\nReturns\nA named tuple containing the dimensions.\n\nReturn type\nConsoleDimensions\n\nstatus(*status*, *\\**, *spinner='dots'*, *spinner\\_style='status.spinner'*, *speed=1.0*, *refresh\\_per\\_second=12.5*)[source]¶\nDisplay a status and spinner.\n\nParameters\n* **status** (*RenderableType*) – A status renderable (str or Text typically).\n* **spinner** (*str**,* *optional*) – Name of spinner animation (see python -m rich.spinner). Defaults to “dots”.\n* **spinner\\_style** (*StyleType**,* *optional*) – Style of spinner. Defaults to “status.spinner”.\n* **speed** (*float**,* *optional*) – Speed factor for spinner animation. Defaults to 1.0.\n* **refresh\\_per\\_second** (*float**,* *optional*) – Number of refreshes per second. Defaults to 12.5.\n\nReturns\nA Status object that may be used as a context manager.\n\nReturn type\nStatus\n\nupdate\\_screen(*renderable*, *\\**, *region=None*, *options=None*)[source]¶\nUpdate the screen at a given offset.\n\nParameters\n* **renderable** (*RenderableType*) – A Rich renderable.\n* **region** (*Region**,* *optional*) – Region of screen to update, or None for entire screen. Defaults to None.\n* **x** (*int**,* *optional*) – x offset. Defaults to 0.\n* **y** (*int**,* *optional*) – y offset. Defaults to 0.\n* **options** (*Optional**[**ConsoleOptions**]*) –\n\nRaises\n**errors.NoAltScreen** – If the Console isn’t in alt screen mode.\n\nupdate\\_screen\\_lines(*lines*, *x=0*, *y=0*)[source]¶\nUpdate lines of the screen at a given offset.\n\nParameters\n* **lines** (*List**[**List**[**Segment**]**]*) – Rendered lines (as produced by `render\\_lines()`).\n* **x** (*int**,* *optional*) – x offset (column no). Defaults to 0.\n* **y** (*int**,* *optional*) – y offset (column no). Defaults to 0.\n\nuse\\_theme(*theme*, *\\**, *inherit=True*)[source]¶\nUse a different theme for the duration of the context manager.\n\nParameters\n* **theme** (*Theme*) – Theme instance to user.\n* **inherit** (*bool**,* *optional*) – Inherit existing console styles. Defaults to True.\n\nReturns\n[description]\n\nReturn type\nThemeContext\n\n*property* width*: int*¶\nGet the width of the console.\n\nReturns\nThe width (in characters) of the console.\n\n\n*class* rich.console.ConsoleDimensions(*width*, *height*)[source]¶\nSize of the terminal.\n\nParameters\n* **width** (*int*) –\n* **height** (*int*) –\n\n\n*property* height¶\nThe height of the console in lines.\n\n*property* width¶\nThe width of the console in ‘cells’.\n\n\n*class* rich.console.ConsoleOptions(*size*, *legacy\\_windows*, *min\\_width*, *max\\_width*, *is\\_terminal*, *encoding*, *max\\_height*, *justify=None*, *overflow=None*, *no\\_wrap=False*, *highlight=None*, *markup=None*, *height=None*)[source]¶\nOptions for \\_\\_rich\\_console\\_\\_ method.\n\nParameters\n* **size** (*ConsoleDimensions*) –\n* **legacy\\_windows** (*bool*) –\n* **min\\_width** (*int*) –\n* **max\\_width** (*int*) –\n* **is\\_terminal** (*bool*) –\n* **encoding** (*str*) –\n* **max\\_height** (*int*) –\n* **justify** (*Optional**[**typing\\_extensions.Literal**[**default**,* *left**,* *center**,* *right**,* *full**]**]*) –\n* **overflow** (*Optional**[**typing\\_extensions.Literal**[**fold**,* *crop**,* *ellipsis**,* *ignore**]**]*) –\n* **no\\_wrap** (*Optional**[**bool**]*) –\n* **highlight** (*Optional**[**bool**]*) –\n* **markup** (*Optional**[**bool**]*) –\n* **height** (*Optional**[**int**]*) –\n\n\n*property* ascii\\_only*: bool*¶\nCheck if renderables should use ascii only.\n\ncopy()[source]¶\nReturn a copy of the options.\n\nReturns\na copy of self.\n\nReturn type\nConsoleOptions\n\nencoding*: str*¶\nEncoding of terminal.\n\nhighlight*: Optional[bool]* *= None*¶\nHighlight override for render\\_str.\n\nis\\_terminal*: bool*¶\nTrue if the target is a terminal, otherwise False.\n\njustify*: Optional[typing\\_extensions.Literal[default, left, center, right, full]]* *= None*¶\nJustify value override for renderable.\n\nlegacy\\_windows*: bool*¶\nflag for legacy windows.\n\nType\nlegacy\\_windows\n\nmarkup*: Optional[bool]* *= None*¶\nEnable markup when rendering strings.\n\nmax\\_height*: int*¶\nHeight of container (starts as terminal)\n\nmax\\_width*: int*¶\nMaximum width of renderable.\n\nmin\\_width*: int*¶\nMinimum width of renderable.\n\nno\\_wrap*: Optional[bool]* *= False*¶\nDisable wrapping for text.\n\noverflow*: Optional[typing\\_extensions.Literal[fold, crop, ellipsis, ignore]]* *= None*¶\nOverflow value override for renderable.\n\nreset\\_height()[source]¶\nReturn a copy of the options with height set to `None`.\n\nReturns\nNew console options instance.\n\nReturn type\n~ConsoleOptions\n\nsize*: ConsoleDimensions*¶\nSize of console.\n\nupdate(*\\**, *width=<rich.console.NoChange object>*, *min\\_width=<rich.console.NoChange object>*, *max\\_width=<rich.console.NoChange object>*, *justify=<rich.console.NoChange object>*, *overflow=<rich.console.NoChange object>*, *no\\_wrap=<rich.console.NoChange object>*, *highlight=<rich.console.NoChange object>*, *markup=<rich.console.NoChange object>*, *height=<rich.console.NoChange object>*)[source]¶\nUpdate values, return a copy.\n\nParameters\n* **width** (*Union**[**int**,* *NoChange**]*) –\n* **min\\_width** (*Union**[**int**,* *NoChange**]*) –\n* **max\\_width** (*Union**[**int**,* *NoChange**]*) –\n* **justify** (*Union**[**typing\\_extensions.Literal**[**default**,* *left**,* *center**,* *right**,* *full**]**,* *None**,* *NoChange**]*) –\n* **overflow** (*Union**[**typing\\_extensions.Literal**[**fold**,* *crop**,* *ellipsis**,* *ignore**]**,* *None**,* *NoChange**]*) –\n* **no\\_wrap** (*Union**[**bool**,* *None**,* *NoChange**]*) –\n* **highlight** (*Union**[**bool**,* *None**,* *NoChange**]*) –\n* **markup** (*Union**[**bool**,* *None**,* *NoChange**]*) –\n* **height** (*Union**[**int**,* *None**,* *NoChange**]*) –\n\nReturn type\n*ConsoleOptions*\n\nupdate\\_dimensions(*width*, *height*)[source]¶\nUpdate the width and height, and return a copy.\n\nParameters\n* **width** (*int*) – New width (sets both min\\_width and max\\_width).\n* **height** (*int*) – New height.\n\nupdate\\_height(*height*)[source]¶\nUpdate the height, and return a copy.\n\nParameters\n**height** (*int*) – New height\n\nReturns\nNew Console options instance.\n\nupdate\\_width(*width*)[source]¶\nUpdate just the width, return a copy.\n\nParameters\n**width** (*int*) – New width (sets both min\\_width and max\\_width)\n\n\n*class* rich.console.ConsoleRenderable(*\\*args*, *\\*\\*kwds*)[source]¶\nAn object that supports the console protocol.\n\n*class* rich.console.ConsoleThreadLocals(*theme\\_stack*, *buffer=<factory>*, *buffer\\_index=0*)[source]¶\nThread local values for Console context.\n\nParameters\n* **theme\\_stack** (*ThemeStack*) –\n* **buffer** (*List**[**Segment**]*) –\n* **buffer\\_index** (*int*) –\n\n*class* rich.console.Group(*\\*renderables*, *fit=True*)[source]¶\nTakes a group of renderables and returns a renderable object that renders the group.\n\nParameters\n* **renderables** (*Iterable**[**RenderableType**]*) – An iterable of renderable objects.\n* **fit** (*bool**,* *optional*) – Fit dimension of group to contents, or fill available space. Defaults to True.\n\n*class* rich.console.NewLine(*count=1*)[source]¶\nA renderable to generate new line(s)\n\nParameters\n**count** (*int*) – \n\n*class* rich.console.PagerContext(*console*, *pager=None*, *styles=False*, *links=False*)[source]¶\nA context manager that ‘pages’ content. See `pager()` for usage.\n\nParameters\n* **console** (*Console*) –\n* **pager** (*Optional**[**Pager**]*) –\n* **styles** (*bool*) –\n* **links** (*bool*) –\n\n*class* rich.console.RenderHook[source]¶\nProvides hooks in to the render process.\n\n\n*abstract* process\\_renderables(*renderables*)[source]¶\nCalled with a list of objects to render.\n\n\nThis method can return a new list of renderables, or modify and return the same list.\n\nParameters\n**renderables** (*List**[**ConsoleRenderable**]*) – A number of renderable objects.\n\nReturns\nA replacement list of renderables.\n\nReturn type\nList[ConsoleRenderable]\n\n\nrich.console.RenderableType¶\nA string or any object that may be rendered by Rich.\n\n\nalias of `Union`[`ConsoleRenderable`, `RichCast`, `str`]\n\n*class* rich.console.RichCast(*\\*args*, *\\*\\*kwds*)[source]¶\nAn object that may be ‘cast’ to a console renderable.\n\n*class* rich.console.ScreenContext(*console*, *hide\\_cursor*, *style=''*)[source]¶\nA context manager that enables an alternative screen. See `screen()` for usage.\n\nParameters\n* **console** (*Console*) –\n* **hide\\_cursor** (*bool*) –\n* **style** (*Union**[**str**,* *Style**]*) –\n\n\nupdate(*\\*renderables*, *style=None*)[source]¶\nUpdate the screen.\n\nParameters\n* **renderable** (*RenderableType**,* *optional*) – Optional renderable to replace current renderable,\nor None for no change. Defaults to None.\n* **style** (*Optional**[**Union**[**str**,* *Style**]**]*) – (Style, optional): Replacement style, or None for no change. Defaults to None.\n* **renderables** (*Union**[**ConsoleRenderable**,* *RichCast**,* *str**]*) –\n\n\n*class* rich.console.ScreenUpdate(*lines*, *x*, *y*)[source]¶\nRender a list of lines at a given offset.\n\nParameters\n* **lines** (*List**[**List**[**Segment**]**]*) –\n* **x** (*int*) –\n* **y** (*int*) –\n\n*class* rich.console.ThemeContext(*console*, *theme*, *inherit=True*)[source]¶\nA context manager to use a temporary theme. See `use\\_theme()` for usage.\n\nParameters\n* **console** (*Console*) –\n* **theme** (*Theme*) –\n* **inherit** (*bool*) –\n\nrich.console.detect\\_legacy\\_windows()[source]¶\nDetect legacy Windows.\n\nrich.console.group(*fit=True*)[source]¶\nA decorator that turns an iterable of renderables in to a group.\n\nParameters\n**fit** (*bool**,* *optional*) – Fit dimension of group to contents, or fill available space. Defaults to True.\n\nReturn type\n*Callable*[[…], *Callable*[[…], *Group*]]\n\n\n# rich.highlighter¶\n\n\n# rich¶\n\n\n\n# rich.measure¶\n\n\n# rich.layout¶\n\n\n*class* rich.layout.ColumnSplitter[source]¶\nSplit a layout region in to columns.\n\n\ndivide(*children*, *region*)[source]¶\nDivide a region amongst several child layouts.\n\nParameters\n* **children** (*Sequence**(**Layout**)*) – A number of child layouts.\n* **region** (*Region*) – A rectangular region to divide.\n\nReturn type\n*Iterable*[*Tuple*[*Layout*, *Region*]]\n\nget\\_tree\\_icon()[source]¶\nGet the icon (emoji) used in layout.tree\n\n\n*class*\n\n==================\n Document 2 \n----------------\n rich.syntax¶\n\n\n*class* rich.syntax.Syntax(*code*, *lexer*, *\\**, *theme='monokai'*, *dedent=False*, *line\\_numbers=False*, *start\\_line=1*, *line\\_range=None*, *highlight\\_lines=None*, *code\\_width=None*, *tab\\_size=4*, *word\\_wrap=False*, *background\\_color=None*, *indent\\_guides=False*, *padding=0*)[source]¶\nConstruct a Syntax object to render syntax highlighted code.\n\nParameters\n* **code** (*str*) – Code to highlight.\n* **lexer** (*Lexer* *|* *str*) – Lexer to use (see https://pygments.org/docs/lexers/)\n* **theme** (*str**,* *optional*) – Color theme, aka Pygments style (see https://pygments.org/docs/styles/#getting-a-list-of-available-styles). Defaults to “monokai”.\n* **dedent** (*bool**,* *optional*) – Enable stripping of initial whitespace. Defaults to False.\n* **line\\_numbers** (*bool**,* *optional*) – Enable rendering of line numbers. Defaults to False.\n* **start\\_line** (*int**,* *optional*) – Starting number for line numbers. Defaults to 1.\n* **line\\_range** (*Tuple**[**int* *|* *None**,* *int* *|* *None**]**,* *optional*) – If given should be a tuple of the start and end line to render.\nA value of None in the tuple indicates the range is open in that direction.\n* **highlight\\_lines** (*Set**[**int**]*) – A set of line numbers to highlight.\n* **code\\_width** (*Optional**[**int**]*) – Width of code to render (not including line numbers), or `None` to use all available width.\n* **tab\\_size** (*int**,* *optional*) – Size of tabs. Defaults to 4.\n* **word\\_wrap** (*bool**,* *optional*) – Enable word wrapping.\n* **background\\_color** (*str**,* *optional*) – Optional background color, or None to use theme color. Defaults to None.\n* **indent\\_guides** (*bool**,* *optional*) – Show indent guides. Defaults to False.\n* **padding** (*PaddingDimensions*) – Padding to apply around the syntax. Defaults to 0 (no padding).\n\n\n*property* default\\_lexer*: Lexer*¶\nA Pygments Lexer to use if one is not specified or invalid.\n\n*classmethod* from\\_path(*path*, *encoding='utf-8'*, *lexer=None*, *theme='monokai'*, *dedent=False*, *line\\_numbers=False*, *line\\_range=None*, *start\\_line=1*, *highlight\\_lines=None*, *code\\_width=None*, *tab\\_size=4*, *word\\_wrap=False*, *background\\_color=None*, *indent\\_guides=False*, *padding=0*)[source]¶\nConstruct a Syntax object from a file.\n\nParameters\n* **path** (*str*) – Path to file to highlight.\n* **encoding** (*str*) – Encoding of file.\n* **lexer** (*str* *|* *Lexer**,* *optional*) – Lexer to use. If None, lexer will be auto-detected from path/file content.\n* **theme** (*str**,* *optional*) – Color theme, aka Pygments style (see https://pygments.org/docs/styles/#getting-a-list-of-available-styles). Defaults to “emacs”.\n* **dedent** (*bool**,* *optional*) – Enable stripping of initial whitespace. Defaults to True.\n* **line\\_numbers** (*bool**,* *optional*) – Enable rendering of line numbers. Defaults to False.\n* **start\\_line** (*int**,* *optional*) – Starting number for line numbers. Defaults to 1.\n* **line\\_range** (*Tuple**[**int**,* *int**]**,* *optional*) – If given should be a tuple of the start and end line to render.\n* **highlight\\_lines** (*Set**[**int**]*) – A set of line numbers to highlight.\n* **code\\_width** (*Optional**[**int**]*) – Width of code to render (not including line numbers), or `None` to use all available width.\n* **tab\\_size** (*int**,* *optional*) – Size of tabs. Defaults to 4.\n* **word\\_wrap** (*bool**,* *optional*) – Enable word wrapping of code.\n* **background\\_color** (*str**,* *optional*) – Optional background color, or None to use theme color. Defaults to None.\n* **indent\\_guides** (*bool**,* *optional*) – Show indent guides. Defaults to False.\n* **padding** (*PaddingDimensions*) – Padding to apply around the syntax. Defaults to 0 (no padding).\n\nReturns\nA Syntax object that may be printed to the console\n\nReturn type\n[Syntax]\n\n*classmethod* get\\_theme(*name*)[source]¶\nGet a syntax theme instance.\n\nParameters\n**name** (*Union**[**str**,* *SyntaxTheme**]*) – \n\nReturn type\n*SyntaxTheme*\n\n*classmethod* guess\\_lexer(*path*, *code=None*)[source]¶\nGuess the alias of the Pygments lexer to use based on a path and an optional string of code.\nIf code is supplied, it will use a combination of the code and the filename to determine the\nbest lexer to use. For example, if the file is `index.html` and the file contains Django\ntemplating syntax, then “html+django” will be returned. If the file is `index.html`, and no\ntemplating language is used, the “html” lexer will be used. If no string of code\nis supplied, the lexer will be chosen based on the file extension..\n\nParameters\n* **path** (*AnyStr*) – The path to the file containing the code you wish to know the lexer for.\n* **code** (*str**,* *optional*) – Optional string of code that will be used as a fallback if no lexer\nis found for the supplied path.\n\nReturns\nThe name of the Pygments lexer that best matches the supplied path/code.\n\nhighlight(*code*, *line\\_range=None*)[source]¶\nHighlight code and return a Text instance.\n\nParameters\n* **code** (*str*) – Code to highlight.\n* **line\\_range** (*Tuple**[**int**,* *int**]**,* *optional*) – Optional line range to highlight.\n\nReturns\nA text instance containing highlighted syntax.\n\n*property* lexer*: Optional[Lexer]*¶\nThe lexer for this syntax, or None if no lexer was found.\n\n\nTries to find the lexer by name if a string was passed to the constructor.\n\nstylize\\_range(*style*, *start*, *end*)[source]¶\nAdds a custom style on a part of the code, that will be applied to the syntax display when it’s rendered.\nLine numbers are 1-based, while column indexes are 0-based.\n\nParameters\n* **style** (*StyleType*) – The style to apply.\n* **start** (*Tuple**[**int**,* *int**]*) – The start of the range, in the form [line number, column index].\n* **end** (*Tuple**[**int**,* *int**]*) – The end of the range, in the form [line number, column index].\n\n\n# rich.theme¶\n\n\n# rich.text¶\n\n\n*class* rich.text.Text(*text=''*, *style=''*, *\\**, *justify=None*, *overflow=None*, *no\\_wrap=None*, *end='\\n'*, *tab\\_size=None*, *spans=None*)[source]¶\nText with color / style.\n\nParameters\n* **text** (*str**,* *optional*) – Default unstyled text. Defaults to “”.\n* **style** (*Union**[**str**,* *Style**]**,* *optional*) – Base style for text. Defaults to “”.\n* **justify** (*str**,* *optional*)\n\n==================\n Document 3 \n----------------\n rich.style¶\n\n\n*class* rich.style.Style(*\\**, *color=None*, *bgcolor=None*, *bold=None*, *dim=None*, *italic=None*, *underline=None*, *blink=None*, *blink2=None*, *reverse=None*, *conceal=None*, *strike=None*, *underline2=None*, *frame=None*, *encircle=None*, *overline=None*, *link=None*, *meta=None*)[source]¶\nA terminal style.\n\n\nA terminal style consists of a color (color), a background color (bgcolor), and a number of attributes, such\nas bold, italic etc. The attributes have 3 states: they can either be on\n(`True`), off (`False`), or not set (`None`).\n\nParameters\n* **color** (*Union**[**Color**,* *str**]**,* *optional*) – Color of terminal text. Defaults to None.\n* **bgcolor** (*Union**[**Color**,* *str**]**,* *optional*) – Color of terminal background. Defaults to None.\n* **bold** (*bool**,* *optional*) – Enable bold text. Defaults to None.\n* **dim** (*bool**,* *optional*) – Enable dim text. Defaults to None.\n* **italic** (*bool**,* *optional*) – Enable italic text. Defaults to None.\n* **underline** (*bool**,* *optional*) – Enable underlined text. Defaults to None.\n* **blink** (*bool**,* *optional*) – Enabled blinking text. Defaults to None.\n* **blink2** (*bool**,* *optional*) – Enable fast blinking text. Defaults to None.\n* **reverse** (*bool**,* *optional*) – Enabled reverse text. Defaults to None.\n* **conceal** (*bool**,* *optional*) – Enable concealed text. Defaults to None.\n* **strike** (*bool**,* *optional*) – Enable strikethrough text. Defaults to None.\n* **underline2** (*bool**,* *optional*) – Enable doubly underlined text. Defaults to None.\n* **frame** (*bool**,* *optional*) – Enable framed text. Defaults to None.\n* **encircle** (*bool**,* *optional*) – Enable encircled text. Defaults to None.\n* **overline** (*bool**,* *optional*) – Enable overlined text. Defaults to None.\n* **link** (*str**,* *link*) – Link URL. Defaults to None.\n* **meta** (*Optional**[**Dict**[**str**,* *Any**]**]*) –\n\n\n*property* background\\_style*: Style*¶\nA Style with background only.\n\n*property* bgcolor*: Optional[Color]*¶\nThe background color or None if it is not set.\n\n*classmethod* chain(*\\*styles*)[source]¶\nCombine styles from positional argument in to a single style.\n\nParameters\n**\\*styles** (*Iterable**[**Style**]*) – Styles to combine.\n\nReturns\nA new style instance.\n\nclear\\_meta\\_and\\_links()[source]¶\nGet a copy of this style with link and meta information removed.\n\nReturns\nNew style object.\n\n*property* color*: Optional[Color]*¶\nThe foreground color or None if it is not set.\n\n*classmethod* combine(*styles*)[source]¶\nCombine styles and get result.\n\nParameters\n**styles** (*Iterable**[**Style**]*) – Styles to combine.\n\ncopy()[source]¶\nGet a copy of this style.\n\nReturns\nA new Style instance with identical attributes.\n\n*classmethod* from\\_color(*color=None*, *bgcolor=None*)[source]¶\nCreate a new style with colors and no attributes.\n\nReturns\nA (foreground) color, or None for no color. Defaults to None.\nbgcolor (Optional[Color]): A (background) color, or None for no color. Defaults to None.\n\nReturn type\ncolor (Optional[Color])\n\nParameters\n* **color** (*Optional**[**Color**]*) –\n* **bgcolor** (*Optional**[**Color**]*) –\n\n*classmethod* from\\_meta(*meta*)[source]¶\nCreate a new style with meta data.\n\nReturns\nA dictionary of meta data. Defaults to None.\n\nReturn type\nmeta (Optional[Dict[str, Any]])\n\nParameters\n**meta** (*Optional**[**Dict**[**str**,* *Any**]**]*) – \n\nget\\_html\\_style(*theme=None*)[source]¶\nGet a CSS style rule.\n\nParameters\n**theme** (*Optional**[**TerminalTheme**]*) – \n\n*property* link*: Optional[str]*¶\nLink text, if set.\n\n*property* link\\_id*: str*¶\nGet a link id, used in ansi code for links.\n\n*property* meta*: Dict[str, Any]*¶\nGet meta information (can not be changed after construction).\n\n*classmethod* normalize(*style*)[source]¶\nNormalize a style definition so that styles with the same effect have the same string\nrepresentation.\n\nParameters\n**style** (*str*) – A style definition.\n\nReturns\nNormal form of style definition.\n\n*classmethod* null()[source]¶\nCreate an ‘null’ style, equivalent to Style(), but more performant.\n\n*classmethod* on(*meta=None*, *\\*\\*handlers*)[source]¶\nCreate a blank style with meta information.\n\n\nstyle = Style.on(click=self.on\\_click)\n\nParameters\n* **meta** (*Optional**[**Dict**[**str**,* *Any**]**]**,* *optional*) – An optional dict of meta information.\n* **\\*\\*handlers** (*Any*) – Keyword arguments are translated in to handlers.\n\nReturns\nA Style with meta information attached.\n\n*classmethod* parse(*style\\_definition*)[source]¶\nParse a style definition.\n\nParameters\n**style\\_definition** (*str*) – A string containing a style.\n\nRaises\n**errors.StyleSyntaxError** – If the style definition syntax is invalid.\n\nReturns\nA Style instance.\n\n*classmethod* pick\\_first(*\\*values*)[source]¶\nPick first non-None style.\n\nParameters\n**values** (*Optional**[**Union**[**str**,* *Style**]**]*) – \n\nReturn type\n*Union*[str, *Style*]\n\nrender(*text=''*, *\\**, *color\\_system=ColorSystem.TRUECOLOR*, *legacy\\_windows=False*)[source]¶\nRender the ANSI codes for the style.\n\nParameters\n* **text** (*str**,* *optional*) – A string to style. Defaults to “”.\n* **color\\_system** (*Optional**[**ColorSystem**]**,* *optional*) – Color system to render to. Defaults to ColorSystem.TRUECOLOR.\n* **legacy\\_windows** (*bool*) –\n\nReturns\nA string containing ANSI style codes.\n\ntest(*text=None*)[source]¶\nWrite text with style directly to terminal.\n\n\nThis method is for testing purposes only.\n\nParameters\n**text** (*Optional**[**str**]**,* *optional*) – Text to style or None for style name.\n\n*property* transparent\\_background*: bool*¶\nCheck if the style specified a transparent background.\n\nupdate\\_link(*link=None*)[source]¶\nGet a copy with a different value for link.\n\nParameters\n**link** (*str**,* *optional*) – New value for link. Defaults to None.\n\nReturns\nA new Style instance.\n\n*property* without\\_color*: Style*¶\nGet a copy of the style with color removed.\n\n\n*class* rich.style.StyleStack(*default\\_style*)[source]¶\nA stack of styles.\n\nParameters\n**default\\_style** (*Style*) – \n\n\n*property* current*: Style*¶\nGet the Style at the top of the stack.\n\npop()[source]¶\nPop last style and discard.\n\nReturns\nNew current style (also available as stack.current)\n\npush(*style*)[source]¶\nPush a new style on to the stack.\n\nParameters\n**style** (*Style*) – New style to combine with current style.\n\n\n# rich.styled¶\n\n\n# rich.table¶\n\n\n# rich.syntax¶\n\n\n*class* rich.syntax.Syntax(*code*, *lexer*, *\\**, *theme='monokai'*, *dedent=False*, *line\\_numbers=False*, *start\\_line=1*, *line\\_range=None*, *highlight\\_lines=None*, *code\\_width=None*, *tab\\_size=4*, *word\\_wrap=False*, *background\\_color=None*, *indent\\_guides=False*, *padding=0*)[source]¶\nConstruct a Syntax object to render syntax highlighted code.\n\nParameters\n* **code** (*str*) – Code to highlight.\n* **lexer** (*Lexer* *|* *str*) – Lexer to use (see\n\n==================\n Document 4 \n----------------\n rich.text¶\n\n\n*class* rich.text.Text(*text=''*, *style=''*, *\\**, *justify=None*, *overflow=None*, *no\\_wrap=None*, *end='\\n'*, *tab\\_size=None*, *spans=None*)[source]¶\nText with color / style.\n\nParameters\n* **text** (*str**,* *optional*) – Default unstyled text. Defaults to “”.\n* **style** (*Union**[**str**,* *Style**]**,* *optional*) – Base style for text. Defaults to “”.\n* **justify** (*str**,* *optional*) – Justify method: “left”, “center”, “full”, “right”. Defaults to None.\n* **overflow** (*str**,* *optional*) – Overflow method: “crop”, “fold”, “ellipsis”. Defaults to None.\n* **no\\_wrap** (*bool**,* *optional*) – Disable text wrapping, or None for default. Defaults to None.\n* **end** (*str**,* *optional*) – Character to end text with. Defaults to “\\n”.\n* **tab\\_size** (*int*) – Number of spaces per tab, or `None` to use `console.tab\\_size`. Defaults to None.\n* **spans** (*List**[**Span**]**,* *optional*) –\n\n\nalign(*align*, *width*, *character=' '*)[source]¶\nAlign text to a given width.\n\nParameters\n* **align** (*AlignMethod*) – One of “left”, “center”, or “right”.\n* **width** (*int*) – Desired width.\n* **character** (*str**,* *optional*) – Character to pad with. Defaults to ” “.\n\nappend(*text*, *style=None*)[source]¶\nAdd text with an optional style.\n\nParameters\n* **text** (*Union**[**Text**,* *str**]*) – A str or Text to append.\n* **style** (*str**,* *optional*) – A style name. Defaults to None.\n\nReturns\nReturns self for chaining.\n\nappend\\_text(*text*)[source]¶\nAppend another Text instance. This method is more performant that Text.append, but\nonly works for Text.\n\nappend\\_tokens(*tokens*)[source]¶\nAppend iterable of str and style. Style may be a Style instance or a str style definition.\n\nParameters\n* **pairs** (*Iterable**[**Tuple**[**str**,* *Optional**[**StyleType**]**]**]*) – An iterable of tuples containing str content and style.\n* **tokens** (*Iterable**[**Tuple**[**str**,* *Optional**[**Union**[**str**,* *Style**]**]**]**]*) –\n\napply\\_meta(*meta*, *start=0*, *end=None*)[source]¶\nApply meta data to the text, or a portion of the text.\n\nParameters\n* **meta** (*Dict**[**str**,* *Any**]*) – A dict of meta information.\n* **start** (*int*) – Start offset (negative indexing is supported). Defaults to 0.\n* **end** (*Optional**[**int**]**,* *optional*) – End offset (negative indexing is supported), or None for end of text. Defaults to None.\n\n*classmethod* assemble(*\\*parts*, *style=''*, *justify=None*, *overflow=None*, *no\\_wrap=None*, *end='\\n'*, *tab\\_size=8*, *meta=None*)[source]¶\nConstruct a text instance by combining a sequence of strings with optional styles.\nThe positional arguments should be either strings, or a tuple of string + style.\n\nParameters\n* **style** (*Union**[**str**,* *Style**]**,* *optional*) – Base style for text. Defaults to “”.\n* **justify** (*str**,* *optional*) – Justify method: “left”, “center”, “full”, “right”. Defaults to None.\n* **overflow** (*str**,* *optional*) – Overflow method: “crop”, “fold”, “ellipsis”. Defaults to None.\n* **end** (*str**,* *optional*) – Character to end text with. Defaults to “\\n”.\n* **tab\\_size** (*int*) – Number of spaces per tab, or `None` to use `console.tab\\_size`. Defaults to None.\n* **meta** (*Dict**[**str**,* *Any**]**,* *optional*) –\n* **parts** (*Union**[**str**,* *Text**,* *Tuple**[**str**,* *Union**[**str**,* *Style**]**]**]*) –\n* **no\\_wrap** (*Optional**[**bool**]*) –\n\nReturns\nA new text instance.\n\nblank\\_copy(*plain=''*)[source]¶\nReturn a new Text instance with copied meta data (but not the string or spans).\n\nParameters\n**plain** (*str*) – \n\n*property* cell\\_len*: int*¶\nGet the number of cells required to render this text.\n\ncopy()[source]¶\nReturn a copy of this instance.\n\ncopy\\_styles(*text*)[source]¶\nCopy styles from another Text instance.\n\nParameters\n**text** (*Text*) – A Text instance to copy styles from, must be the same length.\n\ndetect\\_indentation()[source]¶\nAuto-detect indentation of code.\n\nReturns\nNumber of spaces used to indent code.\n\ndivide(*offsets*)[source]¶\nDivide text in to a number of lines at given offsets.\n\nParameters\n**offsets** (*Iterable**[**int**]*) – Offsets used to divide text.\n\nReturns\nNew RichText instances between offsets.\n\nReturn type\nLines\n\nexpand\\_tabs(*tab\\_size=None*)[source]¶\nConverts tabs to spaces.\n\nParameters\n**tab\\_size** (*int**,* *optional*) – Size of tabs. Defaults to 8.\n\nextend\\_style(*spaces*)[source]¶\nExtend the Text given number of spaces where the spaces have the same style as the last character.\n\nParameters\n**spaces** (*int*) – Number of spaces to add to the Text.\n\nfit(*width*)[source]¶\nFit the text in to given width by chopping in to lines.\n\nParameters\n**width** (*int*) – Maximum characters in a line.\n\nReturns\nLines container.\n\n*classmethod* from\\_ansi(*text*, *\\**, *style=''*, *justify=None*, *overflow=None*, *no\\_wrap=None*, *end='\\n'*, *tab\\_size=8*)[source]¶\nCreate a Text object from a string containing ANSI escape codes.\n\nParameters\n* **text** (*str*) – A string containing escape codes.\n* **style** (*Union**[**str**,* *Style**]**,* *optional*) – Base style for text. Defaults to “”.\n* **justify** (*str**,* *optional*) – Justify method: “left”, “center”, “full”, “right”. Defaults to None.\n* **overflow** (*str**,* *optional*) – Overflow method: “crop”, “fold”, “ellipsis”. Defaults to None.\n* **no\\_wrap** (*bool**,* *optional*) – Disable text wrapping, or None for default. Defaults to None.\n* **end** (*str**,* *optional*) – Character to end text with. Defaults to “\\n”.\n* **tab\\_size** (*int*) – Number of spaces per tab, or `None` to use `console.tab\\_size`. Defaults to None.\n\n*classmethod* from\\_markup(*text*, *\\**, *style=''*, *emoji=True*, *emoji\\_variant=None*, *justify=None*, *overflow=None*, *end='\\n'*)[source]¶\nCreate Text instance from markup.\n\nParameters\n* **text** (*str*) – A string containing console markup.\n* **emoji** (*bool**,* *optional*) – Also render emoji code. Defaults to True.\n* **justify** (*str**,* *optional*) – Justify method: “left”, “center”, “full”, “right”. Defaults to None.\n* **overflow** (*str**,* *optional*) – Overflow method: “crop”, “fold”, “ellipsis”. Defaults to None.\n* **end** (*str**,* *optional*) – Character to end text with. Defaults to “\\n”.\n* **style** (*Union**[**str**,* *Style**]*) –\n* **emoji\\_variant** (*Optional**[**typing\\_extensions.Literal**[**emoji**,* *text**]**]*) –\n\nReturns\nA Text instance with markup rendered.\n\nget\\_style\\_at\\_offset(*console*, *offset*)[source]¶\nGet the style of a character at give offset.\n\nParameters\n* **console** (*~Console*) – Console where text will be rendered.\n* **offset** (*int*) – Offset in to text (negative indexing supported)\n\nhighlight\\_regex(*re\\_highlight*, *style=None*, *\\**, *style\\_prefix=''*)[source]¶\nHighlight text with a regular expression, where group names are\ntranslated to styles.\n\nParameters\n* **re\\_highlight** (*str*) – A regular expression.\n* **style** (*Union**[**GetStyleCallable**,* *StyleType**]*) – Optional style to apply to whole match, or a callable\nwhich accepts the matched text and returns a style. Defaults to None.\n* **style\\_prefix** (*str**,* *optional*) – Optional prefix to add to style group names.\n\nReturns\nNumber of regex matches\n\nhighlight\\_words(*words*, *style*, *\\**, *case\\_sensitive=True*)[source]¶\nHighlight words with a style.\n\nParameters\n* **words** (*Iterable**[**str**]*) – Worlds to highlight.\n* **style** (*Union**[**str**,* *Style**]*) – Style to apply.\n* **case\\_sensitive** (*bool**,* *optional*) – Enable case sensitive matchings. Defaults to True.\n\nReturns\nNumber of words highlighted.\n\njoin(*lines*)[source]¶\nJoin text together with this instance as the separator.\n\nParameters\n**lines** (*Iterable**[**Text**]*) – An iterable of Text instances to join.\n\nReturns\nA new text instance containing join text.\n\n*property* markup*: str*¶\nGet console markup to render this Text.\n\nReturns\nA string potentially creating markup tags.\n\non(*meta=None*, *\\*\\*handlers*)[source]¶\nApply event handlers (used by Textual project).\n\n```\n>>> from rich.text import Text\n>>> text = Text(\"hello world\")\n>>> text.on(click=\"view.toggle('world')\")\n\nParameters\n* **meta** (*Dict**[**str**,* *Any**]*) – Mapping of meta information.\n* **\\*\\*handlers** – Keyword args are prefixed with “@” to defined handlers.\n\nReturns\nSelf is returned to method may be chained.\n\npad(*count*, *character=' '*)[source]¶\nPad left and right with a given number of characters.\n\nParameters\n* **count** (*int*) – Width of padding.\n* **character** (*str*) –\n\npad\\_left(*count*, *character=' '*)[source]¶\nPad the left with a given character.\n\nParameters\n* **count** (*int*) – Number of characters to pad.\n* **character** (*str**,* *optional*) – Character to pad with. Defaults to ” “.\n\npad\\_right(*count*, *character=' '*)[source]¶\nPad the right with a given character.\n\n*property* plain*: str*¶\nGet the text as a single string.\n\nremove\\_suffix(*suffix*)[source]¶\nRemove a suffix if it exists.\n\nParameters\n**suffix** (*str*) – Suffix to remove.\n\nrender(*console*, *end=''*)[source]¶\nRender the text as Segments.\n\nParameters\n* **console** (*Console*) – Console instance.\n* **end** (*Optional**[**str**]**,* *optional*) – Optional end character.\n\nReturns\nResult of render that may be written to the console.\n\nright\\_crop(*amount=1*)[source]¶\nRemove a number of characters from the end of the text.\n\nParameters\n**amount** (*int*) – \n\nrstrip()[source]¶\nStrip whitespace from end of text.\n\nrstrip\\_end(*size*)[source]¶\nRemove whitespace beyond a certain width at the end of the text.\n\nParameters\n**size** (*int*) – The desired size of the text.\n\nset\\_length(*new\\_length*)[source]¶\nSet new length of the text, clipping or padding is required.\n\nParameters\n**new\\_length** (*int*) – \n\n*property* spans*: List[Span]*¶\nGet a reference to the internal list of spans.\n\nsplit(*separator='\\n'*, *\\**, *include\\_separator=False*, *allow\\_blank=False*)[source]¶\nSplit rich text in to lines, preserving styles.\n\nParameters\n* **separator** (*str**,* *optional*) – String to split on. Defaults to “\\n”.\n* **include\\_separator** (*bool**,* *optional*) – Include the separator in the lines. Defaults to False.\n* **allow\\_blank** (*bool**,* *optional*) – Return a blank line if the text ends with a separator. Defaults to False.\n\nReturns\nA list of rich text, one per line of the original.\n\nReturn type\nList[RichText]\n\n*classmethod* styled(*text*, *style=''*, *\\**, *justify=None*, *overflow=None*)[source]¶\nConstruct a Text instance with a pre-applied styled. A style applied in this way won’t be used\nto pad the text when it is justified.\n\nParameters\n* **text** (*str*) – A string containing console markup.\n* **style** (*Union**[**str**,* *Style**]*) – Style to apply to the text. Defaults to “”.\n* **justify** (*str**,* *optional*) – Justify method: “left”, “center”, “full”, “right”. Defaults to None.\n* **overflow** (*str**,* *optional*) – Overflow method: “crop”, “fold”, “ellipsis”. Defaults to None.\n\nReturns\nA text instance with a style applied to the entire string.\n\nstylize(*style*, *start=0*, *end=None*)[source]¶\nApply a style to the text, or a portion of the text.\n\nParameters\n* **style** (*Union**[**str**,* *Style**]*) – Style instance or style definition to apply.\n* **start** (*int*) – Start offset (negative indexing is supported). Defaults to 0.\n* **end** (*Optional**[**int**]**,* *optional*) – End offset (negative indexing is supported), or None for end of text. Defaults to None.\n\nstylize\\_before(*style*, *start=0*, *end=None*)[source]¶\nApply a style to the text, or a portion of the text. Styles will be applied before other styles already present.\n\ntruncate(*max\\_width*, *\\**, *overflow=None*, *pad=False*)[source]¶\nTruncate text if it is longer that a given width.\n\nParameters\n* **max\\_width** (*int*) – Maximum number of characters in text.\n* **overflow** (*str**,* *optional*) – Overflow method: “crop”, “fold”, or “ellipsis”. Defaults to None, to use self.overflow.\n* **pad** (*bool**,* *optional*) – Pad with spaces if the length is less than max\\_width. Defaults to False.\n\nwith\\_indent\\_guides(*indent\\_size=None*, *\\**, *character='│'*, *style='dim green'*)[source]¶\nAdds indent guide lines to text.\n\nParameters\n* **indent\\_size** (*Optional**[**int**]*) – Size of indentation, or None to auto detect. Defaults to None.\n* **character** (*str**,* *optional*) – Character to use for indentation. Defaults to “│”.\n* **style** (*Union**[**Style**,* *str**]**,* *optional*) – Style of indent guides.\n\nReturns\nNew text with indentation guides.\n\nwrap(*console*, *width*, *\\**, *justify=None*, *overflow=None*, *tab\\_size=8*, *no\\_wrap=None*)[source]¶\nWord wrap the text.\n\nParameters\n* **console** (*Console*) – Console instance.\n* **width** (*int*) – Number of characters per line.\n* **emoji** (*bool**,* *optional*) – Also render emoji code. Defaults to True.\n* **justify** (*str**,* *optional*) – Justify method: “default”, “left”, “center”, “full”, “right”. Defaults to “default”.\n* **overflow** (*str**,* *optional*) – Overflow method: “crop”, “fold”, or “ellipsis”. Defaults to None.\n* **tab\\_size** (*int**,* *optional*) – Default tab size. Defaults to 8.\n* **no\\_wrap** (*bool**,* *optional*) – Disable wrapping, Defaults to False.\n\nReturns\nNumber of lines.\n\n# rich.traceback¶\n\n\n*class* rich.traceback.Traceback(*trace=None*, *\\**, *width=100*, *extra\\_lines=3*, *theme=None*, *word\\_wrap=False*, *show\\_locals=False*, *locals\\_max\\_length=10*, *locals\\_max\\_string=80*, *locals\\_hide\\_dunder=True*, *locals\\_hide\\_sunder=False*, *indent\\_guides=True*, *suppress=()*, *max\\_frames=100*)[source]¶\nA Console renderable that renders a traceback.\n\nParameters\n* **trace** (*Trace**,* *optional*) – A Trace object produced from extract. Defaults to None, which uses\nthe last exception.\n* **width**"
    },
    {
        "instruction": "Generate a Python program using the 'rich' API to display a colorful and styled progress bar that shows the progress of a video rendering process. The program should update the progress as the rendering continues.",
        "api": "rich",
        "output": "# Install rich\n#!pip install rich\nimport time\nfrom rich.progress import Progress\n\n# Simulate video rendering process\ndef render_video():\n    for _ in range(100):\n        time.sleep(0.1)\n\n# Create a styled progress bar for video rendering\nwith Progress() as progress:\n    task = progress.add_task(\"[cyan]Rendering video...\", total=100)\n    progress.console.show_cursor(False)\n    \n    # Simulate video rendering and update the progress\n    for _ in range(100):\n        time.sleep(0.1)\n        progress.update(task, completed=1)\n        progress.refresh()\n\n# Display completion message\nprogress.console.show_cursor(True)\nprint(\"[green]Video rendering completed!\")\n\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n rich.progress¶\n\n\n*class* rich.progress.BarColumn(*bar\\_width=40*, *style='bar.back'*, *complete\\_style='bar.complete'*, *finished\\_style='bar.finished'*, *pulse\\_style='bar.pulse'*, *table\\_column=None*)[source]¶\nRenders a visual progress bar.\n\nParameters\n* **bar\\_width** (*Optional**[**int**]**,* *optional*) – Width of bar or None for full width. Defaults to 40.\n* **style** (*StyleType**,* *optional*) – Style for the bar background. Defaults to “bar.back”.\n* **complete\\_style** (*StyleType**,* *optional*) – Style for the completed bar. Defaults to “bar.complete”.\n* **finished\\_style** (*StyleType**,* *optional*) – Style for a finished bar. Defaults to “bar.finished”.\n* **pulse\\_style** (*StyleType**,* *optional*) – Style for pulsing bars. Defaults to “bar.pulse”.\n* **table\\_column** (*Optional**[**Column**]*) –\n\n\nrender(*task*)[source]¶\nGets a progress bar widget for a task.\n\nParameters\n**task** (*Task*) – \n\nReturn type\n*ProgressBar*\n\n\n*class* rich.progress.DownloadColumn(*binary\\_units=False*, *table\\_column=None*)[source]¶\nRenders file size downloaded and total, e.g. ‘0.5/2.3 GB’.\n\nParameters\n* **binary\\_units** (*bool**,* *optional*) – Use binary units, KiB, MiB etc. Defaults to False.\n* **table\\_column** (*Optional**[**Column**]*) –\n\n\nrender(*task*)[source]¶\nCalculate common unit for completed and total.\n\nReturn type\n*Text*\n\n\n*class* rich.progress.FileSizeColumn(*table\\_column=None*)[source]¶\nRenders completed filesize.\n\nParameters\n**table\\_column** (*Optional**[**Column**]*) – \n\n\nrender(*task*)[source]¶\nShow data completed.\n\n\n*class* rich.progress.MofNCompleteColumn(*separator='/'*, *table\\_column=None*)[source]¶\nRenders completed count/total, e.g. ‘ 10/1000’.\n\n\nBest for bounded tasks with int quantities.\n\n\nSpace pads the completed count so that progress length does not change as task progresses\npast powers of 10.\n\nParameters\n* **separator** (*str**,* *optional*) – Text to separate completed and total values. Defaults to “/”.\n* **table\\_column** (*Optional**[**Column**]*) –\n\n\nrender(*task*)[source]¶\nShow completed/total.\n\n\n*class* rich.progress.Progress(*\\*columns*, *console=None*, *auto\\_refresh=True*, *refresh\\_per\\_second=10*, *speed\\_estimate\\_period=30.0*, *transient=False*, *redirect\\_stdout=True*, *redirect\\_stderr=True*, *get\\_time=None*, *disable=False*, *expand=False*)[source]¶\nRenders an auto-updating progress bar(s).\n\nParameters\n* **console** (*Console**,* *optional*) – Optional Console instance. Default will an internal Console instance writing to stdout.\n* **auto\\_refresh** (*bool**,* *optional*) – Enable auto refresh. If disabled, you will need to call refresh().\n* **refresh\\_per\\_second** (*Optional**[**float**]**,* *optional*) – Number of times per second to refresh the progress information or None to use default (10). Defaults to None.\n* **speed\\_estimate\\_period** (*float*) – (float, optional): Period (in seconds) used to calculate the speed estimate. Defaults to 30.\n* **transient** (*bool*) – (bool, optional): Clear the progress on exit. Defaults to False.\n* **redirect\\_stdout** (*bool*) – (bool, optional): Enable redirection of stdout, so `print` may be used. Defaults to True.\n* **redirect\\_stderr** (*bool*) – (bool, optional): Enable redirection of stderr. Defaults to True.\n* **get\\_time** (*Optional**[**Callable**[**[**]**,* *float**]**]*) – (Callable, optional): A callable that gets the current time, or None to use Console.get\\_time. Defaults to None.\n* **disable** (*bool**,* *optional*) – Disable progress display. Defaults to False\n* **expand** (*bool**,* *optional*) – Expand tasks table to fit width. Defaults to False.\n* **columns** (*Union**[**str**,* *ProgressColumn**]*) –\n\n\nadd\\_task(*description*, *start=True*, *total=100.0*, *completed=0*, *visible=True*, *\\*\\*fields*)[source]¶\nAdd a new ‘task’ to the Progress display.\n\nParameters\n* **description** (*str*) – A description of the task.\n* **start** (*bool**,* *optional*) – Start the task immediately (to calculate elapsed time). If set to False,\nyou will need to call start manually. Defaults to True.\n* **total** (*float**,* *optional*) – Number of total steps in the progress if known.\nSet to None to render a pulsing animation. Defaults to 100.\n* **completed** (*int**,* *optional*) – Number of steps completed so far. Defaults to 0.\n* **visible** (*bool**,* *optional*) – Enable display of the task. Defaults to True.\n* **\\*\\*fields** (*str*) – Additional data fields required for rendering.\n\nReturns\nAn ID you can use when calling update.\n\nReturn type\nTaskID\n\nadvance(*task\\_id*, *advance=1*)[source]¶\nAdvance task by a number of steps.\n\nParameters\n* **task\\_id** (*TaskID*) – ID of task.\n* **advance** (*float*) – Number of steps to advance. Default is 1.\n\n*property* finished*: bool*¶\nCheck if all tasks have been completed.\n\n*classmethod* get\\_default\\_columns()[source]¶\n\nGet the default columns used for a new Progress instance:* a text column for the description (TextColumn)\n* the bar itself (BarColumn)\n* a text column showing completion percentage (TextColumn)\n* an estimated-time-remaining column (TimeRemainingColumn)\n\n\nIf the Progress instance is created without passing a columns argument,\nthe default columns defined here will be used.\n\n\nYou can also create a Progress instance using custom columns before\nand/or after the defaults, as in this example:\n\n> \n> \n> progress = Progress(SpinnerColumn(),\n> \\*Progress.default\\_columns(),\n> “Elapsed:”,\n> TimeElapsedColumn(),\n> \n> \n> \n> \n> )\n> \n> \n> \n\n\nThis code shows the creation of a Progress display, containing\na spinner to the left, the default columns, and a labeled elapsed\ntime column.\n\nReturn type\n*Tuple*[*ProgressColumn*, …]\n\nget\\_renderable()[source]¶\nGet a renderable for the progress display.\n\nReturn type\n*Union*[*ConsoleRenderable*, *RichCast*, str]\n\nget\\_renderables()[source]¶\nGet a number of renderables for the progress display.\n\nReturn type\n*Iterable*[*Union*[*ConsoleRenderable*, *RichCast*, str]]\n\nmake\\_tasks\\_table(*tasks*)[source]¶\nGet a table to render the Progress display.\n\nParameters\n**tasks** (*Iterable**[**Task**]*) – An iterable of Task instances, one per row of the table.\n\nopen(*file: Union[str, PathLike[str], bytes]*, *mode: typing\\_extensions.Literal[rb]*, *buffering: int = -1*, *encoding: Optional[str] = None*, *errors: Optional[str] = None*, *newline: Optional[str] = None*, *\\**, *total: Optional[int] = None*, *task\\_id: Optional[TaskID] = None*, *description: str = 'Reading...'*) → BinaryIO[source]¶\n\nopen(*file: Union[str, PathLike[str], bytes]*, *mode: Union[typing\\_extensions.Literal[r], typing\\_extensions.Literal[rt]]*, *buffering: int = -1*, *encoding: Optional[str] = None*, *errors: Optional[str] = None*, *newline: Optional[str] = None*, *\\**, *total: Optional[int] = None*, *task\\_id: Optional[TaskID] = None*, *description: str = 'Reading...'*) → TextIO\nTrack progress while reading from a binary file.\n\nParameters\n* **path** (*Union**[**str**,* *PathLike**[**str**]**]*) – The path to the file to read.\n* **mode** (*str*) – The mode to use to open the file. Only supports “r”, “rb” or “rt”.\n* **buffering** (*int*) – The buffering strategy to use, see `io.open()`.\n* **encoding** (*str**,* *optional*) – The encoding to use when reading in text mode, see `io.open()`.\n* **errors** (*str**,* *optional*) – The error handling strategy for decoding errors, see `io.open()`.\n* **newline** (*str**,* *optional*) – The strategy for handling newlines in text mode, see `io.open()`.\n* **total** (*int**,* *optional*) – Total number of bytes to read. If none given, os.stat(path).st\\_size is used.\n* **task\\_id** (*TaskID*) – Task to track. Default is new task.\n* **description** (*str**,* *optional*) – Description of task, if new task is created.\n\nReturns\nA readable file-like object in binary mode.\n\nReturn type\nBinaryIO\n\nRaises\n**ValueError** – When an invalid mode is given.\n\nrefresh()[source]¶\nRefresh (render) the progress information.\n\nremove\\_task(*task\\_id*)[source]¶\nDelete a task if it exists.\n\nParameters\n**task\\_id** (*TaskID*) – A task ID.\n\nreset(*task\\_id*, *\\**, *start=True*, *total=None*, *completed=0*, *visible=None*, *description=None*, *\\*\\*fields*)[source]¶\nReset a task so completed is 0 and the clock is reset.\n\nParameters\n* **task\\_id** (*TaskID*) – ID of task.\n* **start** (*bool**,* *optional*) – Start the task after reset. Defaults to True.\n* **total** (*float**,* *optional*) – New total steps in task, or None to use current total. Defaults to None.\n* **completed** (*int**,* *optional*) – Number of steps completed. Defaults to 0.\n* **visible** (*bool**,* *optional*) – Enable display of the task. Defaults to True.\n* **description** (*str**,* *optional*) – Change task description if not None. Defaults to None.\n* **\\*\\*fields** (*str*) – Additional data fields required for rendering.\n\nstart()[source]¶\nStart the progress display.\n\nstart\\_task(*task\\_id*)[source]¶\nStart a task.\n\n\nStarts a task (used when calculating elapsed time). You may need to call this manually,\nif you called `add\\_task` with `start=False`.\n\nParameters\n**task\\_id** (*TaskID*) – ID of task.\n\nstop()[source]¶\nStop the progress display.\n\nstop\\_task(*task\\_id*)[source]¶\nStop a task.\n\n\nThis will freeze the elapsed time on the task.\n\n*property* task\\_ids*: List[TaskID]*¶\nA list of task IDs.\n\n*property* tasks*: List[Task]*¶\nGet a list of Task instances.\n\ntrack(*sequence*, *total=None*, *task\\_id=None*, *description='Working...'*, *update\\_period=0.1*)[source]¶\nTrack progress by iterating over a sequence.\n\nParameters\n* **sequence** (*Sequence**[**ProgressType**]*) – A sequence of values you want to iterate over and track progress.\n* **total** (*Optional**[**float**]*) – (float, optional): Total number of steps. Default is len(sequence).\n* **task\\_id** (*Optional**[**TaskID**]*) – (TaskID): Task to track. Default is new task.\n* **description** (*str*) – (str, optional): Description of task, if new task is created.\n* **update\\_period** (*float**,* *optional*) – Minimum time (in seconds) between calls to update(). Defaults to 0.1.\n\nReturns\nAn iterable of values taken from the provided sequence.\n\nReturn type\nIterable[ProgressType]\n\nupdate(*task\\_id*, *\\**, *total=None*, *completed=None*, *advance=None*, *description=None*, *visible=None*, *refresh=False*, *\\*\\*fields*)[source]¶\nUpdate information associated with a task.\n\nParameters\n* **task\\_id** (*TaskID*) – Task id (returned by add\\_task).\n* **total** (*float**,* *optional*) – Updates task.total if not None.\n* **completed** (*float**,* *optional*) – Updates task.completed if not None.\n* **advance** (*float**,* *optional*) – Add a value to task.completed if not None.\n* **description** (*str**,* *optional*) – Change task description if not None.\n* **visible** (*bool**,* *optional*) – Set visible flag if not None.\n* **refresh** (*bool*) – Force a refresh of progress information. Default is False.\n* **\\*\\*fields** (*Any*) – Additional data fields required for rendering.\n\nwrap\\_file(*file*, *total=None*, *\\**, *task\\_id=None*, *description='Reading...'*)[source]¶\nTrack progress file reading from a binary file.\n\nParameters\n* **file** (*BinaryIO*) – A file-like object opened in binary mode.\n* **total** (*int**,* *optional*) – Total number of bytes to read. This must be provided unless a task with a total is also given.\n* **task\\_id** (*TaskID*) – Task to track. Default is new task.\n* **description** (*str**,* *optional*) – Description of task, if new task is created.\n\nRaises\n**ValueError** – When no total value can be extracted from the arguments or the task.\n\n\n*class* rich.progress.ProgressColumn(*table\\_column=None*)[source]¶\nBase class for a widget to use in progress display.\n\n\nget\\_table\\_column()[source]¶\nGet a table column, used to build tasks table.\n\n*abstract* render(*task*)[source]¶\nShould return a renderable object.\n\n\n*class* rich.progress.ProgressSample(*timestamp*, *completed*)[source]¶\nSample of progress for a given time.\n\nParameters\n* **timestamp** (*float*) –\n* **completed** (*float*) –\n\n\n*property* completed¶\nNumber of steps completed.\n\n*property* timestamp¶\nTimestamp of sample.\n\n\n*class* rich.progress.RenderableColumn(*renderable=''*, *\\**, *table\\_column=None*)[source]¶\nA column to insert an arbitrary column.\n\nParameters\n* **renderable** (*RenderableType**,* *optional*) – Any renderable. Defaults to empty string.\n* **table\\_column** (*Optional**[**Column**]*) –\n\n\nrender(*task*)[source]¶\nShould return a renderable object.\n\n\n*class* rich.progress.SpinnerColumn(*spinner\\_name='dots'*, *style='progress.spinner'*, *speed=1.0*, *finished\\_text=' '*, *table\\_column=None*)[source]¶\nA column with a ‘spinner’ animation.\n\nParameters\n* **spinner\\_name** (*str**,* *optional*) – Name of spinner animation. Defaults to “dots”.\n* **style** (*StyleType**,* *optional*) – Style of spinner. Defaults to “progress.spinner”.\n* **speed** (*float**,* *optional*) – Speed factor of spinner. Defaults to 1.0.\n* **finished\\_text** (*TextType**,* *optional*) – Text used when task is finished. Defaults to ” “.\n* **table\\_column** (*Optional**[**Column**]*) –\n\nset\\_spinner(*spinner\\_name*, *spinner\\_style='progress.spinner'*, *speed=1.0*)[source]¶\nSet a new spinner.\n\nParameters\n* **spinner\\_name** (*str*) – Spinner name, see python -m rich.spinner.\n* **spinner\\_style** (*Optional**[**StyleType**]**,* *optional*) – Spinner style. Defaults to “progress.spinner”.\n* **speed** (*float**,* *optional*) – Speed factor of spinner. Defaults to 1.0.\n\n\n*class* rich.progress.Task(*id*, *description*, *total*, *completed*, *\\_get\\_time*, *finished\\_time=None*, *visible=True*, *fields=<factory>*, *finished\\_speed=None*, *\\_lock=<factory>*)[source]¶\nInformation regarding a progress task.\n\n\nThis object should be considered read-only outside of the `Progress` class.\n\nParameters\n* **id** (*TaskID*) –\n* **description** (*str*) –\n* **total** (*Optional**[**float**]*) –\n* **completed** (*float*) –\n* **\\_get\\_time** (*Callable**[**[**]**,* *float**]*) –\n* **finished\\_time** (*Optional**[**float**]*) –\n* **visible** (*bool*) –\n* **fields** (*Dict**[**str**,* *Any**]*) –\n* **finished\\_speed** (*Optional**[**float**]*) –\n* **\\_lock** (*RLock*) –\n\n\ncompleted*: float*¶\nNumber of steps completed\n\nType\nfloat\n\ndescription*: str*¶\nDescription of the task.\n\n*property* elapsed*: Optional[float]*¶\nTime elapsed since task was started, or `None` if the task hasn’t started.\n\nType\nOptional[float]\n\nfields*: Dict[str, Any]*¶\nArbitrary fields passed in via Progress.update.\n\nType\ndict\n\n*property* finished*: bool*¶\nCheck if the task has finished.\n\nfinished\\_speed*: Optional[float]* *= None*¶\nThe last speed for a finished task.\n\nfinished\\_time*: Optional[float]* *= None*¶\nTime task was finished.\n\nget\\_time()[source]¶\nfloat: Get the current time, in seconds.\n\nReturn type\nfloat\n\nid*: TaskID*¶\nTask ID associated with this task (used in Progress methods).\n\n*property* percentage*: float*¶\nGet progress of task as a percentage. If a None total was set, returns 0\n\n*property* remaining*: Optional[float]*¶\nGet the number of steps remaining, if a non-None total was set.\n\n*property* speed*: Optional[float]*¶\nGet the estimated speed in steps per second.\n\nstart\\_time*: Optional[float]* *= None*¶\nTime this task was started, or None if not started.\n\n*property* started*: bool*¶\nCheck if the task as started.\n\nstop\\_time*: Optional[float]* *= None*¶\nTime this task was stopped, or None if not stopped.\n\n*property* time\\_remaining*: Optional[float]*¶\nGet estimated time to completion, or `None` if no data.\n\ntotal*: Optional[float]*¶\nTotal number of steps in this task.\n\nvisible*: bool* *= True*¶\nIndicates if this task is visible in the progress display.\n\n\n*class* rich.progress.TaskProgressColumn(*text\\_format='[progress.percentage]{task.percentage:>3.0f}%'*, *text\\_format\\_no\\_percentage=''*, *style='none'*, *justify='left'*, *markup=True*, *highlighter=None*, *table\\_column=None*, *show\\_speed=False*)[source]¶\nShow task progress as a percentage.\n\nParameters\n* **text\\_format** (*str**,* *optional*) – Format for percentage display. Defaults to “[progress.percentage]{task.percentage:>3.0f}%”.\n* **text\\_format\\_no\\_percentage** (*str**,* *optional*) – Format if percentage is unknown. Defaults to “”.\n* **style** (*StyleType**,* *optional*) – Style of output. Defaults to “none”.\n* **justify** (*JustifyMethod**,* *optional*) – Text justification. Defaults to “left”.\n* **markup** (*bool**,* *optional*) – Enable markup. Defaults to True.\n* **highlighter** (*Optional**[**Highlighter**]**,* *optional*) – Highlighter to apply to output. Defaults to None.\n* **table\\_column** (*Optional**[**Column**]**,* *optional*) – Table Column to use. Defaults to None.\n* **show\\_speed** (*bool**,* *optional*) – Show speed if total is unknown. Defaults to False.\n\n*classmethod* render\\_speed(*speed*)[source]¶\nRender the speed in iterations per second.\n\nParameters\n* **task** (*Task*) – A Task object.\n* **speed** (*Optional**[**float**]*) –\n\nReturns\nText object containing the task speed.\n\n\n*class* rich.progress.TextColumn(*text\\_format*, *style='none'*, *justify='left'*, *markup=True*, *highlighter=None*, *table\\_column=None*)[source]¶\nA column containing text.\n\nParameters\n* **text\\_format** (*str*) –\n* **style** (*Union**[**str**,* *Style**]*) –\n* **justify** (*typing\\_extensions.Literal**[**default**,* *left**,* *center**,* *right**,* *full**]*) –\n* **markup** (*bool*) –\n* **highlighter** (*Optional**[**Highlighter**]*) –\n* **table\\_column** (*Optional**[**Column**]*) –\n\n\n*class* rich.progress.TimeElapsedColumn(*table\\_column=None*)[source]¶\nRenders time elapsed.\n\n\nrender(*task*)[source]¶\nShow time elapsed.\n\n\n*class* rich.progress.TimeRemainingColumn(*compact=False*, *elapsed\\_when\\_finished=False*, *table\\_column=None*)[source]¶\nRenders estimated time remaining.\n\nParameters\n* **compact** (*bool**,* *optional*) – Render MM:SS when time remaining is less than an hour. Defaults to False.\n* **elapsed\\_when\\_finished** (*bool**,* *optional*) – Render time elapsed when the task is finished. Defaults to False.\n* **table\\_column** (*Optional**[**Column**]*) –\n\n\nrender(*task*)[source]¶\nShow time remaining.\n\n\n*class* rich.progress.TotalFileSizeColumn(*table\\_column=None*)[source]¶\nRenders total filesize.\n\n\n*class* rich.progress.TransferSpeedColumn(*table\\_column=None*)[source]¶\nRenders human readable transfer speed.\n\n\nrender(*task*)[source]¶\nShow data transfer speed.\n\n\nrich.progress.open(*file: Union[str, PathLike[str], bytes]*, *mode: Union[typing\\_extensions.Literal[rt], typing\\_extensions.Literal[r]]*, *buffering: int = -1*, *encoding: Optional[str] = None*, *errors: Optional[str] = None*, *newline: Optional[str] = None*, *\\**, *total: Optional[int] = None*, *description: str = 'Reading...'*, *auto\\_refresh: bool = True*, *console: Optional[Console] = None*, *transient: bool = False*, *get\\_time: Optional[Callable[[], float]] = None*, *refresh\\_per\\_second: float = 10*, *style: Union[str, Style] = 'bar.back'*, *complete\\_style: Union[str, Style] = 'bar.complete'*, *finished\\_style: Union[str, Style] = 'bar.finished'*, *pulse\\_style: Union[str, Style] = 'bar.pulse'*, *disable: bool = False*) → AbstractContextManager[TextIO][source]¶\n\nrich.progress.open(*file: Union[str, PathLike[str], bytes]*, *mode: typing\\_extensions.Literal[rb]*, *buffering: int = -1*, *encoding: Optional[str] = None*, *errors: Optional[str] = None*, *newline: Optional[str] = None*, *\\**, *total: Optional[int] = None*, *description: str = 'Reading...'*, *auto\\_refresh: bool = True*, *console: Optional[Console] = None*, *transient: bool = False*, *get\\_time: Optional[Callable[[], float]] = None*, *refresh\\_per\\_second: float = 10*, *style: Union[str, Style] = 'bar.back'*, *complete\\_style: Union[str, Style] = 'bar.complete'*, *finished\\_style: Union[str, Style] = 'bar.finished'*, *pulse\\_style: Union[str, Style] = 'bar.pulse'*, *disable: bool = False*) → AbstractContextManager[BinaryIO]\nRead bytes from a file while tracking progress.\n\nParameters\n* **path** (*Union**[**str**,* *PathLike**[**str**]**,* *BinaryIO**]*) – The path to the file to read, or a file-like object in binary mode.\n* **mode** (*str*) – The mode to use to open the file. Only supports “r”, “rb” or “rt”.\n* **buffering** (*int*) – The buffering strategy to use, see `io.open()`.\n* **encoding** (*str**,* *optional*) – The encoding to use when reading in text mode, see `io.open()`.\n* **errors** (*str**,* *optional*) – The error handling strategy for decoding errors, see `io.open()`.\n* **newline** (*str**,* *optional*) – The strategy for handling newlines in text mode, see `io.open()`\n* **total** – (int, optional): Total number of bytes to read. Must be provided if reading from a file handle. Default for a path is os.stat(file).st\\_size.\n* **description** (*str**,* *optional*) – Description of task show next to progress bar. Defaults to “Reading”.\n* **auto\\_refresh** (*bool**,* *optional*) – Automatic refresh, disable to force a refresh after each iteration. Default is True.\n* **transient** – (bool, optional): Clear the progress on exit. Defaults to False.\n* **console** (*Console**,* *optional*) – Console to write to. Default creates internal Console instance.\n* **refresh\\_per\\_second** (*float*) – Number of times per second to refresh the progress information. Defaults to 10.\n* **style** (*StyleType**,* *optional*) – Style for the bar background. Defaults to “bar.back”.\n* **complete\\_style** (*StyleType**,* *optional*) – Style for the completed bar. Defaults to “bar.complete”.\n* **finished\\_style** (*StyleType**,* *optional*) – Style for a finished bar. Defaults to “bar.finished”.\n* **pulse\\_style** (*StyleType**,* *optional*) – Style for pulsing bars. Defaults to “bar.pulse”.\n* **disable** (*bool**,* *optional*) – Disable display of progress.\n* **encoding** – The encoding to use when reading in text mode.\n\nReturns\nA context manager yielding a progress reader.\n\nReturn type\nContextManager[BinaryIO]\n\nrich.progress.track(*sequence*, *description='Working...'*, *total=None*, *auto\\_refresh=True*, *console=None*, *transient=False*, *get\\_time=None*, *refresh\\_per\\_second=10*, *style='bar.back'*, *complete\\_style='bar.complete'*, *finished\\_style='bar.finished'*, *pulse\\_style='bar.pulse'*, *update\\_period=0.1*, *disable=False*, *show\\_speed=True*)[source]¶\nTrack progress by iterating over a sequence.\n\nParameters\n* **sequence** (*Iterable**[**ProgressType**]*) – A sequence (must support “len”) you wish to iterate over.\n* **description** (*str**,* *optional*) – Description of task show next to progress bar. Defaults to “Working”.\n* **total** (*Optional**[**float**]*) – (float, optional): Total number of steps. Default is len(sequence).\n* **auto\\_refresh** (*bool**,* *optional*) – Automatic refresh, disable to force a refresh after each iteration. Default is True.\n* **transient** (*bool*) – (bool, optional): Clear the progress on exit. Defaults to False.\n* **console** (*Console**,* *optional*) – Console to write to. Default creates internal Console instance.\n* **refresh\\_per\\_second** (*float*) – Number of times per second to refresh the progress information. Defaults to 10.\n* **style** (*StyleType**,* *optional*) – Style for the bar background. Defaults to “bar.back”.\n* **complete\\_style** (*StyleType**,* *optional*) – Style for the completed bar. Defaults to “bar.complete”.\n* **finished\\_style** (*StyleType**,* *optional*) – Style for a finished bar. Defaults to “bar.finished”.\n* **pulse\\_style** (*StyleType**,* *optional*) – Style for pulsing bars. Defaults to “bar.pulse”.\n* **update\\_period** (*float**,* *optional*) – Minimum time (in seconds) between calls to update(). Defaults to 0.1.\n* **disable** (*bool**,* *optional*) – Disable display of progress.\n* **show\\_speed** (*bool**,* *optional*) – Show speed if total isn’t known. Defaults to True.\n* **get\\_time** (*Optional**[**Callable**[**[**]**,* *float**]**]*) –\n\nReturns\nAn iterable of the values in the sequence.\n\nrich.progress.wrap\\_file(*file*, *total*, *\\**, *description='Reading...'*, *auto\\_refresh=True*, *console=None*, *transient=False*, *get\\_time=None*, *refresh\\_per\\_second=10*, *style='bar.back'*, *complete\\_style='bar.complete'*, *finished\\_style='bar.finished'*, *pulse\\_style='bar.pulse'*, *disable=False*)[source]¶\nRead bytes from a file while tracking progress.\n\nParameters\n* **file** (*Union**[**str**,* *PathLike**[**str**]**,* *BinaryIO**]*) – The path to the file to read, or a file-like object in binary mode.\n* **total** (*int*) – Total number of bytes to read.\n* **description** (*str**,* *optional*) – Description of task show next to progress bar. Defaults to “Reading”.\n* **auto\\_refresh** (*bool**,* *optional*) – Automatic refresh, disable to force a refresh after each iteration. Default is True.\n* **transient** (*bool*) – (bool, optional): Clear the progress on exit. Defaults to False.\n* **console** (*Console**,* *optional*) – Console to write to. Default creates internal Console instance.\n* **refresh\\_per\\_second** (*float*) – Number of times per second to refresh the progress information. Defaults to 10.\n* **style** (*StyleType**,* *optional*) – Style for the bar background. Defaults to “bar.back”.\n* **complete\\_style** (*StyleType**,* *optional*) – Style for the completed bar. Defaults to “bar.complete”.\n* **finished\\_style** (*StyleType**,* *optional*) – Style for a finished bar. Defaults to “bar.finished”.\n* **pulse\\_style** (*StyleType**,* *optional*) – Style for pulsing bars. Defaults to “bar.pulse”.\n* **disable** (*bool**,* *optional*) – Disable display of progress.\n* **get\\_time** (*Optional**[**Callable**[**[**]**,* *float**]**]*) –\n\n# rich.prompt¶\n\n\n*class* rich.prompt.Confirm(*prompt=''*, *\\**, *console=None*, *password=False*, *choices=None*, *show\\_default=True*, *show\\_choices=True*)[source]¶\nA yes / no confirmation prompt.\n\n```\n>>> if Confirm.ask(\"Continue\"):\n run\\_job()\n\n\nprocess\\_response(*value*)[source]¶\nConvert choices to a bool.\n\nParameters\n**value** (*str*) – \n\nrender\\_default(*default*)[source]¶\nRender the default as (y) or (n) rather than True/False.\n\nParameters\n**default** (*DefaultType*) – \n\nresponse\\_type¶\nalias of `bool`\n\n\n*class* rich.prompt.FloatPrompt(*prompt=''*, *\\**,\n\n==================\n Document 1 \n----------------\n rich.progress\\_bar¶\n\n\n*class* rich.progress\\_bar.ProgressBar(*total=100.0*, *completed=0*, *width=None*, *pulse=False*, *style='bar.back'*, *complete\\_style='bar.complete'*, *finished\\_style='bar.finished'*, *pulse\\_style='bar.pulse'*, *animation\\_time=None*)[source]¶\nRenders a (progress) bar. Used by rich.progress.\n\nParameters\n* **total** (*float**,* *optional*) – Number of steps in the bar. Defaults to 100. Set to None to render a pulsing animation.\n* **completed** (*float**,* *optional*) – Number of steps completed. Defaults to 0.\n* **width** (*int**,* *optional*) – Width of the bar, or `None` for maximum width. Defaults to None.\n* **pulse** (*bool**,* *optional*) – Enable pulse effect. Defaults to False. Will pulse if a None total was passed.\n* **style** (*StyleType**,* *optional*) – Style for the bar background. Defaults to “bar.back”.\n* **complete\\_style** (*StyleType**,* *optional*) – Style for the completed bar. Defaults to “bar.complete”.\n* **finished\\_style** (*StyleType**,* *optional*) – Style for a finished bar. Defaults to “bar.finished”.\n* **pulse\\_style** (*StyleType**,* *optional*) – Style for pulsing bars. Defaults to “bar.pulse”.\n* **animation\\_time** (*Optional**[**float**]**,* *optional*) – Time in seconds to use for animation, or None to use system time.\n\n\n*property* percentage\\_completed*: Optional[float]*¶\nCalculate percentage complete.\n\nupdate(*completed*, *total=None*)[source]¶\nUpdate progress with new values.\n\nParameters\n* **completed** (*float*) – Number of steps completed.\n* **total** (*float**,* *optional*) – Total number of steps, or `None` to not change. Defaults to None.\n\n# rich.rule¶\n\n\n*class* rich.rule.Rule(*title=''*, *\\**, *characters='─'*, *style='rule.line'*, *end='\\n'*, *align='center'*)[source]¶\nA console renderable to draw a horizontal rule (line).\n\nParameters\n* **title** (*Union**[**str**,* *Text**]**,* *optional*) – Text to render in the rule. Defaults to “”.\n* **characters** (*str**,* *optional*) – Character(s) used to draw the line. Defaults to “─”.\n* **style** (*StyleType**,* *optional*) – Style of Rule. Defaults to “rule.line”.\n* **end** (*str**,* *optional*) – Character at end of Rule. defaults to “\\n”\n* **align** (*str**,* *optional*) – How to align the title, one of “left”, “center”, or “right”. Defaults to “center”.\n\n\n# rich.segment¶\n\n\n*class* rich.segment.ControlType(*value*)[source]¶\nNon-printable control codes which typically translate to ANSI codes.\n\n*class* rich.segment.Segment(*text*, *style=None*, *control=None*)[source]¶\nA piece of text with associated style. Segments are produced by the Console render process and\nare ultimately converted in to strings to be written to the terminal.\n\nParameters\n*\n\n==================\n Document 2 \n----------------\n rich.console¶\n\n\n*class* rich.console.Capture(*console*)[source]¶\nContext manager to capture the result of printing to the console.\nSee `capture()` for how to use.\n\nParameters\n**console** (*Console*) – A console instance to capture output.\n\n\nget()[source]¶\nGet the result of the capture.\n\n\n*exception* rich.console.CaptureError[source]¶\nAn error in the Capture context manager.\n\n*class* rich.console.Console(*\\**, *color\\_system='auto'*, *force\\_terminal=None*, *force\\_jupyter=None*, *force\\_interactive=None*, *soft\\_wrap=False*, *theme=None*, *stderr=False*, *file=None*, *quiet=False*, *width=None*, *height=None*, *style=None*, *no\\_color=None*, *tab\\_size=8*, *record=False*, *markup=True*, *emoji=True*, *emoji\\_variant=None*, *highlight=True*, *log\\_time=True*, *log\\_path=True*, *log\\_time\\_format='[%X]'*, *highlighter=<rich.highlighter.ReprHighlighter object>*, *legacy\\_windows=None*, *safe\\_box=True*, *get\\_datetime=None*, *get\\_time=None*, *\\_environ=None*)[source]¶\nA high level console interface.\n\nParameters\n* **color\\_system** (*str**,* *optional*) – The color system supported by your terminal,\neither `\"standard\"`, `\"256\"` or `\"truecolor\"`. Leave as `\"auto\"` to autodetect.\n* **force\\_terminal** (*Optional**[**bool**]**,* *optional*) – Enable/disable terminal control codes, or None to auto-detect terminal. Defaults to None.\n* **force\\_jupyter** (*Optional**[**bool**]**,* *optional*) – Enable/disable Jupyter rendering, or None to auto-detect Jupyter. Defaults to None.\n* **force\\_interactive** (*Optional**[**bool**]**,* *optional*) – Enable/disable interactive mode, or None to auto detect. Defaults to None.\n* **soft\\_wrap** (*Optional**[**bool**]**,* *optional*) – Set soft wrap default on print method. Defaults to False.\n* **theme** (*Theme**,* *optional*) – An optional style theme object, or `None` for default theme.\n* **stderr** (*bool**,* *optional*) – Use stderr rather than stdout if `file` is not specified. Defaults to False.\n* **file** (*IO**,* *optional*) – A file object where the console should write to. Defaults to stdout.\n* **quiet** (*bool**,* *Optional*) – Boolean to suppress all output. Defaults to False.\n* **width** (*int**,* *optional*) – The width of the terminal. Leave as default to auto-detect width.\n* **height** (*int**,* *optional*) – The height of the terminal. Leave as default to auto-detect height.\n* **style** (*StyleType**,* *optional*) – Style to apply to all output, or None for no style. Defaults to None.\n* **no\\_color** (*Optional**[**bool**]**,* *optional*) – Enabled no color mode, or None to auto detect. Defaults to None.\n* **tab\\_size** (*int**,* *optional*) – Number of spaces used to replace a tab character. Defaults to 8.\n* **record** (*bool**,* *optional*) – Boolean to enable recording of terminal output,\nrequired to call `export\\_html()`, `export\\_svg()`, and `export\\_text()`. Defaults to False.\n* **markup** (*bool**,* *optional*) – Boolean to enable Console Markup. Defaults to True.\n* **emoji** (*bool**,* *optional*) – Enable emoji code. Defaults to True.\n* **emoji\\_variant** (*str**,* *optional*) – Optional emoji variant, either “text” or “emoji”. Defaults to None.\n* **highlight** (*bool**,* *optional*) – Enable automatic highlighting. Defaults to True.\n* **log\\_time** (*bool**,* *optional*) – Boolean to enable logging of time by `log()` methods. Defaults to True.\n* **log\\_path** (*bool**,* *optional*) – Boolean to enable the logging of the caller by `log()`. Defaults to True.\n* **log\\_time\\_format** (*Union**[**str**,* *TimeFormatterCallable**]**,* *optional*) – If `log\\_time` is enabled, either string for strftime or callable that formats the time. Defaults to “[%X] “.\n* **highlighter** (*HighlighterType**,* *optional*) – Default highlighter.\n* **legacy\\_windows** (*bool**,* *optional*) – Enable legacy Windows mode, or `None` to auto detect. Defaults to `None`.\n* **safe\\_box** (*bool**,* *optional*) – Restrict box options that don’t render on legacy Windows.\n* **get\\_datetime** (*Callable**[**[**]**,* *datetime**]**,* *optional*) – Callable that gets the current time as a datetime.datetime object (used by Console.log),\nor None for datetime.now.\n* **get\\_time** (*Callable**[**[**]**,* *time**]**,* *optional*) – Callable that gets the current time in seconds, default uses time.monotonic.\n* **\\_environ** (*Mapping**[**str**,* *str**]*) –\n\n\nbegin\\_capture()[source]¶\nBegin capturing console output. Call `end\\_capture()` to exit capture mode and return output.\n\nbell()[source]¶\nPlay a ‘bell’ sound (if supported by the terminal).\n\ncapture()[source]¶\nA context manager to *capture* the result of print() or log() in a string,\nrather than writing it to the console.\n\n\nExample\n\n```\n>>> from rich.console import Console\n>>> console = Console()\n>>> with console.capture() as capture:\n...     console.print(\"[bold magenta]Hello World[/]\")\n>>> print(capture.get())\n\nReturns\nContext manager with disables writing to the terminal.\n\nReturn type\nCapture\n\nclear(*home=True*)[source]¶\nClear the screen.\n\nParameters\n**home** (*bool**,* *optional*) – Also move the cursor to ‘home’ position. Defaults to True.\n\nclear\\_live()[source]¶\nClear the Live instance.\n\n*property* color\\_system*: Optional[str]*¶\nGet color system string.\n\nReturns\n“standard”, “256” or “truecolor”.\n\nReturn type\nOptional[str]\n\ncontrol(*\\*control*)[source]¶\nInsert non-printing control codes.\n\nParameters\n* **control\\_codes** (*str*) – Control codes, such as those that may move the cursor.\n* **control** (*Control*) –\n\n*property* encoding*: str*¶\nGet the encoding of the console file, e.g. `\"utf-8\"`.\n\nReturns\nA standard encoding string.\n\nend\\_capture()[source]¶\nEnd capture mode and return captured string.\n\nReturns\nConsole output.\n\nexport\\_html(*\\**, *theme=None*, *clear=True*, *code\\_format=None*, *inline\\_styles=False*)[source]¶\nGenerate HTML from console contents (requires record=True argument in constructor).\n\nParameters\n* **theme** (*TerminalTheme**,* *optional*) – TerminalTheme object containing console colors.\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`.\n* **code\\_format** (*str**,* *optional*) – Format string to render HTML. In addition to ‘{foreground}’,\n‘{background}’, and ‘{code}’, should contain ‘{stylesheet}’ if inline\\_styles is `False`.\n* **inline\\_styles** (*bool**,* *optional*) – If `True` styles will be inlined in to spans, which makes files\nlarger but easier to cut and paste markup. If `False`, styles will be embedded in a style tag.\nDefaults to False.\n\nReturns\nString containing console contents as HTML.\n\nexport\\_svg(*\\**, *title='Rich'*, *theme=None*, *clear=True*, *code\\_format='<svg class=\"rich-terminal\" viewBox=\"0 0 {width} {height}\" xmlns=\"http://www.w3.org/2000/svg\">\\n    <!-- Generated with Rich https://www.textualize.io -->\\n    <style>\\n\\n    @font-face {{\\n        font-family: \"Fira Code\";\\n        src: local(\"FiraCode-Regular\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Regular.woff2\") format(\"woff2\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Regular.woff\") format(\"woff\");\\n        font-style: normal;\\n        font-weight: 400;\\n    }}\\n    @font-face {{\\n        font-family: \"Fira Code\";\\n        src: local(\"FiraCode-Bold\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Bold.woff2\") format(\"woff2\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Bold.woff\") format(\"woff\");\\n        font-style: bold;\\n        font-weight: 700;\\n    }}\\n\\n    .{unique\\_id}-matrix {{\\n        font-family: Fira Code, monospace;\\n        font-size: {char\\_height}px;\\n        line-height: {line\\_height}px;\\n        font-variant-east-asian: full-width;\\n    }}\\n\\n    .{unique\\_id}-title {{\\n        font-size: 18px;\\n        font-weight: bold;\\n        font-family: arial;\\n    }}\\n\\n    {styles}\\n    </style>\\n\\n    <defs>\\n    <clipPath id=\"{unique\\_id}-clip-terminal\">\\n      <rect x=\"0\" y=\"0\" width=\"{terminal\\_width}\" height=\"{terminal\\_height}\" />\\n    </clipPath>\\n    {lines}\\n    </defs>\\n\\n    {chrome}\\n    <g transform=\"translate({terminal\\_x}, {terminal\\_y})\" clip-path=\"url(#{unique\\_id}-clip-terminal)\">\\n    {backgrounds}\\n    <g class=\"{unique\\_id}-matrix\">\\n    {matrix}\\n    </g>\\n    </g>\\n</svg>\\n'*, *font\\_aspect\\_ratio=0.61*, *unique\\_id=None*)[source]¶\nGenerate an SVG from the console contents (requires record=True in Console constructor).\n\nParameters\n* **title** (*str**,* *optional*) – The title of the tab in the output image\n* **theme** (*TerminalTheme**,* *optional*) – The `TerminalTheme` object to use to style the terminal\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`\n* **code\\_format** (*str**,* *optional*) – Format string used to generate the SVG. Rich will inject a number of variables\ninto the string in order to form the final SVG output. The default template used and the variables\ninjected by Rich can be found by inspecting the `console.CONSOLE\\_SVG\\_FORMAT` variable.\n* **font\\_aspect\\_ratio** (*float**,* *optional*) – The width to height ratio of the font used in the `code\\_format`\nstring. Defaults to 0.61, which is the width to height ratio of Fira Code (the default font).\nIf you aren’t specifying a different font inside `code\\_format`, you probably don’t need this.\n* **unique\\_id** (*str**,* *optional*) – unique id that is used as the prefix for various elements (CSS styles, node\nids). If not set, this defaults to a computed value based on the recorded content.\n\nexport\\_text(*\\**, *clear=True*, *styles=False*)[source]¶\nGenerate text from console contents (requires record=True argument in constructor).\n\nParameters\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`.\n* **styles** (*bool**,* *optional*) – If `True`, ansi escape codes will be included. `False` for plain text.\nDefaults to `False`.\n\nReturns\nString containing console contents.\n\n*property* file*: IO[str]*¶\nGet the file object to write to.\n\nget\\_style(*name*, *\\**, *default=None*)[source]¶\nGet a Style instance by its theme name or parse a definition.\n\nParameters\n* **name** (*str*) – The name of a style or a style definition.\n* **default** (*Optional**[**Union**[**str**,* *Style**]**]*) –\n\nReturns\nA Style object.\n\nReturn type\nStyle\n\nRaises\n**MissingStyle** – If no style could be parsed from name.\n\n*property* height*: int*¶\nGet the height of the console.\n\nReturns\nThe height (in lines) of the console.\n\ninput(*prompt=''*, *\\**, *markup=True*, *emoji=True*, *password=False*, *stream=None*)[source]¶\nDisplays a prompt and waits for input from the user. The prompt may contain color / style.\n\n\nIt works in the same way as Python’s builtin `input()` function and provides elaborate line editing and history features if Python’s builtin `readline` module is previously loaded.\n\nParameters\n* **prompt** (*Union**[**str**,* *Text**]*) – Text to render in the prompt.\n* **markup** (*bool**,* *optional*) – Enable console markup (requires a str prompt). Defaults to True.\n* **emoji** (*bool**,* *optional*) – Enable emoji (requires a str prompt). Defaults to True.\n* **password** (*bool*) – (bool, optional): Hide typed text. Defaults to False.\n* **stream** (*Optional**[**TextIO**]*) – (TextIO, optional): Optional file to read input from (rather than stdin). Defaults to None.\n\nReturns\nText read from stdin.\n\n*property* is\\_alt\\_screen*: bool*¶\nCheck if the alt screen was enabled.\n\nReturns\nTrue if the alt screen was enabled, otherwise False.\n\nReturn type\nbool\n\n*property* is\\_dumb\\_terminal*: bool*¶\nDetect dumb terminal.\n\nReturns\nTrue if writing to a dumb terminal, otherwise False.\n\n*property* is\\_terminal*: bool*¶\nCheck if the console is writing to a terminal.\n\nReturns\nTrue if the console writing to a device capable of\nunderstanding terminal codes, otherwise False.\n\nline(*count=1*)[source]¶\nWrite new line(s).\n\nParameters\n**count** (*int**,* *optional*) – Number of new lines. Defaults to 1.\n\nlog(*\\*objects*, *sep=' '*, *end='\\n'*, *style=None*, *justify=None*, *emoji=None*, *markup=None*, *highlight=None*, *log\\_locals=False*, *\\_stack\\_offset=1*)[source]¶\nLog rich content to the terminal.\n\nParameters\n* **objects** (*positional args*) – Objects to log to the terminal.\n* **sep** (*str**,* *optional*) – String to write between print data. Defaults to ” “.\n* **end** (*str**,* *optional*) – String to write at end of print data. Defaults to “\\n”.\n* **style** (*Union**[**str**,* *Style**]**,* *optional*) – A style to apply to output. Defaults to None.\n* **justify** (*str**,* *optional*) – One of “left”, “right”, “center”, or “full”. Defaults to `None`.\n* **emoji** (*Optional**[**bool**]**,* *optional*) – Enable emoji code, or `None` to use console default. Defaults to None.\n* **markup** (*Optional**[**bool**]**,* *optional*) – Enable markup, or `None` to use console default. Defaults to None.\n* **highlight** (*Optional**[**bool**]**,* *optional*) – Enable automatic highlighting, or `None` to use console default. Defaults to None.\n* **log\\_locals** (*bool**,* *optional*) – Boolean to enable logging of locals where `log()`\nwas called. Defaults to False.\n* **\\_stack\\_offset** (*int**,* *optional*) – Offset of caller from end of call stack. Defaults to 1.\n\nmeasure(*renderable*, *\\**, *options=None*)[source]¶\nMeasure a renderable. Returns a `Measurement` object which contains\ninformation regarding the number of characters required to print the renderable.\n\nParameters\n* **renderable** (*RenderableType*) – Any renderable or string.\n* **options** (*Optional**[**ConsoleOptions**]**,* *optional*) – Options to use when measuring, or None\nto use default options. Defaults to None.\n\nReturns\nA measurement of the renderable.\n\n*property* options*: ConsoleOptions*¶\nGet default console options.\n\nout(*\\*objects*, *sep=' '*, *end='\\n'*, *style=None*, *highlight=None*)[source]¶\nOutput to the terminal. This is a low-level way of writing to the terminal which unlike\n`print()` won’t pretty print, wrap text, or apply markup, but will\noptionally apply highlighting and a basic style.\n\nParameters\n* **sep** (*str**,* *optional*) – String to write between print data. Defaults to ” “.\n* **end** (*str**,* *optional*) – String to write at end of print data. Defaults to “\\n”.\n* **style** (*Union**[**str**,* *Style**]**,* *optional*) – A style to apply to output. Defaults to None.\n* **highlight** (*Optional**[**bool**]**,* *optional*) – Enable automatic highlighting, or `None` to use\nconsole default. Defaults to `None`.\n* **objects** (*Any*) –\n\npager(*pager=None*, *styles=False*, *links=False*)[source]¶\nA context manager to display anything printed within a “pager”. The pager application\nis defined by the system and will typically support at least pressing a key to scroll.\n\nParameters\n* **pager** (*Pager**,* *optional*) – A pager object, or None to use `SystemPager`. Defaults to None.\n* **styles** (*bool**,* *optional*) – Show styles in pager. Defaults to False.\n* **links** (*bool**,* *optional*) – Show links in pager. Defaults to False.\n\nReturn type\n*PagerContext*\n\n```\n>>> from rich.console import Console\n>>> from rich.\\_\\_main\\_\\_ import make\\_test\\_card\n>>> console = Console()\n>>> with console.pager():\n console.print(make\\_test\\_card())\n\nReturns\nA context manager.\n\nReturn type\nPagerContext\n\nParameters\n* **pager** (*Optional**[**Pager**]*) –\n* **styles** (*bool*) –\n* **links** (*bool*) –\n\npop\\_render\\_hook()[source]¶\nPop the last renderhook from the stack.\n\npop\\_theme()[source]¶\nRemove theme from top of stack, restoring previous theme.\n\nprint(*\\*objects*, *sep=' '*, *end='\\n'*, *style=None*, *justify=None*, *overflow=None*, *no\\_wrap=None*, *emoji=None*, *markup=None*, *highlight=None*, *width=None*, *height=None*, *crop=True*, *soft\\_wrap=None*, *new\\_line\\_start=False*)[source]¶\nPrint to the console.\n\nParameters\n* **objects** (*positional args*) – Objects to log to the terminal.\n* **sep** (*str**,* *optional*) – String to write between print data. Defaults to ” “.\n* **end** (*str**,* *optional*) – String to write at end of print data. Defaults to “\\n”.\n* **style** (*Union**[**str**,* *Style**]**,* *optional*) – A style to apply to output. Defaults to None.\n* **justify** (*str**,* *optional*) – Justify method: “default”, “left”, “right”, “center”, or “full”. Defaults to `None`.\n* **overflow** (*str**,* *optional*) – Overflow method: “ignore”, “crop”, “fold”, or “ellipsis”. Defaults to None.\n* **no\\_wrap** (*Optional**[**bool**]**,* *optional*) – Disable word wrapping. Defaults to None.\n* **emoji** (*Optional**[**bool**]**,* *optional*) – Enable emoji code, or `None` to use console default. Defaults to `None`.\n* **markup** (*Optional**[**bool**]**,* *optional*) – Enable markup, or `None` to use console default. Defaults to `None`.\n* **highlight** (*Optional**[**bool**]**,* *optional*) – Enable automatic highlighting, or `None` to use console default. Defaults to `None`.\n* **width** (*Optional**[**int**]**,* *optional*) – Width of output, or `None` to auto-detect. Defaults to `None`.\n* **crop** (*Optional**[**bool**]**,* *optional*) – Crop output to width of terminal. Defaults to True.\n* **soft\\_wrap** (*bool**,* *optional*) – Enable soft wrap mode which disables word wrapping and cropping of text or `None` for\nConsole default. Defaults to `None`.\n* **new\\_line\\_start** (*bool**,* *False*) – Insert a new line at the start if the output contains more than one line. Defaults to `False`.\n* **height** (*Optional**[**int**]*) –\n\nprint\\_exception(*\\**, *width=100*, *extra\\_lines=3*, *theme=None*, *word\\_wrap=False*, *show\\_locals=False*, *suppress=()*, *max\\_frames=100*)[source]¶\nPrints a rich render of the last exception and traceback.\n\nParameters\n* **width** (*Optional**[**int**]**,* *optional*) – Number of characters used to render code. Defaults to 100.\n* **extra\\_lines** (*int**,* *optional*) – Additional lines of code to render. Defaults to 3.\n* **theme** (*str**,* *optional*) – Override pygments theme used in traceback\n* **word\\_wrap** (*bool**,* *optional*) – Enable word wrapping of long lines. Defaults to False.\n* **show\\_locals** (*bool**,* *optional*) – Enable display of local variables. Defaults to False.\n* **suppress** (*Iterable**[**Union**[**str**,* *ModuleType**]**]*) – Optional sequence of modules or paths to exclude from traceback.\n* **max\\_frames** (*int*) – Maximum number of frames to show in a traceback, 0 for no maximum. Defaults to 100.\n\nprint\\_json(*json=None*, *\\**, *data=None*, *indent=2*, *highlight=True*, *skip\\_keys=False*, *ensure\\_ascii=False*, *check\\_circular=True*, *allow\\_nan=True*, *default=None*, *sort\\_keys=False*)[source]¶\nPretty prints JSON. Output will be valid JSON.\n\nParameters\n* **json** (*Optional**[**str**]*) – A string containing JSON.\n* **data** (*Any*) – If json is not supplied, then encode this data.\n* **indent** (*Union**[**None**,* *int**,* *str**]**,* *optional*) – Number of spaces to indent. Defaults to 2.\n* **highlight** (*bool**,* *optional*) – Enable highlighting of output: Defaults to True.\n* **skip\\_keys** (*bool**,* *optional*) – Skip keys not of a basic type. Defaults to False.\n* **ensure\\_ascii** (*bool**,* *optional*) – Escape all non-ascii characters. Defaults to False.\n* **check\\_circular** (*bool**,* *optional*) – Check for circular references. Defaults to True.\n* **allow\\_nan** (*bool**,* *optional*) – Allow NaN and Infinity values. Defaults to True.\n* **default** (*Callable**,* *optional*) – A callable that converts values that can not be encoded\nin to something that can be JSON encoded. Defaults to None.\n* **sort\\_keys** (*bool**,* *optional*) – Sort dictionary keys. Defaults to False.\n\npush\\_render\\_hook(*hook*)[source]¶\nAdd a new render hook to the stack.\n\nParameters\n**hook** (*RenderHook*) – Render hook instance.\n\npush\\_theme(*theme*, *\\**, *inherit=True*)[source]¶\nPush a new theme on to the top of the stack, replacing the styles from the previous theme.\nGenerally speaking, you should call `use\\_theme()` to get a context manager, rather\nthan calling this method directly.\n\nParameters\n* **theme** (*Theme*) – A theme instance.\n* **inherit** (*bool**,* *optional*) – Inherit existing styles. Defaults to True.\n\nrender(*renderable*, *options=None*)[source]¶\nRender an object in to an iterable of Segment instances.\n\n\nThis method contains the logic for rendering objects with the console protocol.\nYou are unlikely to need to use it directly, unless you are extending the library.\n\nParameters\n* **renderable** (*RenderableType*) – An object supporting the console protocol, or\nan object that may be converted to a string.\n* **options** (*ConsoleOptions**,* *optional*) – An options object, or None to use self.options. Defaults to None.\n\nReturns\nAn iterable of segments that may be rendered.\n\nrender\\_lines(*renderable*, *options=None*, *\\**, *style=None*, *pad=True*, *new\\_lines=False*)[source]¶\nRender objects in to a list of lines.\n\n> \n> The output of render\\_lines is useful when further formatting of rendered console text\n> is required, such as the Panel class which draws a border around any renderable object.\n> \n> \n> \n> Args:renderable (RenderableType): Any object renderable in the console.\n> options (Optional[ConsoleOptions], optional): Console options, or None to use self.options. Default to `None`.\n> style (Style, optional): Optional style to apply to renderables. Defaults to `None`.\n> pad (bool, optional): Pad lines shorter than render width. Defaults to `True`.\n> new\\_lines (bool, optional): Include “\n> \n> \n> \n> \n> \n\n\n” characters at end of lines.\n\n> \n> \n> Returns:List[List[Segment]]: A list of lines, where a line is a list of Segment objects.\n> \n> \n> \n> \n> \n\nParameters\n* **renderable** (*Union**[**ConsoleRenderable**,* *RichCast**,* *str**]*) –\n* **options** (*Optional**[**ConsoleOptions**]*) –\n* **style** (*Optional**[**Style**]*) –\n* **pad** (*bool*) –\n* **new\\_lines** (*bool*) –\n\nrender\\_str(*text*, *\\**, *style=''*, *justify=None*, *overflow=None*, *emoji=None*, *markup=None*, *highlight=None*, *highlighter=None*)[source]¶\nConvert a string to a Text instance. This is called automatically if\nyou print or log a string.\n\nParameters\n* **text** (*str*) – Text to render.\n* **style** (*Union**[**str**,* *Style**]**,* *optional*) – Style to apply to rendered text.\n* **justify** (*str**,* *optional*) – Justify method: “default”, “left”, “center”, “full”, or “right”. Defaults to `None`.\n* **overflow** (*str**,* *optional*) – Overflow method: “crop”, “fold”, or “ellipsis”. Defaults to `None`.\n* **emoji** (*Optional**[**bool**]**,* *optional*) – Enable emoji, or `None` to use Console default.\n* **markup** (*Optional**[**bool**]**,* *optional*) – Enable markup, or `None` to use Console default.\n* **highlight** (*Optional**[**bool**]**,* *optional*) – Enable highlighting, or `None` to use Console default.\n* **highlighter** (*HighlighterType**,* *optional*) – Optional highlighter to apply.\n\nReturns\nRenderable object.\n\nReturn type\nConsoleRenderable\n\nrule(*title=''*, *\\**, *characters='─'*, *style='rule.line'*, *align='center'*)[source]¶\nDraw a line with optional centered title.\n\nParameters\n* **title** (*str**,* *optional*) – Text to render over the rule. Defaults to “”.\n* **characters** (*str**,* *optional*) – Character(s) to form the line. Defaults to “─”.\n* **style** (*str**,* *optional*) – Style of line. Defaults to “rule.line”.\n* **align** (*str**,* *optional*) – How to align the title, one of “left”, “center”, or “right”. Defaults to “center”.\n\nsave\\_html(*path*, *\\**, *theme=None*, *clear=True*, *code\\_format='<!DOCTYPE html>\\n<html>\\n<head>\\n<meta charset=\"UTF-8\">\\n<style>\\n{stylesheet}\\nbody {{\\n    color: {foreground};\\n    background-color: {background};\\n}}\\n</style>\\n</head>\\n<body>\\n    <pre style=\"font-family:Menlo,\\'DejaVu Sans Mono\\',consolas,\\'Courier New\\',monospace\"><code>{code}</code></pre>\\n</body>\\n</html>\\n'*, *inline\\_styles=False*)[source]¶\nGenerate HTML from console contents and write to a file (requires record=True argument in constructor).\n\nParameters\n* **path** (*str*) – Path to write html file.\n* **theme** (*TerminalTheme**,* *optional*) – TerminalTheme object containing console colors.\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`.\n* **code\\_format** (*str**,* *optional*) – Format string to render HTML. In addition to ‘{foreground}’,\n‘{background}’, and ‘{code}’, should contain ‘{stylesheet}’ if inline\\_styles is `False`.\n* **inline\\_styles** (*bool**,* *optional*) – If `True` styles will be inlined in to spans, which makes files\nlarger but easier to cut and paste markup. If `False`, styles will be embedded in a style tag.\nDefaults to False.\n\nsave\\_svg(*path*, *\\**, *title='Rich'*, *theme=None*, *clear=True*, *code\\_format='<svg class=\"rich-terminal\" viewBox=\"0 0 {width} {height}\" xmlns=\"http://www.w3.org/2000/svg\">\\n    <!-- Generated with Rich https://www.textualize.io -->\\n    <style>\\n\\n    @font-face {{\\n        font-family: \"Fira Code\";\\n        src: local(\"FiraCode-Regular\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Regular.woff2\") format(\"woff2\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Regular.woff\") format(\"woff\");\\n        font-style: normal;\\n        font-weight: 400;\\n    }}\\n    @font-face {{\\n        font-family: \"Fira Code\";\\n        src: local(\"FiraCode-Bold\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Bold.woff2\") format(\"woff2\"),\\n                url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Bold.woff\") format(\"woff\");\\n        font-style: bold;\\n        font-weight: 700;\\n    }}\\n\\n    .{unique\\_id}-matrix {{\\n        font-family: Fira Code, monospace;\\n        font-size: {char\\_height}px;\\n        line-height: {line\\_height}px;\\n        font-variant-east-asian: full-width;\\n    }}\\n\\n    .{unique\\_id}-title {{\\n        font-size: 18px;\\n        font-weight: bold;\\n        font-family: arial;\\n    }}\\n\\n    {styles}\\n    </style>\\n\\n    <defs>\\n    <clipPath id=\"{unique\\_id}-clip-terminal\">\\n      <rect x=\"0\" y=\"0\" width=\"{terminal\\_width}\" height=\"{terminal\\_height}\" />\\n    </clipPath>\\n    {lines}\\n    </defs>\\n\\n    {chrome}\\n    <g transform=\"translate({terminal\\_x}, {terminal\\_y})\" clip-path=\"url(#{unique\\_id}-clip-terminal)\">\\n    {backgrounds}\\n    <g class=\"{unique\\_id}-matrix\">\\n    {matrix}\\n    </g>\\n    </g>\\n</svg>\\n'*, *font\\_aspect\\_ratio=0.61*, *unique\\_id=None*)[source]¶\nGenerate an SVG file from the console contents (requires record=True in Console constructor).\n\nParameters\n* **path** (*str*) – The path to write the SVG to.\n* **title** (*str**,* *optional*) – The title of the tab in the output image\n* **theme** (*TerminalTheme**,* *optional*) – The `TerminalTheme` object to use to style the terminal\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`\n* **code\\_format** (*str**,* *optional*) – Format string used to generate the SVG. Rich will inject a number of variables\ninto the string in order to form the final SVG output. The default template used and the variables\ninjected by Rich can be found by inspecting the `console.CONSOLE\\_SVG\\_FORMAT` variable.\n* **font\\_aspect\\_ratio** (*float**,* *optional*) – The width to height ratio of the font used in the `code\\_format`\nstring. Defaults to 0.61, which is the width to height ratio of Fira Code (the default font).\nIf you aren’t specifying a different font inside `code\\_format`, you probably don’t need this.\n* **unique\\_id** (*str**,* *optional*) – unique id that is used as the prefix for various elements (CSS styles, node\nids). If not set, this defaults to a computed value based on the recorded content.\n\nsave\\_text(*path*, *\\**, *clear=True*, *styles=False*)[source]¶\nGenerate text from console and save to a given location (requires record=True argument in constructor).\n\nParameters\n* **path** (*str*) – Path to write text files.\n* **clear** (*bool**,* *optional*) – Clear record buffer after exporting. Defaults to `True`.\n* **styles** (*bool**,* *optional*) – If `True`, ansi style codes will be included. `False` for plain text.\nDefaults to `False`.\n\nscreen(*hide\\_cursor=True*, *style=None*)[source]¶\nContext manager to enable and disable ‘alternative screen’ mode.\n\nParameters\n* **hide\\_cursor** (*bool**,* *optional*) – Also hide the cursor. Defaults to False.\n* **style** (*Style**,* *optional*) – Optional style for screen. Defaults to None.\n\nReturns\nContext which enables alternate screen on enter, and disables it on exit.\n\nReturn type\n~ScreenContext\n\nset\\_alt\\_screen(*enable=True*)[source]¶\nEnables alternative screen mode.\n\n\nNote, if you enable this mode, you should ensure that is disabled before\nthe application exits. See `screen()` for a context manager\nthat handles this for you.\n\nParameters\n**enable** (*bool**,* *optional*) – Enable (True) or disable (False) alternate screen. Defaults to True.\n\nReturns\nTrue if the control codes were written.\n\nset\\_live(*live*)[source]¶\nSet Live instance. Used by Live context manager.\n\nParameters\n**live** (*Live*) – Live instance using this Console.\n\nRaises\n**errors.LiveError** – If this Console has a Live context currently active.\n\nset\\_window\\_title(*title*)[source]¶\nSet the title of the console terminal window.\n\n\nWarning: There is no means within Rich of “resetting” the window title to its\nprevious value, meaning the title you set will persist even after your application\nexits.\n\n\n`fish` shell resets the window title before and after each command by default,\nnegating this issue. Windows Terminal and command prompt will also reset the title for you.\nMost other shells and terminals, however, do not do this.\n\n\nSome terminals may require configuration changes before you can set the title.\nSome terminals may not support setting the title at all.\n\n\nOther software (including the terminal itself, the shell, custom prompts, plugins, etc.)\nmay also set the terminal window title. This could result in whatever value you write\nusing this method being overwritten.\n\nParameters\n**title** (*str*) – The new title of the terminal window.\n\nTrue if the control code to change the terminal title waswritten, otherwise False. Note that a return value of True\ndoes not guarantee that the window title has actually changed,\nsince the feature may be unsupported/disabled in some terminals.\n\n\nReturn type\nbool\n\nshow\\_cursor(*show=True*)[source]¶\nShow or hide the cursor.\n\nParameters\n**show** (*bool**,* *optional*) – Set visibility of the cursor.\n\n*property* size*: ConsoleDimensions*¶\nGet the size of the console.\n\nReturns\nA named tuple containing the dimensions.\n\nReturn type\nConsoleDimensions\n\nstatus(*status*, *\\**, *spinner='dots'*, *spinner\\_style='status.spinner'*, *speed=1.0*, *refresh\\_per\\_second=12.5*)[source]¶\nDisplay a status and spinner.\n\nParameters\n* **status** (*RenderableType*) – A status renderable (str or Text typically).\n* **spinner** (*str**,* *optional*) – Name of spinner animation (see python -m rich.spinner). Defaults to “dots”.\n* **spinner\\_style** (*StyleType**,* *optional*) – Style of spinner. Defaults to “status.spinner”.\n* **speed** (*float**,* *optional*) – Speed factor for spinner animation. Defaults to 1.0.\n* **refresh\\_per\\_second** (*float**,* *optional*) – Number of refreshes per second. Defaults to 12.5.\n\nReturns\nA Status object that may be used as a context manager.\n\nReturn type\nStatus\n\nupdate\\_screen(*renderable*, *\\**, *region=None*, *options=None*)[source]¶\nUpdate the screen at a given offset.\n\nParameters\n* **renderable** (*RenderableType*) – A Rich renderable.\n* **region** (*Region**,* *optional*) – Region of screen to update, or None for entire screen. Defaults to None.\n* **x** (*int**,* *optional*) – x offset. Defaults to 0.\n* **y** (*int**,* *optional*) – y offset. Defaults to 0.\n* **options** (*Optional**[**ConsoleOptions**]*) –\n\nRaises\n**errors.NoAltScreen** – If the Console isn’t in alt screen mode.\n\nupdate\\_screen\\_lines(*lines*, *x=0*, *y=0*)[source]¶\nUpdate lines of the screen at a given offset.\n\nParameters\n* **lines** (*List**[**List**[**Segment**]**]*) – Rendered lines (as produced by `render\\_lines()`).\n* **x** (*int**,* *optional*) – x offset (column no). Defaults to 0.\n* **y** (*int**,* *optional*) – y offset (column no). Defaults to 0.\n\nuse\\_theme(*theme*, *\\**, *inherit=True*)[source]¶\nUse a different theme for the duration of the context manager.\n\nParameters\n* **theme** (*Theme*) – Theme instance to user.\n* **inherit** (*bool**,* *optional*) – Inherit existing console styles. Defaults to True.\n\nReturns\n[description]\n\nReturn type\nThemeContext\n\n*property* width*: int*¶\nGet the width of the console.\n\nReturns\nThe width (in characters) of the console.\n\n\n*class* rich.console.ConsoleDimensions(*width*, *height*)[source]¶\nSize of the terminal.\n\nParameters\n* **width** (*int*) –\n* **height** (*int*) –\n\n\n*property* height¶\nThe height of the console in lines.\n\n*property* width¶\nThe width of the console in ‘cells’.\n\n\n*class* rich.console.ConsoleOptions(*size*, *legacy\\_windows*, *min\\_width*, *max\\_width*, *is\\_terminal*, *encoding*, *max\\_height*, *justify=None*, *overflow=None*, *no\\_wrap=False*, *highlight=None*, *markup=None*, *height=None*)[source]¶\nOptions for \\_\\_rich\\_console\\_\\_ method.\n\nParameters\n* **size** (*ConsoleDimensions*) –\n* **legacy\\_windows** (*bool*) –\n* **min\\_width** (*int*) –\n* **max\\_width** (*int*) –\n* **is\\_terminal** (*bool*) –\n* **encoding** (*str*) –\n* **max\\_height** (*int*) –\n* **justify** (*Optional**[**typing\\_extensions.Literal**[**default**,* *left**,* *center**,* *right**,* *full**]**]*) –\n* **overflow** (*Optional**[**typing\\_extensions.Literal**[**fold**,* *crop**,* *ellipsis**,* *ignore**]**]*) –\n* **no\\_wrap** (*Optional**[**bool**]*) –\n* **highlight** (*Optional**[**bool**]*) –\n* **markup** (*Optional**[**bool**]*) –\n* **height** (*Optional**[**int**]*) –\n\n\n*property* ascii\\_only*: bool*¶\nCheck if renderables should use ascii only.\n\ncopy()[source]¶\nReturn a copy of the options.\n\nReturns\na copy of self.\n\nReturn type\nConsoleOptions\n\nencoding*: str*¶\nEncoding of terminal.\n\nhighlight*: Optional[bool]* *= None*¶\nHighlight override for render\\_str.\n\nis\\_terminal*: bool*¶\nTrue if the target is a terminal, otherwise False.\n\njustify*: Optional[typing\\_extensions.Literal[default, left, center, right, full]]* *= None*¶\nJustify value override for renderable.\n\nlegacy\\_windows*: bool*¶\nflag for legacy windows.\n\nType\nlegacy\\_windows\n\nmarkup*: Optional[bool]* *= None*¶\nEnable markup when rendering strings.\n\nmax\\_height*: int*¶\nHeight of container (starts as terminal)\n\nmax\\_width*: int*¶\nMaximum width of renderable.\n\nmin\\_width*: int*¶\nMinimum width of renderable.\n\nno\\_wrap*: Optional[bool]* *= False*¶\nDisable wrapping for text.\n\noverflow*: Optional[typing\\_extensions.Literal[fold, crop, ellipsis, ignore]]* *= None*¶\nOverflow value override for renderable.\n\nreset\\_height()[source]¶\nReturn a copy of the options with height set to `None`.\n\nReturns\nNew console options instance.\n\nReturn type\n~ConsoleOptions\n\nsize*: ConsoleDimensions*¶\nSize of console.\n\nupdate(*\\**, *width=<rich.console.NoChange object>*, *min\\_width=<rich.console.NoChange object>*, *max\\_width=<rich.console.NoChange object>*, *justify=<rich.console.NoChange object>*, *overflow=<rich.console.NoChange object>*, *no\\_wrap=<rich.console.NoChange object>*, *highlight=<rich.console.NoChange object>*, *markup=<rich.console.NoChange object>*, *height=<rich.console.NoChange object>*)[source]¶\nUpdate values, return a copy.\n\nParameters\n* **width** (*Union**[**int**,* *NoChange**]*) –\n* **min\\_width** (*Union**[**int**,* *NoChange**]*) –\n* **max\\_width** (*Union**[**int**,* *NoChange**]*) –\n* **justify** (*Union**[**typing\\_extensions.Literal**[**default**,* *left**,* *center**,* *right**,* *full**]**,* *None**,* *NoChange**]*) –\n* **overflow** (*Union**[**typing\\_extensions.Literal**[**fold**,* *crop**,* *ellipsis**,* *ignore**]**,* *None**,* *NoChange**]*) –\n* **no\\_wrap** (*Union**[**bool**,* *None**,* *NoChange**]*) –\n* **highlight** (*Union**[**bool**,* *None**,* *NoChange**]*) –\n* **markup** (*Union**[**bool**,* *None**,* *NoChange**]*) –\n* **height** (*Union**[**int**,* *None**,* *NoChange**]*) –\n\nReturn type\n*ConsoleOptions*\n\nupdate\\_dimensions(*width*, *height*)[source]¶\nUpdate the width and height, and return a copy.\n\nParameters\n* **width** (*int*) – New width (sets both min\\_width and max\\_width).\n* **height** (*int*) – New height.\n\nupdate\\_height(*height*)[source]¶\nUpdate the height, and return a copy.\n\nParameters\n**height** (*int*) – New height\n\nReturns\nNew Console options instance.\n\nupdate\\_width(*width*)[source]¶\nUpdate just the width, return a copy.\n\nParameters\n**width** (*int*) – New width (sets both min\\_width and max\\_width)\n\n\n*class* rich.console.ConsoleRenderable(*\\*args*, *\\*\\*kwds*)[source]¶\nAn object that supports the console protocol.\n\n*class* rich.console.ConsoleThreadLocals(*theme\\_stack*, *buffer=<factory>*, *buffer\\_index=0*)[source]¶\nThread local values for Console context.\n\nParameters\n* **theme\\_stack** (*ThemeStack*) –\n* **buffer** (*List**[**Segment**]*) –\n* **buffer\\_index** (*int*) –\n\n*class* rich.console.Group(*\\*renderables*, *fit=True*)[source]¶\nTakes a group of renderables and returns a renderable object that renders the group.\n\nParameters\n* **renderables** (*Iterable**[**RenderableType**]*) – An iterable of renderable objects.\n* **fit** (*bool**,* *optional*) – Fit dimension of group to contents, or fill available space. Defaults to True.\n\n*class* rich.console.NewLine(*count=1*)[source]¶\nA renderable to generate new line(s)\n\nParameters\n**count** (*int*) – \n\n*class* rich.console.PagerContext(*console*, *pager=None*, *styles=False*, *links=False*)[source]¶\nA context manager that ‘pages’ content. See `pager()` for usage.\n\nParameters\n* **console** (*Console*) –\n* **pager** (*Optional**[**Pager**]*) –\n* **styles** (*bool*) –\n* **links** (*bool*) –\n\n*class* rich.console.RenderHook[source]¶\nProvides hooks in to the render process.\n\n\n*abstract* process\\_renderables(*renderables*)[source]¶\nCalled with a list of objects to render.\n\n\nThis method can return a new list of renderables, or modify and return the same list.\n\nParameters\n**renderables** (*List**[**ConsoleRenderable**]*) – A number of renderable objects.\n\nReturns\nA replacement list of renderables.\n\nReturn type\nList[ConsoleRenderable]\n\n\nrich.console.RenderableType¶\nA string or any object that may be rendered by Rich.\n\n\nalias of `Union`[`ConsoleRenderable`, `RichCast`, `str`]\n\n*class* rich.console.RichCast(*\\*args*, *\\*\\*kwds*)[source]¶\nAn object that may be ‘cast’ to a console renderable.\n\n*class* rich.console.ScreenContext(*console*, *hide\\_cursor*, *style=''*)[source]¶\nA context manager that enables an alternative screen. See `screen()` for usage.\n\nParameters\n* **console** (*Console*) –\n* **hide\\_cursor** (*bool*) –\n* **style** (*Union**[**str**,* *Style**]*) –\n\n\nupdate(*\\*renderables*, *style=None*)[source]¶\nUpdate the screen.\n\nParameters\n* **renderable** (*RenderableType**,* *optional*) – Optional renderable to replace current renderable,\nor None for no change. Defaults to None.\n* **style** (*Optional**[**Union**[**str**,* *Style**]**]*) – (Style, optional): Replacement style, or None for no change. Defaults to None.\n* **renderables** (*Union**[**ConsoleRenderable**,* *RichCast**,* *str**]*) –\n\n\n*class* rich.console.ScreenUpdate(*lines*, *x*, *y*)[source]¶\nRender a list of lines at a given offset.\n\nParameters\n* **lines** (*List**[**List**[**Segment**]**]*) –\n* **x** (*int*) –\n* **y** (*int*) –\n\n*class* rich.console.ThemeContext(*console*, *theme*, *inherit=True*)[source]¶\nA context manager to use a temporary theme. See `use\\_theme()` for usage.\n\nParameters\n* **console** (*Console*) –\n* **theme** (*Theme*) –\n* **inherit** (*bool*) –\n\nrich.console.detect\\_legacy\\_windows()[source]¶\nDetect legacy Windows.\n\nrich.console.group(*fit=True*)[source]¶\nA decorator that turns an iterable of renderables in to a group.\n\nParameters\n**fit** (*bool**,* *optional*) – Fit dimension of group to contents, or fill available space. Defaults to True.\n\nReturn type\n*Callable*[[…], *Callable*[[…], *Group*]]\n\n\n# rich.highlighter¶\n\n\n# rich¶\n\n\n\n# rich.measure¶\n\n\n# rich.layout¶\n\n\n*class* rich.layout.ColumnSplitter[source]¶\nSplit a layout region in to columns.\n\n\ndivide(*children*, *region*)[source]¶\nDivide a region amongst several child layouts.\n\nParameters\n* **children** (*Sequence**(**Layout**)*) – A number of child layouts.\n* **region** (*Region*) – A rectangular region to divide.\n\nReturn type\n*Iterable*[*Tuple*[*Layout*, *Region*]]\n\nget\\_tree\\_icon()[source]¶\nGet the icon (emoji) used in layout.tree\n\n\n*class*\n\n==================\n Document 3 \n----------------\n rich.status¶\n\n\n*class* rich.status.Status(*status*, *\\**, *console=None*, *spinner='dots'*, *spinner\\_style='status.spinner'*, *speed=1.0*, *refresh\\_per\\_second=12.5*)[source]¶\nDisplays a status indicator with a ‘spinner’ animation.\n\nParameters\n* **status** (*RenderableType*) – A status renderable (str or Text typically).\n* **console** (*Console**,* *optional*) – Console instance to use, or None for global console. Defaults to None.\n* **spinner** (*str**,* *optional*) – Name of spinner animation (see python -m rich.spinner). Defaults to “dots”.\n* **spinner\\_style** (*StyleType**,* *optional*) – Style of spinner. Defaults to “status.spinner”.\n* **speed** (*float**,* *optional*) – Speed factor for spinner animation. Defaults to 1.0.\n* **refresh\\_per\\_second** (*float**,* *optional*) – Number of refreshes per second. Defaults to 12.5.\n\n\n*property* console*: Console*¶\nGet the Console used by the Status objects.\n\nstart()[source]¶\nStart the status animation.\n\nstop()[source]¶\nStop the spinner animation.\n\nupdate(*status=None*, *\\**, *spinner=None*, *spinner\\_style=None*, *speed=None*)[source]¶\nUpdate status.\n\nParameters\n* **status** (*Optional**[**RenderableType**]**,* *optional*) – New status renderable or None for no change. Defaults to None.\n* **spinner** (*Optional**[**str**]**,* *optional*) – New spinner or None for no change. Defaults to None.\n* **spinner\\_style** (*Optional**[**StyleType**]**,* *optional*) – New spinner style or None for no change. Defaults to None.\n* **speed** (*Optional**[**float**]**,* *optional*) – Speed factor for spinner animation or None for no change. Defaults to None.\n\n# rich.styled¶\n\n\n*class* rich.styled.Styled(*renderable*, *style*)[source]¶\nApply a style to a renderable.\n\nParameters\n* **renderable** (*RenderableType*) – Any renderable.\n* **style** (*StyleType*) – A style to apply across the entire renderable.\n\n\n# rich.table¶\n\n\n*class* rich.table.Column(*header=''*, *footer=''*, *header\\_style=''*, *footer\\_style=''*, *style=''*, *justify='left'*, *vertical='top'*, *overflow='ellipsis'*, *width=None*, *min\\_width=None*, *max\\_width=None*, *ratio=None*, *no\\_wrap=False*, *\\_index=0*, *\\_cells=<factory>*)[source]¶\nDefines a column within a ~Table.\n\nParameters\n* **title** (*Union**[**str**,* *Text**]**,* *optional*) – The title of the table rendered at the top. Defaults to None.\n* **caption**"
    },
    {
        "instruction": "Create a Python program using the 'rustworkx' API to define a directed graph and find the shortest path between two nodes. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish weighted edges between these nodes and print the indices of the added edges. Then, use Dijkstra's shortest path algorithm to find and print the shortest path from node 'A' to node 'C'",
        "api": "rustworkx",
        "output": "#!pip install rustworkx\nimport rustworkx as rx\n\ngraph = rx.PyGraph()\n\n# Each time add node is called, it returns a new node index\na = graph.add_node(\"A\")\nb = graph.add_node(\"B\")\nc = graph.add_node(\"C\")\nd = graph.add_node(\"D\")\n\n# add_edges_from takes tuples of node indices and weights,\n# and returns edge indices\nedge_indices = graph.add_edges_from([(a, b, 1.5), (a, d, 2.0), (b, c, 2.5),(d, c, 1.5)])\nprint(edge_indices)\n\n# Returns the path \npath_mapping = rx.dijkstra_shortest_paths(graph, a, c, weight_fn=float)\nprint(path_mapping)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n\n  \n  \n  \n\n# Rustworkx API Reference#\n\n* Graph Classes\n\t+ PyGraph\n\t+ PyDiGraph\n\t+ PyDAG\n* Algorithm Functions\n\t+ Centrality\n\t+ Connectivity and Cycles\n\t+ DAG Algorithms\n\t+ Graph Operations\n\t+ Isomorphism\n\t+ Link Analysis\n\t+ Matching\n\t+ Other Algorithm Functions\n\t+ Shortest Paths\n\t+ Traversal\n\t+ Tree\n\n\n* Generators\n* Random Graph Generator Functions\n* Layout Functions\n* Serialization\n* Converters\n* API functions for PyDigraph\n* API functions for PyGraph\n* Exceptions\n* Custom Return Types\n\n  \n  \n  \n\n\n# Graph Classes#\n\n|  |  |\n| --- | --- |\n| `rustworkx.PyGraph`([multigraph, attrs]) | A class for creating undirected graphs |\n| `rustworkx.PyDiGraph`([check\\_cycle, ...]) | A class for creating directed graphs |\n| `rustworkx.PyDAG`([check\\_cycle, multigraph, attrs]) | A class for creating direct acyclic graphs. |\n\n# Algorithm Functions#\n\n* Centrality\n\t+ rustworkx.betweenness\\_centrality\n\t+ rustworkx.edge\\_betweenness\\_centrality\n\t+ rustworkx.eigenvector\\_centrality\n\t+ rustworkx.katz\\_centrality\n\t+ rustworkx.closeness\\_centrality\n* Connectivity and Cycles\n\t+ rustworkx.number\\_connected\\_components\n\t+ rustworkx.connected\\_components\n\t+ rustworkx.node\\_connected\\_component\n\t+ rustworkx.is\\_connected\n\t+ rustworkx.strongly\\_connected\\_components\n\t+ rustworkx.number\\_weakly\\_connected\\_components\n\t+ rustworkx.weakly\\_connected\\_components\n\t+ rustworkx.is\\_weakly\\_connected\n\t+ rustworkx.cycle\\_basis\n\t+ rustworkx.simple\\_cycles\n\t+ rustworkx.digraph\\_find\\_cycle\n\t+ rustworkx.articulation\\_points\n\t+ rustworkx.biconnected\\_components\n\t+ rustworkx.chain\\_decomposition\n\t+ rustworkx.all\\_simple\\_paths\n\t+ rustworkx.all\\_pairs\\_all\\_simple\\_paths\n\t+ rustworkx.stoer\\_wagner\\_min\\_cut\n\t+ rustworkx.longest\\_simple\\_path\n* DAG Algorithms\n\t+ rustworkx.dag\\_longest\\_path\n\t+ rustworkx.dag\\_longest\\_path\\_length\n\t+ rustworkx.dag\\_weighted\\_longest\\_path\n\t+ rustworkx.dag\\_weighted\\_longest\\_path\\_length\n\t+ rustworkx.is\\_directed\\_acyclic\\_graph\n\t+ rustworkx.layers\n* Graph Operations\n\t+\n\n==================\n Document 1 \n----------------\n API functions for PyGraph#\n\n\nThese functions are algorithm functions that are type specific for\n`PyGraph` objects. Universal functions from Rustworkx API that\nwork for both graph types internally call the functions from the explicitly\ntyped API based on the data type.\n\n|  |  |\n| --- | --- |\n| `rustworkx.graph\\_is\\_isomorphic`(first, second, /) | Determine if 2 undirected graphs are isomorphic |\n| `rustworkx.graph\\_is\\_subgraph\\_isomorphic`(...) | Determine if 2 undirected graphs are subgraph - isomorphic |\n| `rustworkx.graph\\_vf2\\_mapping`(first, second, /) | Return an iterator over all vf2 mappings between two `PyGraph` objects |\n| `rustworkx.graph\\_distance\\_matrix`(graph, /[, ...]) | Get the distance matrix for an undirected graph |\n| `rustworkx.graph\\_floyd\\_warshall`(graph, /[, ...]) | Find all-pairs shortest path lengths using Floyd's algorithm |\n| `rustworkx.graph\\_floyd\\_warshall\\_numpy`(graph, /) | Find all-pairs shortest path lengths using Floyd's algorithm |\n| `rustworkx.graph\\_adjacency\\_matrix`(graph, /[, ...]) | Return the adjacency matrix for a PyGraph class |\n| `rustworkx.graph\\_all\\_simple\\_paths` | Return all simple paths between 2 nodes in a PyGraph object |\n| `rustworkx.graph\\_all\\_pairs\\_all\\_simple\\_paths`(...) | Return all the simple paths between all pairs of nodes in the graph |\n| `rustworkx.graph\\_astar\\_shortest\\_path`(graph, ...) | Compute the A\\* shortest path for a PyGraph |\n| `rustworkx.graph\\_dijkstra\\_shortest\\_paths` | Find the shortest path from a node |\n| `rustworkx.graph\\_dijkstra\\_shortest\\_path\\_lengths`(...) | Compute the lengths of the shortest paths for a PyGraph object using Dijkstra's algorithm |\n| `rustworkx.graph\\_all\\_pairs\\_dijkstra\\_shortest\\_paths`(...) | For each node in the graph, finds the shortest paths to all others in a `PyGraph` object |\n| `rustworkx.graph\\_k\\_shortest\\_path\\_lengths`(...) | Compute the length of the kth shortest path |\n| `rustworkx.graph\\_all\\_pairs\\_dijkstra\\_path\\_lengths`(...) | For each node in the graph, calculates the lengths of the shortest paths to all others in a `PyGraph` object |\n| `rustworkx.graph\\_bellman\\_ford\\_shortest\\_path\\_lengths`(...) | Compute the lengths of the shortest paths for a PyGraph object using the Bellman-Ford algorithm with the SPFA heuristic. |\n| `rustworkx.graph\\_bellman\\_ford\\_shortest\\_path\\_lengths`(...) | Compute the lengths of the shortest paths for a PyGraph object using the Bellman-Ford algorithm with the SPFA heuristic. |\n| `rustworkx.graph\\_all\\_pairs\\_bellman\\_ford\\_shortest\\_paths`(...) | For each node in the graph, finds the shortest paths to all others in a `PyGraph` object |\n| `rustworkx.graph\\_all\\_pairs\\_bellman\\_ford\\_path\\_lengths`(...) | For each node in the graph, calculates the lengths of the shortest paths to all others in a `PyGraph` object |\n| `rustworkx.graph\\_dfs\\_edges`(graph, /[, source]) | Get an edge list of the tree edges from a depth-first traversal |\n| `rustworkx.graph\\_dfs\\_search`(graph[, source, ...]) | Depth-first traversal of an undirected graph. |\n| `rustworkx.graph\\_transitivity`(graph, /) | Compute the transitivity of an undirected graph. |\n| `rustworkx.graph\\_core\\_number`(graph, /) | Return the core number for each node in the graph. |\n| `rustworkx.graph\\_complement`(graph, /) | Compute the complement of an undirected graph. |\n| `rustworkx.graph\\_union`(first, second, /[, ...]) | Return a new PyGraph by forming a union from two input PyGraph objects |\n| `rustworkx.graph\\_tensor\\_product`(first, second, /) | Return a new PyGraph by forming the tensor product from two input PyGraph objects |\n| `rustworkx.graph\\_token\\_swapper`(graph, mapping, /) | This module performs an approximately optimal Token Swapping algorithm Supports partial mappings (i.e. |\n| `rustworkx.graph\\_cartesian\\_product`(first, ...) | Return a new PyGraph by forming the cartesian product from two input PyGraph objects |\n| `rustworkx.graph\\_random\\_layout` | Generate a random layout |\n| `rustworkx.graph\\_bipartite\\_layout`(graph, ...) | Generate a bipartite layout of the graph |\n| `rustworkx.graph\\_circular\\_layout`(graph, /[, ...]) | Generate a circular layout of the graph |\n| `rustworkx.graph\\_shell\\_layout`(graph, /[, ...]) | Generate a shell layout of the graph |\n| `rustworkx.graph\\_spiral\\_layout`(graph, /[, ...]) | Generate a spiral layout of the graph |\n| `rustworkx.graph\\_spring\\_layout`(graph[, pos, ...]) | Position nodes using Fruchterman-Reingold force-directed algorithm. |\n| `rustworkx.graph\\_num\\_shortest\\_paths\\_unweighted`(...) | Get the number of unweighted shortest paths from a source node |\n| `rustworkx.graph\\_betweenness\\_centrality`(graph, /) | Compute the betweenness centrality of all nodes in a PyGraph. |\n| `rustworkx.graph\\_edge\\_betweenness\\_centrality`(...) | Compute the edge betweenness centrality of all edges in a `PyGraph`. |\n| `rustworkx.graph\\_closeness\\_centrality`(graph) | Compute the closeness centrality of each node in a `PyGraph` object. |\n| `rustworkx.graph\\_eigenvector\\_centrality`(graph, /) | Compute the eigenvector centrality of a `PyGraph`. |\n| `rustworkx.graph\\_katz\\_centrality`(graph, /[, ...]) | Compute the Katz centrality of a `PyGraph`. |\n| `rustworkx.graph\\_unweighted\\_average\\_shortest\\_path\\_length`(...) | Return the average shortest path length for a `PyGraph` with unweighted edges. |\n| `rustworkx.graph\\_bfs\\_search`(graph[, source, ...]) | Breadth-first traversal of an undirected graph. |\n| `rustworkx.graph\\_dijkstra\\_search`(graph[, ...]) | Dijkstra traversal of an undirected graph. |\n| `rustworkx.graph\\_node\\_link\\_json`(graph, /[, ...]) | Generate a JSON object representing a `PyGraph` in a node-link format |\n| `rustworkx.graph\\_longest\\_simple\\_path`(graph, /) | Return a longest simple path in the graph |\n\n# Exceptions#\n\n|  |  |\n| --- | --- |\n| `rustworkx.InvalidNode` |  |\n| `rustworkx.DAGWouldCycle` |  |\n| `rustworkx.NoEdgeBetweenNodes` |  |\n| `rustworkx.DAGHasCycle` |  |\n| `rustworkx.NegativeCycle` |  |\n| `rustworkx.NoSuitableNeighbors` |  |\n| `rustworkx.NoPathFound` |  |\n| `rustworkx.NullGraph` |  |\n| `rustworkx.visit.StopSearch` | Stop graph traversal |\n| `rustworkx.visit.PruneSearch` | Prune part of the search tree while traversing a graph. |\n| `rustworkx.JSONSerializationError` |  |\n\n# Custom Return Types#\n\n|  |  |\n| --- | --- |\n| `rustworkx.BFSSuccessors`() | A custom class for the return from `rustworkx.bfs\\_successors()` |\n| `rustworkx.BFSPredecessors`() | A custom class for the return from `rustworkx.bfs\\_predecessors()` |\n| `rustworkx.NodeIndices`() | A custom class for\n\n==================\n Document 2 \n----------------\n Connectivity and Cycles#\n\n|  |  |\n| --- | --- |\n| `rustworkx.number\\_connected\\_components`(graph, /) | Find the number of connected components in an undirected graph. |\n| `rustworkx.connected\\_components`(graph, /) | Find the connected components in an undirected graph |\n| `rustworkx.node\\_connected\\_component`(graph, ...) | Returns the set of nodes in the component of graph containing node. |\n| `rustworkx.is\\_connected`(graph, /) | Check if the graph is connected. |\n| `rustworkx.strongly\\_connected\\_components`(graph, /) | Compute the strongly connected components for a directed graph |\n| `rustworkx.number\\_weakly\\_connected\\_components`(...) | Find the number of weakly connected components in a directed graph |\n| `rustworkx.weakly\\_connected\\_components`(graph, /) | Find the weakly connected components in a directed graph |\n| `rustworkx.is\\_weakly\\_connected`(graph, /) | Check if the graph is weakly connected |\n| `rustworkx.cycle\\_basis`(graph, /[, root]) | Return a list of cycles which form a basis for cycles of a given PyGraph |\n| `rustworkx.simple\\_cycles`(graph, /) | Find all simple cycles of a `PyDiGraph` |\n| `rustworkx.digraph\\_find\\_cycle`(graph, /[, source]) | Return the first cycle encountered during DFS of a given PyDiGraph, empty list is returned if no cycle is found |\n| `rustworkx.articulation\\_points`(graph, /) | Return the articulation points of an undirected graph. |\n| `rustworkx.biconnected\\_components`(graph, /) | Return the biconnected components of an undirected graph. |\n| `rustworkx.chain\\_decomposition`(graph, /[, source]) | Returns the chain decomposition of a graph. |\n| `rustworkx.all\\_simple\\_paths`(graph, from\\_, to) | Return all simple paths between 2 nodes in a PyGraph object |\n| `rustworkx.all\\_pairs\\_all\\_simple\\_paths`(graph) | Return all the simple paths between all pairs of nodes in the graph |\n| `rustworkx.stoer\\_wagner\\_min\\_cut`(graph, /[, ...]) | Compute a weighted minimum cut using the Stoer-Wagner algorithm. |\n| `rustworkx.longest\\_simple\\_path`(graph) | Return a longest simple path in the graph |\n\n# DAG Algorithms#\n\n|  |  |\n| --- | --- |\n| `rustworkx.dag\\_longest\\_path`(graph, /[, weight\\_fn]) | Find the longest path in a DAG |\n| `rustworkx.dag\\_longest\\_path\\_length`(graph, /) | Find the length of the longest path in a DAG |\n| `rustworkx.dag\\_weighted\\_longest\\_path`(graph, ...) | Find the weighted longest path in a DAG |\n| `rustworkx.dag\\_weighted\\_longest\\_path\\_length`(...) | Find the length of the weighted longest path in a DAG |\n| `rustworkx.is\\_directed\\_acyclic\\_graph`(graph, /) | Check that the PyDiGraph or PyDAG doesn't have a cycle |\n| `rustworkx.layers`(dag, first\\_layer, /[, ...]) | Return a list of layers |\n\n\n# Graph Operations#\n\n|  |  |\n| --- | --- |\n| `rustworkx.complement`(graph) | Compute the complement of a graph. |\n| `rustworkx.union`(first, second[, ...]) | Return a new graph by forming a union from two input graph objects |\n| `rustworkx.cartesian\\_product`(first, second) | Return a new graph by forming the cartesian product from two input graph objects |\n\n\n# Isomorphism#\n\n|  |  |\n| --- | --- |\n| `rustworkx.is\\_isomorphic`(first, second[, ...]) | Determine if 2 graphs are isomorphic |\n| `rustworkx.is\\_subgraph\\_isomorphic`(first, second) | Determine if 2 graphs are subgraph isomorphic |\n| `rustworkx.is\\_isomorphic\\_node\\_match`(first, ...) | Determine if 2 graphs are isomorphic |\n| `rustworkx.vf2\\_mapping`(first, second[, ...]) | Return an iterator over all vf2 mappings between two graphs. |\n\n\n# Link Analysis#\n\n|  |  |\n| --- | --- |\n| `rustworkx.pagerank`(graph, /[, alpha, ...]) | Computes the PageRank of the nodes in a `PyDiGraph`. |\n| `rustworkx.hits`(graph, /[, weight\\_fn, ...]) | Computes the hubs and authorities in a `PyDiGraph`. |\n\n\n# Matching#\n\n|  |  |\n| --- | --- |\n| `rustworkx.max\\_weight\\_matching`(graph, /[, ...]) | Compute a maximum-weighted matching for a `PyGraph` |\n| `rustworkx.is\\_matching`(graph, matching, /) | Check if matching is valid for graph |\n| `rustworkx.is\\_maximal\\_matching`(graph, matching, /) | Check if a matching is a maximal (**not** maximum) matching for a graph |\n\n\n# Other Algorithm Functions#\n\n|  |  |\n| --- | --- |\n| `rustworkx.adjacency\\_matrix`(graph[, ...]) | Return the adjacency matrix for a graph object |\n| `rustworkx.transitivity`(graph) | Compute the transitivity of a graph. |\n| `rustworkx.core\\_number`(graph) | Return the core number for each node in the graph. |\n| `rustworkx.graph\\_greedy\\_color`(graph, /) | Color a `PyGraph` object using a greedy graph coloring algorithm. |\n| `rustworkx.metric\\_closure`(graph, weight\\_fn, /) | Return the metric closure of a graph |\n| `rustworkx.is\\_planar`(graph, /) | Check if an undirected graph is planar. |\n\n# Shortest Paths#\n\n|  |  |\n| --- | --- |\n| `rustworkx.dijkstra\\_shortest\\_paths`(graph, source) | Find the shortest path from a node |\n| `rustworkx.dijkstra\\_shortest\\_path\\_lengths`(...) | Compute the lengths of the shortest paths for a graph object using Dijkstra's algorithm. |\n| `rustworkx.all\\_pairs\\_dijkstra\\_shortest\\_paths`(...)\n\n==================\n Document 3 \n----------------\n Shortest Paths#\n\n|  |  |\n| --- | --- |\n| `rustworkx.dijkstra\\_shortest\\_paths`(graph, source) | Find the shortest path from a node |\n| `rustworkx.dijkstra\\_shortest\\_path\\_lengths`(...) | Compute the lengths of the shortest paths for a graph object using Dijkstra's algorithm. |\n| `rustworkx.all\\_pairs\\_dijkstra\\_shortest\\_paths`(...) | For each node in the graph, finds the shortest paths to all others. |\n| `rustworkx.all\\_pairs\\_dijkstra\\_path\\_lengths`(...) | For each node in the graph, calculates the lengths of the shortest paths to all others. |\n| `rustworkx.bellman\\_ford\\_shortest\\_paths`(graph, ...) | Find the shortest path from a node |\n| `rustworkx.bellman\\_ford\\_shortest\\_path\\_lengths`(...) | Compute the lengths of the shortest paths for a graph object using the Bellman-Ford algorithm with the SPFA heuristic. |\n| `rustworkx.all\\_pairs\\_bellman\\_ford\\_shortest\\_paths`(...) | For each node in the graph, finds the shortest paths to all others. |\n| `rustworkx.all\\_pairs\\_bellman\\_ford\\_path\\_lengths`(...) | For each node in the graph, calculates the lengths of the shortest paths to all others. |\n| `rustworkx.negative\\_edge\\_cycle`(graph, ...) | Check if a negative cycle exists on a graph |\n| `rustworkx.find\\_negative\\_cycle`(graph, ...) | Find a negative cycle of a graph |\n| `rustworkx.distance\\_matrix`(graph[, ...]) | Get the distance matrix for a graph |\n| `rustworkx.floyd\\_warshall`(graph[, weight\\_fn, ...]) | Find all-pairs shortest path lengths using Floyd's algorithm |\n| `rustworkx.floyd\\_warshall\\_numpy`(graph[, ...]) | Find all-pairs shortest path lengths using Floyd's algorithm |\n| `rustworkx.astar\\_shortest\\_path`(graph, node, ...) | Compute the A\\* shortest path for a graph |\n| `rustworkx.k\\_shortest\\_path\\_lengths`(graph, ...) | Compute the length of the kth shortest path |\n| `rustworkx.num\\_shortest\\_paths\\_unweighted`(...) | Get the number of unweighted shortest paths from a source node |\n| `rustworkx.unweighted\\_average\\_shortest\\_path\\_length`(graph) | Return the average shortest path length with unweighted edges. |\n# Traversal#\n\n|  |  |\n| --- | --- |\n| `rustworkx.dfs\\_edges`(graph[, source]) | Get an edge list of the tree edges from a depth-first traversal |\n| `rustworkx.dfs\\_search`(graph, source, visitor) | Depth-first traversal of a directed/undirected graph. |\n| `rustworkx.bfs\\_successors`(graph, node, /)\n\n==================\n Document 4 \n----------------\n Algorithm Functions#\n\n* Centrality\n\t+ rustworkx.betweenness\\_centrality\n\t+ rustworkx.edge\\_betweenness\\_centrality\n\t+ rustworkx.eigenvector\\_centrality\n\t+ rustworkx.katz\\_centrality\n\t+ rustworkx.closeness\\_centrality\n* Connectivity and Cycles\n\t+ rustworkx.number\\_connected\\_components\n\t+ rustworkx.connected\\_components\n\t+ rustworkx.node\\_connected\\_component\n\t+ rustworkx.is\\_connected\n\t+ rustworkx.strongly\\_connected\\_components\n\t+ rustworkx.number\\_weakly\\_connected\\_components\n\t+ rustworkx.weakly\\_connected\\_components\n\t+ rustworkx.is\\_weakly\\_connected\n\t+ rustworkx.cycle\\_basis\n\t+ rustworkx.simple\\_cycles\n\t+ rustworkx.digraph\\_find\\_cycle\n\t+ rustworkx.articulation\\_points\n\t+ rustworkx.biconnected\\_components\n\t+ rustworkx.chain\\_decomposition\n\t+ rustworkx.all\\_simple\\_paths\n\t+ rustworkx.all\\_pairs\\_all\\_simple\\_paths\n\t+ rustworkx.stoer\\_wagner\\_min\\_cut\n\t+ rustworkx.longest\\_simple\\_path\n* DAG Algorithms\n\t+ rustworkx.dag\\_longest\\_path\n\t+ rustworkx.dag\\_longest\\_path\\_length\n\t+ rustworkx.dag\\_weighted\\_longest\\_path\n\t+ rustworkx.dag\\_weighted\\_longest\\_path\\_length\n\t+ rustworkx.is\\_directed\\_acyclic\\_graph\n\t+ rustworkx.layers\n* Graph Operations\n\t+ rustworkx.complement\n\t+ rustworkx.union\n\t+ rustworkx.cartesian\\_product\n* Isomorphism\n\t+ rustworkx.is\\_isomorphic\n\t+ rustworkx.is\\_subgraph\\_isomorphic\n\t+ rustworkx.is\\_isomorphic\\_node\\_match\n\t+ rustworkx.vf2\\_mapping\n* Link Analysis\n\t+ rustworkx.pagerank\n\t+ rustworkx.hits\n* Matching\n\t+ rustworkx.max\\_weight\\_matching\n\t+ rustworkx.is\\_matching\n\t+ rustworkx.is\\_maximal\\_matching\n* Other Algorithm Functions\n\t+ rustworkx.adjacency\\_matrix\n\t+ rustworkx.transitivity\n\t+ rustworkx.core\\_number\n\t+ rustworkx.graph\\_greedy\\_color\n\t+ rustworkx.metric\\_closure\n\t+ rustworkx.is\\_planar\n* Shortest Paths\n\t+ rustworkx.dijkstra\\_shortest\\_paths\n\t+ rustworkx.dijkstra\\_shortest\\_path\\_lengths\n\t+ rustworkx.all\\_pairs\\_dijkstra\\_shortest\\_paths\n\t+ rustworkx.all\\_pairs\\_dijkstra\\_path\\_lengths\n\t+ rustworkx.bellman\\_ford\\_shortest\\_paths\n\t+ rustworkx.bellman\\_ford\\_shortest\\_path\\_lengths\n\t+ rustworkx.all\\_pairs\\_bellman\\_ford\\_shortest\\_paths\n\t+ rustworkx.all\\_pairs\\_bellman\\_ford\\_path\\_lengths\n\t+ rustworkx.negative\\_edge\\_cycle\n\t+ rustworkx.find\\_negative\\_cycle\n\t+ rustworkx.distance\\_matrix\n\t+ rustworkx.floyd\\_warshall\n\t+ rustworkx.floyd\\_warshall\\_numpy\n\t+ rustworkx.astar\\_shortest\\_path\n\t+ rustworkx.k\\_shortest\\_path\\_lengths\n\t+ rustworkx.num\\_shortest\\_paths\\_unweighted\n\t+ rustworkx.unweighted\\_average\\_shortest\\_path\\_length\n* Traversal\n\t+ rustworkx.dfs\\_edges\n\t+ rustworkx.dfs\\_search\n\t+ rustworkx.bfs\\_successors\n\t+ rustworkx.bfs\\_predecessors\n\t+ rustworkx.bfs\\_search\n\t+ rustworkx.dijkstra\\_search\n\t+ rustworkx.topological\\_sort\n\t+ rustworkx.lexicographical\\_topological\\_sort\n\t+ rustworkx.descendants\n\t+ rustworkx.ancestors\n\t+ rustworkx.collect\\_runs\n\t+ rustworkx.collect\\_bicolor\\_runs\n\t+ DFSVisitor\n\t+ BFSVisitor\n\t+ DijkstraVisitor\n\t+ TopologicalSorter\n* Tree\n\t+ rustworkx.minimum\\_spanning\\_edges\n\t+ rustworkx.minimum\\_spanning\\_tree\n\t+ rustworkx.steiner\\_tree\n\n# Centrality#\n\n|  |  |\n| --- | --- |\n| `rustworkx.betweenness\\_centrality`(graph[, ...]) | Returns the betweenness centrality of each node in the graph. |\n| `rustworkx.edge\\_betweenness\\_centrality`(graph) | Compute the edge betweenness centrality of all edges in a graph. |\n| `rustworkx.eigenvector\\_centrality`(graph[, ...]) | Compute the eigenvector centrality of a graph. |\n| `rustworkx.katz\\_centrality`(graph[, alpha, ...]) | Compute the Katz centrality of a graph. |\n| `rustworkx.closeness\\_centrality`(graph[, ...]) | Compute the closeness centrality of each node in a graph object. |\n\n# Connectivity and Cycles#\n\n|  |  |\n| --- | --- |\n| `rustworkx.number\\_connected\\_components`(graph, /) | Find the number of connected components in an undirected graph. |\n| `rustworkx.connected\\_components`(graph, /) | Find the connected components in an undirected graph |\n| `rustworkx.node\\_connected\\_component`(graph, ...)"
    },
    {
        "instruction": "Create a Python program using the 'rustworkx' API to define an undirected graph and find the minimum spanning tree. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish weighted edges between these nodes and print the indices of the added edges. Then, use Kruskal's algorithm to find and print the minimum spanning tree of the graph.",
        "api": "rustworkx",
        "output": "#!pip install rustworkx\nimport rustworkx as rx\n\ngraph = rx.PyGraph()\n\n# Each time add node is called, it returns a new node index\na = graph.add_node(\"A\")\nb = graph.add_node(\"B\")\nc = graph.add_node(\"C\")\nd = graph.add_node(\"D\")\n\n# add_edges_from takes tuples of node indices and weights,\n# and returns edge indices\nedge_indices = graph.add_edges_from([(a, b, 1.5), (a, d, 2.0), (b, c, 2.5),(d, c, 1.5)])\nprint(edge_indices)\n\n# Returns the minimum spanning tree\nmst = rx.minimum_spanning_tree(graph, weight_fn=float)\nprint(mst)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n\n  \n  \n  \n\n# Rustworkx API Reference#\n\n* Graph Classes\n\t+ PyGraph\n\t+ PyDiGraph\n\t+ PyDAG\n* Algorithm Functions\n\t+ Centrality\n\t+ Connectivity and Cycles\n\t+ DAG Algorithms\n\t+ Graph Operations\n\t+ Isomorphism\n\t+ Link Analysis\n\t+ Matching\n\t+ Other Algorithm Functions\n\t+ Shortest Paths\n\t+ Traversal\n\t+ Tree\n\n\n* Generators\n* Random Graph Generator Functions\n* Layout Functions\n* Serialization\n* Converters\n* API functions for PyDigraph\n* API functions for PyGraph\n* Exceptions\n* Custom Return Types\n\n  \n  \n  \n\n\n# Graph Classes#\n\n|  |  |\n| --- | --- |\n| `rustworkx.PyGraph`([multigraph, attrs]) | A class for creating undirected graphs |\n| `rustworkx.PyDiGraph`([check\\_cycle, ...]) | A class for creating directed graphs |\n| `rustworkx.PyDAG`([check\\_cycle, multigraph, attrs]) | A class for creating direct acyclic graphs. |\n\n# Algorithm Functions#\n\n* Centrality\n\t+ rustworkx.betweenness\\_centrality\n\t+ rustworkx.edge\\_betweenness\\_centrality\n\t+ rustworkx.eigenvector\\_centrality\n\t+ rustworkx.katz\\_centrality\n\t+ rustworkx.closeness\\_centrality\n* Connectivity and Cycles\n\t+ rustworkx.number\\_connected\\_components\n\t+ rustworkx.connected\\_components\n\t+ rustworkx.node\\_connected\\_component\n\t+ rustworkx.is\\_connected\n\t+ rustworkx.strongly\\_connected\\_components\n\t+ rustworkx.number\\_weakly\\_connected\\_components\n\t+ rustworkx.weakly\\_connected\\_components\n\t+ rustworkx.is\\_weakly\\_connected\n\t+ rustworkx.cycle\\_basis\n\t+ rustworkx.simple\\_cycles\n\t+ rustworkx.digraph\\_find\\_cycle\n\t+ rustworkx.articulation\\_points\n\t+ rustworkx.biconnected\\_components\n\t+ rustworkx.chain\\_decomposition\n\t+ rustworkx.all\\_simple\\_paths\n\t+ rustworkx.all\\_pairs\\_all\\_simple\\_paths\n\t+ rustworkx.stoer\\_wagner\\_min\\_cut\n\t+ rustworkx.longest\\_simple\\_path\n* DAG Algorithms\n\t+ rustworkx.dag\\_longest\\_path\n\t+ rustworkx.dag\\_longest\\_path\\_length\n\t+ rustworkx.dag\\_weighted\\_longest\\_path\n\t+ rustworkx.dag\\_weighted\\_longest\\_path\\_length\n\t+ rustworkx.is\\_directed\\_acyclic\\_graph\n\t+ rustworkx.layers\n* Graph Operations\n\t+\n\n==================\n Document 1 \n----------------\n API functions for PyGraph#\n\n\nThese functions are algorithm functions that are type specific for\n`PyGraph` objects. Universal functions from Rustworkx API that\nwork for both graph types internally call the functions from the explicitly\ntyped API based on the data type.\n\n|  |  |\n| --- | --- |\n| `rustworkx.graph\\_is\\_isomorphic`(first, second, /) | Determine if 2 undirected graphs are isomorphic |\n| `rustworkx.graph\\_is\\_subgraph\\_isomorphic`(...) | Determine if 2 undirected graphs are subgraph - isomorphic |\n| `rustworkx.graph\\_vf2\\_mapping`(first, second, /) | Return an iterator over all vf2 mappings between two `PyGraph` objects |\n| `rustworkx.graph\\_distance\\_matrix`(graph, /[, ...]) | Get the distance matrix for an undirected graph |\n| `rustworkx.graph\\_floyd\\_warshall`(graph, /[, ...]) | Find all-pairs shortest path lengths using Floyd's algorithm |\n| `rustworkx.graph\\_floyd\\_warshall\\_numpy`(graph, /) | Find all-pairs shortest path lengths using Floyd's algorithm |\n| `rustworkx.graph\\_adjacency\\_matrix`(graph, /[, ...]) | Return the adjacency matrix for a PyGraph class |\n| `rustworkx.graph\\_all\\_simple\\_paths` | Return all simple paths between 2 nodes in a PyGraph object |\n| `rustworkx.graph\\_all\\_pairs\\_all\\_simple\\_paths`(...) | Return all the simple paths between all pairs of nodes in the graph |\n| `rustworkx.graph\\_astar\\_shortest\\_path`(graph, ...) | Compute the A\\* shortest path for a PyGraph |\n| `rustworkx.graph\\_dijkstra\\_shortest\\_paths` | Find the shortest path from a node |\n| `rustworkx.graph\\_dijkstra\\_shortest\\_path\\_lengths`(...) | Compute the lengths of the shortest paths for a PyGraph object using Dijkstra's algorithm |\n| `rustworkx.graph\\_all\\_pairs\\_dijkstra\\_shortest\\_paths`(...) | For each node in the graph, finds the shortest paths to all others in a `PyGraph` object |\n| `rustworkx.graph\\_k\\_shortest\\_path\\_lengths`(...) | Compute the length of the kth shortest path |\n| `rustworkx.graph\\_all\\_pairs\\_dijkstra\\_path\\_lengths`(...) | For each node in the graph, calculates the lengths of the shortest paths to all others in a `PyGraph` object |\n| `rustworkx.graph\\_bellman\\_ford\\_shortest\\_path\\_lengths`(...) | Compute the lengths of the shortest paths for a PyGraph object using the Bellman-Ford algorithm with the SPFA heuristic. |\n| `rustworkx.graph\\_bellman\\_ford\\_shortest\\_path\\_lengths`(...) | Compute the lengths of the shortest paths for a PyGraph object using the Bellman-Ford algorithm with the SPFA heuristic. |\n| `rustworkx.graph\\_all\\_pairs\\_bellman\\_ford\\_shortest\\_paths`(...) | For each node in the graph, finds the shortest paths to all others in a `PyGraph` object |\n| `rustworkx.graph\\_all\\_pairs\\_bellman\\_ford\\_path\\_lengths`(...) | For each node in the graph, calculates the lengths of the shortest paths to all others in a `PyGraph` object |\n| `rustworkx.graph\\_dfs\\_edges`(graph, /[, source]) | Get an edge list of the tree edges from a depth-first traversal |\n| `rustworkx.graph\\_dfs\\_search`(graph[, source, ...]) | Depth-first traversal of an undirected graph. |\n| `rustworkx.graph\\_transitivity`(graph, /) | Compute the transitivity of an undirected graph. |\n| `rustworkx.graph\\_core\\_number`(graph, /) | Return the core number for each node in the graph. |\n| `rustworkx.graph\\_complement`(graph, /) | Compute the complement of an undirected graph. |\n| `rustworkx.graph\\_union`(first, second, /[, ...]) | Return a new PyGraph by forming a union from two input PyGraph objects |\n| `rustworkx.graph\\_tensor\\_product`(first, second, /) | Return a new PyGraph by forming the tensor product from two input PyGraph objects |\n| `rustworkx.graph\\_token\\_swapper`(graph, mapping, /) | This module performs an approximately optimal Token Swapping algorithm Supports partial mappings (i.e. |\n| `rustworkx.graph\\_cartesian\\_product`(first, ...) | Return a new PyGraph by forming the cartesian product from two input PyGraph objects |\n| `rustworkx.graph\\_random\\_layout` | Generate a random layout |\n| `rustworkx.graph\\_bipartite\\_layout`(graph, ...) | Generate a bipartite layout of the graph |\n| `rustworkx.graph\\_circular\\_layout`(graph, /[, ...]) | Generate a circular layout of the graph |\n| `rustworkx.graph\\_shell\\_layout`(graph, /[, ...]) | Generate a shell layout of the graph |\n| `rustworkx.graph\\_spiral\\_layout`(graph, /[, ...]) | Generate a spiral layout of the graph |\n| `rustworkx.graph\\_spring\\_layout`(graph[, pos, ...]) | Position nodes using Fruchterman-Reingold force-directed algorithm. |\n| `rustworkx.graph\\_num\\_shortest\\_paths\\_unweighted`(...) | Get the number of unweighted shortest paths from a source node |\n| `rustworkx.graph\\_betweenness\\_centrality`(graph, /) | Compute the betweenness centrality of all nodes in a PyGraph. |\n| `rustworkx.graph\\_edge\\_betweenness\\_centrality`(...) | Compute the edge betweenness centrality of all edges in a `PyGraph`. |\n| `rustworkx.graph\\_closeness\\_centrality`(graph) | Compute the closeness centrality of each node in a `PyGraph` object. |\n| `rustworkx.graph\\_eigenvector\\_centrality`(graph, /) | Compute the eigenvector centrality of a `PyGraph`. |\n| `rustworkx.graph\\_katz\\_centrality`(graph, /[, ...]) | Compute the Katz centrality of a `PyGraph`. |\n| `rustworkx.graph\\_unweighted\\_average\\_shortest\\_path\\_length`(...) | Return the average shortest path length for a `PyGraph` with unweighted edges. |\n| `rustworkx.graph\\_bfs\\_search`(graph[, source, ...]) | Breadth-first traversal of an undirected graph. |\n| `rustworkx.graph\\_dijkstra\\_search`(graph[, ...]) | Dijkstra traversal of an undirected graph. |\n| `rustworkx.graph\\_node\\_link\\_json`(graph, /[, ...]) | Generate a JSON object representing a `PyGraph` in a node-link format |\n| `rustworkx.graph\\_longest\\_simple\\_path`(graph, /) | Return a longest simple path in the graph |\n\n# Exceptions#\n\n|  |  |\n| --- | --- |\n| `rustworkx.InvalidNode` |  |\n| `rustworkx.DAGWouldCycle` |  |\n| `rustworkx.NoEdgeBetweenNodes` |  |\n| `rustworkx.DAGHasCycle` |  |\n| `rustworkx.NegativeCycle` |  |\n| `rustworkx.NoSuitableNeighbors` |  |\n| `rustworkx.NoPathFound` |  |\n| `rustworkx.NullGraph` |  |\n| `rustworkx.visit.StopSearch` | Stop graph traversal |\n| `rustworkx.visit.PruneSearch` | Prune part of the search tree while traversing a graph. |\n| `rustworkx.JSONSerializationError` |  |\n\n# Custom Return Types#\n\n|  |  |\n| --- | --- |\n| `rustworkx.BFSSuccessors`() | A custom class for the return from `rustworkx.bfs\\_successors()` |\n| `rustworkx.BFSPredecessors`() | A custom class for the return from `rustworkx.bfs\\_predecessors()` |\n| `rustworkx.NodeIndices`() | A custom class for\n\n==================\n Document 2 \n----------------\n Connectivity and Cycles#\n\n|  |  |\n| --- | --- |\n| `rustworkx.number\\_connected\\_components`(graph, /) | Find the number of connected components in an undirected graph. |\n| `rustworkx.connected\\_components`(graph, /) | Find the connected components in an undirected graph |\n| `rustworkx.node\\_connected\\_component`(graph, ...) | Returns the set of nodes in the component of graph containing node. |\n| `rustworkx.is\\_connected`(graph, /) | Check if the graph is connected. |\n| `rustworkx.strongly\\_connected\\_components`(graph, /) | Compute the strongly connected components for a directed graph |\n| `rustworkx.number\\_weakly\\_connected\\_components`(...) | Find the number of weakly connected components in a directed graph |\n| `rustworkx.weakly\\_connected\\_components`(graph, /) | Find the weakly connected components in a directed graph |\n| `rustworkx.is\\_weakly\\_connected`(graph, /) | Check if the graph is weakly connected |\n| `rustworkx.cycle\\_basis`(graph, /[, root]) | Return a list of cycles which form a basis for cycles of a given PyGraph |\n| `rustworkx.simple\\_cycles`(graph, /) | Find all simple cycles of a `PyDiGraph` |\n| `rustworkx.digraph\\_find\\_cycle`(graph, /[, source]) | Return the first cycle encountered during DFS of a given PyDiGraph, empty list is returned if no cycle is found |\n| `rustworkx.articulation\\_points`(graph, /) | Return the articulation points of an undirected graph. |\n| `rustworkx.biconnected\\_components`(graph, /) | Return the biconnected components of an undirected graph. |\n| `rustworkx.chain\\_decomposition`(graph, /[, source]) | Returns the chain decomposition of a graph. |\n| `rustworkx.all\\_simple\\_paths`(graph, from\\_, to) | Return all simple paths between 2 nodes in a PyGraph object |\n| `rustworkx.all\\_pairs\\_all\\_simple\\_paths`(graph) | Return all the simple paths between all pairs of nodes in the graph |\n| `rustworkx.stoer\\_wagner\\_min\\_cut`(graph, /[, ...]) | Compute a weighted minimum cut using the Stoer-Wagner algorithm. |\n| `rustworkx.longest\\_simple\\_path`(graph) | Return a longest simple path in the graph |\n\n# DAG Algorithms#\n\n|  |  |\n| --- | --- |\n| `rustworkx.dag\\_longest\\_path`(graph, /[, weight\\_fn]) | Find the longest path in a DAG |\n| `rustworkx.dag\\_longest\\_path\\_length`(graph, /) | Find the length of the longest path in a DAG |\n| `rustworkx.dag\\_weighted\\_longest\\_path`(graph, ...) | Find the weighted longest path in a DAG |\n| `rustworkx.dag\\_weighted\\_longest\\_path\\_length`(...) | Find the length of the weighted longest path in a DAG |\n| `rustworkx.is\\_directed\\_acyclic\\_graph`(graph, /) | Check that the PyDiGraph or PyDAG doesn't have a cycle |\n| `rustworkx.layers`(dag, first\\_layer, /[, ...]) | Return a list of layers |\n\n\n# Graph Operations#\n\n|  |  |\n| --- | --- |\n| `rustworkx.complement`(graph) | Compute the complement of a graph. |\n| `rustworkx.union`(first, second[, ...]) | Return a new graph by forming a union from two input graph objects |\n| `rustworkx.cartesian\\_product`(first, second) | Return a new graph by forming the cartesian product from two input graph objects |\n\n\n# Isomorphism#\n\n|  |  |\n| --- | --- |\n| `rustworkx.is\\_isomorphic`(first, second[, ...]) | Determine if 2 graphs are isomorphic |\n| `rustworkx.is\\_subgraph\\_isomorphic`(first, second) | Determine if 2 graphs are subgraph isomorphic |\n| `rustworkx.is\\_isomorphic\\_node\\_match`(first, ...) | Determine if 2 graphs are isomorphic |\n| `rustworkx.vf2\\_mapping`(first, second[, ...]) | Return an iterator over all vf2 mappings between two graphs. |\n\n\n# Link Analysis#\n\n|  |  |\n| --- | --- |\n| `rustworkx.pagerank`(graph, /[, alpha, ...]) | Computes the PageRank of the nodes in a `PyDiGraph`. |\n| `rustworkx.hits`(graph, /[, weight\\_fn, ...]) | Computes the hubs and authorities in a `PyDiGraph`. |\n\n\n# Matching#\n\n|  |  |\n| --- | --- |\n| `rustworkx.max\\_weight\\_matching`(graph, /[, ...]) | Compute a maximum-weighted matching for a `PyGraph` |\n| `rustworkx.is\\_matching`(graph, matching, /) | Check if matching is valid for graph |\n| `rustworkx.is\\_maximal\\_matching`(graph, matching, /) | Check if a matching is a maximal (**not** maximum) matching for a graph |\n\n\n# Other Algorithm Functions#\n\n|  |  |\n| --- | --- |\n| `rustworkx.adjacency\\_matrix`(graph[, ...]) | Return the adjacency matrix for a graph object |\n| `rustworkx.transitivity`(graph) | Compute the transitivity of a graph. |\n| `rustworkx.core\\_number`(graph) | Return the core number for each node in the graph. |\n| `rustworkx.graph\\_greedy\\_color`(graph, /) | Color a `PyGraph` object using a greedy graph coloring algorithm. |\n| `rustworkx.metric\\_closure`(graph, weight\\_fn, /) | Return the metric closure of a graph |\n| `rustworkx.is\\_planar`(graph, /) | Check if an undirected graph is planar. |\n\n# Shortest Paths#\n\n|  |  |\n| --- | --- |\n| `rustworkx.dijkstra\\_shortest\\_paths`(graph, source) | Find the shortest path from a node |\n| `rustworkx.dijkstra\\_shortest\\_path\\_lengths`(...) | Compute the lengths of the shortest paths for a graph object using Dijkstra's algorithm. |\n| `rustworkx.all\\_pairs\\_dijkstra\\_shortest\\_paths`(...)\n\n==================\n Document 3 \n----------------\n Generators#\n\n|  |  |\n| --- | --- |\n| `rustworkx.generators.cycle\\_graph`([...]) | Generate an undirected cycle graph |\n| `rustworkx.generators.directed\\_cycle\\_graph`([...]) | Generate a directed cycle graph |\n| `rustworkx.generators.path\\_graph`([num\\_nodes, ...]) | Generate an undirected path graph |\n| `rustworkx.generators.directed\\_path\\_graph`([...]) | Generate a directed path graph |\n| `rustworkx.generators.star\\_graph`([num\\_nodes, ...]) | Generate an undirected star graph |\n| `rustworkx.generators.directed\\_star\\_graph`([...]) | Generate a directed star graph |\n| `rustworkx.generators.mesh\\_graph`([num\\_nodes, ...]) | Generate an undirected mesh (complete) graph where every node is connected to every other |\n| `rustworkx.generators.directed\\_mesh\\_graph`([...]) | Generate a directed mesh (complete) graph where every node is connected to every other |\n| `rustworkx.generators.grid\\_graph`([rows, ...]) | Generate an undirected grid graph. |\n| `rustworkx.generators.directed\\_grid\\_graph`([...]) | Generate a directed grid graph. |\n| `rustworkx.generators.binomial\\_tree\\_graph`(...) | Generate an undirected binomial tree of order n recursively. |\n| `rustworkx.generators.directed\\_binomial\\_tree\\_graph`(...) | Generate a directed binomial tree of order n recursively. |\n| `rustworkx.generators.hexagonal\\_lattice\\_graph`(...) | Generate an undirected hexagonal lattice graph. |\n| `rustworkx.generators.directed\\_hexagonal\\_lattice\\_graph`(...) | Generate a directed hexagonal lattice graph. |\n| `rustworkx.generators.heavy\\_square\\_graph`(d, /) | Generate an undirected heavy square graph. |\n| `rustworkx.generators.directed\\_heavy\\_square\\_graph`(d, /) | Generate an directed heavy square graph. |\n| `rustworkx.generators.heavy\\_hex\\_graph`(d, /[, ...]) | Generate an undirected heavy hex graph. |\n| `rustworkx.generators.directed\\_heavy\\_hex\\_graph`(d, /) | Generate a directed heavy hex graph. |\n| `rustworkx.generators.lollipop\\_graph`([...]) | Generate an undirected lollipop graph where a mesh (complete) graph is connected to a path. |\n| `rustworkx.generators.generalized\\_petersen\\_graph`(n, k, /) | Generate a generalized Petersen graph \\(G(n, k)\\) with \\(2n\\) nodes and \\(3n\\) edges. |\n| `rustworkx.generators.barbell\\_graph`([...]) | Generate an undirected barbell graph where two identical complete graphs are connected by a path. |\n| `rustworkx.generators.full\\_rary\\_tree`(...[, ...]) | Creates a full r-ary tree of n nodes. |\n| `rustworkx.generators.empty\\_graph`(n[, multigraph]) | Generate an undirected empty graph with `n` nodes and no edges. |\n| `rustworkx.generators.directed\\_empty\\_graph`(n) | Generate a directed empty graph with `n` nodes and no edges. |\n| `rustworkx.generators.complete\\_graph`([...]) | Generate an undirected complete graph with `n` nodes. |\n| `rustworkx.generators.directed\\_complete\\_graph`([...]) | Generate a directed complete graph with `n` nodes. |\n# Random Graph Generator Functions#\n\n|  |  |\n| --- | --- |\n| `rustworkx.directed\\_gnp\\_random\\_graph`(...[, seed]) | Return a \\(G\\_{np}\\) directed random graph, also known as an Erdős-Rényi graph or a binomial graph. |\n| `rustworkx.undirected\\_gnp\\_random\\_graph`(...[, ...]) | Return a \\(G\\_{np}\\) random\n\n==================\n Document 4 \n----------------\n Shortest Paths#\n\n|  |  |\n| --- | --- |\n| `rustworkx.dijkstra\\_shortest\\_paths`(graph, source) | Find the shortest path from a node |\n| `rustworkx.dijkstra\\_shortest\\_path\\_lengths`(...) | Compute the lengths of the shortest paths for a graph object using Dijkstra's algorithm. |\n| `rustworkx.all\\_pairs\\_dijkstra\\_shortest\\_paths`(...) | For each node in the graph, finds the shortest paths to all others. |\n| `rustworkx.all\\_pairs\\_dijkstra\\_path\\_lengths`(...) | For each node in the graph, calculates the lengths of the shortest paths to all others. |\n| `rustworkx.bellman\\_ford\\_shortest\\_paths`(graph, ...) | Find the shortest path from a node |\n| `rustworkx.bellman\\_ford\\_shortest\\_path\\_lengths`(...) | Compute the lengths of the shortest paths for a graph object using the Bellman-Ford algorithm with the SPFA heuristic. |\n| `rustworkx.all\\_pairs\\_bellman\\_ford\\_shortest\\_paths`(...) | For each node in the graph, finds the shortest paths to all others. |\n| `rustworkx.all\\_pairs\\_bellman\\_ford\\_path\\_lengths`(...) | For each node in the graph, calculates the lengths of the shortest paths to all others. |\n| `rustworkx.negative\\_edge\\_cycle`(graph, ...) | Check if a negative cycle exists on a graph |\n| `rustworkx.find\\_negative\\_cycle`(graph, ...) | Find a negative cycle of a graph |\n| `rustworkx.distance\\_matrix`(graph[, ...]) | Get the distance matrix for a graph |\n| `rustworkx.floyd\\_warshall`(graph[, weight\\_fn, ...]) | Find all-pairs shortest path lengths using Floyd's algorithm |\n| `rustworkx.floyd\\_warshall\\_numpy`(graph[, ...]) | Find all-pairs shortest path lengths using Floyd's algorithm |\n| `rustworkx.astar\\_shortest\\_path`(graph, node, ...) | Compute the A\\* shortest path for a graph |\n| `rustworkx.k\\_shortest\\_path\\_lengths`(graph, ...) | Compute the length of the kth shortest path |\n| `rustworkx.num\\_shortest\\_paths\\_unweighted`(...) | Get the number of unweighted shortest paths from a source node |\n| `rustworkx.unweighted\\_average\\_shortest\\_path\\_length`(graph) | Return the average shortest path length with unweighted edges. |\n# Traversal#\n\n|  |  |\n| --- | --- |\n| `rustworkx.dfs\\_edges`(graph[, source]) | Get an edge list of the tree edges from a depth-first traversal |\n| `rustworkx.dfs\\_search`(graph, source, visitor) | Depth-first traversal of a directed/undirected graph. |\n| `rustworkx.bfs\\_successors`(graph, node, /)\n\n==================\n Document 5 \n----------------\n Algorithm Functions#\n\n* Centrality\n\t+ rustworkx.betweenness\\_centrality\n\t+ rustworkx.edge\\_betweenness\\_centrality\n\t+ rustworkx.eigenvector\\_centrality\n\t+ rustworkx.katz\\_centrality\n\t+ rustworkx.closeness\\_centrality\n* Connectivity and Cycles\n\t+ rustworkx.number\\_connected\\_components\n\t+ rustworkx.connected\\_components\n\t+ rustworkx.node\\_connected\\_component\n\t+ rustworkx.is\\_connected\n\t+ rustworkx.strongly\\_connected\\_components\n\t+ rustworkx.number\\_weakly\\_connected\\_components\n\t+ rustworkx.weakly\\_connected\\_components\n\t+ rustworkx.is\\_weakly\\_connected\n\t+ rustworkx.cycle\\_basis\n\t+ rustworkx.simple\\_cycles\n\t+ rustworkx.digraph\\_find\\_cycle\n\t+ rustworkx.articulation\\_points\n\t+ rustworkx.biconnected\\_components\n\t+ rustworkx.chain\\_decomposition\n\t+ rustworkx.all\\_simple\\_paths\n\t+ rustworkx.all\\_pairs\\_all\\_simple\\_paths\n\t+ rustworkx.stoer\\_wagner\\_min\\_cut\n\t+ rustworkx.longest\\_simple\\_path\n* DAG Algorithms\n\t+ rustworkx.dag\\_longest\\_path\n\t+ rustworkx.dag\\_longest\\_path\\_length\n\t+ rustworkx.dag\\_weighted\\_longest\\_path\n\t+ rustworkx.dag\\_weighted\\_longest\\_path\\_length\n\t+ rustworkx.is\\_directed\\_acyclic\\_graph\n\t+ rustworkx.layers\n* Graph Operations\n\t+ rustworkx.complement\n\t+ rustworkx.union\n\t+ rustworkx.cartesian\\_product\n* Isomorphism\n\t+ rustworkx.is\\_isomorphic\n\t+ rustworkx.is\\_subgraph\\_isomorphic\n\t+ rustworkx.is\\_isomorphic\\_node\\_match\n\t+ rustworkx.vf2\\_mapping\n* Link Analysis\n\t+ rustworkx.pagerank\n\t+ rustworkx.hits\n* Matching\n\t+ rustworkx.max\\_weight\\_matching\n\t+ rustworkx.is\\_matching\n\t+ rustworkx.is\\_maximal\\_matching\n* Other Algorithm Functions\n\t+ rustworkx.adjacency\\_matrix\n\t+ rustworkx.transitivity\n\t+ rustworkx.core\\_number\n\t+ rustworkx.graph\\_greedy\\_color\n\t+ rustworkx.metric\\_closure\n\t+ rustworkx.is\\_planar\n* Shortest Paths\n\t+ rustworkx.dijkstra\\_shortest\\_paths\n\t+ rustworkx.dijkstra\\_shortest\\_path\\_lengths\n\t+ rustworkx.all\\_pairs\\_dijkstra\\_shortest\\_paths\n\t+ rustworkx.all\\_pairs\\_dijkstra\\_path\\_lengths\n\t+ rustworkx.bellman\\_ford\\_shortest\\_paths\n\t+ rustworkx.bellman\\_ford\\_shortest\\_path\\_lengths\n\t+ rustworkx.all\\_pairs\\_bellman\\_ford\\_shortest\\_paths\n\t+ rustworkx.all\\_pairs\\_bellman\\_ford\\_path\\_lengths\n\t+ rustworkx.negative\\_edge\\_cycle\n\t+ rustworkx.find\\_negative\\_cycle\n\t+ rustworkx.distance\\_matrix\n\t+ rustworkx.floyd\\_warshall\n\t+ rustworkx.floyd\\_warshall\\_numpy\n\t+ rustworkx.astar\\_shortest\\_path\n\t+ rustworkx.k\\_shortest\\_path\\_lengths\n\t+ rustworkx.num\\_shortest\\_paths\\_unweighted\n\t+ rustworkx.unweighted\\_average\\_shortest\\_path\\_length\n* Traversal\n\t+ rustworkx.dfs\\_edges\n\t+ rustworkx.dfs\\_search\n\t+ rustworkx.bfs\\_successors\n\t+ rustworkx.bfs\\_predecessors\n\t+ rustworkx.bfs\\_search\n\t+ rustworkx.dijkstra\\_search\n\t+ rustworkx.topological\\_sort\n\t+ rustworkx.lexicographical\\_topological\\_sort\n\t+ rustworkx.descendants\n\t+ rustworkx.ancestors\n\t+ rustworkx.collect\\_runs\n\t+ rustworkx.collect\\_bicolor\\_runs\n\t+ DFSVisitor\n\t+ BFSVisitor\n\t+ DijkstraVisitor\n\t+ TopologicalSorter\n* Tree\n\t+ rustworkx.minimum\\_spanning\\_edges\n\t+ rustworkx.minimum\\_spanning\\_tree\n\t+ rustworkx.steiner\\_tree\n\n# Centrality#\n\n|  |  |\n| --- | --- |\n| `rustworkx.betweenness\\_centrality`(graph[, ...]) | Returns the betweenness centrality of each node in the graph. |\n| `rustworkx.edge\\_betweenness\\_centrality`(graph) | Compute the edge betweenness centrality of all edges in a graph. |\n| `rustworkx.eigenvector\\_centrality`(graph[, ...]) | Compute the eigenvector centrality of a graph. |\n| `rustworkx.katz\\_centrality`(graph[, alpha, ...]) | Compute the Katz centrality of a graph. |\n| `rustworkx.closeness\\_centrality`(graph[, ...]) | Compute the closeness centrality of each node in a graph object. |\n\n# Connectivity and Cycles#\n\n|  |  |\n| --- | --- |\n| `rustworkx.number\\_connected\\_components`(graph, /) | Find the number of connected components in an undirected graph. |\n| `rustworkx.connected\\_components`(graph, /) | Find the connected components in an undirected graph |\n| `rustworkx.node\\_connected\\_component`(graph, ...)"
    },
    {
        "instruction": "Create a Python program that uses the 'scikit-learn' API to load the LFW (Labeled Faces in the Wild) dataset, split it into training and testing sets, train a Decision Tree classifier, make predictions, calculate accuracy, and save the trained model.",
        "api": "scikit-learn",
        "output": "#!pip install scikit-learn\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.tree import DecisionTreeClassifier\n\n#Load the LFW dataset\nlfw = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\nX, y = lfw.data, lfw.target\n\n#Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n#Initialize the Decision Tree classifier\ndt_classifier = DecisionTreeClassifier()\n\n#Train the classifier\ndt_classifier.fit(X_train, y_train)\n\n#Make predictions\ny_pred = dt_classifier.predict(X_test)\n\n#Calculate accuracy\naccuracy = (y_pred == y_test).mean()\nprint(f'Accuracy: {accuracy:.2f}')\n\n#Serialize and save the model\njoblib.dump(dt_classifier, 'lfw_classifier.pkl')\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n# Examples using `sklearn.datasets.load\\_iris`¶\n\n\nRelease Highlights for scikit-learn 1.2\n\nRelease Highlights for scikit-learn 0.24\n\n\nRelease Highlights for scikit-learn 0.24\n\nRelease Highlights for scikit-learn 0.22\n\n\nRelease Highlights for scikit-learn 0.22\n\nPlot classification probability\n\n\nPlot classification probability\n\nK-means Clustering\n\n\nK-means Clustering\n\nPlot Hierarchical Clustering Dendrogram\n\n\nPlot Hierarchical Clustering Dendrogram\n\nThe Iris Dataset\n\n\nThe Iris Dataset\n\nPlot the decision surface of decision trees trained on the iris dataset\n\n\nPlot the decision surface of decision trees trained on the iris dataset\n\nUnderstanding the decision tree structure\n\n\nUnderstanding the decision tree structure\n\nComparison of LDA and PCA 2D projection of Iris dataset\n\n\nComparison of LDA and PCA 2D projection of Iris dataset\n\nFactor Analysis (with rotation) to visualize patterns\n\n\nFactor Analysis (with rotation) to visualize patterns\n\nIncremental PCA\n\n\nIncremental PCA\n\nPCA example with Iris Data-set\n\n\nPCA example with Iris Data-set\n\nEarly stopping of Gradient Boosting\n\n\nEarly stopping of Gradient Boosting\n\nPlot the decision boundaries of a VotingClassifier\n\n\nPlot the decision boundaries of a VotingClassifier\n\nPlot the decision surfaces of ensembles of trees on the iris dataset\n\n\nPlot the decision surfaces of ensembles of trees on the iris dataset\n\nUnivariate Feature Selection\n\n\nUnivariate Feature Selection\n\nGMM covariances\n\n\nGMM covariances\n\nGaussian process classification (GPC) on iris dataset\n\n\nGaussian process classification (GPC) on iris dataset\n\nLogistic Regression 3-class Classifier\n\n\nLogistic Regression 3-class Classifier\n\nPlot multi-class SGD on the iris dataset\n\n\nPlot multi-class SGD on the iris dataset\n\nRegularization path of L1- Logistic Regression\n\n\nRegularization path of L1- Logistic Regression\n\nIntroducing the set\\_output API\n\n\nIntroducing the set\\_output API\n\nConfusion matrix\n\n\nConfusion matrix\n\nMulticlass Receiver Operating Characteristic (ROC)\n\n\nMulticlass Receiver Operating Characteristic (ROC)\n\nNested versus non-nested cross-validation\n\n\nNested versus non-nested cross-validation\n\nPrecision-Recall\n\n\nPrecision-Recall\n\nReceiver Operating Characteristic (ROC) with cross validation\n\n\nReceiver Operating Characteristic (ROC) with cross validation\n\nTest with permutations the significance of a classification score\n\n\nTest with permutations the significance of a classification score\n\nComparing Nearest Neighbors with and without Neighborhood Components Analysis\n\n\nComparing Nearest Neighbors with and without Neighborhood Components Analysis\n\nNearest Centroid Classification\n\n\nNearest Centroid Classification\n\nNearest Neighbors Classification\n\n\nNearest Neighbors Classification\n\nCompare Stochastic learning strategies for MLPClassifier\n\n\nCompare Stochastic learning strategies for MLPClassifier\n\nConcatenating multiple feature extraction methods\n\n\nConcatenating multiple feature extraction methods\n\nDecision boundary of semi-supervised classifiers versus SVM on the Iris dataset\n\n\nDecision boundary of semi-supervised classifiers versus SVM on the Iris dataset\n\nPlot different SVM classifiers in the iris dataset\n\n\nPlot different SVM classifiers in the iris dataset\n\nRBF SVM parameters\n\n\nRBF SVM parameters\n\nSVM with custom kernel\n\n\nSVM with custom kernel\n\nSVM-Anova: SVM with univariate feature selection\n\n\nSVM-Anova: SVM with univariate feature selection\n\nSVM Exercise\n\n\nSVM Exercise\n\n# `sklearn.datasets`.load\\_linnerud¶\n\n\nsklearn.datasets.load\\_linnerud(*\\**, *return\\_X\\_y=False*, *as\\_frame=False*)[source]¶\nLoad and return the physical exercise Linnerud dataset.\n\n\nThis dataset is suitable for multi-output regression tasks.\n\n\n|  |  |\n| --- | --- |\n| Samples total | 20 |\n| Dimensionality | 3 (for both data and target) |\n|\n\n==================\n Document 1 \n----------------\n## Loaders¶\n\n\n|  |  |\n| --- | --- |\n| `datasets.clear\\_data\\_home`([data\\_home]) | Delete all the content of the data home cache. |\n| `datasets.dump\\_svmlight\\_file`(X, y, f, \\*[, ...]) | Dump the dataset in svmlight / libsvm file format. |\n| `datasets.fetch\\_20newsgroups`(\\*[, data\\_home, ...]) | Load the filenames and data from the 20 newsgroups dataset (classification). |\n| `datasets.fetch\\_20newsgroups\\_vectorized`(\\*[, ...]) | Load and vectorize the 20 newsgroups dataset (classification). |\n| `datasets.fetch\\_california\\_housing`(\\*[, ...]) | Load the California housing dataset (regression). |\n| `datasets.fetch\\_covtype`(\\*[, data\\_home, ...]) | Load the covertype dataset (classification). |\n| `datasets.fetch\\_kddcup99`(\\*[, subset, ...]) | Load the kddcup99 dataset (classification). |\n| `datasets.fetch\\_lfw\\_pairs`(\\*[, subset, ...]) | Load the Labeled Faces in the Wild (LFW) pairs dataset (classification). |\n| `datasets.fetch\\_lfw\\_people`(\\*[, data\\_home, ...]) | Load the Labeled Faces in the Wild (LFW) people dataset (classification). |\n| `datasets.fetch\\_olivetti\\_faces`(\\*[, ...]) | Load the Olivetti faces data-set from AT&T (classification). |\n| `datasets.fetch\\_openml`([name, version, ...]) | Fetch dataset from openml by name or dataset id. |\n| `datasets.fetch\\_rcv1`(\\*[, data\\_home, subset, ...]) | Load the RCV1 multilabel dataset (classification). |\n| `datasets.fetch\\_species\\_distributions`(\\*[, ...]) | Loader for species distribution dataset from Phillips et. |\n| `datasets.get\\_data\\_home`([data\\_home]) | Return the path of the scikit-learn data directory. |\n| `datasets.load\\_breast\\_cancer`(\\*[, return\\_X\\_y, ...]) | Load and return the breast cancer wisconsin dataset (classification). |\n| `datasets.load\\_diabetes`(\\*[, return\\_X\\_y, ...]) | Load and return the diabetes dataset (regression). |\n| `datasets.load\\_digits`(\\*[, n\\_class, ...]) | Load and return the digits dataset (classification). |\n| `datasets.load\\_files`(container\\_path, \\*[, ...]) | Load text files with categories as subfolder names. |\n| `datasets.load\\_iris`(\\*[, return\\_X\\_y, as\\_frame]) | Load and return the iris dataset (classification). |\n| `datasets.load\\_linnerud`(\\*[, return\\_X\\_y, as\\_frame]) | Load and return the physical exercise Linnerud dataset. |\n| `datasets.load\\_sample\\_image`(image\\_name) | Load the numpy array of a single sample image. |\n| `datasets.load\\_sample\\_images`() | Load sample images for image manipulation. |\n| `datasets.load\\_svmlight\\_file`(f, \\*[, ...]) | Load datasets in the svmlight / libsvm format into sparse CSR matrix. |\n| `datasets.load\\_svmlight\\_files`(files, \\*[, ...]) | Load dataset from multiple files in SVMlight format. |\n| `datasets.load\\_wine`(\\*[, return\\_X\\_y, as\\_frame]) | Load and return the wine dataset (classification). |\n\n### Samples generator¶\n\n\n|  |  |\n| --- | --- |\n| `datasets.make\\_biclusters`(shape, n\\_clusters, \\*) | Generate a constant block diagonal structure array for biclustering. |\n| `datasets.make\\_blobs`([n\\_samples, n\\_features, ...]) | Generate isotropic Gaussian blobs for clustering. |\n| `datasets.make\\_checkerboard`(shape, n\\_clusters, \\*) | Generate an array with block checkerboard\n\n==================\n Document 2 \n----------------\n `sklearn.datasets`.fetch\\_lfw\\_pairs¶\n\n\nsklearn.datasets.fetch\\_lfw\\_pairs(*\\**, *subset='train'*, *data\\_home=None*, *funneled=True*, *resize=0.5*, *color=False*, *slice\\_=(slice(70, 195, None), slice(78, 172, None))*, *download\\_if\\_missing=True*)[source]¶\nLoad the Labeled Faces in the Wild (LFW) pairs dataset (classification).\n\n\n|  |  |\n| --- | --- |\n| Classes | 2 |\n| Samples total | 13233 |\n| Dimensionality | 5828 |\n| Features | real, between 0 and 255 |\n\n\nIn the official README.txt this task is described as the\n“Restricted” task. As I am not sure as to implement the\n“Unrestricted” variant correctly, I left it as unsupported for now.\n\n> \n> \n\n\nThe original images are 250 x 250 pixels, but the default slice and resize\narguments reduce them to 62 x 47.\n\n**subset**{‘train’, ‘test’, ‘10\\_folds’}, default=’train’Select the dataset to load: ‘train’ for the development training\nset, ‘test’ for the development test set, and ‘10\\_folds’ for the\nofficial evaluation set that is meant to be used with a 10-folds\ncross validation.\n\n**data\\_home**str, default=NoneSpecify another download and cache folder for the datasets. By\ndefault all scikit-learn data is stored in ‘~/scikit\\_learn\\_data’\nsubfolders.\n\n**funneled**bool, default=TrueDownload and use the funneled variant of the dataset.\n\n**resize**float, default=0.5Ratio used to resize the each face picture.\n\n**color**bool, default=FalseKeep the 3 RGB channels instead of averaging them to a single\ngray level channel. If color is True the shape of the data has\none more dimension than the shape with color = False.\n\n**slice\\_**tuple of slice, default=(slice(70, 195), slice(78, 172))Provide a custom 2D slice (height, width) to extract the\n‘interesting’ part of the jpeg files and avoid use statistical\ncorrelation from the background.\n\ndatandarray of shape (2200, 5828). Shape depends on `subset`.Each row corresponds to 2 ravel’d face images\nof original size 62 x 47 pixels.\nChanging the `slice\\_`, `resize` or `subset` parameters\nwill change the shape of the output.\n\npairsndarray of shape (2200, 2, 62, 47). Shape depends on `subset`Each row has 2 face images corresponding\nto same or different person from the dataset\ncontaining 5749 people. Changing the `slice\\_`,\n`resize` or `subset` parameters will change the shape of the\noutput.\n\ntargetnumpy array of shape (2200,). Shape depends on `subset`.Labels associated to each pair of images.\nThe two label values being different persons or the same person.\n\ntarget\\_namesnumpy array of shape (2,)Explains the target values of the target array.\n0 corresponds to “Different person”, 1 corresponds to “same person”.\n\nDESCRstrDescription of the Labeled Faces in the Wild (LFW) dataset.\n\n# `sklearn.datasets`.fetch\\_lfw\\_people¶\n\n\nsklearn.datasets.fetch\\_lfw\\_people(*\\**, *data\\_home=None*, *funneled=True*, *resize=0.5*, *min\\_faces\\_per\\_person=0*, *color=False*, *slice\\_=(slice(70, 195, None), slice(78, 172, None))*, *download\\_if\\_missing=True*, *return\\_X\\_y=False*)[source]¶\nLoad the Labeled Faces in the Wild (LFW) people dataset (classification).\n\n\n|  |  |\n| --- | --- |\n| Classes | 5749 |\n| Samples total |\n\n==================\n Document 3 \n----------------\n 1.10. Decision Trees¶\n\n\n**Decision Trees (DTs)** are a non-parametric supervised learning method used\nfor classification and regression. The goal is to create a model that predicts the value of a\ntarget variable by learning simple decision rules inferred from the data\nfeatures. A tree can be seen as a piecewise constant approximation.\n\n\nFor instance, in the example below, decision trees learn from data to\napproximate a sine curve with a set of if-then-else decision rules. The deeper\nthe tree, the more complex the decision rules and the fitter the model.\n\n\n\nSome advantages of decision trees are:\n\n> \n> * Simple to understand and to interpret. Trees can be visualized.\n> * Requires little data preparation. Other techniques often require data\n> normalization, dummy variables need to be created and blank values to\n> be removed. Some tree and algorithm combinations support\n> missing values.\n> * The cost of using the tree (i.e., predicting data) is logarithmic in the\n> number of data points used to train the tree.\n> * Able to handle both numerical and categorical data. However, the scikit-learn\n> implementation does not support categorical variables for now. Other\n> techniques are usually specialized in analyzing datasets that have only one type\n> of variable. See algorithms for more\n> information.\n> * Able to handle multi-output problems.\n> * Uses a white box model. If a given situation is observable in a model,\n> the explanation for the condition is easily explained by boolean logic.\n> By contrast, in a black box model (e.g., in an artificial neural\n> network), results may be more difficult to interpret.\n> * Possible to validate a model using statistical tests. That makes it\n> possible to account for the reliability of the model.\n> * Performs well even if its assumptions are somewhat violated by\n> the true model from which the data were generated.\n> \n> \n> \n\n\nThe disadvantages of decision trees include:\n\n> \n> * Decision-tree learners can create over-complex trees that do not\n> generalize the data well. This is called overfitting. Mechanisms\n> such as pruning, setting the minimum number of samples required\n> at a leaf node or setting the maximum depth of the tree are\n> necessary to avoid this problem.\n> * Decision trees can be unstable because small variations in the\n> data might result in a completely different tree being generated.\n> This problem is mitigated by using decision trees within an\n> ensemble.\n> * Predictions of decision trees are neither smooth nor continuous, but\n> piecewise constant approximations as seen in the above figure. Therefore,\n> they are not good at extrapolation.\n> * The problem of learning an optimal decision tree is known to be\n> NP-complete under several aspects of optimality and even for simple\n> concepts. Consequently, practical decision-tree learning algorithms\n> are based on heuristic algorithms such as the greedy algorithm where\n> locally optimal decisions are made at each node. Such algorithms\n> cannot guarantee to return the globally optimal decision tree. This\n> can be mitigated by training multiple trees in an ensemble learner,\n> where the features and samples are randomly sampled with replacement.\n> * There are concepts that are hard to learn because decision trees\n> do not express them easily, such as XOR, parity or multiplexer problems.\n> * Decision tree learners create biased trees if some classes dominate.\n> It is therefore recommended to balance the dataset prior to fitting\n> with the decision tree.\n> \n> \n> \n## 1.10.1. Classification¶\n\n\n`DecisionTreeClassifier` is a class capable of performing multi-class\nclassification on a dataset.\n\n\nAs with other classifiers, `DecisionTreeClassifier` takes as input two arrays:\nan array X, sparse or dense, of shape `(n\\_samples, n\\_features)` holding the\ntraining samples, and an array Y of integer\n\n==================\n Document 4 \n----------------\n# 1.10.1. Classification¶\n\n\n`DecisionTreeClassifier` is a class capable of performing multi-class\nclassification on a dataset.\n\n\nAs with other classifiers, `DecisionTreeClassifier` takes as input two arrays:\nan array X, sparse or dense, of shape `(n\\_samples, n\\_features)` holding the\ntraining samples, and an array Y of integer values, shape `(n\\_samples,)`,\nholding the class labels for the training samples:\n\n```\n>>> from sklearn import tree\n>>> X = [[0, 0], [1, 1]]\n>>> Y = [0, 1]\n>>> clf = tree.DecisionTreeClassifier()\n>>> clf = clf.fit(X, Y)\n\n\nAfter being fitted, the model can then be used to predict the class of samples:\n\n\nIn case that there are multiple classes with the same and highest\nprobability, the classifier will predict the class with the lowest index\namongst those classes.\n\n\nAs an alternative to outputting a specific class, the probability of each class\ncan be predicted, which is the fraction of training samples of the class in a\nleaf:\n\n```\n>>> clf.predict\\_proba([[2., 2.]])\narray([[0., 1.]])\n\n\n`DecisionTreeClassifier` is capable of both binary (where the\nlabels are [-1, 1]) classification and multiclass (where the labels are\n[0, …, K-1]) classification.\n\n\nUsing the Iris dataset, we can construct a tree as follows:\n\n```\n>>> from sklearn.datasets import load\\_iris\n>>> from sklearn import tree\n>>> iris = load\\_iris()\n>>> X, y = iris.data, iris.target\n>>> clf = tree.DecisionTreeClassifier()\n>>> clf = clf.fit(X, y)\n\n\nOnce trained, you can plot the tree with the `plot\\_tree` function:\n\n```\n>>> tree.plot\\_tree(clf)\n[...]\n\n\n\n**Alternative ways to export trees**\nClick for more details\n\n\nWe can also export the tree in Graphviz format using the `export\\_graphviz`\nexporter. If you use the conda package manager, the graphviz binaries\nand the python package can be installed with `conda install python-graphviz`.\n\n\nAlternatively binaries for graphviz can be downloaded from the graphviz project homepage,\nand the Python wrapper installed from pypi with `pip install graphviz`.\n\n\nBelow is an example graphviz export of the above tree trained on the entire\niris dataset; the results are saved in an output file `iris.pdf`:\n\n```\n>>> import graphviz \n>>> dot\\_data = tree.export\\_graphviz(clf, out\\_file=None) \n>>> graph = graphviz.Source(dot\\_data) \n>>> graph.render(\"iris\") \n\n\nThe `export\\_graphviz` exporter also supports a variety of aesthetic\noptions, including coloring nodes by their class (or value for regression) and\nusing explicit variable and class names if desired. Jupyter notebooks also\nrender these plots inline automatically:\n\n```\n>>> dot\\_data = tree.export\\_graphviz(clf, out\\_file=None, \n...                      feature\\_names=iris.feature\\_names,  \n...                      class\\_names=iris.target\\_names,  \n...                      filled=True, rounded=True,  \n...                      special\\_characters=True)  \n>>> graph = graphviz.Source(dot\\_data)  \n>>> graph \n\n\n\n\n\nAlternatively, the tree can also be exported in textual format with the\nfunction `export\\_text`. This method doesn’t require the installation\nof external libraries and is more compact:\n\n```\n>>> from sklearn.datasets import load\\_iris\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> from sklearn.tree import export\\_text\n>>> iris = load\\_iris()\n>>> decision\\_tree = DecisionTreeClassifier(random\\_state=0, max\\_depth=2)\n>>> decision\\_tree = decision\\_tree.fit(iris.data, iris.target)\n>>> r = export\\_text(decision\\_tree, feature\\_names=iris['feature\\_names'])\n>>> print(r)\n|--- petal width (cm) <= 0.80\n| |--- class: 0\n|--- petal width (cm) > 0.80\n| |--- petal width (cm) <= 1.75\n| | |--- class: 1\n| |--- petal width (cm) > 1.75\n| | |--- class: 2\n\n# `sklearn.tree`.DecisionTreeClassifier¶\n\n\n*class* sklearn.tree.DecisionTreeClassifier(*\\**, *criterion='gini'*, *splitter='best'*, *max\\_depth=None*, *min\\_samples\\_split=2*, *min\\_samples\\_leaf=1*, *min\\_weight\\_fraction\\_leaf=0.0*, *max\\_features=None*, *random\\_state=None*, *max\\_leaf\\_nodes=None*, *min\\_impurity\\_decrease=0.0*, *class\\_weight=None*, *ccp\\_alpha=0.0*)[source]¶\nA decision tree classifier.\n\n**criterion**{“gini”, “entropy”, “log\\_loss”}, default=”gini”The function to measure the quality of a split. Supported criteria are\n“gini” for the Gini impurity and “log\\_loss” and “entropy”\n\n==================\n Document 5 \n----------------\n 3.1. Cross-validation: evaluating estimator performance¶\n\n\nLearning the parameters of a prediction function and testing it on the\nsame data is a methodological mistake: a model that would just repeat\nthe labels of the samples that it has just seen would have a perfect\nscore but would fail to predict anything useful on yet-unseen data.\nThis situation is called **overfitting**.\nTo avoid it, it is common practice when performing\na (supervised) machine learning experiment\nto hold out part of the available data as a **test set** `X\\_test, y\\_test`.\nNote that the word “experiment” is not intended\nto denote academic use only,\nbecause even in commercial settings\nmachine learning usually starts out experimentally.\nHere is a flowchart of typical cross validation workflow in model training.\nThe best parameters can be determined by\ngrid search techniques.\n\n\n\nIn scikit-learn a random split into training and test sets\ncan be quickly computed with the `train\\_test\\_split` helper function.\nLet’s load the iris data set to fit a linear support vector machine on it:\n\n```\n>>> import numpy as np\n>>> from sklearn.model\\_selection import train\\_test\\_split\n>>> from sklearn import datasets\n>>> from sklearn import svm\n\n>>> X, y = datasets.load\\_iris(return\\_X\\_y=True)\n>>> X.shape, y.shape\n((150, 4), (150,))\n\n\nWe can now quickly sample a training set while holding out 40% of the\ndata for testing (evaluating) our classifier:\n\n```\n>>> X\\_train, X\\_test, y\\_train, y\\_test = train\\_test\\_split(\n...     X, y, test\\_size=0.4, random\\_state=0)\n\n>>> X\\_train.shape, y\\_train.shape\n((90, 4), (90,))\n>>> X\\_test.shape, y\\_test.shape\n((60, 4), (60,))\n\n>>> clf = svm.SVC(kernel='linear', C=1).fit(X\\_train, y\\_train)\n>>> clf.score(X\\_test, y\\_test)\n0.96...\n\n\nWhen evaluating different settings (“hyperparameters”) for estimators,\nsuch as the `C` setting that must be manually set for an SVM,\nthere is still a risk of overfitting *on the test set*\nbecause the parameters can be tweaked until the estimator performs optimally.\nThis way, knowledge about the test set can “leak” into the model\nand evaluation metrics no longer report on generalization performance.\nTo solve this problem, yet another part of the dataset can be held out\nas a so-called “validation set”: training proceeds on the training set,\nafter which evaluation is done on the validation set,\nand when the experiment seems to be successful,\nfinal evaluation can be done on the test set.\n\n\nHowever, by partitioning the available data into three sets,\nwe drastically reduce the number of samples\nwhich can be used for learning the model,\nand the results can depend on a particular random choice for the pair of\n(train, validation) sets.\n\n\nA solution to this problem is a procedure called\ncross-validation\n(CV for short).\nA test set should still be held out for final evaluation,\nbut the validation set is no longer needed when doing CV.\nIn the basic approach, called *k*-fold CV,\nthe training set is split into *k* smaller sets\n(other approaches are described below,\nbut generally follow the same principles).\nThe following procedure is followed for each of the *k* “folds”:\n\n> \n> * A model is trained using \\(k-1\\) of the folds as training data;\n> * the resulting model is validated on the remaining part of the data\n> (i.e., it is used as a test set to compute a performance measure\n> such as accuracy).\n> \n> \n> \n\n\nThe performance measure reported by *k*-fold cross-validation\nis then the average of the values computed in the loop.\nThis approach can be computationally expensive,\nbut does not waste too much data\n(as is the case when fixing an arbitrary validation set),\nwhich is a major advantage in problems such as inverse inference\nwhere the number of samples is very small.\n\n\n\n## 3.1.1. Computing cross-validated metrics¶\n\n\nThe simplest way to use cross-validation is to call the\n`cross\\_val\\_score` helper function on the estimator and the dataset.\n\n\nThe following example demonstrates how to estimate the accuracy of a linear\nkernel support vector machine on the iris dataset\n\n==================\n Document 6 \n----------------\n 3.4. Validation curves: plotting scores to evaluate models¶\n\n\nEvery estimator has its advantages and drawbacks. Its generalization error\ncan be decomposed in terms of bias, variance and noise. The **bias** of an\nestimator is its average error for different training sets. The **variance**\nof an estimator indicates how sensitive it is to varying training sets. Noise\nis a property of the data.\n\n\nIn the following plot, we see a function \\(f(x) = \\cos (\\frac{3}{2} \\pi x)\\)\nand some noisy samples from that function. We use three different estimators\nto fit the function: linear regression with polynomial features of degree 1,\n4 and 15. We see that the first estimator can at best provide only a poor fit\nto the samples and the true function because it is too simple (high bias),\nthe second estimator approximates it almost perfectly and the last estimator\napproximates the training data perfectly but does not fit the true function\nvery well, i.e. it is very sensitive to varying training data (high variance).\n\n\n\nBias and variance are inherent properties of estimators and we usually have to\nselect learning algorithms and hyperparameters so that both bias and variance\nare as low as possible (see Bias-variance dilemma). Another way to reduce\nthe variance of a model is to use more training data. However, you should only\ncollect more training data if the true function is too complex to be\napproximated by an estimator with a lower variance.\n\n\nIn the simple one-dimensional problem that we have seen in the example it is\neasy to see whether the estimator suffers from bias or variance. However, in\nhigh-dimensional spaces, models can become very difficult to visualize. For\nthis reason, it is often helpful to use the tools described below.\n## 3.4.1. Validation curve¶\n\n\nTo validate a model we need a scoring function (see Metrics and scoring: quantifying the quality of predictions),\nfor example accuracy for classifiers. The proper way of choosing multiple\nhyperparameters of an estimator is of course grid search or\n\n==================\n Document 7 \n----------------\n# 3.4.1. Validation curve¶\n\n\nTo validate a model we need a scoring function (see Metrics and scoring: quantifying the quality of predictions),\nfor example accuracy for classifiers. The proper way of choosing multiple\nhyperparameters of an estimator is of course grid search or similar methods\n(see Tuning the hyper-parameters of an estimator) that select the hyperparameter with the maximum score\non a validation set or multiple validation sets. Note that if we optimize\nthe hyperparameters based on a validation score the validation score is biased\nand not a good estimate of the generalization any longer. To get a proper\nestimate of the generalization we have to compute the score on another test\nset.\n\n\nHowever, it is sometimes helpful to plot the influence of a single\nhyperparameter on the training score and the validation score to find out\nwhether the estimator is overfitting or underfitting for some hyperparameter\nvalues.\n\n\nThe function `validation\\_curve` can help in this case:\n\n```\n>>> import numpy as np\n>>> from sklearn.model\\_selection import validation\\_curve\n>>> from sklearn.datasets import load\\_iris\n>>> from sklearn.svm import SVC\n\n>>> np.random.seed(0)\n>>> X, y = load\\_iris(return\\_X\\_y=True)\n>>> indices = np.arange(y.shape[0])\n>>> np.random.shuffle(indices)\n>>> X, y = X[indices], y[indices]\n\n>>> train\\_scores, valid\\_scores = validation\\_curve(\n...     SVC(kernel=\"linear\"), X, y, param\\_name=\"C\", param\\_range=np.logspace(-7, 3, 3),\n... )\n>>> train\\_scores\narray([[0.90..., 0.94..., 0.91..., 0.89..., 0.92...],\n [0.9... , 0.92..., 0.93..., 0.92..., 0.93...],\n [0.97..., 1... , 0.98..., 0.97..., 0.99...]])\n>>> valid\\_scores\narray([[0.9..., 0.9... , 0.9... , 0.96..., 0.9... ],\n [0.9..., 0.83..., 0.96..., 0.96..., 0.93...],\n [1.... , 0.93..., 1.... , 1.... , 0.9... ]])\n\n\nIf you intend to plot the validation curves only, the class\n`ValidationCurveDisplay` is more direct than\nusing matplotlib manually on the results of a call to `validation\\_curve`.\nYou can use the method\n`from\\_estimator` similarly\nto `validation\\_curve` to generate and plot the validation curve:\n\n```\nfrom sklearn.datasets import load\\_iris\nfrom sklearn.model\\_selection import ValidationCurveDisplay\nfrom sklearn.svm import SVC\nfrom sklearn.utils import shuffle\nX, y = load\\_iris(return\\_X\\_y=True)\nX, y = shuffle(X, y, random\\_state=0)\nValidationCurveDisplay.from\\_estimator(\n   SVC(kernel=\"linear\"), X, y, param\\_name=\"C\", param\\_range=np.logspace(-7, 3, 10)\n)\n\n\n\nIf the training score and the validation score are both low, the estimator will\nbe underfitting. If the training score is high and the validation score is low,\nthe estimator is overfitting and otherwise it is working very well. A low\ntraining score and a high validation score is usually not possible. Underfitting,\noverfitting, and a working model are shown in the in the plot below where we vary\nthe parameter `gamma` of an SVM with an RBF kernel on the digits dataset.\n\n\n## 3.4.2. Learning curve¶\n\n\nA learning curve shows the validation and training score of an estimator\nfor varying numbers of training samples. It is a tool to find out how much\nwe benefit from adding more training data and whether the estimator suffers\nmore"
    },
    {
        "instruction": "Create a Python program that uses the 'scikit-learn' API to load the LFW (Labeled Faces in the Wild) dataset, split it into training and testing sets, train a Random Forest classifier, make predictions, calculate accuracy, and save the trained model.",
        "api": "scikit-learn",
        "output": "#!pip install scikit-learn\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Load the LFW dataset\nlfw = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\nX, y = lfw.data, lfw.target\n\n#Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n#Initialize the Random Forest classifier\nrf_classifier = RandomForestClassifier(n_estimators=100)\n\n#Train the classifier\nrf_classifier.fit(X_train, y_train)\n\n#Make predictions\ny_pred = rf_classifier.predict(X_test)\n\n#Calculate accuracy\naccuracy = (y_pred == y_test).mean()\nprint(f'Accuracy: {accuracy:.2f}')\n\n#Serialize and save the model\njoblib.dump(rf_classifier, 'lfw_classifier.pkl')\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n# 1.11.2. Random forests and other randomized tree ensembles¶\n\n\nThe `sklearn.ensemble` module includes two averaging algorithms based\non randomized decision trees: the RandomForest algorithm\nand the Extra-Trees method. Both algorithms are perturb-and-combine\ntechniques [B1998] specifically designed for trees. This means a diverse\nset of classifiers is created by introducing randomness in the classifier\nconstruction. The prediction of the ensemble is given as the averaged\nprediction of the individual classifiers.\n\n\nAs other classifiers, forest classifiers have to be fitted with two\narrays: a sparse or dense array X of shape `(n\\_samples, n\\_features)`\nholding the training samples, and an array Y of shape `(n\\_samples,)`\nholding the target values (class labels) for the training samples:\n\n```\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> X = [[0, 0], [1, 1]]\n>>> Y = [0, 1]\n>>> clf = RandomForestClassifier(n\\_estimators=10)\n>>> clf = clf.fit(X, Y)\n\n\nLike decision trees, forests of trees also extend to\nmulti-output problems (if Y is an array\nof shape `(n\\_samples, n\\_outputs)`).\n### 1.11.2.1. Random Forests¶\n\n\nIn random forests (see `RandomForestClassifier` and\n`RandomForestRegressor` classes), each tree in the ensemble is built\nfrom a sample drawn with replacement (i.e., a bootstrap sample) from the\ntraining set.\n\n\nFurthermore, when splitting each node during the construction of a tree, the\nbest\n\n==================\n Document 1 \n----------------\n## 1.11.2.1. Random Forests¶\n\n\nIn random forests (see `RandomForestClassifier` and\n`RandomForestRegressor` classes), each tree in the ensemble is built\nfrom a sample drawn with replacement (i.e., a bootstrap sample) from the\ntraining set.\n\n\nFurthermore, when splitting each node during the construction of a tree, the\nbest split is found either from all input features or a random subset of size\n`max\\_features`. (See the parameter tuning guidelines for more details).\n\n\nThe purpose of these two sources of randomness is to decrease the variance of\nthe forest estimator. Indeed, individual decision trees typically exhibit high\nvariance and tend to overfit. The injected randomness in forests yield decision\ntrees with somewhat decoupled prediction errors. By taking an average of those\npredictions, some errors can cancel out. Random forests achieve a reduced\nvariance by combining diverse trees, sometimes at the cost of a slight increase\nin bias. In practice the variance reduction is often significant hence yielding\nan overall better model.\n\n\nIn contrast to the original publication [B2001], the scikit-learn\nimplementation combines classifiers by averaging their probabilistic\nprediction, instead of letting each classifier vote for a single class.\n\n\nA competitive alternative to random forests are\nHistogram-Based Gradient Boosting (HGBT) models:\n\n\n* Building trees: Random forests typically rely on deep trees (that overfit\nindividually) which uses much computational resources, as they require\nseveral splittings and evaluations of candidate splits. Boosting models\nbuild shallow trees (that underfit individually) which are faster to fit\nand predict.\n* Sequential boosting: In HGBT, the decision trees are built sequentially,\nwhere each tree is trained to correct the errors made by the previous ones.\nThis allows them to iteratively improve the model’s performance using\nrelatively few trees. In contrast, random forests use a majority vote to\npredict the outcome, which can require a larger number of trees to achieve\nthe same level of accuracy.\n* Efficient binning: HGBT uses an efficient binning algorithm that can handle\nlarge datasets with a high number of features. The binning algorithm can\npre-process the data to speed up the subsequent tree construction (see\nWhy it’s faster). In contrast, the scikit-learn\nimplementation of random forests does not use binning and relies on exact\nsplitting, which can be computationally expensive.\n\n\nOverall, the computational cost of HGBT versus RF depends on the specific\ncharacteristics of the dataset and the modeling task. It’s a good idea\nto try both models and compare their performance and computational efficiency\non your specific problem to determine which model is the best fit.\n\n### 1.11.2.2. Extremely Randomized Trees¶\n\n\nIn extremely randomized trees (see `ExtraTreesClassifier`\nand `ExtraTreesRegressor` classes), randomness goes one step\nfurther in the way splits are computed. As in random forests, a random\nsubset of candidate features is used, but instead of looking for the\nmost discriminative\n\n==================\n Document 2 \n----------------\n 3.1. Cross-validation: evaluating estimator performance¶\n\n\nLearning the parameters of a prediction function and testing it on the\nsame data is a methodological mistake: a model that would just repeat\nthe labels of the samples that it has just seen would have a perfect\nscore but would fail to predict anything useful on yet-unseen data.\nThis situation is called **overfitting**.\nTo avoid it, it is common practice when performing\na (supervised) machine learning experiment\nto hold out part of the available data as a **test set** `X\\_test, y\\_test`.\nNote that the word “experiment” is not intended\nto denote academic use only,\nbecause even in commercial settings\nmachine learning usually starts out experimentally.\nHere is a flowchart of typical cross validation workflow in model training.\nThe best parameters can be determined by\ngrid search techniques.\n\n\n\nIn scikit-learn a random split into training and test sets\ncan be quickly computed with the `train\\_test\\_split` helper function.\nLet’s load the iris data set to fit a linear support vector machine on it:\n\n```\n>>> import numpy as np\n>>> from sklearn.model\\_selection import train\\_test\\_split\n>>> from sklearn import datasets\n>>> from sklearn import svm\n\n>>> X, y = datasets.load\\_iris(return\\_X\\_y=True)\n>>> X.shape, y.shape\n((150, 4), (150,))\n\n\nWe can now quickly sample a training set while holding out 40% of the\ndata for testing (evaluating) our classifier:\n\n```\n>>> X\\_train, X\\_test, y\\_train, y\\_test = train\\_test\\_split(\n...     X, y, test\\_size=0.4, random\\_state=0)\n\n>>> X\\_train.shape, y\\_train.shape\n((90, 4), (90,))\n>>> X\\_test.shape, y\\_test.shape\n((60, 4), (60,))\n\n>>> clf = svm.SVC(kernel='linear', C=1).fit(X\\_train, y\\_train)\n>>> clf.score(X\\_test, y\\_test)\n0.96...\n\n\nWhen evaluating different settings (“hyperparameters”) for estimators,\nsuch as the `C` setting that must be manually set for an SVM,\nthere is still a risk of overfitting *on the test set*\nbecause the parameters can be tweaked until the estimator performs optimally.\nThis way, knowledge about the test set can “leak” into the model\nand evaluation metrics no longer report on generalization performance.\nTo solve this problem, yet another part of the dataset can be held out\nas a so-called “validation set”: training proceeds on the training set,\nafter which evaluation is done on the validation set,\nand when the experiment seems to be successful,\nfinal evaluation can be done on the test set.\n\n\nHowever, by partitioning the available data into three sets,\nwe drastically reduce the number of samples\nwhich can be used for learning the model,\nand the results can depend on a particular random choice for the pair of\n(train, validation) sets.\n\n\nA solution to this problem is a procedure called\ncross-validation\n(CV for short).\nA test set should still be held out for final evaluation,\nbut the validation set is no longer needed when doing CV.\nIn the basic approach, called *k*-fold CV,\nthe training set is split into *k* smaller sets\n(other approaches are described below,\nbut generally follow the same principles).\nThe following procedure is followed for each of the *k* “folds”:\n\n> \n> * A model is trained using \\(k-1\\) of the folds as training data;\n> * the resulting model is validated on the remaining part of the data\n> (i.e., it is used as a test set to compute a performance measure\n> such as accuracy).\n> \n> \n> \n\n\nThe performance measure reported by *k*-fold cross-validation\nis then the average of the values computed in the loop.\nThis approach can be computationally expensive,\nbut does not waste too much data\n(as is the case when fixing an arbitrary validation set),\nwhich is a major advantage in problems such as inverse inference\nwhere the number of samples is very small.\n\n\n\n## 3.1.1. Computing cross-validated metrics¶\n\n\nThe simplest way to use cross-validation is to call the\n`cross\\_val\\_score` helper function on the estimator and the dataset.\n\n\nThe following example demonstrates how to estimate the accuracy of a linear\nkernel support vector machine on the iris dataset\n\n==================\n Document 3 \n----------------\n## 3.1.2.5. Using cross-validation iterators to split train and test¶\n\n\nThe above group cross-validation functions may also be useful for splitting a\ndataset into training and testing subsets. Note that the convenience\nfunction `train\\_test\\_split` is a wrapper around `ShuffleSplit`\nand thus only allows for stratified splitting (using the class labels)\nand cannot account for groups.\n\n\nTo perform the train and test split, use the indices for the train and test\nsubsets yielded by the generator output by the `split()` method of the\ncross-validation splitter. For example:\n\n```\n>>> import numpy as np\n>>> from sklearn.model\\_selection import GroupShuffleSplit\n\n>>> X = np.array([0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001])\n>>> y = np.array([\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"a\"])\n>>> groups = np.array([1, 1, 2, 2, 3, 3, 4, 4])\n>>> train\\_indx, test\\_indx = next(\n...     GroupShuffleSplit(random\\_state=7).split(X, y, groups)\n... )\n>>> X\\_train, X\\_test, y\\_train, y\\_test = \\\n...     X[train\\_indx], X[test\\_indx], y[train\\_indx], y[test\\_indx]\n>>> X\\_train.shape, X\\_test.shape\n((6,), (2,))\n>>> np.unique(groups[train\\_indx]), np.unique(groups[test\\_indx])\n(array([1, 2, 4]), array([3]))\n\n\n### 3.1.2.6. Cross validation of time series data¶\n\n\nTime series data is characterized by the correlation between observations\nthat are near in time (*autocorrelation*). However, classical\ncross-validation techniques such as `KFold` and\n`ShuffleSplit` assume the samples are independent and\nidentically distributed, and would result in unreasonable correlation\nbetween training and testing instances (yielding poor estimates of\ngeneralization error) on time series data. Therefore, it is very important\nto evaluate our model for time series data on the “future” observations\nleast like those that are used to train the model. To achieve this, one\nsolution is provided by `TimeSeriesSplit`.\n\n#### 3.1.2.6.1. Time Series Split¶\n\n\n`TimeSeriesSplit` is a variation of *k-fold* which\nreturns first \\(k\\) folds as train set and the \\((k+1)\\) th\nfold as test set. Note that unlike standard cross-validation methods,\nsuccessive training sets are supersets of those that come before them.\nAlso,\n\n==================\n Document 4 \n----------------\n### 3.1.2.1.1. K-fold¶\n\n\n`KFold` divides all the samples in \\(k\\) groups of samples,\ncalled folds (if \\(k = n\\), this is equivalent to the *Leave One\nOut* strategy), of equal sizes (if possible). The prediction function is\nlearned using \\(k - 1\\) folds, and the fold left out is used for test.\n\n\nExample of 2-fold cross-validation on a dataset with 4 samples:\n\n```\n>>> import numpy as np\n>>> from sklearn.model\\_selection import KFold\n\n>>> X = [\"a\", \"b\", \"c\", \"d\"]\n>>> kf = KFold(n\\_splits=2)\n>>> for train, test in kf.split(X):\n...     print(\"%s %s\" % (train, test))\n[2 3] [0 1]\n[0 1] [2 3]\n\n\nHere is a visualization of the cross-validation behavior. Note that\n`KFold` is not affected by classes or groups.\n\n\n\nEach fold is constituted by two arrays: the first one is related to the\n*training set*, and the second one to the *test set*.\nThus, one can create the training/test sets using numpy indexing:\n\n```\n>>> X = np.array([[0., 0.], [1., 1.], [-1., -1.], [2., 2.]])\n>>> y = np.array([0, 1, 0, 1])\n>>> X\\_train, X\\_test, y\\_train, y\\_test = X[train], X[test], y[train], y[test]\n\n#### 3.1.2.1.2. Repeated K-Fold¶\n\n\n`RepeatedKFold` repeats K-Fold n times. It can be used when one\nrequires to run `KFold` n times, producing different splits in\neach repetition.\n\n\nExample of 2-fold K-Fold repeated 2 times:\n\n```\n>>> import numpy as np\n>>> from sklearn.model\\_selection import RepeatedKFold\n>>> X =\n\n==================\n Document 5 \n----------------\n 3.3. Metrics and scoring: quantifying the quality of predictions¶\n\n\nThere are 3 different APIs for evaluating the quality of a model’s\npredictions:\n\n\n* **Estimator score method**: Estimators have a `score` method providing a\ndefault evaluation criterion for the problem they are designed to solve.\nThis is not discussed on this page, but in each estimator’s documentation.\n* **Scoring parameter**: Model-evaluation tools using\ncross-validation (such as\n`model\\_selection.cross\\_val\\_score` and\n`model\\_selection.GridSearchCV`) rely on an internal *scoring* strategy.\nThis is discussed in the section The scoring parameter: defining model evaluation rules.\n* **Metric functions**: The `sklearn.metrics` module implements functions\nassessing prediction error for specific purposes. These metrics are detailed\nin sections on Classification metrics,\nMultilabel ranking metrics, Regression metrics and\nClustering metrics.\n\n\nFinally, Dummy estimators are useful to get a baseline\nvalue of those metrics for random predictions.\n\n\nFor “pairwise” metrics, between *samples* and not estimators or\npredictions, see the Pairwise metrics, Affinities and Kernels section.\n\n\n## 3.3.1. The `scoring` parameter: defining model evaluation rules¶\n\n\nModel selection and evaluation using tools, such as\n`model\\_selection.GridSearchCV` and\n`model\\_selection.cross\\_val\\_score`, take a `scoring` parameter that\ncontrols what metric they apply to the estimators evaluated.\n\n### 3.3.1.1. Common cases: predefined values¶\n\n\nFor the most common use cases, you can designate a scorer object with the\n`scoring` parameter; the table below shows all possible values.\nAll scorer objects follow the convention that **higher return values are better\nthan lower return\n\n==================\n Document 6 \n----------------\n 1.16. Probability calibration¶\n\n\nWhen performing classification you often want not only to predict the class\nlabel, but also obtain a probability of the respective label. This probability\ngives you some kind of confidence on the prediction. Some models can give you\npoor estimates of the class probabilities and some even do not support\nprobability prediction (e.g., some instances of\n`SGDClassifier`).\nThe calibration module allows you to better calibrate\nthe probabilities of a given model, or to add support for probability\nprediction.\n\n\nWell calibrated classifiers are probabilistic classifiers for which the output\nof the predict\\_proba method can be directly interpreted as a confidence\nlevel.\nFor instance, a well calibrated (binary) classifier should classify the samples such\nthat among the samples to which it gave a predict\\_proba value close to, say,\n0.8, approximately 80% actually belong to the positive class.\n\n\nBefore we show how to re-calibrate a classifier, we first need a way to detect how\ngood a classifier is calibrated.\n\nNote\n\n\nStrictly proper scoring rules for probabilistic predictions like\n`sklearn.metrics.brier\\_score\\_loss` and\n`sklearn.metrics.log\\_loss` assess calibration (reliability) and\ndiscriminative power (resolution) of a model, as well as the randomness of the data\n(uncertainty) at the same time. This follows from the well-known Brier score\ndecomposition of Murphy [1]. As it is not clear which term dominates, the score is\nof limited use for assessing calibration alone (unless one computes each term of\nthe decomposition). A lower Brier loss, for instance, does not necessarily\nmean a better calibrated model, it could also mean a worse calibrated model with much\nmore discriminatory power, e.g. using many more features.\n\n## 1.16.1. Calibration curves¶\n\n\nCalibration curves, also referred to as *reliability diagrams* (Wilks 1995 [2]),\ncompare how well the probabilistic predictions of a binary classifier are calibrated.\nIt plots the frequency of the positive label (to be more precise, an estimation of the\n*conditional"
    },
    {
        "instruction": "Create a Python program that uses the 'scikit-learn' API to load the Breast Cancer dataset, split it into training and testing sets, train a K-Nearest Neighbors classifier, make predictions, calculate accuracy, and save the trained model.",
        "api": "scikit-learn",
        "output": "#!pip install scikit-learn\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Load the Breast Cancer dataset\nbreast_cancer = load_breast_cancer()\nX, y = breast_cancer.data, breast_cancer.target\n\n#Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n#Initialize the K-Nearest Neighbors classifier\nknn_classifier = KNeighborsClassifier(n_neighbors=3)\n\n#Train the classifier\nknn_classifier.fit(X_train, y_train)\n\n#Make predictions\ny_pred = knn_classifier.predict(X_test)\n\n#Calculate accuracy\naccuracy = (y_pred == y_test).mean()\nprint(f'Accuracy: {accuracy:.2f}')\n\n#Serialize and save the model\njoblib.dump(knn_classifier, 'breast_cancer_classifier.pkl')\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n# 2.8.2. Kernel Density Estimation¶\n\n\nKernel density estimation in scikit-learn is implemented in the\n`KernelDensity` estimator, which uses the\nBall Tree or KD Tree for efficient queries (see Nearest Neighbors for\na discussion of these). Though the above example\nuses a 1D data set for simplicity, kernel density estimation can be\nperformed in any number of dimensions, though in practice the curse of\ndimensionality causes its performance to degrade in high dimensions.\n\n\nIn the following figure, 100 points are drawn from a bimodal distribution,\nand the kernel density estimates are shown for three choices of kernels:\n\n****\n\nIt’s clear how the kernel shape affects the smoothness of the resulting\ndistribution. The scikit-learn kernel density estimator can be used as\nfollows:\n\n```\n>>> from sklearn.neighbors import KernelDensity\n>>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(X)\n>>> kde.score\\_samples(X)\narray([-0.41075698, -0.41075698, -0.41076071, -0.41075698, -0.41075698,\n -0.41076071])\n\n\nHere we have used `kernel='gaussian'`, as seen above.\nMathematically, a kernel is a positive function \\(K(x;h)\\)\nwhich is controlled by the bandwidth parameter \\(h\\).\nGiven this kernel form, the density estimate at a point \\(y\\) within\na group of points \\(x\\_i; i=1\\cdots N\\) is given by:\n\n\\[\\rho\\_K(y) = \\sum\\_{i=1}^{N} K(y - x\\_i; h)\\]\nThe bandwidth here acts as a smoothing parameter, controlling the tradeoff\nbetween bias and variance in the result. A large bandwidth leads to a very\nsmooth (i.e. high-bias) density distribution. A small bandwidth leads\nto an unsmooth (i.e. high-variance) density distribution.\n\n\nThe parameter `bandwidth` controls this smoothing. One can either set\nmanually this parameter or use Scott’s and Silvermann’s estimation\nmethods.\n\n\n`KernelDensity` implements several common kernel\nforms, which are shown in the following figure:\n\n****\n\n**kernels’ mathematical expressions**\nClick for more details\n\n\nThe form of these kernels is as follows:\n\n\n* Gaussian kernel (`kernel = 'gaussian'`)\n\n\n\\(K(x; h) \\propto \\exp(- \\frac{x^2}{2h^2} )\\)\n* Tophat kernel (`kernel = 'tophat'`)\n\n\n\\(K(x; h) \\propto 1\\) if \\(x < h\\)\n* Epanechnikov kernel (`kernel = 'epanechnikov'`)\n\n\n\\(K(x; h) \\propto 1 - \\frac{x^2}{h^2}\\)\n* Exponential kernel (`kernel = 'exponential'`)\n\n\n\\(K(x; h) \\propto \\exp(-x/h)\\)\n* Linear kernel (`kernel = 'linear'`)\n\n\n\\(K(x; h) \\propto 1 - x/h\\) if \\(x < h\\)\n* Cosine kernel (`kernel = 'cosine'`)\n\n\n\\(K(x; h) \\propto \\cos(\\frac{\\pi x}{2h})\\) if \\(x < h\\)\n\n# `sklearn.manifold`.Isomap¶\n\n\n# `sklearn.neighbors`.KNeighborsRegressor¶\n\n\n\n# `sklearn.neighbors`.RadiusNeighborsRegressor¶\n\n\n# `sklearn.metrics`.DistanceMetric¶\n\n\n# `sklearn.metrics.pairwise`.cosine\\_distances¶\n\n\n\n# `sklearn.neighbors`.RadiusNeighborsTransformer¶\n\n\n# `sklearn.neighbors`.NeighborhoodComponentsAnalysis¶\n\n\n\n# 1.2. Linear and Quadratic Discriminant Analysis¶\n\n\n\n# `sklearn.linear\\_model`.Lasso¶\n\n\n\n# `sklearn.linear\\_model`.LassoLarsCV¶\n\n\n\n# `sklearn.linear\\_model`.MultiTaskLasso¶\n\n\n# `sklearn.linear\\_model`.Lars¶\n\n\n# `sklearn.linear\\_model`.lars\\_path¶\n\n\n# `sklearn.linear\\_model`.OrthogonalMatchingPursuit¶\n\n\n\n# `sklearn.linear\\_model`.SGDClassifier¶\n\n\n\n# `sklearn.linear\\_model`.HuberRegressor¶\n\n\n\n# 2.4. Biclustering¶\n\n\n\n# `sklearn.metrics`.homogeneity\\_score¶\n\n\n# `sklearn.metrics`.fowlkes\\_mallows\\_score¶\n\n\n# `sklearn.decomposition`.PCA¶\n\n\n\n# `sklearn.feature\\_extraction.text`.CountVectorizer¶\n\n\n# `sklearn.decomposition`.MiniBatchDictionaryLearning¶\n\n\n\n# `sklearn.decomposition`.FactorAnalysis¶\n\n\n# 1.14. Semi-supervised learning¶\n\n\n# `sklearn.feature\\_selection`.SelectKBest¶\n\n\n\n# `sklearn.feature\\_selection`.SelectPercentile¶\n\n\n# `sklearn.feature\\_selection`.SelectFwe¶\n\n\n# `sklearn.feature\\_selection`.GenericUnivariateSelect¶\n\n\n# `sklearn.feature\\_selection`.r\\_regression¶\n\n\n# `sklearn.feature\\_selection`.f\\_regression¶\n\n\n\n# `sklearn.feature\\_selection`.chi2¶\n\n\n\n# `sklearn.feature\\_selection`.mutual\\_info\\_classif¶\n\n\n\n# `sklearn.feature\\_selection`.SelectFromModel¶\n\n\n\n# `sklearn.feature\\_selection`.SequentialFeatureSelector¶\n\n\n\n# `sklearn.calibration`.CalibratedClassifierCV¶\n\n\n\n# `sklearn.linear\\_model`.LarsCV¶\n\n\n# `sklearn.linear\\_model`.LassoCV¶\n\n\n\n# `sklearn.linear\\_model`.MultiTaskElasticNetCV¶\n\n\n# `sklearn.linear\\_model`.MultiTaskLassoCV¶\n\n\n# `sklearn.linear\\_model`.OrthogonalMatchingPursuitCV¶\n\n\n# `sklearn.linear\\_model`.LassoLarsIC¶\n\n\n# `sklearn.model\\_selection`.train\\_test\\_split¶\n\n\n\n# `sklearn.base`.ClassifierMixin¶\n\n\n\n# `sklearn.model\\_selection`.RepeatedKFold¶\n\n\n\n# `sklearn.model\\_selection`.LeaveOneOut¶\n\n\n# `sklearn.model\\_selection`.ShuffleSplit¶\n\n\n\n# `sklearn.model\\_selection`.GroupKFold¶\n\n\n# `sklearn.model\\_selection`.GroupShuffleSplit¶\n\n\n\n# `sklearn.model\\_selection`.PredefinedSplit¶\n\n\n# `sklearn.model\\_selection`.TimeSeriesSplit¶\n\n\n# 3.1. Cross-validation: evaluating estimator performance¶\n\n\nNone\n\n\n# `sklearn.metrics`.coverage\\_error¶\n\n\n\n# 6.2. Feature extraction¶\n\n\n\n# `sklearn.preprocessing`.MaxAbsScaler¶\n\n\n# `sklearn.preprocessing`.QuantileTransformer¶\n\n\n\n# `sklearn.preprocessing`.FunctionTransformer¶\n\n\n\n# `sklearn.neural\\_network`.BernoulliRBM¶\n\n\n# `sklearn.preprocessing`.binarize¶\n\n\n# `sklearn.preprocessing`.PolynomialFeatures¶\n\n\n\n# 1.9. Naive Bayes¶\n\n\n# 2.3. Clustering¶\n\n\n# 2.5. Decomposing signals in components (matrix factorization problems)¶\n\n\n# `sklearn.feature\\_extraction.image`.img\\_to\\_graph¶\n\n==================\n Document 1 \n----------------\n# Examples using `sklearn.datasets.load\\_diabetes`¶\n\n\nRelease Highlights for scikit-learn 1.2\n\nGradient Boosting regression\n\n\nGradient Boosting regression\n\nPlot individual and voting regression predictions\n\n\nPlot individual and voting regression predictions\n\nModel Complexity Influence\n\n\nModel Complexity Influence\n\nModel-based and sequential feature selection\n\n\nModel-based and sequential feature selection\n\nLasso and Elastic Net\n\n\nLasso and Elastic Net\n\nLasso model selection via information criteria\n\n\nLasso model selection via information criteria\n\nLasso model selection: AIC-BIC / cross-validation\n\n\nLasso model selection: AIC-BIC / cross-validation\n\nLasso path using LARS\n\n\nLasso path using LARS\n\nLinear Regression Example\n\n\nLinear Regression Example\n\nSparsity Example: Fitting only features 1 and 2\n\n\nSparsity Example: Fitting only features 1 and 2\n\nAdvanced Plotting With Partial Dependence\n\n\nAdvanced Plotting With Partial Dependence\n\nImputing missing values before building an estimator\n\n\nImputing missing values before building an estimator\n\nPlotting Cross-Validated Predictions\n\n\nPlotting Cross-Validated Predictions\n\nCross-validation on diabetes Dataset Exercise\n\n\nCross-validation on diabetes Dataset Exercise\n\n# `sklearn.datasets`.load\\_digits¶\n\n\nsklearn.datasets.load\\_digits(*\\**, *n\\_class=10*, *return\\_X\\_y=False*, *as\\_frame=False*)[source]¶\nLoad and return the digits dataset (classification).\n\n\nEach datapoint is a 8x8 image of a digit.\n\n\n|  |  |\n| --- | --- |\n| Classes | 10 |\n| Samples per class | ~180 |\n| Samples total |\n\n==================\n Document 2 \n----------------\n# Examples using `sklearn.datasets.load\\_iris`¶\n\n\nRelease Highlights for scikit-learn 1.2\n\nRelease Highlights for scikit-learn 0.24\n\n\nRelease Highlights for scikit-learn 0.24\n\nRelease Highlights for scikit-learn 0.22\n\n\nRelease Highlights for scikit-learn 0.22\n\nPlot classification probability\n\n\nPlot classification probability\n\nK-means Clustering\n\n\nK-means Clustering\n\nPlot Hierarchical Clustering Dendrogram\n\n\nPlot Hierarchical Clustering Dendrogram\n\nThe Iris Dataset\n\n\nThe Iris Dataset\n\nPlot the decision surface of decision trees trained on the iris dataset\n\n\nPlot the decision surface of decision trees trained on the iris dataset\n\nUnderstanding the decision tree structure\n\n\nUnderstanding the decision tree structure\n\nComparison of LDA and PCA 2D projection of Iris dataset\n\n\nComparison of LDA and PCA 2D projection of Iris dataset\n\nFactor Analysis (with rotation) to visualize patterns\n\n\nFactor Analysis (with rotation) to visualize patterns\n\nIncremental PCA\n\n\nIncremental PCA\n\nPCA example with Iris Data-set\n\n\nPCA example with Iris Data-set\n\nEarly stopping of Gradient Boosting\n\n\nEarly stopping of Gradient Boosting\n\nPlot the decision boundaries of a VotingClassifier\n\n\nPlot the decision boundaries of a VotingClassifier\n\nPlot the decision surfaces of ensembles of trees on the iris dataset\n\n\nPlot the decision surfaces of ensembles of trees on the iris dataset\n\nUnivariate Feature Selection\n\n\nUnivariate Feature Selection\n\nGMM covariances\n\n\nGMM covariances\n\nGaussian process classification (GPC) on iris dataset\n\n\nGaussian process classification (GPC) on iris dataset\n\nLogistic Regression 3-class Classifier\n\n\nLogistic Regression 3-class Classifier\n\nPlot multi-class SGD on the iris dataset\n\n\nPlot multi-class SGD on the iris dataset\n\nRegularization path of L1- Logistic Regression\n\n\nRegularization path of L1- Logistic Regression\n\nIntroducing the set\\_output API\n\n\nIntroducing the set\\_output API\n\nConfusion matrix\n\n\nConfusion matrix\n\nMulticlass Receiver Operating Characteristic (ROC)\n\n\nMulticlass Receiver Operating Characteristic (ROC)\n\nNested versus non-nested cross-validation\n\n\nNested versus non-nested cross-validation\n\nPrecision-Recall\n\n\nPrecision-Recall\n\nReceiver Operating Characteristic (ROC) with cross validation\n\n\nReceiver Operating Characteristic (ROC) with cross validation\n\nTest with permutations the significance of a classification score\n\n\nTest with permutations the significance of a classification score\n\nComparing Nearest Neighbors with and without Neighborhood Components Analysis\n\n\nComparing Nearest Neighbors with and without Neighborhood Components Analysis\n\nNearest Centroid Classification\n\n\nNearest Centroid Classification\n\nNearest Neighbors Classification\n\n\nNearest Neighbors Classification\n\nCompare Stochastic learning strategies for MLPClassifier\n\n\nCompare Stochastic learning strategies for MLPClassifier\n\nConcatenating multiple feature extraction methods\n\n\nConcatenating multiple feature extraction methods\n\nDecision boundary of semi-supervised classifiers versus SVM on the Iris dataset\n\n\nDecision boundary of semi-supervised classifiers versus SVM on the Iris dataset\n\nPlot different SVM classifiers in the iris dataset\n\n\nPlot different SVM classifiers in the iris dataset\n\nRBF SVM parameters\n\n\nRBF SVM parameters\n\nSVM with custom kernel\n\n\nSVM with custom kernel\n\nSVM-Anova: SVM with univariate feature selection\n\n\nSVM-Anova: SVM with univariate feature selection\n\nSVM Exercise\n\n\nSVM Exercise\n\n# `sklearn.datasets`.load\\_linnerud¶\n\n\nsklearn.datasets.load\\_linnerud(*\\**, *return\\_X\\_y=False*, *as\\_frame=False*)[source]¶\nLoad and return the physical exercise Linnerud dataset.\n\n\nThis dataset is suitable for multi-output regression tasks.\n\n\n|  |  |\n| --- | --- |\n| Samples total | 20 |\n| Dimensionality | 3 (for both data and target) |\n|\n\n==================\n Document 3 \n----------------\n 3.1. Cross-validation: evaluating estimator performance¶\n\n\nLearning the parameters of a prediction function and testing it on the\nsame data is a methodological mistake: a model that would just repeat\nthe labels of the samples that it has just seen would have a perfect\nscore but would fail to predict anything useful on yet-unseen data.\nThis situation is called **overfitting**.\nTo avoid it, it is common practice when performing\na (supervised) machine learning experiment\nto hold out part of the available data as a **test set** `X\\_test, y\\_test`.\nNote that the word “experiment” is not intended\nto denote academic use only,\nbecause even in commercial settings\nmachine learning usually starts out experimentally.\nHere is a flowchart of typical cross validation workflow in model training.\nThe best parameters can be determined by\ngrid search techniques.\n\n\n\nIn scikit-learn a random split into training and test sets\ncan be quickly computed with the `train\\_test\\_split` helper function.\nLet’s load the iris data set to fit a linear support vector machine on it:\n\n```\n>>> import numpy as np\n>>> from sklearn.model\\_selection import train\\_test\\_split\n>>> from sklearn import datasets\n>>> from sklearn import svm\n\n>>> X, y = datasets.load\\_iris(return\\_X\\_y=True)\n>>> X.shape, y.shape\n((150, 4), (150,))\n\n\nWe can now quickly sample a training set while holding out 40% of the\ndata for testing (evaluating) our classifier:\n\n```\n>>> X\\_train, X\\_test, y\\_train, y\\_test = train\\_test\\_split(\n...     X, y, test\\_size=0.4, random\\_state=0)\n\n>>> X\\_train.shape, y\\_train.shape\n((90, 4), (90,))\n>>> X\\_test.shape, y\\_test.shape\n((60, 4), (60,))\n\n>>> clf = svm.SVC(kernel='linear', C=1).fit(X\\_train, y\\_train)\n>>> clf.score(X\\_test, y\\_test)\n0.96...\n\n\nWhen evaluating different settings (“hyperparameters”) for estimators,\nsuch as the `C` setting that must be manually set for an SVM,\nthere is still a risk of overfitting *on the test set*\nbecause the parameters can be tweaked until the estimator performs optimally.\nThis way, knowledge about the test set can “leak” into the model\nand evaluation metrics no longer report on generalization performance.\nTo solve this problem, yet another part of the dataset can be held out\nas a so-called “validation set”: training proceeds on the training set,\nafter which evaluation is done on the validation set,\nand when the experiment seems to be successful,\nfinal evaluation can be done on the test set.\n\n\nHowever, by partitioning the available data into three sets,\nwe drastically reduce the number of samples\nwhich can be used for learning the model,\nand the results can depend on a particular random choice for the pair of\n(train, validation) sets.\n\n\nA solution to this problem is a procedure called\ncross-validation\n(CV for short).\nA test set should still be held out for final evaluation,\nbut the validation set is no longer needed when doing CV.\nIn the basic approach, called *k*-fold CV,\nthe training set is split into *k* smaller sets\n(other approaches are described below,\nbut generally follow the same principles).\nThe following procedure is followed for each of the *k* “folds”:\n\n> \n> * A model is trained using \\(k-1\\) of the folds as training data;\n> * the resulting model is validated on the remaining part of the data\n> (i.e., it is used as a test set to compute a performance measure\n> such as accuracy).\n> \n> \n> \n\n\nThe performance measure reported by *k*-fold cross-validation\nis then the average of the values computed in the loop.\nThis approach can be computationally expensive,\nbut does not waste too much data\n(as is the case when fixing an arbitrary validation set),\nwhich is a major advantage in problems such as inverse inference\nwhere the number of samples is very small.\n\n\n\n## 3.1.1. Computing cross-validated metrics¶\n\n\nThe simplest way to use cross-validation is to call the\n`cross\\_val\\_score` helper function on the estimator and the dataset.\n\n\nThe following example demonstrates how to estimate the accuracy of a linear\nkernel support vector machine on the iris dataset\n\n==================\n Document 4 \n----------------\n## 1.11.2.1. Random Forests¶\n\n\nIn random forests (see `RandomForestClassifier` and\n`RandomForestRegressor` classes), each tree in the ensemble is built\nfrom a sample drawn with replacement (i.e., a bootstrap sample) from the\ntraining set.\n\n\nFurthermore, when splitting each node during the construction of a tree, the\nbest split is found either from all input features or a random subset of size\n`max\\_features`. (See the parameter tuning guidelines for more details).\n\n\nThe purpose of these two sources of randomness is to decrease the variance of\nthe forest estimator. Indeed, individual decision trees typically exhibit high\nvariance and tend to overfit. The injected randomness in forests yield decision\ntrees with somewhat decoupled prediction errors. By taking an average of those\npredictions, some errors can cancel out. Random forests achieve a reduced\nvariance by combining diverse trees, sometimes at the cost of a slight increase\nin bias. In practice the variance reduction is often significant hence yielding\nan overall better model.\n\n\nIn contrast to the original publication [B2001], the scikit-learn\nimplementation combines classifiers by averaging their probabilistic\nprediction, instead of letting each classifier vote for a single class.\n\n\nA competitive alternative to random forests are\nHistogram-Based Gradient Boosting (HGBT) models:\n\n\n* Building trees: Random forests typically rely on deep trees (that overfit\nindividually) which uses much computational resources, as they require\nseveral splittings and evaluations of candidate splits. Boosting models\nbuild shallow trees (that underfit individually) which are faster to fit\nand predict.\n* Sequential boosting: In HGBT, the decision trees are built sequentially,\nwhere each tree is trained to correct the errors made by the previous ones.\nThis allows them to iteratively improve the model’s performance using\nrelatively few trees. In contrast, random forests use a majority vote to\npredict the outcome, which can require a larger number of trees to achieve\nthe same level of accuracy.\n* Efficient binning: HGBT uses an efficient binning algorithm that can handle\nlarge datasets with a high number of features. The binning algorithm can\npre-process the data to speed up the subsequent tree construction (see\nWhy it’s faster). In contrast, the scikit-learn\nimplementation of random forests does not use binning and relies on exact\nsplitting, which can be computationally expensive.\n\n\nOverall, the computational cost of HGBT versus RF depends on the specific\ncharacteristics of the dataset and the modeling task. It’s a good idea\nto try both models and compare their performance and computational efficiency\non your specific problem to determine which model is the best fit.\n\n### 1.11.2.2. Extremely Randomized Trees¶\n\n\nIn extremely randomized trees (see `ExtraTreesClassifier`\nand `ExtraTreesRegressor` classes), randomness goes one step\nfurther in the way splits are computed. As in random forests, a random\nsubset of candidate features is used, but instead of looking for the\nmost discriminative\n\n==================\n Document 5 \n----------------\n### 3.1.2.1.1. K-fold¶\n\n\n`KFold` divides all the samples in \\(k\\) groups of samples,\ncalled folds (if \\(k = n\\), this is equivalent to the *Leave One\nOut* strategy), of equal sizes (if possible). The prediction function is\nlearned using \\(k - 1\\) folds, and the fold left out is used for test.\n\n\nExample of 2-fold cross-validation on a dataset with 4 samples:\n\n```\n>>> import numpy as np\n>>> from sklearn.model\\_selection import KFold\n\n>>> X = [\"a\", \"b\", \"c\", \"d\"]\n>>> kf = KFold(n\\_splits=2)\n>>> for train, test in kf.split(X):\n...     print(\"%s %s\" % (train, test))\n[2 3] [0 1]\n[0 1] [2 3]\n\n\nHere is a visualization of the cross-validation behavior. Note that\n`KFold` is not affected by classes or groups.\n\n\n\nEach fold is constituted by two arrays: the first one is related to the\n*training set*, and the second one to the *test set*.\nThus, one can create the training/test sets using numpy indexing:\n\n```\n>>> X = np.array([[0., 0.], [1., 1.], [-1., -1.], [2., 2.]])\n>>> y = np.array([0, 1, 0, 1])\n>>> X\\_train, X\\_test, y\\_train, y\\_test = X[train], X[test], y[train], y[test]\n\n#### 3.1.2.1.2. Repeated K-Fold¶\n\n\n`RepeatedKFold` repeats K-Fold n times. It can be used when one\nrequires to run `KFold` n times, producing different splits in\neach repetition.\n\n\nExample of 2-fold K-Fold repeated 2 times:\n\n```\n>>> import numpy as np\n>>> from sklearn.model\\_selection import RepeatedKFold\n>>> X =\n\n==================\n Document 6 \n----------------\n 1.6. Nearest Neighbors¶\n\n\n`sklearn.neighbors` provides functionality for unsupervised and\nsupervised neighbors-based learning methods. Unsupervised nearest neighbors\nis the foundation of many other learning methods,\nnotably manifold learning and spectral clustering. Supervised neighbors-based\nlearning comes in two flavors: classification for data with\ndiscrete labels, and regression for data with continuous labels.\n\n\nThe principle behind nearest neighbor methods is to find a predefined number\nof training samples closest in distance to the new point, and\npredict the label from these. The number of samples can be a user-defined\nconstant (k-nearest neighbor learning), or vary based\non the local density of points (radius-based neighbor learning).\nThe distance can, in general, be any metric measure: standard Euclidean\ndistance is the most common choice.\nNeighbors-based methods are known as *non-generalizing* machine\nlearning methods, since they simply “remember” all of its training data\n(possibly transformed into a fast indexing structure such as a\nBall Tree or KD Tree).\n\n\nDespite its simplicity, nearest neighbors has been successful in a\nlarge number of classification and regression problems, including\nhandwritten digits and satellite image scenes. Being a non-parametric method,\nit is often successful in classification situations where the decision\nboundary is very irregular.\n\n\nThe classes in `sklearn.neighbors` can handle either NumPy arrays or\n`scipy.sparse` matrices as input. For dense matrices, a large number of\npossible distance metrics are supported. For sparse matrices, arbitrary\nMinkowski metrics are supported for searches.\n\n\nThere are many learning routines which rely on nearest neighbors at their\ncore. One example is kernel density estimation,\ndiscussed in the density estimation section.\n## 1.6.1. Unsupervised Nearest Neighbors¶\n\n\n`NearestNeighbors` implements unsupervised nearest neighbors learning.\nIt acts as a uniform interface to three different nearest neighbors\nalgorithms: `BallTree`, `KDTree`, and a\nbrute-force algorithm based on routines in `sklearn.metrics.pairwise`.\nThe choice of neighbors search algorithm is controlled through the keyword\n`'algorithm'`,\n\n==================\n Document 7 \n----------------\n# 1.6.2. Nearest Neighbors Classification¶\n\n\nNeighbors-based classification is a type of *instance-based learning* or\n*non-generalizing learning*: it does not attempt to construct a general\ninternal model, but simply stores instances of the training data.\nClassification is computed from a simple majority vote of the nearest\nneighbors of each point: a query point is assigned the data class which\nhas the most representatives within the nearest neighbors of the point.\n\n\nscikit-learn implements two different nearest neighbors classifiers:\n`KNeighborsClassifier` implements learning based on the \\(k\\)\nnearest neighbors of each query point, where \\(k\\) is an integer value\nspecified by the user. `RadiusNeighborsClassifier` implements learning\nbased on the number of neighbors within a fixed radius \\(r\\) of each\ntraining point, where \\(r\\) is a floating-point value specified by\nthe user.\n\n\nThe \\(k\\)-neighbors classification in `KNeighborsClassifier`\nis the most commonly used technique. The optimal choice of the value \\(k\\)\nis highly data-dependent: in general a larger \\(k\\) suppresses the effects\nof noise, but makes the classification boundaries less distinct.\n\n\nIn cases where the data is not uniformly sampled, radius-based neighbors\nclassification in `RadiusNeighborsClassifier` can be a better choice.\nThe user specifies a fixed radius \\(r\\), such that points in sparser\nneighborhoods use fewer nearest neighbors for the classification. For\nhigh-dimensional parameter spaces, this method becomes less effective due\nto the so-called “curse of dimensionality”.\n\n\nThe basic nearest neighbors classification uses uniform weights: that is, the\nvalue assigned to a query point is computed from a simple majority vote of\nthe nearest neighbors. Under some circumstances, it is better to weight the\nneighbors such that nearer neighbors contribute more to the fit. This can\nbe accomplished through the `weights` keyword. The default value,\n`weights = 'uniform'`, assigns uniform weights to each neighbor.\n`weights = 'distance'` assigns weights proportional to the inverse of the\ndistance from the query point. Alternatively, a user-defined function of the\ndistance can be supplied to compute the weights.\n\n****\n\n## 1.6.3. Nearest Neighbors Regression¶\n\n\nNeighbors-based regression can be used in cases where the data labels are\ncontinuous rather than discrete variables. The label assigned to a query\npoint is computed based on the mean of the labels of its nearest neighbors.\n\n\nscikit-learn implements"
    },
    {
        "instruction": "Create a Python program that uses the 'scikit-learn' API to load the Breast Cancer dataset, split it into training and testing sets, train a K-Means clustering model, make predictions, and save the trained model.",
        "api": "scikit-learn",
        "output": "#!pip install scikit-learn\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.cluster import KMeans\n\n#Load the Breast Cancer dataset\nbreast_cancer = load_breast_cancer()\nX, y = breast_cancer.data, breast_cancer.target\n\n#Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n#Initialize the K-Means clustering model\nkmeans = KMeans(n_clusters=2)\n\n#Train the model\nkmeans.fit(X_train)\n\n#Make predictions\ny_pred = kmeans.predict(X_test)\n\n#Serialize and save the model\njoblib.dump(kmeans, 'breast_cancer_kmeans_model.pkl')\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n# 2.3.1. Overview of clustering methods¶\n\n\n\nA comparison of the clustering algorithms in scikit-learn¶\n\n\n| Method name | Parameters | Scalability | Usecase | Geometry (metric used) |\n| --- | --- | --- | --- | --- |\n| K-Means | number of clusters | Very large `n\\_samples`, medium `n\\_clusters` with\nMiniBatch code | General-purpose, even cluster size, flat geometry,\nnot too many clusters, inductive | Distances between points |\n| Affinity propagation | damping, sample preference | Not scalable with n\\_samples | Many clusters, uneven cluster size, non-flat geometry, inductive | Graph distance (e.g. nearest-neighbor graph) |\n| Mean-shift | bandwidth | Not scalable with `n\\_samples` | Many clusters, uneven cluster size, non-flat geometry, inductive | Distances between points |\n| Spectral clustering | number of clusters | Medium `n\\_samples`, small `n\\_clusters` | Few clusters, even cluster size, non-flat geometry, transductive | Graph distance (e.g. nearest-neighbor graph) |\n| Ward hierarchical clustering | number of clusters or distance threshold | Large `n\\_samples` and `n\\_clusters` | Many clusters, possibly connectivity constraints, transductive | Distances between points |\n| Agglomerative clustering | number of clusters or distance threshold, linkage type, distance | Large `n\\_samples` and `n\\_clusters` | Many clusters, possibly connectivity constraints, non Euclidean\ndistances, transductive | Any pairwise distance |\n| DBSCAN | neighborhood size | Very large `n\\_samples`, medium `n\\_clusters` | Non-flat geometry, uneven cluster sizes, outlier removal,\ntransductive | Distances between nearest points |\n| HDBSCAN | minimum cluster membership, minimum point neighbors | large `n\\_samples`, medium `n\\_clusters` | Non-flat geometry, uneven cluster sizes, outlier removal,\ntransductive, hierarchical, variable cluster density | Distances between nearest points |\n| OPTICS | minimum cluster membership | Very large `n\\_samples`, large `n\\_clusters` | Non-flat geometry, uneven cluster sizes, variable cluster density,\noutlier removal, transductive | Distances between points |\n| Gaussian mixtures | many | Not scalable | Flat geometry, good for density estimation, inductive | Mahalanobis distances to centers |\n| BIRCH | branching factor, threshold, optional global clusterer. | Large `n\\_clusters` and `n\\_samples` | Large dataset, outlier removal, data reduction, inductive | Euclidean distance between points |\n| Bisecting K-Means | number of clusters | Very large `n\\_samples`, medium `n\\_clusters` | General-purpose, even cluster size, flat geometry,\nno empty clusters, inductive, hierarchical | Distances between points |\n\n\nNon-flat geometry clustering is useful when the clusters have a specific\nshape, i.e. a non-flat manifold, and the standard euclidean distance is\nnot the right metric. This case arises in the two top rows of the figure\nabove.\n\n\nGaussian mixture models, useful for clustering, are described in\nanother chapter of the documentation dedicated to\nmixture models. KMeans can be seen as a special case of Gaussian mixture\nmodel with equal covariance per component.\n\n\nTransductive clustering methods (in contrast to\ninductive clustering methods) are not designed to be applied to new,\nunseen data.\n\n## 2.3.2. K-means¶\n\n\nThe `KMeans` algorithm clusters data by trying to separate samples in n\ngroups of equal variance, minimizing a criterion known as the *inertia* or\nwithin-cluster sum-of-squares (see below). This algorithm requires the number\nof clusters to be specified. It scales well\n\n==================\n Document 1 \n----------------\n# 2.8.2. Kernel Density Estimation¶\n\n\nKernel density estimation in scikit-learn is implemented in the\n`KernelDensity` estimator, which uses the\nBall Tree or KD Tree for efficient queries (see Nearest Neighbors for\na discussion of these). Though the above example\nuses a 1D data set for simplicity, kernel density estimation can be\nperformed in any number of dimensions, though in practice the curse of\ndimensionality causes its performance to degrade in high dimensions.\n\n\nIn the following figure, 100 points are drawn from a bimodal distribution,\nand the kernel density estimates are shown for three choices of kernels:\n\n****\n\nIt’s clear how the kernel shape affects the smoothness of the resulting\ndistribution. The scikit-learn kernel density estimator can be used as\nfollows:\n\n```\n>>> from sklearn.neighbors import KernelDensity\n>>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(X)\n>>> kde.score\\_samples(X)\narray([-0.41075698, -0.41075698, -0.41076071, -0.41075698, -0.41075698,\n -0.41076071])\n\n\nHere we have used `kernel='gaussian'`, as seen above.\nMathematically, a kernel is a positive function \\(K(x;h)\\)\nwhich is controlled by the bandwidth parameter \\(h\\).\nGiven this kernel form, the density estimate at a point \\(y\\) within\na group of points \\(x\\_i; i=1\\cdots N\\) is given by:\n\n\\[\\rho\\_K(y) = \\sum\\_{i=1}^{N} K(y - x\\_i; h)\\]\nThe bandwidth here acts as a smoothing parameter, controlling the tradeoff\nbetween bias and variance in the result. A large bandwidth leads to a very\nsmooth (i.e. high-bias) density distribution. A small bandwidth leads\nto an unsmooth (i.e. high-variance) density distribution.\n\n\nThe parameter `bandwidth` controls this smoothing. One can either set\nmanually this parameter or use Scott’s and Silvermann’s estimation\nmethods.\n\n\n`KernelDensity` implements several common kernel\nforms, which are shown in the following figure:\n\n****\n\n**kernels’ mathematical expressions**\nClick for more details\n\n\nThe form of these kernels is as follows:\n\n\n* Gaussian kernel (`kernel = 'gaussian'`)\n\n\n\\(K(x; h) \\propto \\exp(- \\frac{x^2}{2h^2} )\\)\n* Tophat kernel (`kernel = 'tophat'`)\n\n\n\\(K(x; h) \\propto 1\\) if \\(x < h\\)\n* Epanechnikov kernel (`kernel = 'epanechnikov'`)\n\n\n\\(K(x; h) \\propto 1 - \\frac{x^2}{h^2}\\)\n* Exponential kernel (`kernel = 'exponential'`)\n\n\n\\(K(x; h) \\propto \\exp(-x/h)\\)\n* Linear kernel (`kernel = 'linear'`)\n\n\n\\(K(x; h) \\propto 1 - x/h\\) if \\(x < h\\)\n* Cosine kernel (`kernel = 'cosine'`)\n\n\n\\(K(x; h) \\propto \\cos(\\frac{\\pi x}{2h})\\) if \\(x < h\\)\n\n# `sklearn.manifold`.Isomap¶\n\n\n# `sklearn.neighbors`.KNeighborsRegressor¶\n\n\n\n# `sklearn.neighbors`.RadiusNeighborsRegressor¶\n\n\n# `sklearn.metrics`.DistanceMetric¶\n\n\n# `sklearn.metrics.pairwise`.cosine\\_distances¶\n\n\n\n# `sklearn.neighbors`.RadiusNeighborsTransformer¶\n\n\n# `sklearn.neighbors`.NeighborhoodComponentsAnalysis¶\n\n\n\n# 1.2. Linear and Quadratic Discriminant Analysis¶\n\n\n\n# `sklearn.linear\\_model`.Lasso¶\n\n\n\n# `sklearn.linear\\_model`.LassoLarsCV¶\n\n\n\n# `sklearn.linear\\_model`.MultiTaskLasso¶\n\n\n# `sklearn.linear\\_model`.Lars¶\n\n\n# `sklearn.linear\\_model`.lars\\_path¶\n\n\n# `sklearn.linear\\_model`.OrthogonalMatchingPursuit¶\n\n\n\n# `sklearn.linear\\_model`.SGDClassifier¶\n\n\n\n# `sklearn.linear\\_model`.HuberRegressor¶\n\n\n\n# 2.4. Biclustering¶\n\n\n\n# `sklearn.metrics`.homogeneity\\_score¶\n\n\n# `sklearn.metrics`.fowlkes\\_mallows\\_score¶\n\n\n# `sklearn.decomposition`.PCA¶\n\n\n\n# `sklearn.feature\\_extraction.text`.CountVectorizer¶\n\n\n# `sklearn.decomposition`.MiniBatchDictionaryLearning¶\n\n\n\n# `sklearn.decomposition`.FactorAnalysis¶\n\n\n# 1.14. Semi-supervised learning¶\n\n\n# `sklearn.feature\\_selection`.SelectKBest¶\n\n\n\n# `sklearn.feature\\_selection`.SelectPercentile¶\n\n\n# `sklearn.feature\\_selection`.SelectFwe¶\n\n\n# `sklearn.feature\\_selection`.GenericUnivariateSelect¶\n\n\n# `sklearn.feature\\_selection`.r\\_regression¶\n\n\n# `sklearn.feature\\_selection`.f\\_regression¶\n\n\n\n# `sklearn.feature\\_selection`.chi2¶\n\n\n\n# `sklearn.feature\\_selection`.mutual\\_info\\_classif¶\n\n\n\n# `sklearn.feature\\_selection`.SelectFromModel¶\n\n\n\n# `sklearn.feature\\_selection`.SequentialFeatureSelector¶\n\n\n\n# `sklearn.calibration`.CalibratedClassifierCV¶\n\n\n\n# `sklearn.linear\\_model`.LarsCV¶\n\n\n# `sklearn.linear\\_model`.LassoCV¶\n\n\n\n# `sklearn.linear\\_model`.MultiTaskElasticNetCV¶\n\n\n# `sklearn.linear\\_model`.MultiTaskLassoCV¶\n\n\n# `sklearn.linear\\_model`.OrthogonalMatchingPursuitCV¶\n\n\n# `sklearn.linear\\_model`.LassoLarsIC¶\n\n\n# `sklearn.model\\_selection`.train\\_test\\_split¶\n\n\n\n# `sklearn.base`.ClassifierMixin¶\n\n\n\n# `sklearn.model\\_selection`.RepeatedKFold¶\n\n\n\n# `sklearn.model\\_selection`.LeaveOneOut¶\n\n\n# `sklearn.model\\_selection`.ShuffleSplit¶\n\n\n\n# `sklearn.model\\_selection`.GroupKFold¶\n\n\n# `sklearn.model\\_selection`.GroupShuffleSplit¶\n\n\n\n# `sklearn.model\\_selection`.PredefinedSplit¶\n\n\n# `sklearn.model\\_selection`.TimeSeriesSplit¶\n\n\n# 3.1. Cross-validation: evaluating estimator performance¶\n\n\nNone\n\n\n# `sklearn.metrics`.coverage\\_error¶\n\n\n\n# 6.2. Feature extraction¶\n\n\n\n# `sklearn.preprocessing`.MaxAbsScaler¶\n\n\n# `sklearn.preprocessing`.QuantileTransformer¶\n\n\n\n# `sklearn.preprocessing`.FunctionTransformer¶\n\n\n\n# `sklearn.neural\\_network`.BernoulliRBM¶\n\n\n# `sklearn.preprocessing`.binarize¶\n\n\n# `sklearn.preprocessing`.PolynomialFeatures¶\n\n\n\n# 1.9. Naive Bayes¶\n\n\n# 2.3. Clustering¶\n\n\n# 2.5. Decomposing signals in components (matrix factorization problems)¶\n\n\n# `sklearn.feature\\_extraction.image`.img\\_to\\_graph¶\n\n==================\n Document 2 \n----------------\n# 2.3.2. K-means¶\n\n\nThe `KMeans` algorithm clusters data by trying to separate samples in n\ngroups of equal variance, minimizing a criterion known as the *inertia* or\nwithin-cluster sum-of-squares (see below). This algorithm requires the number\nof clusters to be specified. It scales well to large numbers of samples and has\nbeen used across a large range of application areas in many different fields.\n\n\nThe k-means algorithm divides a set of \\(N\\) samples \\(X\\) into\n\\(K\\) disjoint clusters \\(C\\), each described by the mean \\(\\mu\\_j\\)\nof the samples in the cluster. The means are commonly called the cluster\n“centroids”; note that they are not, in general, points from \\(X\\),\nalthough they live in the same space.\n\n\nThe K-means algorithm aims to choose centroids that minimise the **inertia**,\nor **within-cluster sum-of-squares criterion**:\n\n\\[\\sum\\_{i=0}^{n}\\min\\_{\\mu\\_j \\in C}(||x\\_i - \\mu\\_j||^2)\\]\nInertia can be recognized as a measure of how internally coherent clusters are.\nIt suffers from various drawbacks:\n\n\n* Inertia makes the assumption that clusters are convex and isotropic,\nwhich is not always the case. It responds poorly to elongated clusters,\nor manifolds with irregular shapes.\n* Inertia is not a normalized metric: we just know that lower values are\nbetter and zero is optimal. But in very high-dimensional spaces, Euclidean\ndistances tend to become inflated\n(this is an instance of the so-called “curse of dimensionality”).\nRunning a dimensionality reduction algorithm such as Principal component analysis (PCA) prior to\nk-means clustering can alleviate this problem and speed up the\ncomputations.\n\n\n\nK-means is often referred to as Lloyd’s algorithm. In basic terms, the\nalgorithm has three steps. The first step chooses the initial centroids, with\nthe most basic method being to choose \\(k\\) samples from the dataset\n\\(X\\). After initialization, K-means consists of looping between the\ntwo other steps. The first step assigns each sample to its nearest centroid.\nThe second step creates new centroids by taking the mean value of all of the\nsamples assigned to each previous centroid. The difference between the old\nand the new centroids are computed and the algorithm repeats these last two\nsteps until this value is less than a threshold. In other words, it repeats\nuntil the centroids do not move significantly.\n\n\n\nK-means is equivalent to the expectation-maximization algorithm\nwith a small, all-equal, diagonal covariance matrix.\n\n\nThe algorithm can also be understood through the concept of Voronoi diagrams. First the Voronoi diagram of\nthe points is calculated using the current centroids. Each segment in the\nVoronoi diagram becomes a separate cluster. Secondly, the centroids are updated\nto the mean of each segment. The algorithm then repeats this until a stopping\ncriterion is fulfilled. Usually, the algorithm stops when the relative decrease\nin the objective function between iterations is less than the given tolerance\nvalue. This is not the case in this implementation: iteration stops when\ncentroids move less than the tolerance.\n\n\nGiven enough time, K-means will always converge, however this may be to a local\nminimum. This is highly dependent on the initialization of the centroids.\nAs a result, the computation is often done several times, with different\ninitializations of the centroids. One method to help address this issue is the\nk-means++ initialization scheme, which has been implemented in scikit-learn\n(use the `init='k-means++'` parameter). This initializes the centroids to be\n(generally) distant from each other, leading to probably better results than\nrandom initialization, as shown in the reference.\n\n\nK-means++ can also be called independently to select seeds for other\nclustering algorithms, see `sklearn.cluster.kmeans\\_plusplus` for details\nand example usage.\n\n\nThe algorithm supports sample weights, which can be given by a parameter\n`sample\\_weight`. This allows to assign more weight to some samples when\ncomputing cluster centers and values of inertia. For example, assigning a\nweight of 2 to a sample is equivalent to adding a duplicate of that sample\nto the dataset \\(X\\).\n\n\nK-means can be used for vector quantization. This is achieved using the\ntransform method of a trained model of `KMeans`.\n\n### 2.3.2.1. Low-level parallelism¶\n\n\n`KMeans` benefits from OpenMP based parallelism through Cython. Small\nchunks of data (256 samples) are processed in parallel, which in addition\nyields a low memory footprint. For more details on how to control the number of\nthreads, please refer to our Parallelism notes.\n\n\n### 2.3.2.2. Mini Batch K-Means¶\n\n\nThe `MiniBatchKMeans` is a variant of the `KMeans` algorithm\nwhich uses mini-batches to reduce the computation time, while still attempting\nto optimise the same objective function. Mini-batches are subsets of the input\ndata, randomly sampled in each training iteration."
    },
    {
        "instruction": "Create a Python program using the 'scipy' API to apply various image transformations to a sample image. The program should perform operations like shifting and rotating the image and display the original and transformed images.",
        "api": "scipy",
        "output": "#!pip install scipy matplotlib\nimport scipy as sp\nimport matplotlib.pyplot as plt\n\nface = sp.datasets.face(gray=True)\n\nshifted_face = sp.ndimage.shift(face, (50, 50))\nrotated_face = sp.ndimage.rotate(face, 30)\n\nplt.figure(figsize=(15, 3))\nplt.subplot(151)\nplt.imshow(face, cmap=plt.cm.gray)\nplt.axis(\"off\")\n\nplt.subplot(152)\nplt.imshow(shifted_face, cmap=plt.cm.gray)\nplt.axis(\"off\")\n\nplt.subplot(153)\nplt.imshow(rotated_face, cmap=plt.cm.gray)\nplt.axis(\"off\")\n\nplt.subplots_adjust(wspace=0.05, left=0.01, bottom=0.01, right=0.99, top=0.99)\n\nplt.show()",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# SciPy API#\n\n## Importing from SciPy#\n\n\nIn Python, the distinction between what is the public API of a library and what\nare private implementation details is not always clear. Unlike in other\nlanguages like Java, it is possible in Python to access âprivateâ functions or\nobjects.\n\n==================\n Document 1 \n----------------\n# Usage information#\n\n\nThere are seven available sparse matrix types:\n\n> \n> 1. csc\\_matrix: Compressed Sparse Column format\n> 2. csr\\_matrix: Compressed Sparse Row format\n> 3. bsr\\_matrix: Block Sparse Row format\n> 4. lil\\_matrix: List of Lists format\n> 5. dok\\_matrix: Dictionary of Keys format\n> 6. coo\\_matrix: COOrdinate format (aka IJV, triplet format)\n> 7. dia\\_matrix: DIAgonal format\n> \n> \n> \n\n\nTo construct a matrix efficiently, use either dok\\_matrix or lil\\_matrix.\nThe lil\\_matrix class supports basic slicing and fancy indexing with a\nsimilar syntax to NumPy arrays. As illustrated below, the COO format\nmay also be used to efficiently construct matrices. Despite their\nsimilarity to NumPy arrays, it is **strongly discouraged** to use NumPy\nfunctions directly on these matrices because NumPy may not properly convert\nthem for computations, leading to unexpected (and incorrect) results. If you\ndo want to apply a NumPy function to these matrices, first check if SciPy has\nits own implementation for the given sparse matrix class, or **convert the\nsparse matrix to a NumPy array** (e.g., using the *toarray()* method of the\nclass) first before applying the method.\n\n\nTo perform manipulations such as multiplication or inversion, first\nconvert the matrix to either CSC or CSR format. The lil\\_matrix format is\nrow-based, so conversion to CSR is efficient, whereas conversion to CSC\nis less so.\n\n\nAll conversions among the CSR, CSC, and COO formats are efficient,\nlinear-time operations.\n### Matrix vector product#\n\n\nTo do a vector product between a sparse matrix and a vector simply use\nthe matrix *dot* method, as described in its docstring:\n\n```\n>>> import numpy as np\n>>> from scipy.sparse import csr\\_matrix\n>>> A = csr\\_matrix([[1, 2, 0], [0, 0,\n\n==================\n Document 2 \n----------------\n Miscellaneous routines (`scipy.misc`)#\n\nDeprecated since version 1.10.0: This module is deprecated and will be completely\nremoved in SciPy v2.0.0.\n\nVarious utilities that donât have another home.\n\n\n|  |  |\n| --- | --- |\n| `ascent`() | Get an 8-bit grayscale bit-depth, 512 x 512 derived image for easy use in demos |\n| `central\\_diff\\_weights`(Np[,Â ndiv]) | Return weights for an Np-point central derivative. |\n| `derivative`(func,Â x0[,Â dx,Â n,Â args,Â order]) | Find the nth derivative of a function at a point. |\n| `face`([gray]) | Get a 1024 x 768, color image of a raccoon face. |\n| `electrocardiogram`() | Load an electrocardiogram as an example for a 1-D signal. |\n\n# Multidimensional image processing (`scipy.ndimage`)#\n\n\nThis package contains various functions for multidimensional image\nprocessing.\n\n## Filters#\n\n\n|  |  |\n| --- | --- |\n| `convolve`(input,Â weights[,Â output,Â mode,Â ...]) | Multidimensional convolution. |\n| `convolve1d`(input,Â weights[,Â axis,Â output,Â ...]) | Calculate a 1-D convolution along the given axis. |\n| `correlate`(input,Â weights[,Â output,Â mode,Â ...]) | Multidimensional correlation. |\n| `correlate1d`(input,Â weights[,Â axis,Â output,Â ...]) | Calculate a 1-D correlation along the given\n\n==================\n Document 3 \n----------------\n# Importing from SciPy#\n\n\nIn Python, the distinction between what is the public API of a library and what\nare private implementation details is not always clear. Unlike in other\nlanguages like Java, it is possible in Python to access âprivateâ functions or\nobjects. Occasionally this may be convenient, but be aware that if you do so\nyour code may break without warning in future releases. Some widely understood\nrules for what is and isnât public in Python are:\n\n\n* Methods / functions / classes and module attributes whose names begin with a\nleading underscore are private.\n* If a class name begins with a leading underscore, none of its members are\npublic, whether or not they begin with a leading underscore.\n* If a module name in a package begins with a leading underscore none of\nits members are public, whether or not they begin with a leading underscore.\n* If a module or package defines `\\_\\_all\\_\\_`, that authoritatively defines the\npublic interface.\n* If a module or package doesnât define `\\_\\_all\\_\\_`, then all names that donât\nstart with a leading underscore are public.\n\nNote\n\n\nReading the above guidelines one could draw the conclusion that every\nprivate module or object starts with an underscore. This is not the\ncase; the presence of underscores do mark something as private, but\nthe absence of underscores do not mark something as public.\n\nIn SciPy there are modules whose names donât start with an underscore, but that\nshould be considered private. To clarify which modules these are, we define\nbelow what the public API is for SciPy, and give some recommendations for how\nto import modules/functions/objects from SciPy.\n\n## Guidelines for importing functions from SciPy#\n\n\nThe scipy namespace itself only contains functions imported from numpy. These\nfunctions still exist for backwards compatibility, but should be imported from\nnumpy directly.\n\n\nEverything in the namespaces of scipy submodules is public. In general, it is\nrecommended\n\n==================\n Document 4 \n----------------\n# B-splines#\n\n\n|  |  |\n| --- | --- |\n| `bspline`(x,Â n) | `bspline` is deprecated! `scipy.signal.bspline` is deprecated in SciPy 1.11 and will be removed in SciPy 1.13. |\n| `cubic`(x) | `cubic` is deprecated! `scipy.signal.cubic` is deprecated in SciPy 1.11 and will be removed in SciPy 1.13. |\n| `quadratic`(x) | `quadratic` is deprecated! `scipy.signal.quadratic` is deprecated in SciPy 1.11 and will be removed in SciPy 1.13. |\n| `gauss\\_spline`(x,Â n) | Gaussian approximation to B-spline basis function of order n. |\n| `cspline1d`(signal[,Â lamb]) | Compute cubic spline coefficients for rank-1 array. |\n| `qspline1d`(signal[,Â lamb]) | Compute quadratic spline coefficients for rank-1 array. |\n| `cspline2d`(input[,Â lambda,Â precision]) | Coefficients for 2-D cubic (3rd order) B-spline. |\n| `qspline2d`(input[,Â lambda,Â precision]) | Coefficients for 2-D quadratic (2nd order) B-spline: |\n| `cspline1d\\_eval`(cj,Â newx[,Â dx,Â x0]) | Evaluate a cubic spline at the new set of points. |\n| `qspline1d\\_eval`(cj,Â newx[,Â dx,Â x0]) | Evaluate a quadratic spline at the new set of points. |\n| `spline\\_filter`(Iin[,Â lmbda]) | Smoothing spline (cubic) filtering of a rank-2 array. |\n\n## Filtering#\n\n\n|  |  |\n| --- | --- |\n| `order\\_filter`(a,Â domain,Â rank) | Perform an order filter on an N-D array. |\n| `medfilt`(volume[,Â kernel\\_size]) | Perform a median filter on an N-dimensional array. |\n| `medfilt2d`(input[,Â kernel\\_size]) | Median filter a 2-dimensional array. |\n|\n\n==================\n Document 5 \n----------------\n# How dataset retrieval and storage works#\n\n\nSciPy dataset files are stored within individual github repositories under the\nSciPy GitHub organization, following a naming convention as\n`'dataset-<name>'`, for example `scipy.datasets.face` files live at\nhttps://github.com/scipy/dataset-face. The `scipy.datasets` submodule utilizes\nand depends on Pooch, a Python\npackage built to simplify fetching data files. Pooch uses these repos to\nretrieve the respective dataset files when calling the dataset function.\n\n\nA registry of all the datasets, essentially a mapping of filenames with their\nSHA256 hash and repo urls are maintained, which Pooch uses to handle and verify\nthe downloads on function call. After downloading the dataset once, the files\nare saved in the system cache directory under `'scipy-data'`.\n\n\nDataset cache locations may vary on different platforms.\n\n\nFor macOS:\n\n```\n'~/Library/Caches/scipy-data'\n\n\nFor Linux and other Unix-like platforms:\n\n```\n'~/.cache/scipy-data'  # or the value of the XDG\\_CACHE\\_HOME env var, if defined\n\n\nFor Windows:\n\n```\n'C:\\Users\\<user>\\AppData\\Local\\<AppAuthor>\\scipy-data\\Cache'\n\n\nIn environments with constrained network connectivity for various security\nreasons or on systems without continuous internet connections, one may manually\nload the cache of the datasets by placing the contents of the dataset repo in\nthe above mentioned cache directory to avoid fetching dataset errors without\nthe internet connectivity.\n\n\n# Discrete Fourier transforms (`scipy.fft`)#\n\n## Fast Fourier Transforms (FFTs)#\n\n\n|  |  |\n| --- | --- |\n| `fft`(x[,Â n,Â axis,Â norm,Â overwrite\\_x,Â ...]) | Compute the 1-D discrete Fourier Transform. |\n| `ifft`(x[,Â n,Â axis,Â norm,Â overwrite\\_x,Â ...]) | Compute the 1-D inverse discrete Fourier Transform. |\n| `fft2`(x[,Â s,Â axes,Â norm,Â overwrite\\_x,Â ...]) | Compute the 2-D discrete Fourier Transform"
    },
    {
        "instruction": "Create a Python program using the 'scipy' API to perform numerical optimization with nonlinear inequality constraints. The program should define an objective function to be minimized or maximized, specify the nonlinear inequality constraints, and use the 'scipy.optimize.minimize' function with the 'constraints' parameter to find the optimal solution.",
        "api": "scipy",
        "output": "import scipy as sp\nfrom scipy.optimize import minimize\n\n# Define the objective function\ndef objective(x):\n    return x[0]**2 + x[1]**2\n\n# Define the nonlinear inequality constraints\ndef constraint(x):\n    return x[0]**2 + x[1]**2 - 1\n\n# Specify the optimization method\nmethod = 'SLSQP'\n\n# Specify the initial guess for the variables\nx0 = [0, 0]\n\n# Specify the constraints\nconstraints = [{'type': 'ineq', 'fun': constraint}]\n\n# Find the optimal solution with nonlinear inequality constraints\nresult = minimize(objective, x0, method=method, constraints=constraints)\n\n# Display the optimal solution\nprint(\"Optimal solution:\", result.x)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n# SciPy API#\n\n## Importing from SciPy#\n\n\nIn Python, the distinction between what is the public API of a library and what\nare private implementation details is not always clear. Unlike in other\nlanguages like Java, it is possible in Python to access âprivateâ functions or\nobjects.\n\n==================\n Document 1 \n----------------\n## Old API#\n\n\nThese are the routines developed earlier for SciPy. They wrap older solvers\nimplemented in Fortran (mostly ODEPACK). While the interface to them is not\nparticularly convenient and certain features are missing compared to the new\nAPI, the solvers themselves are of good quality and work fast as compiled\nFortran code. In some cases, it might be worth using this old API.\n\n\n|  |  |\n| --- | --- |\n| `odeint`(func,Â y0,Â t[,Â args,Â Dfun,Â col\\_deriv,Â ...]) | Integrate a system of ordinary differential equations. |\n| `ode`(f[,Â jac]) | A generic interface class to numeric integrators. |\n| `complex\\_ode`(f[,Â jac]) | A wrapper of ode for complex systems. |\n\n## Solving boundary value problems for ODE systems#\n\n\n|  |  |\n| --- | --- |\n| `solve\\_bvp`(fun,Â bc,Â x,Â y[,Â p,Â S,Â fun\\_jac,Â ...]) | Solve a boundary value problem for a system of ODEs. |\n\n\n\n# Interpolation (`scipy.interpolate`)#\n\n\nSub-package for objects used in interpolation.\n\n\nAs listed below, this sub-package contains spline functions and classes,\n1-D and multidimensional (univariate and multivariate)\ninterpolation classes, Lagrange and Taylor polynomial interpolators, and\nwrappers for FITPACK\nand DFITPACK functions.\n\n## Univariate interpolation#\n\n\n|  |  |\n| --- | --- |\n| `interp1d`(x,Â y[,Â kind,Â axis,Â copy,Â ...]) | Interpolate a 1-D function. |\n| `BarycentricInterpolator`(xi[,Â yi,Â axis]) | The interpolating polynomial for a set of points |\n| `KroghInterpolator`(xi,Â yi[,Â axis]) | Interpolating polynomial for a set of points. |\n| `barycentric\\_interpolate`(xi,Â yi,Â x[,Â axis])\n\n==================\n Document 2 \n----------------\n# Importing from SciPy#\n\n\nIn Python, the distinction between what is the public API of a library and what\nare private implementation details is not always clear. Unlike in other\nlanguages like Java, it is possible in Python to access âprivateâ functions or\nobjects. Occasionally this may be convenient, but be aware that if you do so\nyour code may break without warning in future releases. Some widely understood\nrules for what is and isnât public in Python are:\n\n\n* Methods / functions / classes and module attributes whose names begin with a\nleading underscore are private.\n* If a class name begins with a leading underscore, none of its members are\npublic, whether or not they begin with a leading underscore.\n* If a module name in a package begins with a leading underscore none of\nits members are public, whether or not they begin with a leading underscore.\n* If a module or package defines `\\_\\_all\\_\\_`, that authoritatively defines the\npublic interface.\n* If a module or package doesnât define `\\_\\_all\\_\\_`, then all names that donât\nstart with a leading underscore are public.\n\nNote\n\n\nReading the above guidelines one could draw the conclusion that every\nprivate module or object starts with an underscore. This is not the\ncase; the presence of underscores do mark something as private, but\nthe absence of underscores do not mark something as public.\n\nIn SciPy there are modules whose names donât start with an underscore, but that\nshould be considered private. To clarify which modules these are, we define\nbelow what the public API is for SciPy, and give some recommendations for how\nto import modules/functions/objects from SciPy.\n\n## Guidelines for importing functions from SciPy#\n\n\nThe scipy namespace itself only contains functions imported from numpy. These\nfunctions still exist for backwards compatibility, but should be imported from\nnumpy directly.\n\n\nEverything in the namespaces of scipy submodules is public. In general, it is\nrecommended\n\n==================\n Document 3 \n----------------\n## Basic usage#\n\n\n1. Define the function you want to fit against.:\n\n```\ndef f(B, x):\n '''Linear function y = m\\*x + b'''\n    # B is a vector of the parameters.\n    # x is an array of the current x values.\n    # x is in the same format as the x passed to Data or RealData.\n    #\n    # Return an array in the same format as y passed to Data or RealData.\n    return B[0]\\*x + B[1]\n\n```\n2. Create a Model.:\n\n```\nlinear = Model(f)\n\n```\n3. Create a Data or RealData instance.:\n\n```\nmydata = Data(x, y, wd=1./power(sx,2), we=1./power(sy,2))\n\n\nor, when the actual covariances are known:\n\n```\nmydata = RealData(x, y, sx=sx, sy=sy)\n\n```\n4. Instantiate ODR with your data, model and initial parameter estimate.:\n\n```\nmyodr = ODR(mydata, linear, beta0=[1., 2.])\n\n```\n5. Run the fit.:\n\n```\nmyoutput = myodr.run()\n\n```\n6. Examine output.:\n\n```\nmyoutput.pprint()\n\n\n### References#\n\n\n[1]\nP. T. Boggs and J. E. Rogers, âOrthogonal Distance Regression,â\nin âStatistical analysis of measurement error models and\napplications: proceedings of the AMS-IMS-SIAM joint summer research\nconference held June 10-16, 1989,â Contemporary Mathematics,\nvol. 112, pg. 186, 1990.\n\n\n# Optimization and root finding (`scipy.optimize`)#\n\n\nSciPy `optimize` provides functions for minimizing (or maximizing)\nobjective functions, possibly subject to constraints. It includes\nsolvers for nonlinear problems (with support for both local and global\noptimization algorithms), linear programing, constrained\nand nonlinear least-squares, root finding, and curve fitting.\n\n\nCommon functions and objects, shared across different solvers, are:\n\n\n|  |  |\n| --- | --- |\n| `show\\_options`([solver,Â method,Â disp]) | Show documentation for additional options of optimization solvers. |\n| `OptimizeResult` | Represents the optimization result. |\n| `OptimizeWarning` |  |\n\n\n## Optimization#\n\n\n### Scalar functions optimization#\n\n\n|  |  |\n| --- | --- |\n| `minimize\\_scalar`(fun[,Â bracket,Â bounds,Â ...]) | Minimization of scalar function of one variable. |\n\n\nThe `minimize\\_scalar` function supports the following methods:\n\n* minimize\\_scalar(method=âbrentâ)\n* minimize\\_scalar(method=âboundedâ)\n* minimize\\_scalar(method=âgoldenâ)\n\n### Local (multivariate) optimization#\n\n\n|  |  |\n| --- | --- |\n| `minimize`(fun,Â x0[,Â args,Â method,Â jac,Â hess,Â ...]) | Minimization of scalar function of one or more variables. |\n\n\nThe `minimize` function supports the following methods:\n\n* minimize(method=âNelder-Meadâ)\n* minimize(method=âPowellâ)\n* minimize(method=âCGâ)\n* minimize(method=âBFGSâ)\n* minimize(method=âNewton-CGâ)\n* minimize(method=âL-BFGS-Bâ)\n* minimize(method=âTNCâ)\n* minimize(method=âCOBYLAâ)\n* minimize(method=âSLSQPâ)\n* minimize(method=âtrust-constrâ)\n*\n\n==================\n Document 4 \n----------------\n## Local (multivariate) optimization#\n\n\n|  |  |\n| --- | --- |\n| `minimize`(fun,Â x0[,Â args,Â method,Â jac,Â hess,Â ...]) | Minimization of scalar function of one or more variables. |\n\n\nThe `minimize` function supports the following methods:\n\n* minimize(method=âNelder-Meadâ)\n* minimize(method=âPowellâ)\n* minimize(method=âCGâ)\n* minimize(method=âBFGSâ)\n* minimize(method=âNewton-CGâ)\n* minimize(method=âL-BFGS-Bâ)\n* minimize(method=âTNCâ)\n* minimize(method=âCOBYLAâ)\n* minimize(method=âSLSQPâ)\n* minimize(method=âtrust-constrâ)\n* minimize(method=âdoglegâ)\n* minimize(method=âtrust-ncgâ)\n* minimize(method=âtrust-krylovâ)\n* minimize(method=âtrust-exactâ)\n\nConstraints are passed to `minimize` function as a single object or\nas a list of objects from the following classes:\n\n\n|  |  |\n| --- | --- |\n| `NonlinearConstraint`(fun,Â lb,Â ub[,Â jac,Â ...]) | Nonlinear constraint on the variables. |\n| `LinearConstraint`(A[,Â lb,Â ub,Â keep\\_feasible]) | Linear constraint on the variables. |\n\n\nSimple bound constraints are handled separately and there is a special class\nfor them:\n\n\n|  |  |\n| --- | --- |\n| `Bounds`([lb,Â ub,Â keep\\_feasible]) | Bounds constraint on the variables. |\n\n\nQuasi-Newton strategies implementing `HessianUpdateStrategy`\ninterface can be used to approximate the Hessian in `minimize`\nfunction (available only for the âtrust-constrâ method). Available\nquasi-Newton methods implementing this interface are:\n\n\n|  |  |\n| --- | --- |\n| `BFGS`([exception\\_strategy,Â min\\_curvature,Â ...]) | Broyden-Fletcher-Goldfarb-Shanno (BFGS) Hessian update strategy. |\n| `SR1`([min\\_denominator,Â init\\_scale]) | Symmetric-rank-1 Hessian update strategy. |\n\n### Global optimization#\n\n\n|  |  |\n| --- | --- |\n| `basinhopping`(func,Â x0[,Â niter,Â T,Â stepsize,Â ...]) | Find the global minimum of a function using the basin-hopping algorithm. |\n| `brute`(func,Â ranges[,Â args,Â Ns,Â full\\_output,Â ...]) | Minimize a function over a given range by brute force. |\n| `differential\\_evolution`(func,Â bounds[,Â args,Â ...]) |"
    },
    {
        "instruction": "Create a Python program using the 'spacy' API to analyze text and extract named entities. The program should identify and label different types of named entities such as persons, organizations, and locations.",
        "api": "spacy",
        "output": "#!pip install spacy\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef extract_named_entities(text):\n    doc = nlp(text)\n    entities = []\n    for ent in doc.ents:\n        entities.append((ent.text, ent.label_))\n    return entities\n\ntext = \"Apple Inc. is a technology company headquartered in Cupertino, California. Tim Cook is the CEO of Apple.\"\nentities = extract_named_entities(text)\nfor entity in entities:\n    print(entity)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n# Library Architecture\n\nThe central data structures in spaCy are the `Language` class,\nthe `Vocab` and the `Doc` object. The `Language` class\nis used to process a text and turn it into a `Doc` object. It’s typically stored\nas a variable called `nlp`. The\n\n==================\n Document 1 \n----------------\n Library Architecture\n\nThe central data structures in spaCy are the `Language` class,\nthe `Vocab` and the `Doc` object. The `Language` class\nis used to process a text and turn it into a `Doc` object. It’s typically stored\nas a variable called `nlp`. The `Doc` object owns the **sequence of tokens** and\nall their annotations. By centralizing strings, word vectors and lexical\nattributes in the `Vocab`, we avoid storing multiple copies of this data. This\nsaves memory, and ensures there’s a **single source of truth**.\n\n\nText annotations are also designed to allow a single source of truth: the `Doc`\nobject owns the data, and `Span` and `Token` are\n**views that point into it**. The `Doc` object is constructed by the\n`Tokenizer`, and then **modified in place** by the components\nof the pipeline. The `Language` object coordinates these components. It takes\nraw text and sends it through the pipeline, returning an **annotated document**.\nIt also orchestrates training and serialization.\n\n\n### Container objects\n\n\n| Name | Description |\n| --- | --- |\n| `Doc` | A container for accessing linguistic annotations. |\n| `DocBin` | A collection of `Doc` objects for efficient binary serialization. Also used for training data. |\n| `Example` | A\n\n==================\n Document 2 \n----------------\n## Calling the training function from Python  v3.2\n\nThe training CLI exposes a `train` helper function that lets you run the\ntraining just like `spacy train`. Usually it’s easier to use the command line\ndirectly, but if you need to kick off training from code this is how to do it.\n\n| Name | Description |\n| --- | --- |\n| `config_path` | Path to the config to use for training. Union[str,Path] |\n| `output_path` | Optional name of directory to save output model in. If not provided a model will not be saved. Optional[Union[str,Path]] |\n| *keyword-only* |  |\n| `use_gpu` | Which GPU to use. Defaults to -1 for no GPU. int |\n| `overrides` | Values to override config settings. Dict[str, Any] |\n\n## pretrain  commandexperimental\n\nPretrain the “token to vector” (`Tok2vec`) layer of pipeline\ncomponents on raw text, using an approximate language-modeling objective.\nSpecifically, we load pretrained vectors, and train a component like a CNN,\nBiLSTM, etc to predict vectors which match the pretrained ones.\n\n==================\n Document 3 \n----------------\n### spacy.NER.v1\n\nThe original version of the built-in NER task supports both zero-shot and\nfew-shot prompting.\n\n| Argument | Description |\n| --- | --- |\n| `labels` | Comma-separated list of labels. str |\n| `examples` | Optional function that generates examples for few-shot learning. Defaults to `None`. Optional[Callable[[], Iterable[Any]]] |\n| `normalizer` | Function that normalizes the labels as returned by the LLM. If `None`, defaults to `spacy.LowercaseNormalizer.v1`. Optional[Callable[[str], str]] |\n| `alignment_mode` | Alignment mode in case the LLM returns entities that do not align with token boundaries. Options are `\"strict\"`, `\"contract\"` or `\"expand\"`. Defaults to `\"contract\"`. str |\n| `case_sensitive_matching` | Whether to search without case sensitivity. Defaults to `False`. bool |\n| `single_match` | Whether to match an entity in the LLM’s response only once (the first hit) or multiple times. Defaults to `False`. bool |\n\nThe NER task implementation doesn’t currently ask the LLM for specific offsets,\nbut simply expects a list of strings that represent the enties in the document.\nThis means that a form of string matching is required. This can be configured by\nthe following parameters:\n\n* The `single_match` parameter is typically set to `False` to allow for multiple\nmatches. For instance, the response from the LLM might only mention the entity\n“Paris” once, but you’d still want to mark it every time it occurs in the\ndocument.\n* The case-sensitive matching is typically set to `False` to be robust against\ncase variances in the LLM’s output.\n* The `alignment_mode` argument is used to match entities as returned by the LLM\nto the tokens from the original `Doc` - specifically it’s used as argument in\nthe call to `doc.char_span()`. The `\"strict\"` mode will\nonly keep spans that strictly adhere to the given token boundaries.\n`\"contract\"` will only keep those tokens that are fully within the given\nrange, e.g. reducing `\"New Y\"` to `\"New\"`. Finally, `\"expand\"` will expand the\nspan to the next token boundaries, e.g. expanding `\"New Y\"` out to\n`\"New York\"`.\n\n### SpanCat\n\nThe SpanCat task identifies potentially overlapping entities in text.\n\n#### spacy.SpanCat.v3\n\nThe built-in SpanCat v3 task is a simple adaptation of the NER v3 task to\nsupport overlapping entities and store its annotations in `doc.spans`.\n\n| Argument | Description |\n| --- | --- |\n| `labels` | List of labels or str of\n\n==================\n Document 4 \n----------------\n## Annotation format for creating training examples\n\nAn `Example` object holds the information for one training\ninstance. It stores two `Doc` objects: one for holding the\ngold-standard reference data, and one for holding the predictions of the\npipeline. Examples can be created using the\n`Example.from_dict` method with a reference `Doc` and\na dictionary of gold-standard annotations.\n\n| Name | Description |\n| --- | --- |\n| `text` | Raw text. str |\n| `words` | List of gold-standard tokens. List[str] |\n| `lemmas` | List of lemmas. List[str] |\n| `spaces` | List of boolean values indicating whether the corresponding tokens is followed by a space or not. List[bool] |\n| `tags` | List of fine-grained POS tags. List[str] |\n| `pos` | List of coarse-grained POS tags. List[str] |\n| `morphs` | List of morphological features. List[str] |\n| `sent_starts` | List of boolean values indicating whether each token is the first of a sentence or not. List[bool] |\n| `deps` | List of string values indicating the dependency relation of a token to its head. List[str] |\n| `heads` | List of integer values indicating the dependency head of each token, referring to the absolute index of each token in the text. List[int] |\n| `entities` | **Option 1:** List of BILUO tags per token of the format `\"{action}-{label}\"`, or `None` for unannotated tokens. List[str] |\n| `entities` | **Option 2:** List of `(start_char, end_char, label)` tuples defining all entities in the text. List[Tuple[int, int, str]] |\n| `cats` | Dictionary of `label`/`value` pairs indicating how relevant a certain text category is for the text. Dict[str, float] |\n| `links` | Dictionary of `offset`/`dict` pairs defining named entity links. The character offsets are linked to a dictionary of relevant knowledge base IDs. Dict[Tuple[int, int], Dict] |\n| `spans` | Dictionary of `spans_key`/`List[Tuple]` pairs defining the spans for each spans key as `(start_char, end_char, label, kb_id)` tuples. Dict[str, List[Tuple[int, int, str, str]] |\n\n\n```\n#### Examples\n\n\n```\n\n\n## Lexical data for vocabulary\n\nThis data file can be provided via the `vocab_data` setting in the\n`[initialize]` block of the training config to pre-define the lexical data to\ninitialize the `nlp` object’s vocabulary with. The file should contain one\nlexical entry per line. The first line defines the language and vocabulary\nsettings. All other lines are expected to be JSON objects describing an\nindividual lexeme. The lexical attributes will be then set as attributes on\nspaCy’s `Lexeme` object.\n\n\n```\n\n#### First line\n\n```\n\n#### Entry structure\n\n\n```\nHere’s an example of the 20 most frequent lexemes in the English training data:\n\n\n```\n`explosion/spaCy/master/extra/example_data/vocab-data.jsonl`\n```\n\n## Pipeline meta\n\nThe pipeline meta is available as the file `meta.json` and exported\nautomatically when you save an `nlp` object to disk. Its contents are available\nas `nlp.meta`.\n\n| Name | Description |\n| --- | --- |\n| `lang` | Pipeline language ISO code."
    },
    {
        "instruction": "Create a Python program using the 'spacy' API to perform sentiment analysis on a given text. The program should analyze the sentiment of the text and classify it as positive, negative, or neutral.",
        "api": "spacy",
        "output": "#!pip install spacy\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef sentiment_analysis(text):\n    doc = nlp(text)\n    sentiment = \"Neutral\"\n    # Perform sentiment analysis based on the content of the text\n    # Add your sentiment analysis logic here\n    return sentiment\n\ntext = \"I love this movie!\"\nsentiment = sentiment_analysis(text)\nprint(sentiment)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n# Library Architecture\n\nThe central data structures in spaCy are the `Language` class,\nthe `Vocab` and the `Doc` object. The `Language` class\nis used to process a text and turn it into a `Doc` object. It’s typically stored\nas a variable called `nlp`. The\n\n==================\n Document 1 \n----------------\n Library Architecture\n\nThe central data structures in spaCy are the `Language` class,\nthe `Vocab` and the `Doc` object. The `Language` class\nis used to process a text and turn it into a `Doc` object. It’s typically stored\nas a variable called `nlp`. The `Doc` object owns the **sequence of tokens** and\nall their annotations. By centralizing strings, word vectors and lexical\nattributes in the `Vocab`, we avoid storing multiple copies of this data. This\nsaves memory, and ensures there’s a **single source of truth**.\n\n\nText annotations are also designed to allow a single source of truth: the `Doc`\nobject owns the data, and `Span` and `Token` are\n**views that point into it**. The `Doc` object is constructed by the\n`Tokenizer`, and then **modified in place** by the components\nof the pipeline. The `Language` object coordinates these components. It takes\nraw text and sends it through the pipeline, returning an **annotated document**.\nIt also orchestrates training and serialization.\n\n\n### Container objects\n\n\n| Name | Description |\n| --- | --- |\n| `Doc` | A container for accessing linguistic annotations. |\n| `DocBin` | A collection of `Doc` objects for efficient binary serialization. Also used for training data. |\n| `Example` | A\n\n==================\n Document 2 \n----------------\n## Annotation format for creating training examples\n\nAn `Example` object holds the information for one training\ninstance. It stores two `Doc` objects: one for holding the\ngold-standard reference data, and one for holding the predictions of the\npipeline. Examples can be created using the\n`Example.from_dict` method with a reference `Doc` and\na dictionary of gold-standard annotations.\n\n| Name | Description |\n| --- | --- |\n| `text` | Raw text. str |\n| `words` | List of gold-standard tokens. List[str] |\n| `lemmas` | List of lemmas. List[str] |\n| `spaces` | List of boolean values indicating whether the corresponding tokens is followed by a space or not. List[bool] |\n| `tags` | List of fine-grained POS tags. List[str] |\n| `pos` | List of coarse-grained POS tags. List[str] |\n| `morphs` | List of morphological features. List[str] |\n| `sent_starts` | List of boolean values indicating whether each token is the first of a sentence or not. List[bool] |\n| `deps` | List of string values indicating the dependency relation of a token to its head. List[str] |\n| `heads` | List of integer values indicating the dependency head of each token, referring to the absolute index of each token in the text. List[int] |\n| `entities` | **Option 1:** List of BILUO tags per token of the format `\"{action}-{label}\"`, or `None` for unannotated tokens. List[str] |\n| `entities` | **Option 2:** List of `(start_char, end_char, label)` tuples defining all entities in the text. List[Tuple[int, int, str]] |\n| `cats` | Dictionary of `label`/`value` pairs indicating how relevant a certain text category is for the text. Dict[str, float] |\n| `links` | Dictionary of `offset`/`dict` pairs defining named entity links. The character offsets are linked to a dictionary of relevant knowledge base IDs. Dict[Tuple[int, int], Dict] |\n| `spans` | Dictionary of `spans_key`/`List[Tuple]` pairs defining the spans for each spans key as `(start_char, end_char, label, kb_id)` tuples. Dict[str, List[Tuple[int, int, str, str]] |\n\n\n```\n#### Examples\n\n\n```\n\n\n## Lexical data for vocabulary\n\nThis data file can be provided via the `vocab_data` setting in the\n`[initialize]` block of the training config to pre-define the lexical data to\ninitialize the `nlp` object’s vocabulary with. The file should contain one\nlexical entry per line. The first line defines the language and vocabulary\nsettings. All other lines are expected to be JSON objects describing an\nindividual lexeme. The lexical attributes will be then set as attributes on\nspaCy’s `Lexeme` object.\n\n\n```\n\n#### First line\n\n```\n\n#### Entry structure\n\n\n```\nHere’s an example of the 20 most frequent lexemes in the English training data:\n\n\n```\n`explosion/spaCy/master/extra/example_data/vocab-data.jsonl`\n```\n\n## Pipeline meta\n\nThe pipeline meta is available as the file `meta.json` and exported\nautomatically when you save an `nlp` object to disk. Its contents are available\nas `nlp.meta`.\n\n| Name | Description |\n| --- | --- |\n| `lang` | Pipeline language ISO code.\n\n==================\n Document 3 \n----------------\n### spacy.SpanCat.v1\n\nThe original version of the built-in SpanCat task is a simple adaptation of the\nv1 NER task to support overlapping entities and store its annotations in\n`doc.spans`.\n\n| Argument | Description |\n| --- | --- |\n| `labels` | Comma-separated list of labels. str |\n| `spans_key` | Key of the `Doc.spans` dict to save the spans under. Defaults to `\"sc\"`. str |\n| `examples` | Optional function that generates examples for few-shot learning. Defaults to `None`. Optional[Callable[[], Iterable[Any]]] |\n| `normalizer` | Function that normalizes the labels as returned by the LLM. If `None`, defaults to `spacy.LowercaseNormalizer.v1`. Optional[Callable[[str], str]] |\n| `alignment_mode` | Alignment mode in case the LLM returns entities that do not align with token boundaries. Options are `\"strict\"`, `\"contract\"` or `\"expand\"`. Defaults to `\"contract\"`. str |\n| `case_sensitive_matching` | Whether to search without case sensitivity. Defaults to `False`. bool |\n| `single_match` | Whether to match an entity in the LLM’s response only once (the first hit) or multiple times. Defaults to `False`. bool |\n\nExcept for the `spans_key` parameter, the SpanCat v1 task reuses the\nconfiguration from the NER v1 task. Refer to its documentation for\nmore insight.\n\n### TextCat\n\nThe TextCat task labels documents with relevant categories.\n\n#### spacy.TextCat.v3\n\nOn top of the functionality from v2, version 3 of the built-in TextCat tasks\nallows setting definitions of labels. Those definitions are included in the\nprompt.\n\n| Argument | Description |\n| --- | --- |\n| `labels` | List of labels or str\n\n==================\n Document 4 \n----------------\n## Calling the training function from Python  v3.2\n\nThe training CLI exposes a `train` helper function that lets you run the\ntraining just like `spacy train`. Usually it’s easier to use the command line\ndirectly, but if you need to kick off training from code this is how to do it.\n\n| Name | Description |\n| --- | --- |\n| `config_path` | Path to the config to use for training. Union[str,Path] |\n| `output_path` | Optional name of directory to save output model in. If not provided a model will not be saved. Optional[Union[str,Path]] |\n| *keyword-only* |  |\n| `use_gpu` | Which GPU to use. Defaults to -1 for no GPU. int |\n| `overrides` | Values to override config settings. Dict[str, Any] |\n\n## pretrain  commandexperimental\n\nPretrain the “token to vector” (`Tok2vec`) layer of pipeline\ncomponents on raw text, using an approximate language-modeling objective.\nSpecifically, we load pretrained vectors, and train a component like a CNN,\nBiLSTM, etc to predict vectors which match the pretrained ones."
    },
    {
        "instruction": "Develop a Python program using the 'stumpy' API to perform motif discovery in a given time series. The program should allow the user to specify the desired motif length and find the motifs within the time series. It should return the indices of all identified motifs.",
        "api": "stumpy",
        "output": "#!pip install stumpy\nimport stumpy\nimport numpy as np\n\n# Generate a random time series\nyour_time_series = np.random.rand(1000)\n\n# Specify the motif length\nmotif_length = 30\n\n# Perform motif discovery\nmotif_indices = stumpy.motif(your_time_series, motif_length)\n\n# Print the indices of identified motifs\nprint(f\"Motif Indices: {motif_indices}\")",
        "documentation": "\n\n==================\n Document 0 \n----------------\n# motifs#\n\n\nstumpy.motifs(*T*, *P*, *min\\_neighbors=1*, *max\\_distance=None*, *cutoff=None*, *max\\_matches=10*, *max\\_motifs=1*, *atol=1e-08*, *normalize=True*, *p=2.0*, *T\\_subseq\\_isconstant=None*)[source]#\nDiscover the top motifs for time series T\n\n\nA subsequence, Q, becomes a candidate motif if there are at least min\\_neighbor\nnumber of other subsequence matches in T (outside the exclusion zone) with a\ndistance less or equal to max\\_distance.\n\n\nNote that, in the best case scenario, the returned arrays would have shape\n(max\\_motifs, max\\_matches) and contain all finite values. However, in reality,\nmany conditions (see below) need to be satisfied in order for this to be true. Any\ntruncation in the number of rows (i.e., motifs) may be the result of insufficient\ncandidate motifs with matches greater than or equal to min\\_neighbors or that the\nmatrix profile value for the candidate motif was larger than cutoff. Similarly,\nany truncation in the number of columns (i.e., matches) may be the result of\ninsufficient matches being found with distances (to their corresponding candidate\nmotif) that are equal to or less than max\\_distance. Only motifs and matches that\nsatisfy all of these constraints will be returned.\n\n\nIf you must return a shape of (max\\_motifs, max\\_matches), then you may consider\nspecifying a smaller min\\_neighbors, a larger max\\_distance, and/or a larger\ncutoff. For example, while it is ill advised, setting min\\_neighbors=1,\nmax\\_distance=np.inf, and cutoff=np.inf will ensure that the shape of the output\narrays will be (max\\_motifs, max\\_matches). However, given the lack of constraints,\nthe quality of each motif and the quality of each match may be drastically\ndifferent. Setting appropriate conditions will help ensure appropriately\nconstrained results that may be easier to interpret.\n\nParameters:\n* **T** (*numpy.ndarray*) – The time series or sequence\n* **P** (*numpy.ndarray*) – The (1-dimensional) matrix profile of T. In the case where the matrix\nprofile was computed with k > 1 (i.e., top-k nearest neighbors), you\nmust summarize the top-k nearest-neighbor distances for each subsequence\ninto a single value (e.g., np.mean, np.min, etc) and then use that\nderived value as your P.\n* **min\\_neighbors** (*int**,* *default 1*) – The minimum number of similar matches a subsequence needs to have in order\nto be considered a motif. This defaults to 1, which means that a subsequence\nmust have at least one similar match in order to be considered a motif.\n* **max\\_distance** (*float* *or* *function**,* *default None*) – For a candidate motif, Q, and a non-trivial subsequence, S, max\\_distance\nis the maximum distance allowed between Q and S so that S is considered\na match of Q. If max\\_distance is a function, then it must be a function\nthat accepts a single parameter, D, in its function signature, which is the\ndistance profile between Q and T. If None, this defaults to\nnp.nanmax([np.nanmean(D) - 2.0 \\* np.nanstd(D), np.nanmin(D)]).\n* **cutoff** (*float**,* *default None*) – The largest matrix profile value (distance) that a candidate motif is allowed\nto have. If None, this defaults to\nnp.nanmax([np.nanmean(P) - 2.0 \\* np.nanstd(P), np.nanmin(P)])\n* **max\\_matches** (*int**,* *default 10*) – The maximum amount of similar matches of a motif representative to be returned.\nThe resulting matches are sorted by distance, so a value of 10 means that the\nindices of the most similar 10 subsequences is returned.\nIf None, all matches within max\\_distance of the motif representative\nwill be returned. Note that the first match is always the\nself-match/trivial-match for each motif.\n* **max\\_motifs** (*int**,* *default 1*) – The maximum number of motifs to return\n* **atol** (*float**,* *default 1e-8*) – The absolute tolerance parameter. This value will be added to max\\_distance\nwhen comparing distances between subsequences.\n* **normalize** (*bool**,* *default True*) – When set to True, this z-normalizes subsequences prior to computing distances.\nOtherwise, this function gets re-routed to its complementary non-normalized\nequivalent set in the @core.non\\_normalized function decorator.\n* **p** (*float**,* *default 2.0*) – The p-norm to apply for computing the Minkowski distance. Minkowski distance is\ntypically used with p being 1 or 2, which correspond to the Manhattan distance\nand the Euclidean distance, respectively. This parameter is ignored when\nnormalize == True.\n* **T\\_subseq\\_isconstant** (*numpy.ndarray* *or* *function**,* *default None*) – A boolean array that indicates whether a subsequence in T is constant\n(True). Alternatively, a custom, user-defined function that returns a\nboolean array that indicates whether a subsequence in T is constant\n(True). The function must only take two arguments, a, a 1-D array,\nand w, the window size, while additional arguments may be specified\nby currying the user-defined function using functools.partial. Any\nsubsequence with at least one np.nan/np.inf will automatically have its\ncorresponding value set to False in this boolean array.\n\nReturns:\n* **motif\\_distances** (*numpy.ndarray*) – The distances corresponding to a set of subsequence matches for each motif.\nNote that the first column always corresponds to the distance for the\nself-match/trivial-match for each motif.\n* **motif\\_indices** (*numpy.ndarray*) – The indices corresponding to a set of subsequences matches for each motif.\nNote that the first column always corresponds to the index for the\nself-match/trivial-match for each motif.\n\n`stumpy.match`Find all matches of a query Q in a time series T\n\n`stumpy.mmotifs`Discover the top motifs for the multi-dimensional time series T\n\n```\n>>> import stumpy\n>>> import numpy as np\n>>> mp = stumpy.stump(np.array([584., -11., 23., 79., 1001., 0., -19.]), m=3)\n>>> stumpy.motifs(\n...     np.array([584., -11., 23., 79., 1001., 0., -19.]),\n...     mp[:, 0],\n...     max\\_distance=2.0)\n(array([[0. , 0.11633857]]), array([[0, 4]]))\n## match#\n\n\nstumpy.match(*Q*, *T*, *M\\_T=None*, *Σ\\_T=None*, *max\\_distance=None*, *max\\_matches=None*, *atol=1e-08*, *query\\_idx=None*, *normalize=True*, *p=2.0*, *T\\_subseq\\_isfinite=None*, *T\\_subseq\\_isconstant=None*, *Q\\_subseq\\_isconstant=None*)[source]#\nFind all matches of a query Q in a time series T\n\n\nThe indices of subsequences whose distances to Q are less than or equal to\nmax\\_distance, sorted by\n\n==================\n Document 1 \n----------------\n# mmotifs#\n\n\nstumpy.mmotifs(*T*, *P*, *I*, *min\\_neighbors=1*, *max\\_distance=None*, *cutoffs=None*, *max\\_matches=10*, *max\\_motifs=1*, *atol=1e-08*, *k=None*, *include=None*, *normalize=True*, *p=2.0*, *T\\_subseq\\_isconstant=None*)[source]#\nDiscover the top motifs for the multi-dimensional time series T\n\nParameters:\n* **T** (*numpy.ndarray*) – The multi-dimensional time series or sequence\n* **P** (*numpy.ndarray*) – Multi-dimensional Matrix Profile of T\n* **I** (*numpy.ndarray*) – Multi-dimensional Matrix Profile indices\n* **min\\_neighbors** (*int**,* *default 1*) – The minimum number of similar matches a subsequence needs to have in order\nto be considered a motif. This defaults to 1, which means that a subsequence\nmust have at least one similar match in order to be considered a motif.\n* **max\\_distance** (*flaot**,* *default None*) – Maximal distance that is allowed between a query subsequence\n(a candidate motif) and all subsequences in T to be considered as a match.\nIf None, this defaults to\nnp.nanmax([np.nanmean(D) - 2 \\* np.nanstd(D), np.nanmin(D)])\n(i.e. at least the closest match will be returned).\n* **cutoffs** (*numpy.ndarray* *or* *float**,* *default None*) – The largest matrix profile value (distance) for each dimension of the\nmultidimensional matrix profile that a multidimenisonal candidate motif is\nallowed to have. If cutoffs is a scalar value, then this value will be\napplied to every dimension.\n* **max\\_matches** (*int**,* *default 10*) – The maximum number of similar matches (nearest neighbors) to return for each\nmotif. The first match is always the self/trivial-match for each motif.\n* **max\\_motifs** (*int**,* *default 1*) – The maximum number of motifs to return\n* **atol** (*float**,* *default 1e-8*) – The absolute tolerance parameter. This value will be added to max\\_distance\nwhen comparing distances between subsequences.\n* **k** (*int**,* *default None*) – The number of dimensions (k + 1) required for discovering all motifs. This\nvalue is available for doing guided search or, together with include, for\nconstrained search. If k is None, then this will be automatically be computed\nfor each motif using MDL (unconstrained search).\n* **include** (*numpy.ndarray**,* *default None*) – A list of (zero based) indices corresponding to the dimensions in T that must be\nincluded in the constrained multidimensional motif search.\n* **normalize** (*bool**,* *default True*) – When set to True, this z-normalizes subsequences prior to computing distances.\nOtherwise, this function gets re-routed to its complementary non-normalized\nequivalent set in the @core.non\\_normalized function decorator.\n* **p** (*float**,* *default 2.0*) – The p-norm to apply for computing the Minkowski distance. Minkowski distance is\ntypically used with p being 1 or 2, which correspond to the Manhattan distance\nand the Euclidean distance, respectively. This parameter is ignored when\nnormalize == True.\n* **T\\_subseq\\_isconstant** (*numpy.ndarray**,* *function**, or* *list**,* *default None*) – A parameter that is used to show whether a subsequence of a time series in T\nis constant (True) or not. T\\_subseq\\_isconstant can be a 2D boolean numpy.ndarry\nor a function that can be applied to each time series in T. Alternatively, for\nmaximum flexibility, a list (with length equal to the total number of time\nseries) may also be used. In this case, T\\_subseq\\_isconstant[i] corresponds to\nthe i-th time series T[i] and each element in the list can either be a 1D\nboolean np.ndarray, a function, or None.\n\nReturns:\n* **motif\\_distances** (*numpy.ndarray*) – The distances corresponding to a set of subsequence matches for each motif.\n* **motif\\_indices** (*numpy.ndarray*) – The indices corresponding to a set of subsequences matches for each motif.\n* **motif\\_subspaces** (*list*) – A list consisting of arrays that contain the k-dimensional\nsubspace for each motif.\n* **motif\\_mdls** (*list*) – A list consisting of arrays that contain the mdl results for\nfinding the dimension of each motif\n\n`stumpy.motifs`Find the top motifs for time series T\n\n\nFor more information on include and search types, see Section IV D and IV E\n\n```\n>>> import stumpy\n>>> import numpy as np\n>>> mps, indices = stumpy.mstump(\n...     np.array([[584., -11., 23., 79., 1001., 0., -19.],\n...               [  1.,   2.,  4.,  8.,   16., 0.,  32.]]),\n...     m=3)\n>>> stumpy.mmotifs(\n...     np.array([[584., -11., 23., 79., 1001., 0., -19.],\n...               [  1.,   2.,  4.,  8.,   16., 0.,  32.]]),\n...     mps,\n...     indices)\n(array([[4.47034836e-08, 4.47034836e-08]]), array([[0, 2]]), [array([1])],\n [array([ 80. , 111.509775])])\n## snippets#\n\n\nstumpy.snippets(*T*, *m*, *k*, *percentage=1.0*, *s=None*, *mpdist\\_percentage=0.05*, *mpdist\\_k=None*, *normalize=True*, *p=2.0*, *mpdist\\_T\\_subseq\\_isconstant=None*)[source]#\nIdentify the top k snippets that best represent the time series, T\n\nParameters:\n* **T** (*numpy.ndarray*) – The time series or sequence for which to find the snippets\n* **m** (*int*) – The\n\n==================\n Document 2 \n----------------\n\n# STUMPY API#\n\n\nOverview\n\n\n|  |  |\n| --- | --- |\n| `stumpy.stump` | Compute the z-normalized matrix profile |\n| `stumpy.stumped` | Compute the z-normalized matrix profile with a distributed dask/ray cluster |\n| `stumpy.gpu\\_stump` | Compute the z-normalized matrix profile with"
    },
    {
        "instruction": "Develop a Python program using the 'stumpy' API to generate a matrix profile for a given random time series data. The program should compute the e z-normalized matrix profile, and the resulting profile should be saved to a text file.",
        "api": "stumpy",
        "output": "#!pip install stumpy\nimport stumpy\nimport numpy as np\n\n# Generate a random time series\nyour_time_series = np.random.rand(10000)\nwindow_size = 50  \n\n# Compute the e z-normalized matrix profile \nmatrix_profile = stumpy.stump(your_time_series, m=window_size)\n\noutput_file_path = \"matrix_profile.txt\"\nnp.savetxt(output_file_path, matrix_profile)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n# STUMPY API#\n\n\nOverview\n\n\n|  |  |\n| --- | --- |\n| `stumpy.stump` | Compute the z-normalized matrix profile |\n| `stumpy.stumped` | Compute the z-normalized matrix profile with a distributed dask/ray cluster |\n| `stumpy.gpu\\_stump` | Compute the z-normalized matrix profile with\n\n==================\n Document 1 \n----------------\n# stimp#\n\n\nstumpy.stimp(*T*, *min\\_m=3*, *max\\_m=None*, *step=1*, *percentage=0.01*, *pre\\_scrump=True*, *normalize=True*, *p=2.0*, *T\\_subseq\\_isconstant\\_func=None*)[source]#\nCompute the Pan Matrix Profile\n\n\nThis is based on the SKIMP algorithm.\n\nParameters:\n* **T** (*numpy.ndarray*) – The time series or sequence for which to compute the pan matrix profile\n* **min\\_m** (*int**,* *default 3*) – The starting (or minimum) subsequence window size for which a matrix profile\nmay be computed\n* **max\\_m** (*int**,* *default None*) – The stopping (or maximum) subsequence window size for which a matrix profile\nmay be computed. When max\\_m = Non, this is set to the maximum allowable\nsubsequence window size\n* **step** (*int**,* *default 1*) – The step between subsequence window sizes\n* **percentage** (*float**,* *default 0.01*) – The percentage of the full matrix profile to compute for each subsequence\nwindow size. When percentage < 1.0, then the scrump algorithm is used.\nOtherwise, the stump algorithm is used when the exact matrix profile is\nrequested.\n* **pre\\_scrump** (*bool**,* *default True*) – A flag for whether or not to perform the PreSCRIMP calculation prior to\ncomputing SCRIMP. If set to True, this is equivalent to computing\nSCRIMP++. This parameter is ignored when percentage = 1.0.\n* **normalize** (*bool**,* *default True*) – When set to True, this z-normalizes subsequences prior to computing distances.\nOtherwise, this function gets re-routed to its complementary non-normalized\nequivalent set in the @core.non\\_normalized function decorator.\n* **p** (*float**,* *default 2.0*) – The p-norm to apply for computing the Minkowski distance. This parameter is\nignored when normalize == True.\n* **T\\_subseq\\_isconstant\\_func** (*function**,* *default None*) – A custom, user-defined function that returns a boolean array that indicates\nwhether a subsequence in T is constant (True). The function must only take\ntwo arguments, a, a 1-D array, and w, the window size, while additional\narguments may be specified by currying the user-defined function using\nfunctools.partial. Any subsequence with at least one np.nan/np.inf will\nautomatically have its corresponding value set to False in this boolean array.\n\n\nstumpy.PAN\\_#\nThe transformed (i.e., normalized, contrasted, binarized, and repeated)\npan matrix profile\n\nstumpy.M\\_#\nThe full list of (breadth first search (level) ordered) subsequence window\nsizes\n\nupdate():\nCompute the next matrix profile using the next available (breadth-first-search\n(level) ordered) subsequence window size and update the pan matrix profile\n\n`stumpy.stimped`Compute the Pan Matrix Profile with a distributed dask cluster\n\n`stumpy.gpu\\_stimp`Compute the Pan Matrix Profile with with one or more GPU devices\n\n\nDOI: 10.1109/ICBK.2019.00031\n\n```\n>>> import stumpy\n>>> import numpy as np\n>>> pmp = stumpy.stimp(np.array([584., -11., 23., 79., 1001., 0., -19.]))\n>>> pmp.update()\n>>> pmp.PAN\\_\narray([[0., 1., 1., 1., 1., 1., 1.],\n [0., 1., 1., 1., 1., 1., 1.]])\n## stimped#\n\n\nstumpy.stimped(*client*, *T*, *min\\_m=3*, *max\\_m=None*, *step=1*, *normalize=True*, *p=2.0*, *T\\_subseq\\_isconstant\\_func=None*)[source]#\nCompute the Pan Matrix Profile with a distributed dask/ray cluster\n\nParameters:\n* **client** (*client*) – A Dask or Ray Distributed client. Setting up a distributed cluster is beyond\nthe scope of this library. Please refer\n\n==================\n Document 2 \n----------------\n# stimped#\n\n\nstumpy.stimped(*client*, *T*, *min\\_m=3*, *max\\_m=None*, *step=1*, *normalize=True*, *p=2.0*, *T\\_subseq\\_isconstant\\_func=None*)[source]#\nCompute the Pan Matrix Profile with a distributed dask/ray cluster\n\nParameters:\n* **client** (*client*) – A Dask or Ray Distributed client. Setting up a distributed cluster is beyond\nthe scope of this library. Please refer to the Dask or Ray Distributed\ndocumentation.\n* **T** (*numpy.ndarray*) – The time series or sequence for which to compute the pan matrix profile\n* **min\\_m** (*int**,* *default 3*) – The starting (or minimum) subsequence window size for which a matrix profile\nmay be computed\n* **max\\_m** (*int**,* *default None*) – The stopping (or maximum) subsequence window size for which a matrix profile\nmay be computed. When max\\_m = Non, this is set to the maximum allowable\nsubsequence window size\n* **step** (*int**,* *default 1*) – The step between subsequence window sizes\n* **normalize** (*bool**,* *default True*) – When set to True, this z-normalizes subsequences prior to computing distances.\nOtherwise, this function gets re-routed to its complementary non-normalized\nequivalent set in the @core.non\\_normalized function decorator.\n* **p** (*float**,* *default 2.0*) – The p-norm to apply for computing the Minkowski distance. This parameter is\nignored when normalize == True.\n* **T\\_subseq\\_isconstant\\_func** (*function**,* *default None*) – A custom, user-defined function that returns a boolean array that indicates\nwhether a subsequence in T is constant (True). The function must only take\ntwo arguments, a, a 1-D array, and w, the window size, while additional\narguments may be specified by currying the user-defined function using\nfunctools.partial. Any subsequence with at least one np.nan/np.inf will\nautomatically have its corresponding value set to False in this boolean\narray.\n\n`stumpy.stimp`Compute the Pan Matrix Profile\n\n```\n>>> import stumpy\n>>> import numpy as np\n>>> from dask.distributed import Client\n>>> if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\":\n...     with Client() as dask\\_client:\n...         pmp = stumpy.stimped(\n...             dask\\_client,\n...             np.array([584., -11., 23., 79., 1001., 0., -19.]))\n...         pmp.update()\n...         pmp.PAN\\_\narray([[0., 1., 1., 1., 1., 1., 1.],\n [0., 1., 1., 1., 1., 1., 1.]])\n## gpu\\_stimp#\n\n\nstumpy.gpu\\_stimp(*T*, *min\\_m=3*, *max\\_m=None*, *step=1*, *device\\_id=0*, *normalize=True*, *p=2.0*, *T\\_subseq\\_isconstant\\_func=None*)#\nCompute the Pan Matrix Profile with with one or more GPU devices\n\nParameters:\n* **T** (*numpy.ndarray*) – The time series or sequence for which to compute the pan matrix profile\n* **min\\_m** (*int**,* *default 3*)\n\n==================\n Document 3 \n----------------\n# stumpi#\n\n\nstumpy.stumpi(*T*, *m*, *egress=True*, *normalize=True*, *p=2.0*, *k=1*, *mp=None*, *T\\_subseq\\_isconstant\\_func=None*)[source]#\nCompute an incremental z-normalized matrix profile for streaming data\n\n\nThis is based on the on-line STOMPI and STAMPI algorithms.\n\nParameters:\n* **T** (*numpy.ndarray*) – The time series or sequence for which the matrix profile and matrix profile\nindices will be returned\n* **m** (*int*) – Window size\n* **egress** (*bool**,* *default True*) – If set to True, the oldest data point in the time series is removed and\nthe time series length remains constant rather than forever increasing\n* **normalize** (*bool**,* *default True*) – When set to True, this z-normalizes subsequences prior to computing distances.\nOtherwise, this class gets re-routed to its complementary non-normalized\nequivalent set in the @core.non\\_normalized class decorator.\n* **p** (*float**,* *default 2.0*) – The p-norm to apply for computing the Minkowski distance. This parameter is\nignored when normalize == True.\n* **k** (*int**,* *default 1*) – The number of top k smallest distances used to construct the matrix profile.\nNote that this will increase the total computational time and memory usage\nwhen k > 1.\n* **mp** (*numpy.ndarry**,* *default None*) – A pre-computed matrix profile (and corresponding matrix profile indices).\nThis is a 2D array of shape (len(T) - m + 1, 2 \\* k + 2), where the first k\ncolumns are top-k matrix profile, and the next k columns are their\ncorresponding indices. The last two columns correspond to the top-1 left and\ntop-1 right matrix profile indices. When None (default), this array is computed\ninternally using stumpy.stump.\n* **T\\_subseq\\_isconstant\\_func** (*function**,* *default None*) – A custom, user-defined function that returns a boolean array that indicates\nwhether a subsequence in T is constant (True). The function must only take\ntwo arguments, a, a 1-D array, and w, the window size, while additional\narguments may be specified by currying the user-defined function using\nfunctools.partial. Any subsequence with at least one np.nan/np.inf will\nautomatically have its corresponding value set to False in this boolean array.\n\n\nstumpy.P\\_#\nThe updated (top-k) matrix profile for T. When k=1 (default), the first\n(and only) column in this 2D array consists of the matrix profile. When\nk > 1, the output has exactly k columns consisting of the top-k matrix\nprofile.\n\nstumpy.I\\_#\nThe updated (top-k) matrix profile indices for T. When k=1 (default),\nthe first (and only) column in this 2D array consists of the matrix profile\nindices. When k > 1, the output has exactly k columns consisting of the\ntop-k matrix profile indices.\n\nstumpy.left\\_P\\_#\nThe updated left (top-1) matrix profile for T\n\nstumpy.left\\_I\\_#\nThe updated left (top-1) matrix profile indices for T\n\nstumpy.T\\_#\nThe updated time series or sequence for which the matrix profile and matrix\nprofile indices are computed\n\nstumpy.update(*t*)#\nAppend a single new data point, t, to the time series, T, and update the\nmatrix profile\n\n\nDOI: 10.1007/s10618-017-0519-9\n\n\nSee Table V\n\n\nNote that line 11 is missing an important sqrt operation!\n\n```\n>>> import stumpy\n>>> import numpy as np\n>>> stream = stumpy.stumpi(\n...     np.array([584., -11., 23., 79., 1001., 0.]),\n...     m=3)\n>>> stream.update(-19.0)\n>>> stream.left\\_P\\_\narray([ inf, 3.00009263, 2.69407392, 3.05656417])\n>>> stream.left\\_I\\_\narray([-1, 0, 1, 2])\n## mstump#\n\n\nstumpy.mstump(*T*, *m*, *include=None*, *discords=False*, *normalize=True*, *p=2.0*, *T\\_subseq\\_isconstant=None*)[source]#\nCompute the multi-dimensional z-normalized matrix profile\n\n\nThis is a convenience wrapper around the Numba JIT-compiled parallelized\n\\_mstump function which computes the multi-dimensional matrix profile and\nmulti-dimensional matrix profile index according to mSTOMP, a variant of\nmSTAMP. Note\n\n==================\n Document 4 \n----------------\n# stump#\n\n\nstumpy.stump(*T\\_A*, *m*, *T\\_B=None*, *ignore\\_trivial=True*, *normalize=True*, *p=2.0*, *k=1*, *T\\_A\\_subseq\\_isconstant=None*, *T\\_B\\_subseq\\_isconstant=None*)[source]#\nCompute the z-normalized matrix profile\n\n\nThis is a convenience wrapper around the Numba JIT-compiled parallelized\n\\_stump function which computes the (top-k) matrix profile according to\nSTOMPopt with Pearson correlations.\n\nParameters:\n* **T\\_A** (*numpy.ndarray*) – The time series or sequence for which to compute the matrix profile\n* **m** (*int*) – Window size\n* **T\\_B** (*numpy.ndarray**,* *default None*) – The time series or sequence that will be used to annotate T\\_A. For every\nsubsequence in T\\_A, its nearest neighbor in T\\_B will be recorded. Default is\nNone which corresponds to a self-join.\n* **ignore\\_trivial** (*bool**,* *default True*) – Set to True if this is a self-join. Otherwise, for AB-join, set this\nto False. Default is True.\n* **normalize** (*bool**,* *default True*) – When set to True, this z-normalizes subsequences prior to computing distances.\nOtherwise, this function gets re-routed to its complementary non-normalized\nequivalent set in the @core.non\\_normalized function decorator.\n* **p** (*float**,* *default 2.0*) – The p-norm to apply for computing the Minkowski distance. Minkowski distance is\ntypically used with p being 1 or 2, which correspond to the Manhattan distance\nand the Euclidean distance, respectively. This parameter is ignored when\nnormalize == True.\n* **k** (*int**,* *default 1*) – The number of top k smallest distances used to construct the matrix profile.\nNote that this will increase the total computational time and memory usage\nwhen k > 1. If you have access to a GPU device, then you may be able to\nleverage gpu\\_stump for better performance and scalability.\n* **T\\_A\\_subseq\\_isconstant** (*numpy.ndarray* *or* *function**,* *default None*) – A boolean array that indicates whether a subsequence in T\\_A is constant\n(True). Alternatively, a custom, user-defined function that returns a\nboolean array that indicates whether a subsequence in T\\_A is constant\n(True). The function must only take two arguments, a, a 1-D array,\nand w, the window size, while additional arguments may be specified\nby currying the user-defined function using functools.partial. Any\nsubsequence with at least one np.nan/np.inf will automatically have its\ncorresponding value set to False in this boolean array.\n* **T\\_B\\_subseq\\_isconstant** (*numpy.ndarray* *or* *function**,* *default None*) – A boolean array that indicates whether a subsequence in T\\_B is constant\n(True). Alternatively, a custom, user-defined function that returns a\nboolean array that indicates whether a subsequence in T\\_B is constant\n(True). The function must only take two arguments, a, a 1-D array,\nand w, the window size, while additional arguments may be specified\nby currying the user-defined function using functools.partial. Any\nsubsequence with at least one np.nan/np.inf will automatically have its\ncorresponding value set to False in this boolean array.\n\nReturns:\n**out** – When k = 1 (default), the first column consists of the matrix profile,\nthe second column consists of the matrix profile indices, the third column\nconsists of the left matrix profile indices, and the fourth column consists\nof the right matrix profile indices. However, when k > 1, the output array\nwill contain exactly 2 \\* k + 2 columns. The first k columns (i.e., out[:, :k])\nconsists of the top-k matrix profile, the next set of k columns\n(i.e., out[:, k:2k]) consists of the corresponding top-k matrix profile\nindices, and the last two columns (i.e., out[:, 2k] and out[:, 2k+1] or,\nequivalently, out[:, -2] and out[:, -1]) correspond to the top-1 left\nmatrix profile indices and the top-1 right matrix profile indices, respectively.\n\nReturn type:\nnumpy.ndarray\n\nSee also\n\n`stumpy.stumped`Compute the z-normalized matrix profile with a distributed dask cluster\n\n`stumpy.gpu\\_stump`Compute the z-normalized matrix profile with one or more GPU devices\n\n`stumpy.scrump`Compute an approximate z-normalized matrix profile\n\nNotes\n\n\nDOI: 10.1007/s10115-017-1138-x\n\n\nSee Section 4.5\n\n\nThe above reference outlines a general approach for traversing the distance\nmatrix in a diagonal fashion rather than in a row-wise fashion.\n\n\nDOI: 10.1145/3357223.3362721\n\n\nSee Section 3.1 and Section 3.3\n\n\nThe above reference outlines the use of the Pearson correlation via Welford’s\ncentered sum-of-products along each diagonal of the distance matrix in place of the\nsliding window dot product found in the original STOMP method.\n\n\nDOI: 10.1109/ICDM.2016.0085\n\n\nSee Table II\n\n\nTimeseries, T\\_A, will be annotated with the distance location\n(or index) of all its subsequences in another times series, T\\_B.\n\n\nReturn: For every subsequence, Q, in T\\_A, you will get a distance\nand index for the closest subsequence in T\\_B. Thus, the array\nreturned will have length T\\_A.shape[0]-m+1. Additionally, the\nleft and right matrix profiles are also returned.\n\n\nNote: Unlike in the Table II where T\\_A.shape is expected to be equal\nto T\\_B.shape, this implementation is generalized so that the shapes of\nT\\_A and T\\_B can be different. In the case where T\\_A.shape == T\\_B.shape,\nthen our algorithm reduces down to the same algorithm found in Table II.\n\n\nAdditionally, unlike STAMP where the exclusion zone is m/2, the default\nexclusion zone for STOMP is m/4 (See Definition 3 and Figure 3).\n\n\nFor self-joins, set ignore\\_trivial = True in order to avoid the\ntrivial match.\n\n\nNote that left and right matrix profiles are only available for self-joins.\n\n\nExamples\n\n```\n>>> import stumpy\n>>> import numpy as np\n>>> stumpy.stump(np.array([584., -11., 23., 79., 1001., 0., -19.]), m=3)\narray([[0.11633857113691416, 4, -1, 4],\n [2.694073918063438, 3, -1, 3],\n [3.0000926340485923, 0, 0, 4],\n [2.694073918063438, 1, 1, -1],\n [0.11633857113691416, 0, 0, -1]], dtype=object)\n\n```\n## stumped#\n\n\nstumpy.stumped(*client*, *T\\_A*, *m*, *T\\_B=None*, *ignore\\_trivial=True*, *normalize=True*, *p=2.0*, *k=1*, *T\\_A\\_subseq\\_isconstant=None*, *T\\_B\\_subseq\\_isconstant=None*)[source]#\nCompute the z-normalized matrix profile with a distributed dask/ray cluster\n\n\nThis is a highly distributed implementation around the Numba JIT-compiled\nparallelized \\_stump function which computes the (top-k) matrix profile according\nto STOMPopt with\n\n==================\n Document 5 \n----------------\n# gpu\\_stimp#\n\n\nstumpy.gpu\\_stimp(*T*, *min\\_m=3*, *max\\_m=None*, *step=1*, *device\\_id=0*, *normalize=True*, *p=2.0*, *T\\_subseq\\_isconstant\\_func=None*)#\nCompute the Pan Matrix Profile with with one or more GPU devices\n\nParameters:\n* **T** (*numpy.ndarray*) – The time series or sequence for which to compute the pan matrix profile\n* **min\\_m** (*int**,* *default 3*) – The starting (or minimum) subsequence window size for which a matrix profile\nmay be computed\n* **max\\_m** (*int**,* *default None*) – The stopping (or maximum) subsequence window size for which a matrix profile\nmay be computed. When m\\_stop = Non, this is set to the maximum allowable\nsubsequence window size\n* **step** (*int**,* *default 1*) – The step between subsequence window sizes\n* **device\\_id** (*int* *or* *list**,* *default 0*) – The (GPU) device number to use. The default value is 0. A list of\nvalid device ids (int) may also be provided for parallel GPU-STUMP\ncomputation. A list of all valid device ids can be obtained by\nexecuting [device.id for device in numba.cuda.list\\_devices()].\n* **normalize** (*bool**,* *default True*) – When set to True, this z-normalizes subsequences prior to computing distances.\nOtherwise, this function gets re-routed to its complementary non-normalized\nequivalent set in the @core.non\\_normalized function decorator.\n* **p** (*float**,* *default 2.0*) – The p-norm to apply for computing the Minkowski distance. Minkowski distance is\ntypically used with p being 1 or 2, which correspond to the Manhattan distance\nand the Euclidean distance, respectively. This parameter is ignored when\nnormalize == True.\n* **T\\_subseq\\_isconstant\\_func** (*function**,* *default None*) – A custom, user-defined function that returns a boolean array that indicates\nwhether a subsequence in T is constant (True). The function must only take\ntwo arguments, a, a 1-D array, and w, the window size, while additional\narguments may be specified by currying the user-defined function using\nfunctools.partial. Any subsequence with at least one np.nan/np.inf will\nautomatically have its corresponding value set to False in this boolean array.\n\n```\n>>> import stumpy\n>>> import numpy as np\n>>> from numba import cuda\n>>> if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\":\n...     all\\_gpu\\_devices = [device.id for device in cuda.list\\_devices()]\n...     pmp = stumpy.gpu\\_stimp(\n...         np.array([584., -11., 23., 79., 1001., 0., -19.]),\n...         device\\_id=all\\_gpu\\_devices)\n...     pmp.update()\n...     pmp.PAN\\_\narray([[0., 1., 1., 1., 1., 1., 1.],\n [0., 1., 1., 1., 1., 1., 1.]])"
    },
    {
        "instruction": "Develop a Python program that utilizes the 'supervision' API for object detection using a YOLO model on a webcam feed. Annotate detected objects with labels and draw a red polygon zone on the video frame. Use the Ultralytics YOLO model for object detection.",
        "api": "supervision",
        "output": "#!pip install ultralytics supervision opencv-python\nimport cv2\nfrom ultralytics import YOLO\nimport supervision as sv\nimport numpy as np\n\nZONE_POLYGON = np.array([\n    [0, 0],\n    [0.5, 0],\n    [0.5, 1],\n    [0, 1]\n])\nwebcam_resolution = (1280, 720)\n\nframe_width, frame_height = webcam_resolution\n\ncap = cv2.VideoCapture(0)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, frame_width)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, frame_height)\n\nmodel = YOLO(\"yolov8s.pt\")\n\nbox_annotator = sv.BoxAnnotator(\n        thickness=2,\n        text_thickness=2,\n        text_scale=1\n    )\n\nzone_polygon = (ZONE_POLYGON * np.array(webcam_resolution)).astype(int)\nzone = sv.PolygonZone(polygon=zone_polygon, frame_resolution_wh=tuple(webcam_resolution))\nzone_annotator = sv.PolygonZoneAnnotator(\n        zone=zone, \n        color=sv.Color.red(),\n        thickness=2,\n        text_thickness=4,\n        text_scale=2\n    )\n\nwhile True:\n  _, frame = cap.read()\n\n  result = model(frame, agnostic_nms=True)[0]\n  detections = sv.Detections.from_ultralytics(result)\n  print(detections)\n  labels = [\n            f\"{model.model.names[class_id]} {confidence:0.2f}\"\n            for _, _, confidence, class_id, _\n            in detections\n        ]\n  frame = box_annotator.annotate(\n            scene=frame, \n            detections=detections, \n            labels=labels\n        )\n\n  zone.trigger(detections=detections)\n  frame = zone_annotator.annotate(scene=frame)      \n        \n  cv2.imshow(\"yolov8\", frame)\n\n  if (cv2.waitKey(30) == 27):\n      break",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n\n# Home\n\n\n\n\n## 👋 Hello¶\n\n\nWe write your reusable computer vision tools. Whether you need to load your dataset from your hard drive, draw detections on an image or video, or count how many detections are in a zone. You can count on us!\n\n\n\n## 💻 Install¶\n\n\nYou can install `supervision` with pip in a\n**3.11>=Python>=3.8** environment.\n\npip install (recommended)\n\n\nheadlessdesktop\n\n\nThe headless installation of `supervision` is designed for environments where graphical user interfaces (GUI) are not needed, making it more lightweight and suitable for server-side applications.\n\n```\npip install supervision\n\n```\n\nIf you require the full version of `supervision` with GUI support you can install the desktop version. This version includes the GUI components of OpenCV, allowing you to display images and videos on the screen.\n\n```\npip install supervision[desktop]\n\n\ngit clone (for development)\n\n\nvirtualenvpoetry\n\n```\n\n# clone repository and navigate to root directory\ngit clone https://github.com/roboflow/supervision.git\ncd supervision\n\n\n# setup python environment and activate it\npython3 -m venv venv\nsource venv/bin/activate\n\n\n# headless install\npip install -e \".\"\n\n\n# desktop install\npip install -e \".[desktop]\"\n\n\n```\n\n# clone repository and navigate to root directory\ngit clone https://github.com/roboflow/supervision.git\ncd supervision\n\n\n# setup python environment and activate it\npoetry env use python 3.10\npoetry shell\n\n\n# headless install\npoetry install\n\n\n# desktop install\npoetry install --extras \"desktop\"\n\n\n\n# Detections\n\n\n\n## advanced filtering¶\n\n\nThe advanced filtering capabilities of the `Detections` class offer users a versatile and efficient way to narrow down\nand refine object detections. This section outlines various filtering methods, including filtering by specific class\nor a set of classes, confidence, object area, bounding box area, relative area, box dimensions, and designated zones.\nEach method is demonstrated with concise code examples to provide users with a clear understanding of how to implement\nthe filters in their applications.\n\n\n\n### by specific class¶\n\n\nAllows you to select detections that belong only to one selected class.\n\n\nAfterBefore\n\n```\nimport supervision as sv\n\ndetections = sv.Detections(...)\ndetections = detections[detections.class\\_id == 0]\n\n\n\n\n\n```\nimport supervision as sv\n\n\n\n\n\n\n### by set of classes¶\n\n\nAllows you to select detections that belong only to selected set of classes.\n\n```\nimport numpy as np\nimport supervision as sv\n\nselected\\_classes = [0, 2, 3]\ndetections = sv.Detections(...)\ndetections = detections[np.isin(detections.class\\_id, selected\\_classes)]\n\n\n\n\n\n```\nimport numpy as np\nimport supervision as sv\n\nclass\\_id = [0, 2, 3]\ndetections = sv.Detections(...)\ndetections = detections[np.isin(detections.class\\_id, class\\_id)]\n\n\n\n### by confidence¶\n\n\nAllows you to select detections with specific confidence value, for example higher than selected threshold.\n\ndetections = sv.Detections(...)\ndetections = detections[detections.confidence > 0.5]\n\n\n\n\n\n\n### by area¶\n\n\nAllows you to select detections based on their size. We define the area as the number of pixels occupied by the\ndetection in the image. In the example below, we have sifted out the detections that are too small.\n\ndetections = sv.Detections(...)\ndetections = detections[detections.area > 1000]\n\n\n\n\n\n### by relative area¶\n\n\nAllows you to select detections based on their size in relation to the size of whole image. Sometimes the concept of\ndetection size changes depending on the image. Detection occupying 10000 square px can be large on a\n\n==================\n Document 1 \n----------------\n## 0.13.0 August 8, 2023¶\n\n\n* Added #236: support for mean average precision (mAP) for object detection models with `sv.MeanAveragePrecision`.\n\n>>> model = YOLO(...)\n>>> def callback(image: np.ndarray) -> sv.Detections:\n...     result = model(image)[0]\n...     return sv.Detections.from\\_yolov8(result)\n\n* Added #256: support for ByteTrack for object tracking with `sv.ByteTrack`.\n* Added #222: `sv.Detections.from_ultralytics` to enable seamless integration with Ultralytics framework. This will enable you to use `supervision` with all models that Ultralytics supports.\n\n\n`sv.Detections.from_yolov8` is now deprecated and will be removed with `supervision-0.15.0` release.\n\n* Added #191: `sv.Detections.from_paddledet` to enable seamless integration with PaddleDetection framework.\n* Added #245: support for loading PASCAL VOC segmentation datasets with `sv.DetectionDataset.`.\n\n\n### 0.12.0 July 24, 2023¶\n\n\nWith the `supervision-0.12.0` release, we are terminating official support for Python 3.7.\n\n* Added #177: initial support for object detection model benchmarking with `sv.ConfusionMatrix`.\n\n* Added #173: `Detections.from_mmdetection` to enable seamless integration with MMDetection framework.\n* Added #130: ability to install package in `headless` or `desktop` mode.\n* Changed #180: packing method from `setup.py` to `pyproject.toml`.\n* Fixed #188: `sv.DetectionDataset.from_cooc` can't be loaded when there are images without annotations.\n* Fixed #226: `sv.DetectionDataset.from_yolo` can't load background instances.\n\n\n\n### 0.11.1 June 29, 2023¶\n\n\n* Fix #165: `as_folder_structure` fails to save `sv.ClassificationDataset` when it is result of inference.\n\n\n\n### 0.11.0 June 28, 2023¶\n\n\n* Added #150: ability to load and save `sv.DetectionDataset` in COCO format using `as_coco` and `from_coco` methods.\n\n>>> ds = sv.DetectionDataset.from\\_coco(\n...     images\\_directory\\_path='...',\n...     annotations\\_path='...'\n... )\n\n>>> ds.as\\_coco(\n...     images\\_directory\\_path='...',\n...     annotations\\_path='...'\n... )\n\n* Added #158: ability to marge multiple `sv.DetectionDataset` together using `merge` method.\n\n* Added #162: additional `start` and `end` arguments to `sv.get_video_frames_generator` allowing to generate frames only for a selected part of the video.\n* Fix #157: incorrect loading of YOLO dataset class names from `data.yaml`.\n\n\n\n### 0.10.0 June 14, 2023¶\n\n\n* Added #125: ability to load and save `sv.ClassificationDataset` in a folder structure format.\n\n>>> cs = sv.ClassificationDataset.from\\_folder\\_structure(\n...     root\\_directory\\_path='...'\n... )\n\n>>> cs.as\\_folder\\_structure(\n...     root\\_directory\\_path='...'\n... )\n\n* Added #125: support for `sv.ClassificationDataset.split` allowing to divide `sv.ClassificationDataset` into two parts.\n* Added #110: ability to extract masks from Roboflow API results using `sv.Detections.from_roboflow`.\n* Added commit hash: Supervision Quickstart notebook where you can learn more about Detection, Dataset and Video APIs.\n* Changed #135: `sv.get_video_frames_generator` documentation to better describe actual behavior.\n\n\n### 0.9.0 June 7, 2023¶\n\n\n* Added #118: ability to select `sv.Detections` by index, list of indexes or slice. Here is an example illustrating the new selection methods.\n\n>>> detections = sv.Detections(...)\n>>> len(detections[0])\n1\n>>> len(detections[[0, 1]])\n2\n>>> len(detections[0:2])\n2\n\n* Added #101: ability to extract masks\n\n==================\n Document 2 \n----------------\n## 0.14.0 August 31, 2023¶\n\n\n* Added #282: support for SAHI inference technique with `sv.InferenceSlicer`.\n\n* Added #297: `Detections.from_deepsparse` to enable seamless integration with DeepSparse framework.\n* Added #281: `sv.Classifications.from_ultralytics` to enable seamless integration with Ultralytics framework. This will enable you to use supervision with all models that Ultralytics supports.\n\n\nsv.Detections.from\\_yolov8 and sv.Classifications.from\\_yolov8 are now deprecated and will be removed with supervision-0.16.0 release.\n\n* Added #341: First supervision usage example script showing how to detect and track objects on video using YOLOv8 + Supervision.\n* Changed #296: `sv.ClassificationDataset` and `sv.DetectionDataset` now use image path (not image name) as dataset keys.\n* Fixed #300: `Detections.from_roboflow` to filter out polygons with less than 3 points.\n\n### 0.13.0 August 8, 2023¶\n\n\n* Added #236: support for mean average precision (mAP) for object detection models with `sv.MeanAveragePrecision`.\n\n>>> model = YOLO(...)\n>>> def callback(image: np.ndarray) -> sv.Detections:\n...     result = model(image)[0]\n...     return sv.Detections.from\\_yolov8(result)\n\n*\n\n==================\n Document 3 \n----------------\n# PolygonZoneAnnotator¶\n\nA class for annotating a polygon-shaped zone within a\n frame with a count of detected objects.\n\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `zone` | `PolygonZone` | \nThe polygon zone to be annotated\n |\n| `color` | `Color` | \nThe color to draw the polygon lines\n |\n| `thickness` | `int` | \nThe thickness of the polygon lines, default is 2\n |\n| `text_color` | `Color` | \nThe color of the text on the polygon, default is black\n |\n| `text_scale` | `float` | \nThe scale of the text on the polygon, default is 0.5\n |\n| `text_thickness` | `int` | \nThe thickness of the text on the polygon, default is 1\n |\n| `text_padding` | `int` | \nThe padding around the text on the polygon, default is 10\n |\n| `font` | `int` | \nThe font type for the text on the polygon,\ndefault is cv2.FONT\\_HERSHEY\\_SIMPLEX\n |\n| `center` | `Tuple[int, int]` | \nThe center of the polygon for text placement\n |\n\n\n|  |  |\n| --- | --- |\n| \n```\n 70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n```\n | \n```\nclass PolygonZoneAnnotator:\n \"\"\"\n A class for annotating a polygon-shaped zone within a\n frame with a count of detected objects.\n\n Attributes:\n zone (PolygonZone): The polygon zone to be annotated\n color (Color): The color to draw the polygon lines\n thickness (int): The thickness of the polygon lines, default is 2\n text\\_color (Color): The color of the text on the polygon, default is black\n text\\_scale (float): The scale of the text on the polygon, default is 0.5\n text\\_thickness (int): The thickness of the text on the polygon, default is 1\n text\\_padding (int): The padding around the text on the polygon, default is 10\n font (int): The font type for the text on the polygon,\n default is cv2.FONT\\_HERSHEY\\_SIMPLEX\n center (Tuple[int, int]): The center of the polygon for text placement\n \"\"\"\n\n    def \\_\\_init\\_\\_(\n        self,\n        zone: PolygonZone,\n        color: Color,\n        thickness: int = 2,\n        text\\_color: Color = Color.black(),\n        text\\_scale: float = 0.5,\n        text\\_thickness: int = 1,\n        text\\_padding: int = 10,\n    ):\n        self.zone = zone\n        self.color = color\n        self.thickness = thickness\n        self.text\\_color = text\\_color\n        self.text\\_scale = text\\_scale\n        self.text\\_thickness = text\\_thickness\n        self.text\\_padding = text\\_padding\n        self.font = cv2.FONT\\_HERSHEY\\_SIMPLEX\n        self.center = get\\_polygon\\_center(polygon=zone.polygon)\n\n    def annotate(self, scene: np.ndarray, label: Optional[str] = None) -> np.ndarray:\n \"\"\"\n Annotates the polygon zone within a frame with a count of detected objects.\n\n Parameters:\n scene (np.ndarray): The image on which the polygon zone will be annotated\n label (Optional[str]): An optional label for the count of detected objects\n within the polygon zone (default: None)\n\n Returns:\n np.ndarray: The image with the polygon zone and count of detected objects\n \"\"\"\n        annotated\\_frame = draw\\_polygon(\n            scene=scene,\n            polygon=self.zone.polygon,\n            color=self.color,\n            thickness=self.thickness,\n        )\n\n        annotated\\_frame = draw\\_text(\n            scene=annotated\\_frame,\n            text=str(self.zone.current\\_count) if label is None else label,\n            text\\_anchor=self.center,\n            background\\_color=self.color,\n            text\\_color=self.text\\_color,\n            text\\_scale=self.text\\_scale,\n            text\\_thickness=self.text\\_thickness,\n            text\\_padding=self.text\\_padding,\n            text\\_font=self.font,\n        )\n\n        return annotated\\_frame\n## \n`annotate(scene, label=None)`\n¶\n\nAnnotates the polygon zone within a frame with a count of detected objects.\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `scene` | `ndarray` | \nThe image on which the\n\n==================\n Document 4 \n----------------\n# \n`annotate(scene, label=None)`\n¶\n\nAnnotates the polygon zone within a frame with a count of detected objects.\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `scene` | `ndarray` | \nThe image on which the polygon zone will be annotated\n | *required* |\n| `label` | `Optional[str]` | \nAn optional label for the count of detected objects\nwithin the polygon zone (default: None)\n | `None` |\n\n\n| Type | Description |\n| --- | --- |\n| `ndarray` | \nnp.ndarray: The image with the polygon zone and count of detected objects\n |\n\n\n|  |  |\n| --- | --- |\n| \n```\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n```\n | \n```\ndef annotate(self, scene: np.ndarray, label: Optional[str] = None) -> np.ndarray:\n \"\"\"\n Annotates the polygon zone within a frame with a count of detected objects.\n\n Returns:\n np.ndarray: The image with the polygon zone and count of detected objects\n \"\"\"\n    annotated\\_frame = draw\\_polygon(\n        scene=scene,\n        polygon=self.zone.polygon,\n        color=self.color,\n        thickness=self.thickness,\n    )\n\n    annotated\\_frame = draw\\_text(\n        scene=annotated\\_frame,\n        text=str(self.zone.current\\_count) if label is None else label,\n        text\\_anchor=self.center,\n        background\\_color=self.color,\n        text\\_color=self.text\\_color,\n        text\\_scale=self.text\\_scale,\n        text\\_thickness=self.text\\_thickness,\n        text\\_padding=self.text\\_padding,\n        text\\_font=self.font,\n    )\n\n    return annotated\\_frame\n\n# Inference Slicer\n\n\n## InferenceSlicer¶\n\nInferenceSlicer performs slicing-based inference for small target detection. This\nmethod, often referred to as Slicing Adaptive Inference (SAHI), involves dividing a\nlarger image into smaller slices, performing inference on each slice, and then\nmerging the detections.\n\n\n| Name | Type | Description |\n|\n\n==================\n Document 5 \n----------------\n# \n`annotate(scene, detections, labels=None, skip\\_label=False)`\n¶\n\nDraws bounding boxes on the frame using the detections provided.\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `scene` | `ndarray` | \nThe image on which the bounding boxes will be drawn\n | *required* |\n| `detections` | `Detections` | \nThe detections for which the\nbounding boxes will be drawn\n | *required* |\n| `labels` | `Optional[List[str]]` | \nAn optional list of labels\ncorresponding to each detection. If `labels` are not provided,\ncorresponding `class_id` will be used as label.\n | `None` |\n| `skip_label` | `bool` | \nIs set to `True`, skips bounding box label annotation.\n | `False` |\n\n\nReturns:\n np.ndarray: The image with the bounding boxes drawn on it\n\n>>> classes = ['person', ...]\n>>> image = ...\n>>> detections = sv.Detections(...)\n\n>>> box\\_annotator = sv.BoxAnnotator()\n>>> labels = [\n...     f\"{classes[class\\_id]} {confidence:0.2f}\"\n...     for \\_, \\_, confidence, class\\_id, \\_\n...     in detections\n... ]\n>>> annotated\\_frame = box\\_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections,\n...     labels=labels\n... )\n\n\n|  |  |\n| --- | --- |\n| \n```\n 44\n 45\n 46\n 47\n 48\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n 65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n```\n | \n```\ndef annotate(\n    self,\n    scene: np.ndarray,\n    detections: Detections,\n    labels: Optional[List[str]] = None,\n    skip\\_label: bool = False,\n) -> np.ndarray:\n \"\"\"\n Draws bounding boxes on the frame using the detections provided.\n\n >>> box\\_annotator = sv.BoxAnnotator()\n >>> labels = [\n ... f\"{classes[class\\_id]} {confidence:0.2f}\"\n ... for \\_, \\_, confidence, class\\_id, \\_\n ... in detections\n ... ]\n >>> annotated\\_frame = box\\_annotator.annotate(\n ... scene=image.copy(),\n ... detections=detections,\n ... labels=labels\n ... )\n ```\n \"\"\"\n    font = cv2.FONT\\_HERSHEY\\_SIMPLEX\n    for i in range(len(detections)):\n        x1, y1, x2, y2 = detections.xyxy[i].astype(int)\n        class\\_id = (\n            detections.class\\_id[i] if detections.class\\_id is not None else None\n        )\n        idx = class\\_id if class\\_id is not None else i\n        color = (\n            self.color.by\\_idx(idx)\n            if isinstance(self.color, ColorPalette)\n            else self.color\n        )\n        cv2.rectangle(\n            img=scene,\n            pt1=(x1, y1),\n            pt2=(x2, y2),\n            color=color.as\\_bgr(),\n            thickness=self.thickness,\n        )\n        if skip\\_label:\n            continue\n\n        text = (\n            f\"{class\\_id}\"\n            if (labels is None or len(detections) != len(labels))\n            else labels[i]\n        )\n\n        text\\_width, text\\_height = cv2.getTextSize(\n            text=text,\n            fontFace=font,\n            fontScale=self.text\\_scale,\n            thickness=self.text\\_thickness,\n        )[0]\n\n        text\\_x = x1 + self.text\\_padding\n        text\\_y = y1 - self.text\\_padding\n\n        text\\_background\\_x1 = x1\n        text\\_background\\_y1 = y1 - 2 \\* self.text\\_padding - text\\_height\n\n        text\\_background\\_x2 = x1 + 2 \\* self.text\\_padding + text\\_width\n        text\\_background\\_y2 = y1\n\n        cv2.rectangle(\n            img=scene,\n            pt1=(text\\_background\\_x1, text\\_background\\_y1),\n            pt2=(text\\_background\\_x2, text\\_background\\_y2),\n            color=color.as\\_bgr(),\n            thickness=cv2.FILLED,\n        )\n        cv2.putText(\n            img=scene,\n            text=text,\n            org=(text\\_x, text\\_y),\n            fontFace=font,\n            fontScale=self.text\\_scale,\n            color=self.text\\_color.as\\_rgb(),\n            thickness=self.text\\_thickness,\n            lineType=cv2.LINE\\_AA,\n        )\n    return scene\n## MaskAnnotator¶\n\nA class for overlaying masks on an image using detections provided.\n\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `color` | `Union[Color, ColorPalette]` | \nThe color to fill the mask,\ncan be a single color or"
    },
    {
        "instruction": "Develop a Python program that utilizes the 'supervision' API to perform image classification using a pre-trained ResNet model. Load an image from a file, classify it using the ResNet model, and display the predicted class label along with the confidence score.",
        "api": "supervision",
        "output": "#!pip install supervision torchvision\nimport torch\nimport torchvision.transforms as transforms\nimport supervision as sv\n\n# Load the pre-trained ResNet model\nmodel = torch.hub.load('pytorch/vision:v0.9.0', 'resnet18', pretrained=True)\nmodel.eval()\n\n# Define the image transformation pipeline\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Load the image from file\nimage_path = 'image.jpg'\nimage = sv.Image.from_file(image_path)\n\n# Apply the transformation pipeline to the image\ninput_tensor = transform(image)\n\n# Add a batch dimension to the input tensor\ninput_tensor = input_tensor.unsqueeze(0)\n\n# Perform the classification\nwith torch.no_grad():\n    output = model(input_tensor)\n\n# Get the predicted class label and confidence score\n_, predicted_class = torch.max(output, 1)\nconfidence = torch.nn.functional.softmax(output, dim=1)[0, predicted_class]\n\n# Display the predicted class label and confidence score\nprint(f\"Predicted class: {predicted_class.item()}\")\nprint(f\"Confidence: {confidence.item()}\")",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n\n# Home\n\n\n\n\n## 👋 Hello¶\n\n\nWe write your reusable computer vision tools. Whether you need to load your dataset from your hard drive, draw detections on an image or video, or count how many detections are in a zone. You can count on us!\n\n\n\n## 💻 Install¶\n\n\nYou can install `supervision` with pip in a\n**3.11>=Python>=3.8** environment.\n\npip install (recommended)\n\n\nheadlessdesktop\n\n\nThe headless installation of `supervision` is designed for environments where graphical user interfaces (GUI) are not needed, making it more lightweight and suitable for server-side applications.\n\n```\npip install supervision\n\n```\n\nIf you require the full version of `supervision` with GUI support you can install the desktop version. This version includes the GUI components of OpenCV, allowing you to display images and videos on the screen.\n\n```\npip install supervision[desktop]\n\n\ngit clone (for development)\n\n\nvirtualenvpoetry\n\n```\n\n# clone repository and navigate to root directory\ngit clone https://github.com/roboflow/supervision.git\ncd supervision\n\n\n# setup python environment and activate it\npython3 -m venv venv\nsource venv/bin/activate\n\n\n# headless install\npip install -e \".\"\n\n\n# desktop install\npip install -e \".[desktop]\"\n\n\n```\n\n# clone repository and navigate to root directory\ngit clone https://github.com/roboflow/supervision.git\ncd supervision\n\n\n# setup python environment and activate it\npoetry env use python 3.10\npoetry shell\n\n\n# headless install\npoetry install\n\n\n# desktop install\npoetry install --extras \"desktop\"\n\n\n\n# Detections\n\n\n\n## advanced filtering¶\n\n\nThe advanced filtering capabilities of the `Detections` class offer users a versatile and efficient way to narrow down\nand refine object detections. This section outlines various filtering methods, including filtering by specific class\nor a set of classes, confidence, object area, bounding box area, relative area, box dimensions, and designated zones.\nEach method is demonstrated with concise code examples to provide users with a clear understanding of how to implement\nthe filters in their applications.\n\n\n\n### by specific class¶\n\n\nAllows you to select detections that belong only to one selected class.\n\n\nAfterBefore\n\n```\nimport supervision as sv\n\ndetections = sv.Detections(...)\ndetections = detections[detections.class\\_id == 0]\n\n\n\n\n\n```\nimport supervision as sv\n\n\n\n\n\n\n### by set of classes¶\n\n\nAllows you to select detections that belong only to selected set of classes.\n\n```\nimport numpy as np\nimport supervision as sv\n\nselected\\_classes = [0, 2, 3]\ndetections = sv.Detections(...)\ndetections = detections[np.isin(detections.class\\_id, selected\\_classes)]\n\n\n\n\n\n```\nimport numpy as np\nimport supervision as sv\n\nclass\\_id = [0, 2, 3]\ndetections = sv.Detections(...)\ndetections = detections[np.isin(detections.class\\_id, class\\_id)]\n\n\n\n### by confidence¶\n\n\nAllows you to select detections with specific confidence value, for example higher than selected threshold.\n\ndetections = sv.Detections(...)\ndetections = detections[detections.confidence > 0.5]\n\n\n\n\n\n\n### by area¶\n\n\nAllows you to select detections based on their size. We define the area as the number of pixels occupied by the\ndetection in the image. In the example below, we have sifted out the detections that are too small.\n\ndetections = sv.Detections(...)\ndetections = detections[detections.area > 1000]\n\n\n\n\n\n### by relative area¶\n\n\nAllows you to select detections based on their size in relation to the size of whole image. Sometimes the concept of\ndetection size changes depending on the image. Detection occupying 10000 square px can be large on a\n\n==================\n Document 1 \n----------------\n## 0.13.0 August 8, 2023¶\n\n\n* Added #236: support for mean average precision (mAP) for object detection models with `sv.MeanAveragePrecision`.\n\n>>> model = YOLO(...)\n>>> def callback(image: np.ndarray) -> sv.Detections:\n...     result = model(image)[0]\n...     return sv.Detections.from\\_yolov8(result)\n\n* Added #256: support for ByteTrack for object tracking with `sv.ByteTrack`.\n* Added #222: `sv.Detections.from_ultralytics` to enable seamless integration with Ultralytics framework. This will enable you to use `supervision` with all models that Ultralytics supports.\n\n\n`sv.Detections.from_yolov8` is now deprecated and will be removed with `supervision-0.15.0` release.\n\n* Added #191: `sv.Detections.from_paddledet` to enable seamless integration with PaddleDetection framework.\n* Added #245: support for loading PASCAL VOC segmentation datasets with `sv.DetectionDataset.`.\n\n\n### 0.12.0 July 24, 2023¶\n\n\nWith the `supervision-0.12.0` release, we are terminating official support for Python 3.7.\n\n* Added #177: initial support for object detection model benchmarking with `sv.ConfusionMatrix`.\n\n* Added #173: `Detections.from_mmdetection` to enable seamless integration with MMDetection framework.\n* Added #130: ability to install package in `headless` or `desktop` mode.\n* Changed #180: packing method from `setup.py` to `pyproject.toml`.\n* Fixed #188: `sv.DetectionDataset.from_cooc` can't be loaded when there are images without annotations.\n* Fixed #226: `sv.DetectionDataset.from_yolo` can't load background instances.\n\n\n\n### 0.11.1 June 29, 2023¶\n\n\n* Fix #165: `as_folder_structure` fails to save `sv.ClassificationDataset` when it is result of inference.\n\n\n\n### 0.11.0 June 28, 2023¶\n\n\n* Added #150: ability to load and save `sv.DetectionDataset` in COCO format using `as_coco` and `from_coco` methods.\n\n>>> ds = sv.DetectionDataset.from\\_coco(\n...     images\\_directory\\_path='...',\n...     annotations\\_path='...'\n... )\n\n>>> ds.as\\_coco(\n...     images\\_directory\\_path='...',\n...     annotations\\_path='...'\n... )\n\n* Added #158: ability to marge multiple `sv.DetectionDataset` together using `merge` method.\n\n* Added #162: additional `start` and `end` arguments to `sv.get_video_frames_generator` allowing to generate frames only for a selected part of the video.\n* Fix #157: incorrect loading of YOLO dataset class names from `data.yaml`.\n\n\n\n### 0.10.0 June 14, 2023¶\n\n\n* Added #125: ability to load and save `sv.ClassificationDataset` in a folder structure format.\n\n>>> cs = sv.ClassificationDataset.from\\_folder\\_structure(\n...     root\\_directory\\_path='...'\n... )\n\n>>> cs.as\\_folder\\_structure(\n...     root\\_directory\\_path='...'\n... )\n\n* Added #125: support for `sv.ClassificationDataset.split` allowing to divide `sv.ClassificationDataset` into two parts.\n* Added #110: ability to extract masks from Roboflow API results using `sv.Detections.from_roboflow`.\n* Added commit hash: Supervision Quickstart notebook where you can learn more about Detection, Dataset and Video APIs.\n* Changed #135: `sv.get_video_frames_generator` documentation to better describe actual behavior.\n\n\n### 0.9.0 June 7, 2023¶\n\n\n* Added #118: ability to select `sv.Detections` by index, list of indexes or slice. Here is an example illustrating the new selection methods.\n\n>>> detections = sv.Detections(...)\n>>> len(detections[0])\n1\n>>> len(detections[[0, 1]])\n2\n>>> len(detections[0:2])\n2\n\n* Added #101: ability to extract masks\n\n==================\n Document 2 \n----------------\n## 0.14.0 August 31, 2023¶\n\n\n* Added #282: support for SAHI inference technique with `sv.InferenceSlicer`.\n\n* Added #297: `Detections.from_deepsparse` to enable seamless integration with DeepSparse framework.\n* Added #281: `sv.Classifications.from_ultralytics` to enable seamless integration with Ultralytics framework. This will enable you to use supervision with all models that Ultralytics supports.\n\n\nsv.Detections.from\\_yolov8 and sv.Classifications.from\\_yolov8 are now deprecated and will be removed with supervision-0.16.0 release.\n\n* Added #341: First supervision usage example script showing how to detect and track objects on video using YOLOv8 + Supervision.\n* Changed #296: `sv.ClassificationDataset` and `sv.DetectionDataset` now use image path (not image name) as dataset keys.\n* Fixed #300: `Detections.from_roboflow` to filter out polygons with less than 3 points.\n\n### 0.13.0 August 8, 2023¶\n\n\n* Added #236: support for mean average precision (mAP) for object detection models with `sv.MeanAveragePrecision`.\n\n>>> model = YOLO(...)\n>>> def callback(image: np.ndarray) -> sv.Detections:\n...     result = model(image)[0]\n...     return sv.Detections.from\\_yolov8(result)\n\n*"
    },
    {
        "instruction": "Create a Python program that uses the 'tensorflow' API to build a simple feedforward neural network for binary classification. The program should generate synthetic data, compile the model, train it on the data, and evaluate its performance.",
        "api": "tensorflow",
        "output": "#pip install tensorflow\nimport tensorflow as tf\nimport numpy as np\n\n# Generate synthetic data for binary classification\ndata_size = 1000\ndata_x = np.random.rand(data_size, 2)\ndata_y = (data_x[:, 0] + data_x[:, 1] > 1).astype(int)\n\n# Build a simple feedforward neural network\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(2,)),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model on the synthetic data\nprint(\"Training............\")\nmodel.fit(data_x, data_y, epochs=10)\n\n\nprint(\"\\nEvaluation............\")\nloss, accuracy = model.evaluate(data_x, data_y)\nprint(\"Accuracy: \",accuracy)\nprint(\"Loss: \",loss)\n\npredictions = model.predict(data_x)\nprint(\"Are all predictions between 0 and 1?\",np.all(predictions >= 0) and np.all(predictions <= 1))",
        "documentation": "\n\n==================\n Document 0 \n----------------\n Input builders\ndef input_fn_train:\n  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n  # index.\n  pass\ndef input_fn_eval:\n  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n  # index.\n  pass\ndef input_fn_predict:\n  # Returns tf.data.Dataset of (x, None) tuple.\n  pass\nestimator.train(input_fn=input_fn_train)\nmetrics = estimator.evaluate(input_fn=input_fn_eval)\npredictions = estimator.predict(input_fn=input_fn_predict)\n\nInput of `train` and `evaluate` should have following features,\notherwise there will be a `KeyError`:\n\n\n* if `weight_column` is not `None`, a feature with `key=weight_column` whose\nvalue is a `Tensor`.\n* for each `column` in `feature_columns`:\n\t+ if `column` is a `CategoricalColumn`, a feature with `key=column.name`\n\twhose `value` is a `SparseTensor`.\n\t+ if `column` is a `WeightedCategoricalColumn`, two features: the first\n\twith `key` the id column name, the second with `key` the weight column\n\tname. Both features' `value` must be a `SparseTensor`.\n\t+ if `column` is a `DenseColumn`, a feature with `key=column.name`\n\twhose `value` is a `Tensor`.\n\n\nLoss is calculated by using softmax cross entropy.\n\n# tf.compat.v1.estimator.DNNEstimator\n\nAn estimator for TensorFlow DNN models with user-specified head. (deprecated) (deprecated)\n\ntf.compat.v1.estimator.DNNEstimator(\n    head,\n    hidden_units,\n    feature_columns,\n    model_dir=None,\n    optimizer='Adagrad',\n    activation_fn=`tf.nn.relu`,\n    dropout=None,\n    input_layer_partitioner=None,\n    config=None,\n    warm_start_from=None,\n    batch_norm=False\n)\n\n```\nsparse_feature_a = sparse_column_with_hash_bucket(...)\nsparse_feature_b = sparse_column_with_hash_bucket(...)\n\nsparse_feature_a_emb = embedding_column(sparse_id_column=sparse_feature_a,\n                                        ...)\nsparse_feature_b_emb = embedding_column(sparse_id_column=sparse_feature_b,\n                                        ...)\n\nestimator = tf.estimator.DNNEstimator(\n    head=tf.estimator.MultiLabelHead(n_classes=3),\n    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n    hidden_units=[1024, 512, 256])\n\n\n# Or estimator using the ProximalAdagradOptimizer optimizer with\n\n# regularization.\nestimator = tf.estimator.DNNEstimator(\n    head=tf.estimator.MultiLabelHead(n_classes=3),\n    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n    hidden_units=[1024, 512, 256],\n    optimizer=tf.compat.v1.train.ProximalAdagradOptimizer(\n      learning_rate=0.1,\n      l1_regularization_strength=0.001\n    ))\n\n\n# Or estimator using an optimizer with a learning rate decay.\nestimator = tf.estimator.DNNEstimator(\n    head=tf.estimator.MultiLabelHead(n_classes=3),\n    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n    hidden_units=[1024, 512, 256],\n    optimizer=lambda: tf.keras.optimizers.Adam(\n        learning_rate=tf.compat.v1.train.exponential_decay(\n            learning_rate=0.1,\n            global_step=tf.compat.v1.train.get_global_step(),\n            decay_steps=10000,\n            decay_rate=0.96))\n\n\n# Or estimator with warm-starting from a previous checkpoint.\nestimator = tf.estimator.DNNEstimator(\n    head=tf.estimator.MultiLabelHead(n_classes=3),\n    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n    hidden_units=[1024, 512, 256],\n    warm_start_from=\"/path/to/checkpoint/dir\")\n\n\nLoss and predicted output are determined by the specified head.\n\n\n# tf.compat.v1.estimator.DNNLinearCombinedClassifier\n\nAn estimator for TensorFlow Linear and DNN joined classification models. (deprecated) (deprecated)\n\ntf.compat.v1.estimator.DNNLinearCombinedClassifier(\n    model_dir=None,\n    linear_feature_columns=None,\n    linear_optimizer='Ftrl',\n    dnn_feature_columns=None,\n    dnn_optimizer='Adagrad',\n    dnn_hidden_units=None,\n    dnn_activation_fn=`tf.nn.relu`,\n    dnn_dropout=None,\n    n_classes=2,\n    weight_column=None,\n    label_vocabulary=None,\n    input_layer_partitioner=None,\n    config=None,\n    warm_start_from=None,\n    loss_reduction=tf.compat.v1.losses.Reduction.SUM,\n    batch_norm=False,\n    linear_sparse_combiner='sum'\n)\n\n```\nnumeric_feature = numeric_column(...)\ncategorical_column_a = categorical_column_with_hash_bucket(...)\ncategorical_column_b = categorical_column_with_hash_bucket(...)\n\ncategorical_feature_a_x_categorical_feature_b = crossed_column(...)\ncategorical_feature_a_emb = embedding_column(\n    categorical_column=categorical_feature_a, ...)\ncategorical_feature_b_emb = embedding_column(\n    categorical_id_column=categorical_feature_b, ...)\n\nestimator = tf.estimator.DNNLinearCombinedClassifier(\n    # wide settings\n    linear_feature_columns=[categorical_feature_a_x_categorical_feature_b],\n    linear_optimizer=tf.keras.optimizers.Ftrl(...),\n    # deep settings\n    dnn_feature_columns=[\n        categorical_feature_a_emb, categorical_feature_b_emb,\n        numeric_feature],\n    dnn_hidden_units=[1000, 500, 100],\n    dnn_optimizer=tf.keras.optimizers.Adagrad(...),\n    # warm-start settings\n    warm_start_from=\"/path/to/checkpoint/dir\")\n\n\n# To apply L1 and L2 regularization, you can set dnn_optimizer to:\ntf.compat.v1.train.ProximalAdagradOptimizer(\n    learning_rate=0.1,\n    l1_regularization_strength=0.001,\n    l2_regularization_strength=0.001)\n\n# To apply learning rate decay, you can set dnn_optimizer to a callable:\nlambda: tf.keras.optimizers.Adam(\n    learning_rate=tf.compat.v1.train.exponential_decay(\n        learning_rate=0.1,\n        global_step=tf.compat.v1.train.get_global_step(),\n        decay_steps=10000,\n        decay_rate=0.96)\n\n# It is the same for linear_optimizer.\n\n# Input builders\ndef input_fn_train:\n  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n  # index.\n  pass\ndef input_fn_eval:\n  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n  # index.\n\n==================\n Document 1 \n----------------\n### Use for a single program\n\n```\nwith tf.Graph().as_default():\n  ...add operations to the graph...\n  # Create a Supervisor that will checkpoint the model in '/tmp/mydir'.\n  sv = Supervisor(logdir='/tmp/mydir')\n  # Get a TensorFlow session managed by the supervisor.\n  with sv.managed_session(FLAGS.master) as sess:\n    # Use the session to train the graph.\n    while not sv.should_stop():\n      sess.run(<my_train_op>)\n\nWithin the `with sv.managed_session()` block all variables in the graph have\nbeen initialized. In addition, a few services have been started to\ncheckpoint the model and add summaries to the event log.\n\n\nIf the program crashes and is restarted, the managed session automatically\nreinitialize variables from the most recent checkpoint.\n\n\nThe supervisor is notified of any exception raised by one of the services.\nAfter an exception is raised, `should_stop()` returns `True`. In that case\nthe training loop should also stop. This is why the training loop has to\ncheck for `sv.should_stop()`.\n\n\nExceptions that indicate that the training inputs have been exhausted,\n`tf.errors.OutOfRangeError`, also cause `sv.should_stop()` to return `True`\nbut are not re-raised from the `with` block: they indicate a normal\ntermination.\n\n\n#### Use for multiple replicas\n\n\nTo train with replicas you deploy the same program in a `Cluster`.\nOne of the tasks must be identified as the *chief*: the task that handles\ninitialization, checkpoints, summaries, and recovery. The other tasks\ndepend on the *chief* for these services.\n\n\nThe only change you have to do to the single program code is to indicate\nif the program is running as the *chief*.\n\n```\n\n# Choose a task as the chief. This could be based on server_def.task_index,\n\n# or job_def.name, or job_def.tasks. It's entirely up to the end user.\n# But there can be only one *chief*.\nis_chief = (server_def.task_index == 0)\nserver = tf.distribute.Server(server_def)\n\nwith tf.Graph().as_default():\n  ...add operations to the graph...\n  # Create a Supervisor that uses log directory on a shared file system.\n  # Indicate if you\n\n==================\n Document 2 \n----------------\n Input builders\ndef input_fn_train:\n  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n  # index.\n  pass\ndef input_fn_eval:\n  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n  # index.\n  pass\ndef input_fn_predict:\n  # Returns tf.data.Dataset of (x, None) tuple.\n  pass\nestimator.train(input_fn=input_fn_train)\nmetrics = estimator.evaluate(input_fn=input_fn_eval)\npredictions = estimator.predict(input_fn=input_fn_predict)\n\n\n* if `weight_column` is not `None`, a feature with `key=weight_column` whose\nvalue is a `Tensor`.\n* for each `column` in `feature_columns`:\n\t+ if `column` is a `SparseColumn`, a feature with `key=column.name`\n\twhose `value` is a `SparseTensor`.\n\t+ if `column` is a `WeightedSparseColumn`, two features: the first with\n\t`key` the id column name, the second with `key` the weight column name.\n\tBoth features' `value` must be a `SparseTensor`.\n\t+ if `column` is a `RealValuedColumn`, a feature with `key=column.name`\n\twhose `value` is a `Tensor`.\n\n# tf.compat.v1.estimator.LinearEstimator\n\nAn estimator for TensorFlow linear models with user-specified head. (deprecated) (deprecated)\n\ntf.compat.v1.estimator.LinearEstimator(\n    head,\n    feature_columns,\n    model_dir=None,\n    optimizer='Ftrl',\n    config=None,\n    partitioner=None,\n    sparse_combiner='sum',\n    warm_start_from=None\n)\n\n\n# Estimator using the default optimizer.\nestimator = tf.estimator.LinearEstimator(\n    head=tf.estimator.MultiLabelHead(n_classes=3),\n    feature_columns=[categorical_column_a,\n                     categorical_feature_a_x_categorical_feature_b])\n\n\n# Or estimator using an optimizer with a learning rate decay.\nestimator = tf.estimator.LinearEstimator(\n    head=tf.estimator.MultiLabelHead(n_classes=3),\n    feature_columns=[categorical_column_a,\n                     categorical_feature_a_x_categorical_feature_b],\n    optimizer=lambda: tf.keras.optimizers.Ftrl(\n        learning_rate=tf.compat.v1.train.exponential_decay(\n            learning_rate=0.1,\n            global_step=tf.compat.v1.train.get_global_step(),\n            decay_steps=10000,\n            decay_rate=0.96))\n\n# Or estimator using the FTRL optimizer with regularization.\nestimator = tf.estimator.LinearEstimator(\n    head=tf.estimator.MultiLabelHead(n_classes=3),\n    feature_columns=[categorical_column_a,\n                     categorical_feature_a_x_categorical_feature_b])\n\n==================\n Document 3 \n----------------\n Input builders\ndef input_fn_train:\n  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n  # index.\n  pass\ndef input_fn_eval:\n  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n  # index.\n  pass\ndef input_fn_predict:\n  # Returns tf.data.Dataset of (x, None) tuple.\n  pass\nestimator.train(input_fn=input_fn_train, steps=100)\nmetrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)\npredictions = estimator.predict(input_fn=input_fn_predict)\n\n\n* for each `column` in `dnn_feature_columns` + `linear_feature_columns`:\n\t+ if `column` is a `CategoricalColumn`, a feature with `key=column.name`\n\twhose `value` is a `SparseTensor`.\n\t+ if `column` is a `WeightedCategoricalColumn`, two features: the first\n\twith `key` the id column name, the second with `key` the weight column\n\tname. Both features' `value` must be a `SparseTensor`.\n\t+ if `column` is a `DenseColumn`, a feature with `key=column.name`\n\twhose `value` is a `Tensor`.\n\n# tf.compat.v1.estimator.DNNLinearCombinedEstimator\n\nAn estimator for TensorFlow Linear and DNN joined models with custom head. (deprecated) (deprecated)\n\ntf.compat.v1.estimator.DNNLinearCombinedEstimator(\n    head,\n    model_dir=None,\n    linear_feature_columns=None,\n    linear_optimizer='Ftrl',\n    dnn_feature_columns=None,\n    dnn_optimizer='Adagrad',\n    dnn_hidden_units=None,\n    dnn_activation_fn=`tf.nn.relu`,\n    dnn_dropout=None,\n    input_layer_partitioner=None,\n    config=None,\n    batch_norm=False,\n    linear_sparse_combiner='sum'\n)\n\ncategorical_feature_a_x_categorical_feature_b = crossed_column(...)\ncategorical_feature_a_emb = embedding_column(\n    categorical_column=categorical_feature_a, ...)\ncategorical_feature_b_emb = embedding_column(\n    categorical_column=categorical_feature_b, ...)\n\nestimator = tf.estimator.DNNLinearCombinedEstimator(\n    head=tf.estimator.MultiLabelHead(n_classes=3),\n    # wide settings\n    linear_feature_columns=[categorical_feature_a_x_categorical_feature_b],\n    linear_optimizer=tf.keras.optimizers.Ftrl(...),\n    # deep settings\n    dnn_feature_columns=[\n        categorical_feature_a_emb, categorical_feature_b_emb,\n        numeric_feature],\n    dnn_hidden_units=[1000, 500, 100],\n    dnn_optimizer=tf.keras.optimizers.Adagrad(...))\n\n\nLoss is calculated by using mean squared error.\n\n\n# tf.compat.v1.estimator.DNNLinearCombinedRegressor\n\nAn estimator for TensorFlow Linear and DNN joined models for regression. (deprecated) (deprecated)\n\ntf.compat.v1.estimator.DNNLinearCombinedRegressor(\n    model_dir=None,\n    linear_feature_columns=None,\n    linear_optimizer='Ftrl',\n    dnn_feature_columns=None,\n    dnn_optimizer='Adagrad',\n    dnn_hidden_units=None,\n    dnn_activation_fn=`tf.nn.relu`,\n    dnn_dropout=None,\n    label_dimension=1,\n    weight_column=None,\n    input_layer_partitioner=None,\n    config=None,\n    warm_start_from=None,\n    loss_reduction=tf.compat.v1.losses.Reduction.SUM,\n    batch_norm=False,\n    linear_sparse_combiner='sum'\n)\n\nestimator = tf.estimator.DNNLinearCombinedRegressor(\n    # wide settings\n    linear_feature_columns=[categorical_feature_a_x_categorical_feature_b],\n    linear_optimizer=tf.keras.optimizers.Ftrl(...),\n    # deep settings\n    dnn_feature_columns=[\n        categorical_feature_a_emb, categorical_feature_b_emb,\n        numeric_feature],\n    dnn_hidden_units=[1000, 500, 100],\n    dnn_optimizer=tf.keras.optimizers.Adagrad(...),\n    # warm-start settings\n    warm_start_from=\"/path/to/checkpoint/dir\")\n\n\n# tf.compat.v1.estimator.DNNRegressor\n\nA regressor for TensorFlow DNN models. (deprecated) (deprecated)\n\ntf.compat.v1.estimator.DNNRegressor(\n    hidden_units,\n    feature_columns,\n    model_dir=None,\n    label_dimension=1,\n    weight_column=None,\n    optimizer='Adagrad',\n    activation_fn=`tf.nn.relu`,\n    dropout=None,\n    input_layer_partitioner=None,\n    config=None,\n    warm_start_from=None,\n    loss_reduction=tf.compat.v1.losses.Reduction.SUM,\n    batch_norm=False\n)\n\nestimator = tf.estimator.DNNRegressor(\n    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],\n    hidden_units=[1024, 512, 256])\n\n\n# Or estimator using the ProximalAdagradOptimizer optimizer with\n\n# regularization.\nestimator = tf.estimator.DNNRegressor(\n    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],\n    hidden_units=[1024, 512, 256],\n    optimizer=tf.compat.v1.train.ProximalAdagradOptimizer(\n      learning_rate=0.1,\n      l1_regularization_strength=0.001\n    ))\n\n\n# Or estimator using an optimizer with a learning rate decay.\nestimator = tf.estimator.DNNRegressor(\n    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],\n    hidden_units=[1024, 512, 256],\n    optimizer=lambda: tf.keras.optimizers.Adam(\n        learning_rate=tf.compat.v1.train.exponential_decay(\n            learning_rate=0.1,\n            global_step=tf.compat.v1.train.get_global_step(),\n            decay_steps=10000,\n            decay_rate=0.96))\n\n\n# Or estimator with warm-starting from a previous checkpoint.\nestimator = tf.estimator.DNNRegressor(\n    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],\n    hidden_units=[1024, 512, 256],\n    warm_start_from=\"/path/to/checkpoint/dir\")\n\n# tf.compat.v1.estimator.Estimator\n\nEstimator class to train and evaluate TensorFlow models. (deprecated)\n\ntf.compat.v1.estimator.Estimator(\n    model_fn, model_dir=None, config=None, params=None, warm_start_from=None\n)\n\n\n| Used in the guide | Used in the tutorials |\n| --- | --- |\n| * Migrate early stopping\n* Migrate LoggingTensorHook and StopAtStepHook"
    },
    {
        "instruction": "Create a Python program that uses the 'tensorflow' API to implement a recurrent neural network (RNN) for sequence prediction. The program should use a sequence dataset, preprocess it, build an RNN model, train it, and generate predictions on new sequences.",
        "api": "tensorflow",
        "output": "#pip install tensorflow\nimport tensorflow as tf\nimport numpy as np\n\n# Generate a synthetic sequence dataset\nseq_length = 100\nsequences = [np.random.randint(0, 10, size=seq_length) for _ in range(1000)]\ntargets = [np.sum(seq) for seq in sequences]\n\n# Preprocess the data\nsequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post')\ntargets = np.array(targets)\n\n# Build a simple recurrent neural network\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=10, output_dim=16, input_length=seq_length),\n    tf.keras.layers.LSTM(64, activation='relu', return_sequences=False),\n    tf.keras.layers.Dense(1)\n])\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='mean_squared_error')\n\n# Train the model on the synthetic sequence data\nmodel.fit(sequences, targets, epochs=5)\n\n# Generate predictions on new sequences\nnew_sequence = np.array([np.random.randint(0, 10, size=seq_length)])\nprediction = model.predict(new_sequence)\nprint(\"Predicted sum of the new sequence:\", prediction[0][0])",
        "documentation": "\n\n==================\n Document 0 \n----------------\n\n\n* TensorFlow\n* API\n* TensorFlow v2.14.0\n\n# TensorFlow API Versions\n\n\n \n Stay organized with collections\n \n\n \n Save and categorize content based on your preferences.\n \n\n\nThe following versions of the TensorFlow api-docs are currently available.\nMajor features, improvements, and changes of each version are available in the\nrelease notes.\n\n\n\n## TensorFlow 2\n\n\n* r2.12 -\nrelease notes\n* r2.11 -\nrelease notes\n* r2.10 -\nrelease notes\n* r2.9 - \nrelease notes\n* r2.8 - \nrelease notes\n* r2.7- \nrelease notes\n* r2.6- \nrelease notes\n* r2.5- \nrelease notes\n* r2.4- \nrelease notes\n* r2.3- \nrelease notes\n* r2.2- \nrelease notes\n* r2.1- \nrelease notes\n* r2.0- \nrelease notes\n\n\n## TensorFlow 1\n\n\n* r1.15 -\nrelease notes\n* r1.14 -\nrelease notes\n* r1.13 -\nrelease notes\n* r1.12 -\nrelease notes\n* r1.11 -\nrelease notes\n* r1.10 -\nrelease notes\n* r1.9 -\nrelease notes\n* r1.8 -\nrelease notes\n* r1.7 -\nrelease notes\n* r1.6 -\nrelease notes\n* r1.5 -\nrelease notes\n* r1.4 -\nrelease notes\n* r1.3\n\n==================\n Document 1 \n----------------\n Input builders\ndef input_fn_train:\n  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n  # index.\n  pass\ndef input_fn_eval:\n  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n  # index.\n  pass\ndef input_fn_predict:\n  # Returns tf.data.Dataset of (x, None) tuple.\n  pass\nestimator.train(input_fn=input_fn_train)\nmetrics = estimator.evaluate(input_fn=input_fn_eval)\npredictions = estimator.predict(input_fn=input_fn_predict)\n\nInput of `train` and `evaluate` should have following features,\notherwise there will be a `KeyError`:\n\n\n* if `weight_column` is not `None`, a feature with `key=weight_column` whose\nvalue is a `Tensor`.\n* for each `column` in `feature_columns`:\n\t+ if `column` is a `CategoricalColumn`, a feature with `key=column.name`\n\twhose `value` is a `SparseTensor`.\n\t+ if `column` is a `WeightedCategoricalColumn`, two features: the first\n\twith `key` the id column name, the second with `key` the weight column\n\tname. Both features' `value` must be a `SparseTensor`.\n\t+ if `column` is a `DenseColumn`, a feature with `key=column.name`\n\twhose `value` is a `Tensor`.\n\n\nLoss is calculated by using softmax cross entropy.\n\n# tf.compat.v1.estimator.DNNEstimator\n\nAn estimator for TensorFlow DNN models with user-specified head. (deprecated) (deprecated)\n\ntf.compat.v1.estimator.DNNEstimator(\n    head,\n    hidden_units,\n    feature_columns,\n    model_dir=None,\n    optimizer='Adagrad',\n    activation_fn=`tf.nn.relu`,\n    dropout=None,\n    input_layer_partitioner=None,\n    config=None,\n    warm_start_from=None,\n    batch_norm=False\n)\n\n```\nsparse_feature_a = sparse_column_with_hash_bucket(...)\nsparse_feature_b = sparse_column_with_hash_bucket(...)\n\nsparse_feature_a_emb = embedding_column(sparse_id_column=sparse_feature_a,\n                                        ...)\nsparse_feature_b_emb = embedding_column(sparse_id_column=sparse_feature_b,\n                                        ...)\n\nestimator = tf.estimator.DNNEstimator(\n    head=tf.estimator.MultiLabelHead(n_classes=3),\n    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n    hidden_units=[1024, 512, 256])\n\n\n# Or estimator using the ProximalAdagradOptimizer optimizer with\n\n# regularization.\nestimator = tf.estimator.DNNEstimator(\n    head=tf.estimator.MultiLabelHead(n_classes=3),\n    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n    hidden_units=[1024, 512, 256],\n    optimizer=tf.compat.v1.train.ProximalAdagradOptimizer(\n      learning_rate=0.1,\n      l1_regularization_strength=0.001\n    ))\n\n\n# Or estimator using an optimizer with a learning rate decay.\nestimator = tf.estimator.DNNEstimator(\n    head=tf.estimator.MultiLabelHead(n_classes=3),\n    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n    hidden_units=[1024, 512, 256],\n    optimizer=lambda: tf.keras.optimizers.Adam(\n        learning_rate=tf.compat.v1.train.exponential_decay(\n            learning_rate=0.1,\n            global_step=tf.compat.v1.train.get_global_step(),\n            decay_steps=10000,\n            decay_rate=0.96))\n\n\n# Or estimator with warm-starting from a previous checkpoint.\nestimator = tf.estimator.DNNEstimator(\n    head=tf.estimator.MultiLabelHead(n_classes=3),\n    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n    hidden_units=[1024, 512, 256],\n    warm_start_from=\"/path/to/checkpoint/dir\")\n\n\nLoss and predicted output are determined by the specified head.\n\n\n# tf.compat.v1.estimator.DNNLinearCombinedClassifier\n\nAn estimator for TensorFlow Linear and DNN joined classification models. (deprecated) (deprecated)\n\ntf.compat.v1.estimator.DNNLinearCombinedClassifier(\n    model_dir=None,\n    linear_feature_columns=None,\n    linear_optimizer='Ftrl',\n    dnn_feature_columns=None,\n    dnn_optimizer='Adagrad',\n    dnn_hidden_units=None,\n    dnn_activation_fn=`tf.nn.relu`,\n    dnn_dropout=None,\n    n_classes=2,\n    weight_column=None,\n    label_vocabulary=None,\n    input_layer_partitioner=None,\n    config=None,\n    warm_start_from=None,\n    loss_reduction=tf.compat.v1.losses.Reduction.SUM,\n    batch_norm=False,\n    linear_sparse_combiner='sum'\n)\n\n```\nnumeric_feature = numeric_column(...)\ncategorical_column_a = categorical_column_with_hash_bucket(...)\ncategorical_column_b = categorical_column_with_hash_bucket(...)\n\ncategorical_feature_a_x_categorical_feature_b = crossed_column(...)\ncategorical_feature_a_emb = embedding_column(\n    categorical_column=categorical_feature_a, ...)\ncategorical_feature_b_emb = embedding_column(\n    categorical_id_column=categorical_feature_b, ...)\n\nestimator = tf.estimator.DNNLinearCombinedClassifier(\n    # wide settings\n    linear_feature_columns=[categorical_feature_a_x_categorical_feature_b],\n    linear_optimizer=tf.keras.optimizers.Ftrl(...),\n    # deep settings\n    dnn_feature_columns=[\n        categorical_feature_a_emb, categorical_feature_b_emb,\n        numeric_feature],\n    dnn_hidden_units=[1000, 500, 100],\n    dnn_optimizer=tf.keras.optimizers.Adagrad(...),\n    # warm-start settings\n    warm_start_from=\"/path/to/checkpoint/dir\")\n\n\n# To apply L1 and L2 regularization, you can set dnn_optimizer to:\ntf.compat.v1.train.ProximalAdagradOptimizer(\n    learning_rate=0.1,\n    l1_regularization_strength=0.001,\n    l2_regularization_strength=0.001)\n\n# To apply learning rate decay, you can set dnn_optimizer to a callable:\nlambda: tf.keras.optimizers.Adam(\n    learning_rate=tf.compat.v1.train.exponential_decay(\n        learning_rate=0.1,\n        global_step=tf.compat.v1.train.get_global_step(),\n        decay_steps=10000,\n        decay_rate=0.96)\n\n# It is the same for linear_optimizer.\n\n# Input builders\ndef input_fn_train:\n  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n  # index.\n  pass\ndef input_fn_eval:\n  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n  # index.\n\n==================\n Document 2 \n----------------\n training or inference mode, e.g. applying dropout.\ntraining = True\nrating = tf.feature_column.sequence_numeric_column('rating')\nwatches = tf.feature_column.sequence_categorical_column_with_identity(\n    'watches', num_buckets=1000)\nwatches_embedding = tf.feature_column.embedding_column(watches,\n                                            dimension=10)\ncolumns = [rating, watches_embedding]\n\nfeatures = {\n 'rating': tf.sparse.from_dense([[1.0,1.1, 0, 0, 0],\n                                             [2.0,2.1,2.2, 2.3, 2.5]]),\n 'watches': tf.sparse.from_dense([[2, 85, 0, 0, 0],[33,78, 2, 73, 1]])\n}\n\nsequence_input_layer = tf.keras.experimental.SequenceFeatures(columns)\nsequence_input, sequence_length = sequence_input_layer(\n   features, training=training)\nsequence_length_mask = tf.sequence_mask(sequence_length)\nhidden_size = 32\nrnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)\nrnn_layer = tf.keras.layers.RNN(rnn_cell)\noutputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)\n\n\n| `feature_columns` | \nAn iterable of dense sequence columns. Valid columns\nare |\n\n* `embedding_column` that wraps a\n`sequence_categorical_column_with_*`\n* `sequence_numeric_column`.\n\n| `trainable` | \nBoolean, whether the layer's variables will be updated via\ngradient descent during training.\n |\n| `name` | \nName to give to the SequenceFeatures.\n |\n| `**kwargs` | \nKeyword arguments to construct a layer.\n |\n\n\n| `ValueError` | \nIf any of the `feature_columns` is not a\n`SequenceDenseColumn`.\n |\n# tf.keras.experimental.SidecarEvaluator\n\nDeprecated. Please use `tf.keras.utils.SidecarEvaluator` instead.\n\n\nInherits From: `SidecarEvaluator`\n\ntf.keras.experimental.SidecarEvaluator(\n    *args, **kwargs\n)\n\n\n| `model` | \nModel to use for evaluation. The model object used here should\nbe a `tf.keras.Model`, and should be the same as the one that is\nused in training,\n\n==================\n Document 3 \n----------------\n tf.keras.layers.SimpleRNN\n\nFully-connected RNN where the output is to be fed back to input.\n\ntf.keras.layers.SimpleRNN(\n    units,\n    activation='tanh',\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    recurrent_initializer='orthogonal',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    recurrent_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    recurrent_constraint=None,\n    bias_constraint=None,\n    dropout=0.0,\n    recurrent_dropout=0.0,\n    return_sequences=False,\n    return_state=False,\n    go_backwards=False,\n    stateful=False,\n    unroll=False,\n    **kwargs\n)\n\n\n| `units` | \nPositive integer, dimensionality of the output space.\n |\n| `activation` | \nActivation function to use.\nDefault: hyperbolic tangent (`tanh`).\nIf you pass None, no activation is applied\n(ie. \"linear\" activation: `a(x) = x`).\n |\n| `use_bias` | \nBoolean, (default `True`), whether the layer uses a bias vector.\n |\n| `kernel_initializer` | \nInitializer for the `kernel` weights matrix,\nused for the linear transformation of the inputs. Default:\n`glorot_uniform`.\n |\n| `recurrent_initializer` | \nInitializer for the `recurrent_kernel`\nweights matrix, used for the linear transformation of the recurrent\nstate. Default: `orthogonal`.\n |\n| `bias_initializer` | \nInitializer for the bias vector. Default: `zeros`.\n |\n| `kernel_regularizer` | \nRegularizer function applied to the `kernel` weights\nmatrix. Default: `None`.\n |\n| `recurrent_regularizer` | \nRegularizer function applied to the\n`recurrent_kernel` weights matrix. Default: `None`.\n |\n| `bias_regularizer` | \nRegularizer function applied to the bias vector.\nDefault: `None`.\n |\n| `activity_regularizer` | \nRegularizer function applied to the output of the\nlayer (its \"activation\"). Default: `None`.\n |\n| `kernel_constraint` | \nConstraint function applied to the `kernel` weights\nmatrix. Default: `None`.\n |\n| `recurrent_constraint` | \nConstraint function applied to the\n`recurrent_kernel` weights matrix. Default: `None`.\n |\n| `bias_constraint` | \nConstraint function applied to the bias vector. Default:\n`None`.\n |\n| `dropout` | \nFloat between 0 and 1.\nFraction of the units to drop for the linear transformation of the\ninputs. Default: 0.\n |\n| `recurrent_dropout` | \nFloat between 0 and 1.\nFraction of the units to drop for the linear transformation of the\nrecurrent state. Default: 0.\n |\n| `return_sequences` | \nBoolean. Whether to return the last output\nin the output sequence, or the full sequence. Default: `False`.\n |\n| `return_state` | \nBoolean. Whether to return the last state\nin addition to the output. Default: `False` |\n| `go_backwards` | \nBoolean (default False).\nIf True, process the input sequence backwards and return the\nreversed sequence.\n |\n| `stateful` | \nBoolean (default False). If True, the last state\nfor each sample at index i in a batch will be used as initial\nstate for the sample of index i in the following batch.\n |\n| `unroll` | \nBoolean (default False).\nIf True, the network will be unrolled,\nelse a symbolic loop will be used.\nUnrolling can speed-up a RNN,\nalthough it tends to be more memory-intensive.\nUnrolling is only suitable for short sequences.\n |\n\n\n| `inputs` | \nA 3D tensor, with shape `[batch, timesteps, feature]`.\n |\n| `mask` | \nBinary tensor of shape `[batch, timesteps]` indicating whether\na given timestep should be masked. An individual `True` entry indicates\nthat the corresponding timestep should be utilized, while a `False`\nentry indicates that the corresponding timestep should be ignored.\n |\n| `training` | \nPython boolean indicating whether the layer should behave in\ntraining mode or in inference mode. This argument is passed to the cell\nwhen calling it. This is only relevant if `dropout` or\n`recurrent_dropout` is used.\n |\n| `initial_state` | \nList of initial state tensors to be passed to the first\ncall of the cell.\n |\n\n```\ninputs = np.random.random([32, 10, 8]).astype(np.float32)\nsimple_rnn = tf.keras.layers.SimpleRNN(4)\n\noutput = simple_rnn(inputs)  # The output has shape `[32, 4]`.\n\nsimple_rnn = tf.keras.layers.SimpleRNN(\n    4, return_sequences=True, return_state=True)\n\n# whole_sequence_output has shape `[32, 10, 4]`.\n\n# final_state has shape `[32, 4]`.\nwhole_sequence_output, final_state = simple_rnn(inputs)\n\n# tf.keras.layers.SimpleRNNCell\n\nCell class for SimpleRNN.\n\ntf.keras.layers.SimpleRNNCell(\n    units,\n    activation='tanh',\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    recurrent_initializer='orthogonal',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    recurrent_regularizer=None,\n\n==================\n Document 4 \n----------------\n tf.data.Dataset\n\ntf.data.Dataset(\n    variant_tensor\n)\n\n\n| Used in the guide | Used in the tutorials |\n| --- | --- |\n| * tf.data: Build TensorFlow input pipelines\n* Better performance with the tf.data API\n* Extension types\n* Migrate from Estimator to Keras APIs\n* Distributed training with TensorFlow\n | * Distributed Input\n* Parameter server training with ParameterServerStrategy\n* Load CSV data\n* Custom training with tf.distribute.Strategy\n* pix2pix: Image-to-image translation with a conditional GAN\n |\n\n\nThe `tf.data.Dataset` API supports writing descriptive and efficient input\npipelines. `Dataset` usage follows a common pattern:\n\n\n1. Create a source dataset from your input data.\n2. Apply dataset transformations to preprocess the data.\n3. Iterate over the dataset and process the elements.\n\n\nIteration happens in a streaming fashion, so the full dataset does not need to\nfit into memory.\n\n\n#### Source Datasets:\n\n\nThe simplest way to create a dataset is to create it from a python `list`:\n\nTo process lines from files, use `tf.data.TextLineDataset`:\n\ndataset = tf.data.TextLineDataset([\"file1.txt\", \"file2.txt\"])\n\nTo process records written in the `TFRecord` format, use `TFRecordDataset`:\n\ndataset = tf.data.TFRecordDataset([\"file1.tfrecords\", \"file2.tfrecords\"])\n\nTo create a dataset of all files matching a pattern, use\n`tf.data.Dataset.list_files`:\n\n```\ndataset = tf.data.Dataset.list_files(\"/path/*.txt\")\n\nSee `tf.data.FixedLengthRecordDataset` and `tf.data.Dataset.from_generator`\nfor more ways to create datasets.\n\n\n\n#### Transformations:\n\n\nOnce you have a dataset, you can apply transformations to prepare the data for\nyour model:\n\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.map(lambda x: x*2)\nlist(dataset.as_numpy_iterator())\n[2, 4, 6]\n\n\n#### Common Terms:\n\n\n**Element**: A single output from calling `next()` on a dataset iterator.\n Elements may be nested structures containing multiple components. For\n example, the element `(1, (3, \"apple\"))` has one tuple nested in another\n tuple. The components are `1`, `3`, and `\"apple\"`.\n\n\n**Component**: The leaf in the nested structure of an element.\n\n\n#### Supported types:\n\n\nElements can be nested structures of tuples, named tuples, and dictionaries.\nNote that Python lists are *not* treated as nested structures of components.\nInstead, lists are converted to tensors and treated as components. For\nexample, the element `(1, [1, 2, 3])`"
    },
    {
        "instruction": "Create a Python program using the 'validators' API to validate user-provided URLs, checking if they adhere to the standard URL format. Additionally, the program should also check if the URL is reachable by making a request to it and display the response status code. If the URL is not reachable, the program should display an appropriate error message. Finally, the program should also check if the URL is secure (HTTPS) and display a message indicating its security status.",
        "api": "validators",
        "output": "#!pip install validators\nimport validators\nimport requests\n\ndef validate_url(url):\n    if validators.url(url):\n        try:\n            response = requests.get(url)\n            if response.status_code == 200:\n                if validators.url(url, public=True, schemes=['https']):\n                    return \"Valid URL, reachable, and secure (HTTPS). Response status code: \" + str(response.status_code)\n                else:\n                    return \"Valid URL, reachable, but not secure (HTTP). Response status code: \" + str(response.status_code)\n            else:\n                return \"Valid URL but unreachable. Response status code: \" + str(response.status_code)\n        except requests.exceptions.RequestException:\n            return \"Valid URL but unreachable.\"\n    else:\n        return \"Invalid URL.\"\n\n# Get user inputs\nuser_url = input(\"Enter a URL: \")\n\n# Validate and display results\nprint(validate_url(user_url))\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n Output: ValidationError(func=url, ...)\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `value` | `str` | \nURL string to validate.\n | *required* |\n| `skip_ipv6_addr` | `bool` | \nWhen URL string cannot contain an IPv6 address.\n | `False` |\n| `skip_ipv4_addr` | `bool` | \nWhen URL string cannot contain an IPv4 address.\n | `False` |\n| `may_have_port` | `bool` | \nURL string may contain port number.\n | `True` |\n| `simple_host` | `bool` | \nURL string maybe only hyphens and alpha-numerals.\n | `False` |\n| `strict_query` | `bool` | \nFail validation on query string parsing error.\n | `True` |\n| `rfc_1034` | `bool` | \nAllow trailing dot in domain/host name.\nRef: RFC 1034.\n | `False` |\n| `rfc_2782` | `bool` | \nDomain/Host name is of type service record.\nRef: RFC 2782.\n | `False` |\n\nNote\n* *In version 0.11.3*:\n\t+ Added support for URLs containing localhost.\n* *In version 0.11.0*:\n\t+ Made the regular expression case insensitive.\n* *In version 0.10.3*:\n\t+ Added a `public` parameter.\n* *In version 0.10.2*:\n\t+ Added support for various exotic URLs.\n\t+ Fixed various false positives.\n\nSource code in `/opt/hostedtoolcache/Python/3.11.5/x64/lib/python3.11/site-packages/validators/url.py`\n\n\n|  |  |\n| --- | --- |\n| \n```\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n```\n | \n```\n@validator\ndef url(\n    value: str,\n    /,\n    \\*,\n    skip\\_ipv6\\_addr: bool = False,\n    skip\\_ipv4\\_addr: bool = False,\n    may\\_have\\_port: bool = True,\n    simple\\_host: bool = False,\n    strict\\_query: bool = True,\n    rfc\\_1034: bool = False,\n    rfc\\_2782: bool = False,\n):\n r\"\"\"Return whether or not given value is a valid URL.\n\n This validator was inspired from [URL validator of dperini][1].\n The following diagram is from [urlly][2].\n\n foo://admin:hunter1@example.com:8042/over/there?name=ferret#nose\n \\\\_/ \\\\_\\_\\_/ \\\\_\\_\\_\\_\\_/ \\\\_\\_\\_\\_\\_\\_\\_\\_\\_/ \\\\_\\_/\\\\_\\_\\_\\_\\_\\_\\_\\_\\_/ \\\\_\\_\\_\\_\\_\\_\\_\\_\\_/ \\\\_\\_/\n | | | | | | | |\n scheme username password hostname port path query fragment\n\n [1]: https://gist.github.com/dperini/729294\n [2]: https://github.com/treeform/urlly\n\n Examples:\n >>> url('http://duck.com')\n # Output: True\n >>> url('ftp://foobar.dk')\n # Output: True\n >>> url('http://10.0.0.1')\n # Output: True\n >>> url('http://example.com/\">user@example.com')\n # Output: ValidationError(func=url, ...)\n\n Args:\n value:\n URL string to validate.\n skip\\_ipv6\\_addr:\n When URL string cannot contain an IPv6 address.\n skip\\_ipv4\\_addr:\n When URL string cannot contain an IPv4 address.\n may\\_have\\_port:\n URL string may contain port number.\n simple\\_host:\n URL string maybe only hyphens and alpha-numerals.\n strict\\_query:\n Fail validation on query string parsing error.\n rfc\\_1034:\n Allow trailing dot in domain/host name.\n Ref: [RFC 1034](https://www.rfc-editor.org/rfc/rfc1034).\n rfc\\_2782:\n Domain/Host name is of type service record.\n Ref: [RFC 2782](https://www.rfc-editor.org/rfc/rfc2782).\n\n Note:\n - \\*In version 0.11.3\\*:\n - Added support for URLs containing localhost.\n - \\*In version 0.11.0\\*:\n - Made the regular expression case insensitive.\n - \\*In version 0.10.3\\*:\n - Added a `public` parameter.\n - \\*In version 0.10.2\\*:\n - Added support for various exotic URLs.\n - Fixed various false positives.\n\n > \\*New in version 0.2.0\\*.\n \"\"\"\n    if not value or re.search(r\"\\s\", value):\n        # url must not contain any white\n        # spaces, they must be encoded\n        return False\n\n    try:\n        scheme, netloc, path, query, fragment = urlsplit(value)\n    except ValueError:\n        return False\n\n    return (\n        \\_validate\\_scheme(scheme)\n        and \\_validate\\_netloc(\n            netloc,\n            skip\\_ipv6\\_addr,\n            skip\\_ipv4\\_addr,\n            may\\_have\\_port,\n            simple\\_host,\n            rfc\\_1034,\n            rfc\\_2782,\n        )\n        and \\_validate\\_optionals(path, query, fragment, strict\\_query)\n    )\n\n# utils¶\n\n## \n`validators.utils.ValidationError`\n¶\n\n\n Bases: `Exception`\n\n\nException class when validation failure occurs.\n\nSource code in `/opt/hostedtoolcache/Python/3.11.5/x64/lib/python3.11/site-packages/validators/utils.py`\n\n\n|  |  |\n| --- | --- |\n| \n```\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n```\n | \n```\nclass ValidationError(Exception):\n \"\"\"Exception class when validation failure occurs.\"\"\"\n\n    def \\_\\_init\\_\\_(self, function: Callable[..., Any], arg\\_dict: Dict[str,\n\n==================\n Document 1 \n----------------\n Output: ValidationError(func=slug, args={'value': 'my.slug'})\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `value` | `str` | \nSlug string to validate.\n | *required* |\n\n\n| Type | Description |\n| --- | --- |\n| `Literal[True]` | \nIf `value` is a valid slug.\n |\n| `ValidationError` | \nIf `value` is an invalid slug.\n |\n\n> \n> *New in version 0.6.0*.\n> \n> \n> \n\nSource code in `/opt/hostedtoolcache/Python/3.11.5/x64/lib/python3.11/site-packages/validators/slug.py`\n\n\n|  |  |\n| --- | --- |\n| \n```\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n```\n | \n```\n@validator\ndef slug(value: str, /):\n \"\"\"Validate whether or not given value is valid slug.\n\n Valid slug can contain only lowercase alphanumeric characters and hyphens.\n It starts and ends with these lowercase alphanumeric characters.\n\n Examples:\n >>> slug('my-slug-2134')\n # Output: True\n >>> slug('my.slug')\n # Output: ValidationError(func=slug, args={'value': 'my.slug'})\n\n Args:\n value:\n Slug string to validate.\n\n Returns:\n (Literal[True]):\n If `value` is a valid slug.\n (ValidationError):\n If `value` is an invalid slug.\n\n > \\*New in version 0.6.0\\*.\n \"\"\"\n    return re.match(r\"^[a-z0-9]+(?:-[a-z0-9]+)\\*$\", value) if value else False\n\n# url¶\n\n\n## \n`validators.url.url(value, /, \\*, skip\\_ipv6\\_addr=False, skip\\_ipv4\\_addr=False, may\\_have\\_port=True, simple\\_host=False, strict\\_query=True, rfc\\_1034=False, rfc\\_2782=False)`\n¶\n\nReturn whether or not given value is a valid URL.\n\n\nThis validator was inspired from URL validator of dperini.\nThe following diagram is from urlly.\n\n```\n    foo://admin:hunter1@example.com:8042/over/there?name=ferret#nose\n    \\_/   \\___/ \\_____/ \\_________/ \\__/\\_________/ \\_________/ \\__/\n     |      |       |       |        |       |          |         |\n  scheme username password hostname port    path      query    fragment\n\n**Examples:**\n\n```\n>>> url('http://duck.com')\n\n# Output: True\n>>> url('ftp://foobar.dk')\n\n# Output: True\n>>> url('http://10.0.0.1')\n\n# Output: True\n>>> url('http://example.com/\">user@example.com')\n# Output: ValidationError(func=url, ...)\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `value` | `str` | \nURL string to validate.\n | *required* |\n| `skip_ipv6_addr` | `bool` | \nWhen URL string cannot contain\n\n==================\n Document 2 \n----------------\n Output: True\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `value` | `str` | \nCountry code string to validate.\n | *required* |\n| `iso_format` | `str` | \nISO format to be used. Available options are:\n`auto`, `alpha2`, `alpha3` and `numeric`.\n | `'auto'` |\n\n\n| Type | Description |\n| --- | --- |\n| `Literal[True]` | \nIf `value` is a valid country code.\n |\n| `ValidationError` | \nIf `value` is an invalid country code.\n |\n\nSource code in `/opt/hostedtoolcache/Python/3.11.5/x64/lib/python3.11/site-packages/validators/country_code.py`\n\n\n|  |  |\n| --- | --- |\n| \n```\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n```\n | \n```\n@validator\ndef country\\_code(value: str, /, \\*, iso\\_format: str = \"auto\"):\n \"\"\"Validates given country code.\n\n This performs a case-sensitive [ISO 3166][1] country code validation.\n\n [1]: https://www.iso.org/iso-3166-country-codes.html\n\n Examples:\n >>> country\\_code('GB', iso\\_format='alpha3')\n # Output: False\n >>> country\\_code('USA')\n # Output: True\n >>> country\\_code('840', iso\\_format='numeric')\n # Output: True\n >>> country\\_code('iN', iso\\_format='alpha2')\n # Output: False\n >>> country\\_code('ZWE', iso\\_format='alpha3')\n # Output: True\n\n Args:\n value:\n Country code string to validate.\n iso\\_format:\n ISO format to be used. Available options are:\n `auto`, `alpha2`, `alpha3` and `numeric`.\n\n Returns:\n (Literal[True]):\n If `value` is a valid country code.\n (ValidationError):\n If `value` is an invalid country code.\n \"\"\"\n    if not value:\n        return False\n\n    if not (1 < len(value) < 4):\n        return False\n\n    if iso\\_format == \"auto\" and (iso\\_format := get\\_code\\_type(value)) == \"invalid\":\n        return False\n\n    if iso\\_format == \"alpha2\":\n        return value in alpha\\_2\n    if iso\\_format == \"alpha3\":\n        return value in alpha\\_3\n    return value in numeric if iso\\_format == \"numeric\" else False\n\n# domain¶\n\n\n## \n`validators.domain.domain(value, /, \\*, rfc\\_1034=False, rfc\\_2782=False)`\n¶\n\nReturn whether or not given value is a valid domain.\n\n```\n>>> domain('example.com')\n\n# Output: True\n>>> domain('example.com/')\n\n# Output: ValidationError(func=domain, ...)\n>>> # Supports IDN domains as well::\n>>> domain('xn----gtbspbbmkef.xn--p1ai')\n# Output: True\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `value` | `str` | \nDomain string to validate.\n | *required* |\n| `rfc_1034` | `bool` | \nAllow trailing dot in domain name.\nRef:\n\n==================\n Document 3 \n----------------\n Output: ValidationError(func=iban, ...)\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `value` | `str` | \nIBAN string to validate.\n | *required* |\n\n\n| Type | Description |\n| --- | --- |\n| `Literal[True]` | \nIf `value` is a valid IBAN code.\n |\n| `ValidationError` | \nIf `value` is an invalid IBAN code.\n |\n\n> \n> *New in version 0.8.0*\n> \n> \n> \n\nSource code in `/opt/hostedtoolcache/Python/3.11.5/x64/lib/python3.11/site-packages/validators/iban.py`\n\n\n|  |  |\n| --- | --- |\n| \n```\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n```\n | \n```\n@validator\ndef iban(value: str, /):\n \"\"\"Return whether or not given value is a valid IBAN code.\n\n Examples:\n >>> iban('DE29100500001061045672')\n # Output: True\n >>> iban('123456')\n # Output: ValidationError(func=iban, ...)\n\n Args:\n value:\n IBAN string to validate.\n\n Returns:\n (Literal[True]):\n If `value` is a valid IBAN code.\n (ValidationError):\n If `value` is an invalid IBAN code.\n\n > \\*New in version 0.8.0\\*\n \"\"\"\n    return (\n        (re.match(r\"^[A-Z]{2}[0-9]{2}[A-Z0-9]{11,30}$\", value) and \\_mod\\_check(value))\n        if value\n        else False\n    )\n\n# ip\\_address¶\n\n\n## \n`validators.ip\\_address.ipv4(value, /, \\*, cidr=True, strict=False, host\\_bit=True)`\n¶\n\nReturns whether a given value is a valid IPv4 address.\n\n\nFrom Python version 3.9.5 leading zeros are no longer tolerated\nand are treated as an error. The initial version of ipv4 validator\nwas inspired from WTForms IPAddress validator.\n\n```\n>>> ipv4('123.0.0.7')\n\n# Output: True\n>>> ipv4('1.1.1.1/8')\n\n# Output: True\n>>> ipv4('900.80.70.11')\n# Output: ValidationError(func=ipv4, args={'value': '900.80.70.11'})\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `value` | `str` | \nIP address string to validate.\n | *required* |\n| `cidr` | `bool` | \nIP address string\n\n==================\n Document 4 \n----------------\n Output: ValidationError(email=email, args={'value': 'bogus@@'})\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `value` | `str` | \neMail string to validate.\n | *required* |\n| `ipv6_address` | `bool` | \nWhen the domain part is an IPv6 address.\n | `False` |\n| `ipv4_address` | `bool` | \nWhen the domain part is an IPv4 address.\n | `False` |\n| `simple_host` | `bool` | \nWhen the domain part is a simple hostname.\n | `False` |\n| `rfc_1034` | `bool` | \nAllow trailing dot in domain name.\nRef: RFC 1034.\n | `False` |\n| `rfc_2782` | `bool` | \nDomain name is of type service record.\nRef: RFC 2782.\n | `False` |\n\n\n| Type | Description |\n| --- | --- |\n| `Literal[True]` | \nIf `value` is a valid eMail.\n |\n| `ValidationError` | \nIf `value` is an invalid eMail.\n |\n\n> \n> *New in version 0.1.0*.\n> \n> \n> \n\nSource code in `/opt/hostedtoolcache/Python/3.11.5/x64/lib/python3.11/site-packages/validators/email.py`\n\n\n|  |  |\n| --- | --- |\n| \n```\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n```\n | \n```\n@validator\ndef email(\n    value: str,\n    /,\n    \\*,\n    ipv6\\_address: bool = False,\n    ipv4\\_address: bool = False,\n    simple\\_host: bool = False,\n    rfc\\_1034: bool = False,\n    rfc\\_2782: bool = False,\n):\n \"\"\"Validate an email address.\n\n This was inspired from [Django's email validator][1].\n Also ref: [RFC 1034][2], [RFC 5321][3] and [RFC 5322][4].\n\n [1]: https://github.com/django/django/blob/main/django/core/validators.py#L174\n [2]: https://www.rfc-editor.org/rfc/rfc1034\n [3]: https://www.rfc-editor.org/rfc/rfc5321\n [4]: https://www.rfc-editor.org/rfc/rfc5322\n\n Examples:\n >>> email('someone@example.com')\n # Output: True\n >>> email('bogus@@')\n # Output: ValidationError(email=email, args={'value': 'bogus@@'})\n\n Args:\n value:\n eMail string to validate.\n ipv6\\_address:\n When the domain part is an IPv6 address.\n ipv4\\_address:\n When the domain part is an IPv4 address.\n simple\\_host:\n When the domain part is a simple hostname.\n rfc\\_1034:\n Allow trailing dot in domain name.\n Ref: [RFC 1034](https://www.rfc-editor.org/rfc/rfc1034).\n rfc\\_2782:\n Domain name is of type service record.\n Ref: [RFC 2782](https://www.rfc-editor.org/rfc/rfc2782).\n\n Returns:\n (Literal[True]):\n If `value` is a valid eMail.\n (ValidationError):\n If `value` is an invalid eMail.\n\n > \\*New in version 0.1.0\\*.\n \"\"\"\n    if not value or value.count(\"@\") != 1:\n        return False\n\n    username\\_part, domain\\_part = value.rsplit(\"@\", 1)\n\n    if len(username\\_part) > 64 or len(domain\\_part) > 253:\n        # ref: RFC 1034 and 5231\n        return False\n\n    if ipv6\\_address or ipv4\\_address:\n        if domain\\_part.startswith(\"[\") and domain\\_part.endswith(\"]\"):\n            # ref: RFC 5321\n            domain\\_part = domain\\_part.lstrip(\"[\").rstrip(\"]\")\n        else:\n            return False\n\n    return (\n        bool(\n            hostname(\n                domain\\_part,\n                skip\\_ipv6\\_addr=not ipv6\\_address,\n                skip\\_ipv4\\_addr=not ipv4\\_address,\n                may\\_have\\_port=False,\n                maybe\\_simple=simple\\_host,\n                rfc\\_1034=rfc\\_1034,\n                rfc\\_2782=rfc\\_2782,\n            )\n        )\n        if re.match(\n            # dot-atom\n            r\"(^[-!#$%&'\\*+/=?^\\_`{}|~0-9A-Z]+(\\.[-!#$%&'\\*+/=?^\\_`{}|~0-9A-Z]+)\\*$\"\n            # quoted-string\n            + r'|^\"([\\001-\\010\\013\\014\\016-\\037!#-\\[\\]-\\177]|\\\\[\\001-\\011\\013\\014\\016-\\177])\\*\"$)',\n            username\\_part,\n            re.IGNORECASE,\n        )\n        else False\n    )\n\n# hashes¶\n\n\n## \n`validators.hashes.md5(value)`\n¶\n\nReturn whether or not given value is a valid MD5 hash.\n\n```\n>>> md5('d41d8cd98f00b204e9800998ecf8427e')\n\n# Output: True\n>>> md5('900zz11')\n# Output: ValidationError(func=md5, args={'value': '900zz11'})\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `value` | `str` | \nMD5 string to validate.\n | *required* |\n\n\n| Type | Description |\n| --- | --- |\n|"
    },
    {
        "instruction": "Create a Python program using the 'validators' API to validate user-provided email addresses, checking if they adhere to the standard email format. Additionally, the program should also check if the email address is deliverable by making a request to the email server.",
        "api": "validators",
        "output": "#!pip install validators\nimport validators\nimport smtplib\n\ndef validate_email(email):\n    if validators.email(email):\n        try:\n            server = smtplib.SMTP('smtp.gmail.com', 587)\n            server.starttls()\n            server.login(\"your_email_address\", \"your_email_password\")\n            response = server.sendmail(\"your_email_address\", email, \"Test message\")\n            server.quit()\n            if response == {}:\n                return \"Valid email address and deliverable.\"\n            else:\n                return \"Valid email address but undeliverable.\"\n        except smtplib.SMTPException:\n            return \"Valid email address but undeliverable.\"\n    else:\n        return \"Invalid email address.\"\n\n# Get user inputs\nuser_email = input(\"Enter an email address: \")\n\n# Validate and display results\nprint(validate_email(user_email))\n",
        "documentation": "\n\n==================\n Document 0 \n----------------\n Output: ValidationError(email=email, args={'value': 'bogus@@'})\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `value` | `str` | \neMail string to validate.\n | *required* |\n| `ipv6_address` | `bool` | \nWhen the domain part is an IPv6 address.\n | `False` |\n| `ipv4_address` | `bool` | \nWhen the domain part is an IPv4 address.\n | `False` |\n| `simple_host` | `bool` | \nWhen the domain part is a simple hostname.\n | `False` |\n| `rfc_1034` | `bool` | \nAllow trailing dot in domain name.\nRef: RFC 1034.\n | `False` |\n| `rfc_2782` | `bool` | \nDomain name is of type service record.\nRef: RFC 2782.\n | `False` |\n\n\n| Type | Description |\n| --- | --- |\n| `Literal[True]` | \nIf `value` is a valid eMail.\n |\n| `ValidationError` | \nIf `value` is an invalid eMail.\n |\n\n> \n> *New in version 0.1.0*.\n> \n> \n> \n\nSource code in `/opt/hostedtoolcache/Python/3.11.5/x64/lib/python3.11/site-packages/validators/email.py`\n\n\n|  |  |\n| --- | --- |\n| \n```\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n```\n | \n```\n@validator\ndef email(\n    value: str,\n    /,\n    \\*,\n    ipv6\\_address: bool = False,\n    ipv4\\_address: bool = False,\n    simple\\_host: bool = False,\n    rfc\\_1034: bool = False,\n    rfc\\_2782: bool = False,\n):\n \"\"\"Validate an email address.\n\n This was inspired from [Django's email validator][1].\n Also ref: [RFC 1034][2], [RFC 5321][3] and [RFC 5322][4].\n\n [1]: https://github.com/django/django/blob/main/django/core/validators.py#L174\n [2]: https://www.rfc-editor.org/rfc/rfc1034\n [3]: https://www.rfc-editor.org/rfc/rfc5321\n [4]: https://www.rfc-editor.org/rfc/rfc5322\n\n Examples:\n >>> email('someone@example.com')\n # Output: True\n >>> email('bogus@@')\n # Output: ValidationError(email=email, args={'value': 'bogus@@'})\n\n Args:\n value:\n eMail string to validate.\n ipv6\\_address:\n When the domain part is an IPv6 address.\n ipv4\\_address:\n When the domain part is an IPv4 address.\n simple\\_host:\n When the domain part is a simple hostname.\n rfc\\_1034:\n Allow trailing dot in domain name.\n Ref: [RFC 1034](https://www.rfc-editor.org/rfc/rfc1034).\n rfc\\_2782:\n Domain name is of type service record.\n Ref: [RFC 2782](https://www.rfc-editor.org/rfc/rfc2782).\n\n Returns:\n (Literal[True]):\n If `value` is a valid eMail.\n (ValidationError):\n If `value` is an invalid eMail.\n\n > \\*New in version 0.1.0\\*.\n \"\"\"\n    if not value or value.count(\"@\") != 1:\n        return False\n\n    username\\_part, domain\\_part = value.rsplit(\"@\", 1)\n\n    if len(username\\_part) > 64 or len(domain\\_part) > 253:\n        # ref: RFC 1034 and 5231\n        return False\n\n    if ipv6\\_address or ipv4\\_address:\n        if domain\\_part.startswith(\"[\") and domain\\_part.endswith(\"]\"):\n            # ref: RFC 5321\n            domain\\_part = domain\\_part.lstrip(\"[\").rstrip(\"]\")\n        else:\n            return False\n\n    return (\n        bool(\n            hostname(\n                domain\\_part,\n                skip\\_ipv6\\_addr=not ipv6\\_address,\n                skip\\_ipv4\\_addr=not ipv4\\_address,\n                may\\_have\\_port=False,\n                maybe\\_simple=simple\\_host,\n                rfc\\_1034=rfc\\_1034,\n                rfc\\_2782=rfc\\_2782,\n            )\n        )\n        if re.match(\n            # dot-atom\n            r\"(^[-!#$%&'\\*+/=?^\\_`{}|~0-9A-Z]+(\\.[-!#$%&'\\*+/=?^\\_`{}|~0-9A-Z]+)\\*$\"\n            # quoted-string\n            + r'|^\"([\\001-\\010\\013\\014\\016-\\037!#-\\[\\]-\\177]|\\\\[\\001-\\011\\013\\014\\016-\\177])\\*\"$)',\n            username\\_part,\n            re.IGNORECASE,\n        )\n        else False\n    )\n\n# hashes¶\n\n\n## \n`validators.hashes.md5(value)`\n¶\n\nReturn whether or not given value is a valid MD5 hash.\n\n```\n>>> md5('d41d8cd98f00b204e9800998ecf8427e')\n\n# Output: True\n>>> md5('900zz11')\n# Output: ValidationError(func=md5, args={'value': '900zz11'})\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `value` | `str` | \nMD5 string to validate.\n | *required* |\n\n\n| Type | Description |\n| --- | --- |\n|\n\n==================\n Document 1 \n----------------\n Output: True\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `value` | `str` | \nDomain string to validate.\n | *required* |\n| `rfc_1034` | `bool` | \nAllow trailing dot in domain name.\nRef: RFC 1034.\n | `False` |\n| `rfc_2782` | `bool` | \nDomain name is of type service record.\nRef: RFC 2782.\n | `False` |\n\n\n| Type | Description |\n| --- | --- |\n| `Literal[True]` | \nIf `value` is a valid domain name.\n |\n| `ValidationError` | \nIf `value` is an invalid domain name.\n |\n\nNote\n* *In version 0.10.0*:\n\t+ Added support for internationalized domain name (IDN) validation.\n\n\n \n> \n> *New in version 0.9.0*.\n> \n> \n> \n\nSource code in `/opt/hostedtoolcache/Python/3.11.5/x64/lib/python3.11/site-packages/validators/domain.py`\n\n\n|  |  |\n| --- | --- |\n| \n```\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n```\n | \n```\n@validator\ndef domain(value: str, /, \\*, rfc\\_1034: bool = False, rfc\\_2782: bool = False):\n \"\"\"Return whether or not given value is a valid domain.\n\n Examples:\n >>> domain('example.com')\n # Output: True\n >>> domain('example.com/')\n # Output: ValidationError(func=domain, ...)\n >>> # Supports IDN domains as well::\n >>> domain('xn----gtbspbbmkef.xn--p1ai')\n # Output: True\n\n Args:\n value:\n Domain string to validate.\n rfc\\_1034:\n Allow trailing dot in domain name.\n Ref: [RFC 1034](https://www.rfc-editor.org/rfc/rfc1034).\n rfc\\_2782:\n Domain name is of type service record.\n Ref: [RFC 2782](https://www.rfc-editor.org/rfc/rfc2782).\n\n\n Returns:\n (Literal[True]):\n If `value` is a valid domain name.\n (ValidationError):\n If `value` is an invalid domain name.\n\n Note:\n - \\*In version 0.10.0\\*:\n - Added support for internationalized domain name (IDN) validation.\n\n > \\*New in version 0.9.0\\*.\n \"\"\"\n    if not value:\n        return False\n    try:\n        return not re.search(r\"\\s\", value) and re.match(\n            # First character of the domain\n            rf\"^(?:[a-zA-Z0-9{'\\_'if rfc\\_2782 else ''}]\"\n            # Sub domain + hostname\n            + r\"(?:[a-zA-Z0-9-\\_]{0,61}[A-Za-z0-9])?\\.)\"\n            # First 61 characters of the gTLD\n            + r\"+[A-Za-z0-9][A-Za-z0-9-\\_]{0,61}\"\n            # Last character of the gTLD\n            + rf\"[A-Za-z]{r'.$' if rfc\\_1034 else r'$'}\",\n            value.encode(\"idna\").decode(\"utf-8\"),\n            re.IGNORECASE,\n        )\n    except UnicodeError:\n        return False\n\n# email¶\n\n\n## \n`validators.email.email(value, /, \\*, ipv6\\_address=False, ipv4\\_address=False, simple\\_host=False, rfc\\_1034=False, rfc\\_2782=False)`\n¶\n\nValidate an email address.\n\n\nThis was inspired from Django's email validator.\nAlso ref: RFC 1034, RFC 5321 and RFC 5322.\n\n```\n>>> email('someone@example.com')\n\n# Output: True\n>>> email('bogus@@')\n# Output: ValidationError(email=email, args={'value': 'bogus@@'})\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `value` | `str` | \neMail string to validate.\n | *required* |\n| `ipv6_address` | `bool` | \nWhen the domain part\n\n==================\n Document 2 \n----------------\n Output: ValidationError(func=btc\\_address, args=...)\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `value` | `str` | \nBitcoin address string to validate.\n | *required* |\n\n\n| Type | Description |\n| --- | --- |\n| `Literal[True]` | \nIf `value` is a valid bitcoin address.\n |\n| `ValidationError` | \nIf `value` is an invalid bitcoin address.\n |\n\n> \n> *New in version 0.18.0*.\n> \n> \n> \n\nSource code in `/opt/hostedtoolcache/Python/3.11.5/x64/lib/python3.11/site-packages/validators/btc_address.py`\n\n\n|  |  |\n| --- | --- |\n| \n```\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n```\n | \n```\n@validator\ndef btc\\_address(value: str, /):\n \"\"\"Return whether or not given value is a valid bitcoin address.\n\n Full validation is implemented for P2PKH and P2SH addresses.\n For segwit addresses a regexp is used to provide a reasonable\n estimate on whether the address is valid.\n\n Examples:\n >>> btc\\_address('3Cwgr2g7vsi1bXDUkpEnVoRLA9w4FZfC69')\n # Output: True\n >>> btc\\_address('1BvBMsEYstWetqTFn5Au4m4GFg7xJaNVN2')\n # Output: ValidationError(func=btc\\_address, args=...)\n\n Args:\n value:\n Bitcoin address string to validate.\n\n Returns:\n (Literal[True]):\n If `value` is a valid bitcoin address.\n (ValidationError):\n If `value` is an invalid bitcoin address.\n\n > \\*New in version 0.18.0\\*.\n \"\"\"\n    if not value:\n        return False\n\n    return (\n        # segwit pattern\n        re.compile(r\"^(bc|tc)[0-3][02-9ac-hj-np-z]{14,74}$\").match(value)\n        if value[:2] in (\"bc\", \"tb\")\n        else \\_validate\\_old\\_btc\\_address(value)\n    )\n\n# card¶\n\n\n## \n`validators.card.amex(value)`\n¶\n\nReturn whether or not given value is a valid American Express card number.\n\n```\n>>> amex('378282246310005')\n\n# Output: True\n>>> amex('4242424242424242')\n# Output: ValidationError(func=amex, args={'value': '4242424242424242'})\n\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `value` | `str` | \nAmerican Express card number string to validate\n | *required* |\n\n\n| Type | Description |\n| ---"
    },
    {
        "instruction": "Create a Python program using the 'xarray' API to read and manipulate a NetCDF file. The program should read a NetCDF file, extract a specific variable, perform a mathematical operation on the variable, and save the modified variable back to a new NetCDF file.",
        "api": "xarray",
        "output": "#!pip install xarray\nimport xarray as xr\n\n# Read the NetCDF file\nds = xr.open_dataset('input.nc')\n\n# Extract the variable\nvar = ds['temperature']\n\n# Perform a mathematical operation on the variable\nvar_modified = var * 2\n\n# Save the modified variable to a new NetCDF file\nvar_modified.to_netcdf('output.nc')",
        "documentation": "\n\n==================\n Document 0 \n----------------\n## DataArray methods#\n\n\n|  |  |\n| --- | --- |\n| `load\\_dataarray`(filename\\_or\\_obj, \\*\\*kwargs) | Open, load into memory, and close a DataArray from a file or file-like object containing a single data variable. |\n| `open\\_dataarray`(filename\\_or\\_obj, \\*[, engine, ...]) | Open an DataArray from a file or file-like object containing a single data variable. |\n| `DataArray.as\\_numpy`() | Coerces wrapped data and coordinates into numpy arrays, returning a DataArray. |\n| `DataArray.from\\_cdms2`(variable) | Convert a cdms2.Variable into an xarray.DataArray |\n| `DataArray.from\\_dict`(d) | Convert a dictionary into an xarray.DataArray |\n| `DataArray.from\\_iris`(cube) | Convert a iris.cube.Cube into an xarray.DataArray |\n| `DataArray.from\\_series`(series[, sparse]) | Convert a pandas.Series into an xarray.DataArray. |\n| `DataArray.to\\_cdms2`() | Convert this array into a cdms2.Variable |\n| `DataArray.to\\_dask\\_dataframe`([dim\\_order, ...]) | Convert this array into a dask.dataframe.DataFrame. |\n| `DataArray.to\\_dataframe`([name, dim\\_order]) | Convert this array and its coordinates into a tidy pandas.DataFrame. |\n| `DataArray.to\\_dataset`([dim, name, promote\\_attrs]) | Convert a DataArray to a Dataset. |\n| `DataArray.to\\_dict`([data, encoding]) | Convert this xarray.DataArray into a dictionary following xarray naming conventions. |\n| `DataArray.to\\_index`() | Convert this variable to a pandas.Index. |\n| `DataArray.to\\_iris`() | Convert this array into a iris.cube.Cube |\n| `DataArray.to\\_masked\\_array`([copy]) | Convert this array into a numpy.ma.MaskedArray |\n| `DataArray.to\\_netcdf`([path, mode, format, ...]) | Write DataArray contents to a netCDF file. |\n| `DataArray.to\\_numpy`() | Coerces wrapped data to numpy and returns a numpy.ndarray. |\n| `DataArray.to\\_pandas`() | Convert this array into a pandas object with the same shape. |\n| `DataArray.to\\_series`() | Convert this array into a pandas.Series. |\n| `DataArray.to\\_zarr`([store, chunk\\_store, ...]) | Write DataArray contents to a Zarr store |\n| `DataArray.chunk`([chunks, name\\_prefix, ...]) | Coerce this array's data into a dask arrays with the given chunks. |\n| `DataArray.close`() | Release any resources linked to this object. |\n| `DataArray.compute`(\\*\\*kwargs) | Manually trigger loading of this array's data from disk or a remote source into memory and return a new array. |\n| `DataArray.persist`(\\*\\*kwargs) | Trigger computation in constituent dask arrays |\n| `DataArray.load`(\\*\\*kwargs) | Manually trigger loading of this array's data from disk or a remote source into memory and return this array. |\n| `DataArray.unify\\_chunks`() | Unify chunk size along all chunked dimensions of this DataArray. |\n\n## Coordinates objects#\n\n\n### Dataset#\n\n\n|  |  |\n| --- | --- |\n| `core.coordinates.DatasetCoordinates`(dataset) | Dictionary like container for Dataset coordinates (variables + indexes). |\n| `core.coordinates.DatasetCoordinates.dtypes` | Mapping from coordinate names to dtypes. |\n\n\n\n### DataArray#\n\n\n|  |  |\n| --- | --- |\n| `core.coordinates.DataArrayCoordinates`(dataarray) | Dictionary like container for DataArray coordinates (variables + indexes). |\n| `core.coordinates.DataArrayCoordinates.dtypes` | Mapping from coordinate names to dtypes. |\n\n## Plotting#\n\n\n|  |  |\n| --- | --- |\n| `Dataset.plot.scatter`(\\*args[, x, y, z, hue, ...]) | Scatter variables against each other. |\n| `Dataset.plot.quiver`(\\*args[, x, y, u, v, ...]) | Quiver plot of Dataset variables. |\n| `Dataset.plot.streamplot`(\\*args[, x, y, u, v, ...]) | Plot streamlines of Dataset variables. |\n\n\n|  |  |\n| --- |\n\n==================\n Document 1 \n----------------\n# Advanced API#\n\n\n|  |  |\n| --- | --- |\n| `Coordinates`([coords, indexes]) | Dictionary like container for Xarray coordinates (variables + indexes). |\n| `Dataset.variables` | Low level interface to Dataset contents as dict of Variable objects. |\n| `DataArray.variable` | Low level interface to the Variable object for this DataArray. |\n| `Variable`(dims, data[, attrs, encoding, fastpath]) | A netcdf-like variable consisting of dimensions, data and attributes which describe a single Array. |\n| `IndexVariable`(dims, data[, attrs, encoding, ...]) | Wrapper for accommodating a pandas.Index in an xarray.Variable. |\n| `as\\_variable`(obj[, name]) | Convert an object into a Variable. |\n| `Index`() | Base class inherited by all xarray-compatible indexes. |\n| `IndexSelResult`(dim\\_indexers[, indexes, ...]) | Index query results. |\n| `Context`(func) | object carrying the information of a call |\n| `register\\_dataset\\_accessor`(name) | Register a custom property on xarray.Dataset objects. |\n| `register\\_dataarray\\_accessor`(name) | Register a custom accessor on xarray.DataArray objects. |\n| `Dataset.set\\_close`(close) | Register the function that releases any resources linked to this object. |\n| `backends.BackendArray`() |  |\n| `backends.BackendEntrypoint`() | `BackendEntrypoint` is a class container and it is the main interface for the backend plugins, see BackendEntrypoint subclassing. |\n| `backends.list\\_engines`() | Return a dictionary of available engines and their BackendEntrypoint objects. |\n| `backends.refresh\\_engines`() | Refreshes the backend engines based on installed packages. |\n\n\nDefault, pandas-backed indexes built-in Xarray:\n\n> \n> indexes.PandasIndex\n> indexes.PandasMultiIndex\n> \n> \n> \n\n\nThese backends provide a low-level interface for lazily loading data from\nexternal file-formats or protocols, and can be manually invoked to create\narguments for the `load\\_store` and `dump\\_to\\_store` Dataset methods:\n\n\n|  |  |\n| --- | --- |\n| `backends.NetCDF4DataStore`(manager[, group, ...]) | Store for reading and writing data via the Python-NetCDF4 library. |\n| `backends.H5NetCDFStore`(manager[, group, ...]) | Store for reading and writing data via h5netcdf |\n| `backends.PseudoNetCDFDataStore`(manager[, lock]) | Store for accessing datasets via PseudoNetCDF |\n| `backends.PydapDataStore`(ds) | Store for accessing OpenDAP datasets with pydap. |\n| `backends.ScipyDataStore`(filename\\_or\\_obj[, ...]) | Store for reading and writing data via scipy.io.netcdf. |\n| `backends.ZarrStore`(zarr\\_group[, mode, ...]) | Store for reading and writing data via zarr |\n| `backends.FileManager`() | Manager for acquiring and closing a file object. |\n| `backends.CachingFileManager`(opener, \\*args[, ...]) | Wrapper for automatically opening and closing file objects. |\n| `backends.DummyFileManager`(value) | FileManager that simply wraps an open file in the FileManager interface. |\n\n\nThese BackendEntrypoints provide a basic interface to the most commonly\nused filetypes in the xarray universe.\n\n\n|  |  |\n| --- | --- |\n| `backends.NetCDF4BackendEntrypoint`() | Backend for netCDF files based on the netCDF4 package. |\n| `backends.H5netcdfBackendEntrypoint`() | Backend for netCDF files based on the h5netcdf package. |\n| `backends.PseudoNetCDFBackendEntrypoint`() | Backend for netCDF-like data formats in the air quality field based on the PseudoNetCDF package. |\n| `backends.PydapBackendEntrypoint`() | Backend for steaming datasets over the internet using the Data Access Protocol, also known as DODS or OPeNDAP based on the pydap package. |\n| `backends.ScipyBackendEntrypoint`() | Backend for netCDF files based on the scipy package. |\n| `backends.StoreBackendEntrypoint`() |  |\n| `backends.ZarrBackendEntrypoint`() | Backend for \".zarr\" files based on the zarr package. |\n\n## Deprecated / Pending Deprecation#\n\n\n|  |  |\n| --- | --- |\n| `Dataset.drop`([labels, dim, errors]) | Backward compatible method based on drop\\_vars and drop\\_sel |\n| `DataArray.drop`([labels, dim, errors]) | Backward compatible method based on drop\\_vars and drop\\_sel |\n| `Dataset.apply`(func[, keep\\_attrs, args]) | Backward compatible implementation\n\n==================\n Document 2 \n----------------\n## Dataset methods#\n\n\n|  |  |\n| --- | --- |\n| `load\\_dataset`(filename\\_or\\_obj, \\*\\*kwargs) | Open, load into memory, and close a Dataset from a file or file-like object. |\n| `open\\_dataset`(filename\\_or\\_obj, \\*[, engine, ...]) | Open and decode a dataset from a file or file-like object. |\n| `open\\_mfdataset`(paths[, chunks, concat\\_dim, ...]) | Open multiple files as a single dataset. |\n| `open\\_zarr`(store[, group, synchronizer, ...]) | Load and decode a dataset from a Zarr store. |\n| `save\\_mfdataset`(datasets, paths[, mode, ...]) | Write multiple datasets to disk as netCDF files simultaneously. |\n| `Dataset.as\\_numpy`() | Coerces wrapped data and coordinates into numpy arrays, returning a Dataset. |\n| `Dataset.from\\_dataframe`(dataframe[, sparse]) | Convert a pandas.DataFrame into an xarray.Dataset |\n| `Dataset.from\\_dict`(d) | Convert a dictionary into an xarray.Dataset. |\n| `Dataset.to\\_array`([dim, name]) | Convert this dataset into an xarray.DataArray |\n| `Dataset.to\\_dataframe`([dim\\_order]) | Convert this dataset into a pandas.DataFrame. |\n| `Dataset.to\\_dask\\_dataframe`([dim\\_order, set\\_index]) | Convert this dataset into a dask.dataframe.DataFrame. |\n| `Dataset.to\\_dict`([data, encoding]) | Convert this dataset to a dictionary following xarray naming conventions. |\n| `Dataset.to\\_netcdf`([path, mode, format, ...]) | Write dataset contents to a netCDF file. |\n| `Dataset.to\\_pandas`() | Convert this dataset into a pandas object without changing the number of dimensions. |\n| `Dataset.to\\_zarr`([store, chunk\\_store, mode, ...]) | Write dataset contents to a zarr group. |\n| `Dataset.chunk`([chunks, name\\_prefix, token, ...]) | Coerce all arrays in this dataset into dask arrays with the given chunks. |\n| `Dataset.close`() | Release any resources linked to this object. |\n| `Dataset.compute`(\\*\\*kwargs) | Manually trigger loading and/or computation of this dataset's data from disk or a remote source into memory and return a new dataset. |\n| `Dataset.filter\\_by\\_attrs`(\\*\\*kwargs) | Returns a `Dataset` with variables that match specific conditions. |\n| `Dataset.info`([buf]) | Concise summary of a Dataset variables and attributes. |\n| `Dataset.load`(\\*\\*kwargs) | Manually trigger loading and/or computation of this dataset's data from disk or a remote source into memory and return this dataset. |\n| `Dataset.persist`(\\*\\*kwargs) | Trigger computation, keeping data as dask arrays |\n| `Dataset.unify\\_chunks`() | Unify chunk size along all chunked dimensions of this Dataset. |\n\n### DataArray methods#\n\n\n|  |  |\n| --- | --- |\n| `load\\_dataarray`(filename\\_or\\_obj, \\*\\*kwargs) | Open, load into memory, and close a DataArray from a file or file-like object containing a single data variable. |\n| `open\\_dataarray`(filename\\_or\\_obj, \\*[, engine, ...]) | Open an DataArray from a\n\n==================\n Document 3 \n----------------\n## ndarray methods#\n\n\n|  |  |\n| --- | --- |\n| `Dataset.argsort`([axis, kind, order]) | Returns the indices that would sort this array. |\n| `Dataset.astype`(dtype, \\*[, order, casting, ...]) | Copy of the xarray object, with data cast to a specified type. |\n| `Dataset.clip`([min, max, keep\\_attrs]) | Return an array whose values are limited to `[min, max]`. |\n| `Dataset.conj`() | Complex-conjugate all elements. |\n| `Dataset.conjugate`() | Return the complex conjugate, element-wise. |\n| `Dataset.imag` | The imaginary part of each data variable. |\n| `Dataset.round`(\\*args, \\*\\*kwargs) | Evenly round to the given number of decimals. |\n| `Dataset.real` | The real part of each data variable. |\n| `Dataset.rank`(dim[, pct, keep\\_attrs]) | Ranks the data. |\n\n### Reshaping and reorganizing#\n\n\n|  |  |\n| --- | --- |\n| `Dataset.transpose`(\\*dims[, missing\\_dims]) | Return a new Dataset object with all array dimensions transposed. |\n| `Dataset.stack`([dimensions, create\\_index, ...]) | Stack any number of existing dimensions into a single new dimension. |\n| `Dataset.unstack`([dim, fill\\_value, sparse])\n\n==================\n Document 4 \n----------------\n# Top-level functions#\n\n\n|  |  |\n| --- | --- |\n| `apply\\_ufunc`(func, \\*args[, input\\_core\\_dims, ...]) | Apply a vectorized function for unlabeled arrays on xarray objects. |\n| `align`(\\*objects[, join, copy, indexes, ...]) | Given any number of Dataset and/or DataArray objects, returns new objects with aligned indexes and dimension sizes. |\n| `broadcast`(\\*args[, exclude]) | Explicitly broadcast any number of DataArray or Dataset objects against one another. |\n| `concat`(objs, dim[, data\\_vars, coords, ...]) | Concatenate xarray objects along a new or existing dimension. |\n| `merge`(objects[, compat, join, fill\\_value, ...]) | Merge any number of xarray objects into a single Dataset as variables. |\n| `combine\\_by\\_coords`([data\\_objects, compat, ...]) | Attempt to auto-magically combine the given datasets (or data arrays) into one by using dimension coordinates. |\n| `combine\\_nested`(datasets, concat\\_dim[, ...]) | Explicitly combine an N-dimensional grid of datasets into one by using a succession of concat and merge operations along each dimension of the grid. |\n| `where`(cond, x, y[, keep\\_attrs]) | Return elements from x or y depending on cond. |\n| `infer\\_freq`(index) | Infer the most likely frequency given the input index. |\n| `full\\_like`(other, fill\\_value[, dtype, ...]) | Return a new object with the same shape and type as a given object. |\n| `zeros\\_like`(other[, dtype, chunks, ...]) | Return a new object of zeros with the same shape and type as a given dataarray or dataset. |\n| `ones\\_like`(other[, dtype, chunks, ...]) | Return a new object of ones with the same shape and type as a given dataarray or dataset. |\n| `cov`(da\\_a, da\\_b[, dim, ddof]) | Compute covariance between two DataArray objects along a shared dimension. |\n| `corr`(da\\_a, da\\_b[, dim]) | Compute the Pearson correlation coefficient between two DataArray objects along a shared dimension. |\n| `cross`(a, b, \\*, dim) | Compute the cross product of two (arrays of) vectors. |\n| `dot`(\\*arrays[, dims]) | Generalized dot product for xarray objects. |\n| `polyval`(coord, coeffs[, degree\\_dim]) | Evaluate a polynomial at specific values |\n| `map\\_blocks`(func, obj[, args, kwargs, template]) | Apply a function to each block of a DataArray or Dataset. |\n| `show\\_versions`([file]) | print the versions of xarray and its dependencies |\n| `set\\_options`(\\*\\*kwargs) | Set options for xarray in a controlled context. |\n| `get\\_options`() | Get options for xarray. |\n| `unify\\_chunks`(\\*objects) | Given any number of Dataset and/or DataArray objects, returns new objects with unified chunk size along all chunked dimensions. |\n\n\n## Dataset#\n\n\n### Creating a dataset#\n\n\n|  |  |\n| --- | --- |\n| `Dataset`([data\\_vars, coords, attrs]) | A multi-dimensional, in memory, array database. |\n| `decode\\_cf`(obj[, concat\\_characters, ...]) | Decode the given Dataset or Datastore according to CF conventions into a new Dataset. |\n\n\n### Attributes#\n\n\n|  |  |\n| --- | --- |\n| `Dataset.dims` | Mapping from dimension names to lengths. |\n| `Dataset.sizes` | Mapping from dimension names to lengths. |\n| `Dataset.dtypes` | Mapping from data variable names to dtypes. |\n| `Dataset.data\\_vars` |"
    },
    {
        "instruction": "Create a Python program using the 'xarray' API to perform data slicing, masking, and aggregation on a multi-dimensional data array. The program should read a NetCDF file, select a specific variable, slice the data along one or more dimensions, apply a mask based on a condition, and then calculate the mean, standard deviation, and maximum value of the masked and sliced data along another specified dimension. Finally, the program should print the calculated statistics.",
        "api": "xarray",
        "output": "#!pip install xarray\nimport xarray as xr\n\n# Read the NetCDF file\nds = xr.open_dataset('input.nc')\n\n# Select the variable\nvar = ds['temperature']\n\n# Slice the data along one or more dimensions\nsliced_data = var.sel(time=slice('2000-01-01', '2000-12-31'), lat=slice(0, 90))\n\n# Apply a mask based on a condition\nmask = sliced_data > 25\nmasked_data = sliced_data.where(mask)\n\n# Calculate the mean, standard deviation, and maximum value of the masked and sliced data along another specified dimension\nmean = masked_data.mean(dim='time')\nstd_dev = masked_data.std(dim='time')\nmax_value = masked_data.max(dim='time')\n\n# Print the calculated statistics\nprint(\"Mean: \", mean)\nprint(\"Standard Deviation: \", std_dev)\nprint(\"Maximum Value: \", max_value)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n## DataArray methods#\n\n\n|  |  |\n| --- | --- |\n| `load\\_dataarray`(filename\\_or\\_obj, \\*\\*kwargs) | Open, load into memory, and close a DataArray from a file or file-like object containing a single data variable. |\n| `open\\_dataarray`(filename\\_or\\_obj, \\*[, engine, ...]) | Open an DataArray from a file or file-like object containing a single data variable. |\n| `DataArray.as\\_numpy`() | Coerces wrapped data and coordinates into numpy arrays, returning a DataArray. |\n| `DataArray.from\\_cdms2`(variable) | Convert a cdms2.Variable into an xarray.DataArray |\n| `DataArray.from\\_dict`(d) | Convert a dictionary into an xarray.DataArray |\n| `DataArray.from\\_iris`(cube) | Convert a iris.cube.Cube into an xarray.DataArray |\n| `DataArray.from\\_series`(series[, sparse]) | Convert a pandas.Series into an xarray.DataArray. |\n| `DataArray.to\\_cdms2`() | Convert this array into a cdms2.Variable |\n| `DataArray.to\\_dask\\_dataframe`([dim\\_order, ...]) | Convert this array into a dask.dataframe.DataFrame. |\n| `DataArray.to\\_dataframe`([name, dim\\_order]) | Convert this array and its coordinates into a tidy pandas.DataFrame. |\n| `DataArray.to\\_dataset`([dim, name, promote\\_attrs]) | Convert a DataArray to a Dataset. |\n| `DataArray.to\\_dict`([data, encoding]) | Convert this xarray.DataArray into a dictionary following xarray naming conventions. |\n| `DataArray.to\\_index`() | Convert this variable to a pandas.Index. |\n| `DataArray.to\\_iris`() | Convert this array into a iris.cube.Cube |\n| `DataArray.to\\_masked\\_array`([copy]) | Convert this array into a numpy.ma.MaskedArray |\n| `DataArray.to\\_netcdf`([path, mode, format, ...]) | Write DataArray contents to a netCDF file. |\n| `DataArray.to\\_numpy`() | Coerces wrapped data to numpy and returns a numpy.ndarray. |\n| `DataArray.to\\_pandas`() | Convert this array into a pandas object with the same shape. |\n| `DataArray.to\\_series`() | Convert this array into a pandas.Series. |\n| `DataArray.to\\_zarr`([store, chunk\\_store, ...]) | Write DataArray contents to a Zarr store |\n| `DataArray.chunk`([chunks, name\\_prefix, ...]) | Coerce this array's data into a dask arrays with the given chunks. |\n| `DataArray.close`() | Release any resources linked to this object. |\n| `DataArray.compute`(\\*\\*kwargs) | Manually trigger loading of this array's data from disk or a remote source into memory and return a new array. |\n| `DataArray.persist`(\\*\\*kwargs) | Trigger computation in constituent dask arrays |\n| `DataArray.load`(\\*\\*kwargs) | Manually trigger loading of this array's data from disk or a remote source into memory and return this array. |\n| `DataArray.unify\\_chunks`() | Unify chunk size along all chunked dimensions of this DataArray. |\n\n## Coordinates objects#\n\n\n### Dataset#\n\n\n|  |  |\n| --- | --- |\n| `core.coordinates.DatasetCoordinates`(dataset) | Dictionary like container for Dataset coordinates (variables + indexes). |\n| `core.coordinates.DatasetCoordinates.dtypes` | Mapping from coordinate names to dtypes. |\n\n\n\n### DataArray#\n\n\n|  |  |\n| --- | --- |\n| `core.coordinates.DataArrayCoordinates`(dataarray) | Dictionary like container for DataArray coordinates (variables + indexes). |\n| `core.coordinates.DataArrayCoordinates.dtypes` | Mapping from coordinate names to dtypes. |\n\n## Plotting#\n\n\n|  |  |\n| --- | --- |\n| `Dataset.plot.scatter`(\\*args[, x, y, z, hue, ...]) | Scatter variables against each other. |\n| `Dataset.plot.quiver`(\\*args[, x, y, u, v, ...]) | Quiver plot of Dataset variables. |\n| `Dataset.plot.streamplot`(\\*args[, x, y, u, v, ...]) | Plot streamlines of Dataset variables. |\n\n\n|  |  |\n| --- |\n\n==================\n Document 1 \n----------------\n# Top-level functions#\n\n\n|  |  |\n| --- | --- |\n| `apply\\_ufunc`(func, \\*args[, input\\_core\\_dims, ...]) | Apply a vectorized function for unlabeled arrays on xarray objects. |\n| `align`(\\*objects[, join, copy, indexes, ...]) | Given any number of Dataset and/or DataArray objects, returns new objects with aligned indexes and dimension sizes. |\n| `broadcast`(\\*args[, exclude]) | Explicitly broadcast any number of DataArray or Dataset objects against one another. |\n| `concat`(objs, dim[, data\\_vars, coords, ...]) | Concatenate xarray objects along a new or existing dimension. |\n| `merge`(objects[, compat, join, fill\\_value, ...]) | Merge any number of xarray objects into a single Dataset as variables. |\n| `combine\\_by\\_coords`([data\\_objects, compat, ...]) | Attempt to auto-magically combine the given datasets (or data arrays) into one by using dimension coordinates. |\n| `combine\\_nested`(datasets, concat\\_dim[, ...]) | Explicitly combine an N-dimensional grid of datasets into one by using a succession of concat and merge operations along each dimension of the grid. |\n| `where`(cond, x, y[, keep\\_attrs]) | Return elements from x or y depending on cond. |\n| `infer\\_freq`(index) | Infer the most likely frequency given the input index. |\n| `full\\_like`(other, fill\\_value[, dtype, ...]) | Return a new object with the same shape and type as a given object. |\n| `zeros\\_like`(other[, dtype, chunks, ...]) | Return a new object of zeros with the same shape and type as a given dataarray or dataset. |\n| `ones\\_like`(other[, dtype, chunks, ...]) | Return a new object of ones with the same shape and type as a given dataarray or dataset. |\n| `cov`(da\\_a, da\\_b[, dim, ddof]) | Compute covariance between two DataArray objects along a shared dimension. |\n| `corr`(da\\_a, da\\_b[, dim]) | Compute the Pearson correlation coefficient between two DataArray objects along a shared dimension. |\n| `cross`(a, b, \\*, dim) | Compute the cross product of two (arrays of) vectors. |\n| `dot`(\\*arrays[, dims]) | Generalized dot product for xarray objects. |\n| `polyval`(coord, coeffs[, degree\\_dim]) | Evaluate a polynomial at specific values |\n| `map\\_blocks`(func, obj[, args, kwargs, template]) | Apply a function to each block of a DataArray or Dataset. |\n| `show\\_versions`([file]) | print the versions of xarray and its dependencies |\n| `set\\_options`(\\*\\*kwargs) | Set options for xarray in a controlled context. |\n| `get\\_options`() | Get options for xarray. |\n| `unify\\_chunks`(\\*objects) | Given any number of Dataset and/or DataArray objects, returns new objects with unified chunk size along all chunked dimensions. |\n\n\n## Dataset#\n\n\n### Creating a dataset#\n\n\n|  |  |\n| --- | --- |\n| `Dataset`([data\\_vars, coords, attrs]) | A multi-dimensional, in memory, array database. |\n| `decode\\_cf`(obj[, concat\\_characters, ...]) | Decode the given Dataset or Datastore according to CF conventions into a new Dataset. |\n\n\n### Attributes#\n\n\n|  |  |\n| --- | --- |\n| `Dataset.dims` | Mapping from dimension names to lengths. |\n| `Dataset.sizes` | Mapping from dimension names to lengths. |\n| `Dataset.dtypes` | Mapping from data variable names to dtypes. |\n| `Dataset.data\\_vars` |\n\n==================\n Document 2 \n----------------\n## ndarray methods#\n\n\n|  |  |\n| --- | --- |\n| `Dataset.argsort`([axis, kind, order]) | Returns the indices that would sort this array. |\n| `Dataset.astype`(dtype, \\*[, order, casting, ...]) | Copy of the xarray object, with data cast to a specified type. |\n| `Dataset.clip`([min, max, keep\\_attrs]) | Return an array whose values are limited to `[min, max]`. |\n| `Dataset.conj`() | Complex-conjugate all elements. |\n| `Dataset.conjugate`() | Return the complex conjugate, element-wise. |\n| `Dataset.imag` | The imaginary part of each data variable. |\n| `Dataset.round`(\\*args, \\*\\*kwargs) | Evenly round to the given number of decimals. |\n| `Dataset.real` | The real part of each data variable. |\n| `Dataset.rank`(dim[, pct, keep\\_attrs]) | Ranks the data. |\n\n### Reshaping and reorganizing#\n\n\n|  |  |\n| --- | --- |\n| `Dataset.transpose`(\\*dims[, missing\\_dims]) | Return a new Dataset object with all array dimensions transposed. |\n| `Dataset.stack`([dimensions, create\\_index, ...]) | Stack any number of existing dimensions into a single new dimension. |\n| `Dataset.unstack`([dim, fill\\_value, sparse])\n\n==================\n Document 3 \n----------------\n## Dataset methods#\n\n\n|  |  |\n| --- | --- |\n| `load\\_dataset`(filename\\_or\\_obj, \\*\\*kwargs) | Open, load into memory, and close a Dataset from a file or file-like object. |\n| `open\\_dataset`(filename\\_or\\_obj, \\*[, engine, ...]) | Open and decode a dataset from a file or file-like object. |\n| `open\\_mfdataset`(paths[, chunks, concat\\_dim, ...]) | Open multiple files as a single dataset. |\n| `open\\_zarr`(store[, group, synchronizer, ...]) | Load and decode a dataset from a Zarr store. |\n| `save\\_mfdataset`(datasets, paths[, mode, ...]) | Write multiple datasets to disk as netCDF files simultaneously. |\n| `Dataset.as\\_numpy`() | Coerces wrapped data and coordinates into numpy arrays, returning a Dataset. |\n| `Dataset.from\\_dataframe`(dataframe[, sparse]) | Convert a pandas.DataFrame into an xarray.Dataset |\n| `Dataset.from\\_dict`(d) | Convert a dictionary into an xarray.Dataset. |\n| `Dataset.to\\_array`([dim, name]) | Convert this dataset into an xarray.DataArray |\n| `Dataset.to\\_dataframe`([dim\\_order]) | Convert this dataset into a pandas.DataFrame. |\n| `Dataset.to\\_dask\\_dataframe`([dim\\_order, set\\_index]) | Convert this dataset into a dask.dataframe.DataFrame. |\n| `Dataset.to\\_dict`([data, encoding]) | Convert this dataset to a dictionary following xarray naming conventions. |\n| `Dataset.to\\_netcdf`([path, mode, format, ...]) | Write dataset contents to a netCDF file. |\n| `Dataset.to\\_pandas`() | Convert this dataset into a pandas object without changing the number of dimensions. |\n| `Dataset.to\\_zarr`([store, chunk\\_store, mode, ...]) | Write dataset contents to a zarr group. |\n| `Dataset.chunk`([chunks, name\\_prefix, token, ...]) | Coerce all arrays in this dataset into dask arrays with the given chunks. |\n| `Dataset.close`() | Release any resources linked to this object. |\n| `Dataset.compute`(\\*\\*kwargs) | Manually trigger loading and/or computation of this dataset's data from disk or a remote source into memory and return a new dataset. |\n| `Dataset.filter\\_by\\_attrs`(\\*\\*kwargs) | Returns a `Dataset` with variables that match specific conditions. |\n| `Dataset.info`([buf]) | Concise summary of a Dataset variables and attributes. |\n| `Dataset.load`(\\*\\*kwargs) | Manually trigger loading and/or computation of this dataset's data from disk or a remote source into memory and return this dataset. |\n| `Dataset.persist`(\\*\\*kwargs) | Trigger computation, keeping data as dask arrays |\n| `Dataset.unify\\_chunks`() | Unify chunk size along all chunked dimensions of this Dataset. |\n\n### DataArray methods#\n\n\n|  |  |\n| --- | --- |\n| `load\\_dataarray`(filename\\_or\\_obj, \\*\\*kwargs) | Open, load into memory, and close a DataArray from a file or file-like object containing a single data variable. |\n| `open\\_dataarray`(filename\\_or\\_obj, \\*[, engine, ...]) | Open an DataArray from a"
    },
    {
        "instruction": "Create a Python program that employs the 'ydata-profiling' API to profile a JSONL file named 'data.jsonl'. The profiling should include a count of records, data types of columns, and a table of unique values for a column named 'user_id'. Save the profiling report as an HTML file named 'jsonl_data_profile.html'.",
        "api": "ydata-profiling",
        "output": "#! pip install ydata-profiling\nimport pandas as pd\nfrom ydata_profiling import ProfileReport\n\n# Load the JSONL data\ndata = pd.read_json('data.jsonl', lines=True)\n\n# Perform data profiling\nprofiling_report = ProfileReport(data)\n\n# Save the report as an HTML file\nprofiling_report.to_file('jsonl_data_profile.html')",
        "documentation": "\n\n==================\n Document 0 \n----------------\n Profiling large datasets\n\n\nBy default, `ydata-profiling` comprehensively summarizes the input\ndataset in a way that gives the most insights for data analysis. For\nsmall datasets, these computations can be performed in *quasi*\nreal-time. For larger datasets, deciding upfront which calculations to\nmake might be required. Whether a computation scales to a large datasets\nnot only depends on the exact size of the dataset, but also on its\ncomplexity and on whether fast computations are available. If the\ncomputation time of the profiling becomes a bottleneck,\n`ydata-profiling` offers several alternatives to overcome it.\n\nScale in a fully managed system\n\n\nLooking for an fully managed system that is able to scale the profiling\nfor large datasets? Sign up Fabric\ncommunity for distributed data profiling.\n\n## Pyspark\n\n\nSpark\n\nMinimal mode\n\n\nThis mode was introduced in version v4.0.0\n\n`ydata-profiling` now supports Spark Dataframes profiling. You can find\nan example of the integration\nhere.\n\n\n**Features supported:** - Univariate variables' analysis - Head and Tail\ndataset sample - Correlation matrices: Pearson and Spearman\n\n\n*Coming soon* - Missing values analysis - Interactions - Improved\nhistogram computation\n\n\nKeep an eye on the\nGitHub page to\nfollow the updates on the implementation of Pyspark Dataframes\nsupport.\n\n\n\n## Minimal mode\n\n**Minimal mode**\n\n\nThis mode was introduced in version v2.4.0\n\n`ydata-profiling` includes a minimal configuration file where the most\nexpensive computations are turned off by default. This is the\nrecommended starting point for larger datasets.\n\n```\nprofile = ProfileReport(large\\_dataset, minimal=True)\nprofile.to\\_file(\"output.html\")\n\nThis configuration file can be found here:\nconfig\\_minimal.yaml.\nMore details on settings and configuration are available in\n`../advanced_usage/available_settings`.\n\n\n## Sample the dataset\n\n\nAn alternative way to handle really large datasets is to use a portion\nof it to generate the profiling report. Several users report this is a\ngood way to scale back the computation time while maintaining\nrepresentativity.\n\n> \n> pandas-profiling is\n\n==================\n Document 1 \n----------------\n Time-Series data\n\n\n`ydata-profiling` can be used for a quick Exploratory Data Analysis on\ntime-series data. This is useful for a quick understanding on the\nbehaviour of time dependent variables regarding behaviours such as time\nplots, seasonality, trends, stationary and data gaps.\n\n\nCombined with the profiling reports compare, you're able to compare the\nevolution and data behaviour through time, in terms of time-series\nspecific statistics such as PACF and ACF plots. It also provides the\nidentification of gaps in the time series, caused either by missing\nvalues or by entries missing in the time index.\n\nTime series EDA tutorial\n\n\nDo you want to learn how to interpret the time-series profiling\ncheck out blog content here. \n\n\nYou can find the a [otebook with the\nfull code in our examples folder.\n\n```\n\n\n```\n\n\n\nTime-series profiling report\n\n## Profiling time-series dataset\n\n\nThe following syntax can be used to generate a profile under the\nassumption that the dataset includes time dependent features:\n\n\n| Setting the configurations for time-series profiling |\n| --- |\n| \n```\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n```\n | \n```\nimport pandas as pd\n\nfrom ydata\\_profiling.utils.cache import cache\\_file\nfrom ydata\\_profiling import ProfileReport\n\nfile\\_name = cache\\_file(\n    \"pollution\\_us\\_2000\\_2016.csv\",\n    \"https://query.data.world/s/mz5ot3l4zrgvldncfgxu34nda45kvb\",\n)\n\ndf = pd.read\\_csv(file\\_name, index\\_col=[0])\n\n\n# Filtering time-series to profile a single site\nsite = df[df[\"Site Num\"] == 3003]\n\n\n#Enable tsmode to True to automatically identify time-series variables\n\n#Provide the column name that provides the chronological order of your time-series\nprofile = ProfileReport(df, tsmode=True, sortby=\"Date Local\", title=\"Time-Series EDA\")\n\nprofile.to\\_file(\"report\\_timeseries.html\")\n\n\nTo enable a time-series report to be generated `ts_mode` needs to be set\nto `True`. If `True` the variables that have temporal dependence\nwill be automatically identified based on the presence of\nautocorrection. The time-series report uses the `sortby` attribute to\norder the dataset. If not provided it is assumed that the dataset is\nalready ordered.\nYou can set up the correlation level to detect to apply the time-series \nvalidation by setting the x configuration. \n\n\n\n### Warnings and validations\n\n\nSpecific to time-series analysis, 2 new warnings were added to the `ydata-profiling`\nwarnings family: **NON\\_STATIONARY** and **SEASONAL**.\n\n\n\n#### Stationarity\n\n\nIn the realm of time-series analysis, a stationary time-series is a dataset \nwhere statistical properties, such as mean, variance, and autocorrelation, \nremain constant over time. This property is essential for many time-series \nforecasting and modeling techniques because they often assume that the underlying \ndata is stationary. Stationarity simplifies the modeling process by making\nit easier to detect patterns and trends.\n\n\n`ydata-profiling` stationary warning is based on an **Augmented Dickey-Fuller(ADF)** test.\nNevertheless, you should always combine the output of this warning with a visual\ninspection to your time-series behaviour and search for variance of the \nrolling statistics analysis. \n\n\n\n#### Seasonality\n\n\nA seasonal time-series is a specific type of time-series data that exhibits\nrecurring patterns or fluctuations at regular intervals. These patterns\nare known as seasonality and are often observed in data associated with yearly,\nmonthly, weekly, or daily cycles. Seasonal time-series data can be challenging\nto model accurately without addressing the underlying seasonality.\n\n\n`ydata-profiling` seasonality warning is based on an **Augmented Dickey-Fuller(ADF)** test.\nNevertheless, you should always combine the output of this warning with a seasonal decomposition\nPACF and ACF plots (also computed in your time-series profiling).\n\n\n### Time-series missing gaps\n\n\n\nTime-series missing data visualization\n\nAs a data scientist, one of the critical aspects of working with time-series data\nis understanding and analyzing time-series gaps. Time-series gaps refer to the\nintervals within your time-series data where observations are missing or incomplete.\nWhile\n\n==================\n Document 2 \n----------------\n# Dataset schema\n\n\nIn addition to providing dataset details, users often want to include\nset type schemas. This is particularly important when integrating\n`ydata-profiling` generation with the information already in a data\ncatalog. When using `ydata-profiling` ProfileReport, users can set the\ntype\\_schema property to control the generated profiling data types. By\ndefault, the `type_schema` is automatically inferred with visions.\n\n\n| Set the variable type schema to Generate the profile report |\n| --- |\n| \n```\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n```\n | \n```\nimport json\nimport pandas as pd\n\nfrom ydata\\_profiling import ProfileReport\nfrom ydata\\_profiling.utils.cache import cache\\_file\n\nfile\\_name = cache\\_file(\n    \"titanic.csv\",\n    \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\",\n)\ndf = pd.read\\_csv(file\\_name)\n\ntype\\_schema = {\"Survived\": \"categorical\", \"Embarked\": \"categorical\"}\n\n# We can set the type\\_schema only for the variables that we are certain of their types. All the other will be automatically inferred.\nreport = ProfileReport(df, title=\"Titanic EDA\", type\\_schema=type\\_schema)\n\n\n# Data quality Profiling with a Collaborative experience\n\nNote\n\n\nSign-up Fabric community to try the **data catalog**\nand **collaborative** experience for data profiling at scale!\n\nYData Fabric is a Data-Centric AI\ndevelopment platform. YData Fabric provides all capabilities of\nydata-profiling in a hosted environment combined with a guided UI\nexperience.\n\n\nFabric's Data Catalog\nprovides a comprehensive and powerful tool designed to enable data\nprofessionals, including data scientists and data engineers, to manage\nand understand data within an organization. The Data Catalog act as a\nsearchable repository, that captures the schema and metadata, while\nproviding a unified view of all datasets.\n\n\n\n## Profiling in a Data Catalog\n\n\n\n### Built-in connectors\n\n\nFabric's Data Catalog experience, provides pre-configured interfaces\nfor a variety of data sources. The built-in connectors simplify and\nexpedite the data integration, while reducing developing time and\nenhancing data availability ensuring reliable and consistent data\nexchange.\n\n\n\n### Metadata management\n\n\nThe Data Catalog captures and stores automatically the datasets'\nmetadata, providing essential information about your data types, source,\nlast time of update, relationships among other characteristics, such as\nthe presence of potential Personally identifiable Information (PII). It\nsupports automated metadata ingestion from a variety of data sources,\nwhich allows to keep the catalog always up-to-date.\n\n\n\n### Data profiling and relationships\n\n\nAn interactive experience that allows to drill-down in a comprehensive data profiling\nand relationship analysis, providing deep insights into data structure,\ndistributions and interactions for improved data preparation.\n\n\n\n### Data quality indexes\n\n\nAccess and navigate indicators and data quality statistics, such as completeness, uniqueness\nand consistency. This feature ensures that your teams are working with\ntrusted, complete and reliable data while developing data initiatives.\n\n\n\n### Collaborative\n\n\nThe Data Catalog enables a collaborative experience through dataset\ndescriptions and tags for ease of search. This fosters collaboration\namong team members, sharing domain knowledge and experience, and leading\nto better, more informed decisions.\n\n\n\n### Security and Compliance\n\n\nThrough built-in connectors and flexible infrastructure enforce data access control per\nusers and per project. YData Fabric Data Catalog helps in maintaining\nregulatory compliance by identifying any sensitive data.\n\n\nTry today the Catalog experience in with Fabric Community\nversion!\n\n\n# Handling sensitive data\n\n\nIn certain data-sensitive contexts (for instance, private health\nrecords), sharing a report that includes a sample would violate privacy\nconstraints. The following configuration shorthand groups together\nvarious options so that only aggregate information is provided in the\nreport and no individual records are shown:\n\n\n|  |  |\n| --- | --- |\n| \n```\n1\n```\n | \n```\nreport = df.profile\\_report(sensitive=True)\n\n\nAdditionally, `pandas-profiling` does not send data to external\nservices, making it suitable for private data.\n\n\n## Sample and duplicates\n\n\nExplicitly showing a dataset\\'as sample and duplicate rows can be\ndisabled, to guarantee the report does not directly leak any data:\n\n\n|  |  |\n| --- | --- |\n| \n```\n1\n```\n | \n```\nreport = df.profile\\_report(duplicates=None, samples=None)\n\n\nAlternatively, it is possible\n\n==================\n Document 3 \n----------------\n# Column descriptions\n\n\nIn addition to providing dataset details, often users want to include\ncolumn-specific descriptions when sharing reports with team members and\nstakeholders. `ydata-profiling` supports creating these descriptions, so\nthat the report includes a built-in data dictionary. By default, the\ndescriptions are presented in the *Overview* section of the report, next\nto each variable.\n\n\n| Generate a report with per-variable descriptions |\n| --- |\n| \n```\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n```\n | \n```\nprofile = df.profile\\_report(\n    variables={\n        \"descriptions\": {\n            \"files\": \"Files in the filesystem, # variable name: variable description\",\n            \"datec\": \"Creation date\",\n            \"datem\": \"Modification date\",\n        }\n    }\n)\n\nprofile.to\\_file(report.html)\n\n\nAlternatively, column descriptions can be loaded from a JSON file:\n\n\n| dataset\\_column\\_definition.json |\n| --- |\n| \n```\n1\n2\n3\n4\n```\n | \n```\n{\n    column name 1: column 1 definition,\n    column name 2: column 2 definition\n}\n\n\n| Generate a report with descriptions per variable from a JSON definitions file |\n| --- |\n| \n```\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n```\n | \n```\nimport json\nimport pandas as pd\nimport ydata\\_profiling\n\ndefinition\\_file = dataset\\_column\\_definition.json\n\n# Read the variable descriptions\nwith open(definition\\_file, r) as f:\n    definitions = json.load(f)\n\n\n# By default, the descriptions are presented in the Overview section, next to each variable\nreport = df.profile\\_report(variable={\"descriptions\": definitions})\n\n\n# We can disable showing the descriptions next to each variable\nreport = df.profile\\_report(\n    variable={\"descriptions\": definitions}, show\\_variable\\_description=False\n)\n\nreport.to\\_file(\"report.html\")\n\n\n## Dataset schema\n\n\nIn addition to providing dataset details, users often want to include\nset type schemas. This is particularly important when integrating\n`ydata-profiling` generation with the information already in a data\ncatalog. When using `ydata-profiling` ProfileReport, users can set the\ntype\\_schema property to control"
    },
    {
        "instruction": "Create a Python program that uses the \"ydata-profiling\" API to perform data profiling on a CSV file named \"data.csv.\" The profiling should include statistics like mean, median, and histograms for numeric columns. Save the profiling report as an HTML file named \"data_profile.html.\"",
        "api": "ydata-profiling",
        "output": "#! pip install ydata-profiling\nimport pandas as pd\nfrom ydata_profiling import ProfileReport\n\n# Load the CSV data\ndata = pd.read_csv(\"data.csv\")\n\n# Perform data profiling\nprofiling_report = ProfileReport(data)\n\n# Save the report as an HTML file\nprofiling_report.to_file(\"data_profile.html\")",
        "documentation": "\n\n==================\n Document 0 \n----------------\n# Sample the dataset\n\n\nAn alternative way to handle really large datasets is to use a portion\nof it to generate the profiling report. Several users report this is a\ngood way to scale back the computation time while maintaining\nrepresentativity.\n\n> \n> pandas-profiling is a nifty tool to compute descriptive statistics on a dataframe and issue warnings on columns with many missing values, high skew, high cardinality categorical values, high correlation, etc: https://t.co/57IezPW1nI demo notebook: https://t.co/JpqTO9FK1p\n> \n> — Olivier Grisel (@ogrisel) January 11, 2018\n\n\n| Sampling a large dataset |\n| --- |\n| \n```\n1\n2\n3\n4\n5\n```\n | \n```\n# Sample 10.000 rows\nsample = large\\_dataset.sample(10000)\n\nprofile = ProfileReport(sample, minimal=True)\nprofile.to\\_file(\"output.html\")\n\n\nThe reader of the report might want to know that the profile is\ngenerated using a sample from the data. This can be done by adding a\ndescription to the report (see `metadata`\nfor details).\n\n\n| Sample 5% of your dataset |\n| --- |\n| \n```\n1\n2\n3\n4\n5\n```\n | \n```\ndescription = \"Disclaimer: this profiling report was generated using a sample of 5% of the original dataset.\"\nsample = large\\_dataset.sample(frac=0.05)\n\nprofile = sample.profile\\_report(dataset={\"description\": description}, minimal=True)\nprofile.to\\_file(\"output.html\")\n\n\n\n## Disable expensive computations\n\n\nTo decrease the computational burden in particularly large datasets but\nstill maintain some information of interest that may stem from them,\nsome computations can be filtered only for certain columns.\nParticularly, a list of targets can be provided to **Interactions**, so\nthat only the interactions with these variables in specific are\ncomputed.\n\n\n| Disable expensive computations |\n| --- |\n| \n```\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n```\n | \n```\nfrom ydata\\_profiling import ProfileReport\nimport pandas as pd\n\n\n# Reading the data\ndata = pd.read\\_csv(\n    \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n)\n\n\n# Creating the profile without specifying the data source, to allow editing the configuration\nprofile = ProfileReport()\nprofile.config.interactions.targets = [\"Name\", \"Sex\", \"Age\"]\n\n\n# Assigning a DataFrame and exporting to a file, triggering computation\nprofile.df = data\nprofile.to\\_file(\"report.html\")\n\n\nThe setting controlling this, `ìnteractions.targets`, can be changed via\nmultiple interfaces (configuration files or environment variables). For\ndetails, see `../advanced_usage/changing_settings`{.interpreted-text\nrole=\"doc\"}.\n\n\n\n# Concurrency\n\n\n`ydata-profiling` is a project under active development. One of the\nhighly desired features is the addition of a scalable backend such as\nModin or\nDask.\n\n\nKeep an eye on the\nGitHub page to\nfollow the updates on the implementation of a concurrent and highly\nscalable backend. Specifically, development of a Spark backend is\ncurrently\nunderway.\n\n\n# Customizing the report\n\n\nIn some situations, a user might want to customize the appearance\nof the report to match personal preferences or a corporate brand.\n`ydata-profiling` offers two major customization dimensions: \nthe **styling of the HTML report** and the **styling of the visualizations\nand plots** contained within. \n\n\n## Customizing the report's theme\n\n\nSeveral aspects of the report can be customized. The table below shows the available settings:\n\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| `html.minify_html` | bool | `True`\n\n==================\n Document 1 \n----------------\n Dataset Comparison\n\nDataframes compare support\n\n\nProfiling compare is supported from\nydata-profiling version 3.5.0 onwards.\nProfiling compare is not *(yet!)* available for Spark Dataframes\n\n`ydata-profiling` can be used to compare multiple version of the same\ndataset. This is useful when comparing data from multiple time periods,\nsuch as two years. Another common scenario is to view the dataset\nprofile for training, validation and test sets in machine learning.\n\n\nThe following syntax can be used to compare two datasets:\n\n\n| Comparing 2 datasets |\n| --- |\n| \n```\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n```\n | \n```\nfrom ydata\\_profiling import ProfileReport\n\ntrain\\_df = pd.read\\_csv(\"train.csv\")\ntrain\\_report = ProfileReport(train\\_df, title=\"Train\")\n\ntest\\_df = pd.read\\_csv(\"test.csv\")\ntest\\_report = ProfileReport(test\\_df, title=\"Test\")\n\ncomparison\\_report = train\\_report.compare(test\\_report)\ncomparison\\_report.to\\_file(\"comparison.html\")\n\n\nThe comparison report uses the `title` attribute out of `Settings` as a\nlabel throughout. The colors are configured in\n`settings.html.style.primary_colors`. The numeric precision parameter\n`settings.report.precision` can be played with to obtain some additional\nspace in reports.\n\n\nIn order to compare more than two reports, the following syntax can be\nused:\n\n\n| Comparing more than 2 datasets |\n| --- |\n| \n```\n1\n2\n3\n4\n5\n6\n7\n8\n9\n```\n | \n```\nfrom ydata\\_profiling import ProfileReport, compare\n\ncomparison\\_report = compare([train\\_report, validation\\_report, test\\_report])\n\n# Obtain merged statistics\nstatistics = comparison\\_report.get\\_description()\n\n\n# Save report to file\ncomparison\\_report.to\\_file(\"comparison.html\")\n\n\nThis functionality only ensures the support report comparison\nof two datasets. It is possible to obtain the statistics - the report\nmay have formatting issues. One of the settings that can be changed is\n`settings.report.precision`. As a rule of thumb, the value 10 can be\nused for a single report and 8 for comparing two reports.\n\n\n# Profiling large datasets\n\n\nBy default, `ydata-profiling` comprehensively summarizes the input\ndataset in a way that gives the most insights for data analysis. For\nsmall datasets, these computations can be performed in *quasi*\nreal-time. For larger datasets, deciding upfront which calculations to\nmake might be\n\n==================\n Document 2 \n----------------\n Time-Series data\n\n\n`ydata-profiling` can be used for a quick Exploratory Data Analysis on\ntime-series data. This is useful for a quick understanding on the\nbehaviour of time dependent variables regarding behaviours such as time\nplots, seasonality, trends, stationary and data gaps.\n\n\nCombined with the profiling reports compare, you're able to compare the\nevolution and data behaviour through time, in terms of time-series\nspecific statistics such as PACF and ACF plots. It also provides the\nidentification of gaps in the time series, caused either by missing\nvalues or by entries missing in the time index.\n\nTime series EDA tutorial\n\n\nDo you want to learn how to interpret the time-series profiling\ncheck out blog content here. \n\n\nYou can find the a [otebook with the\nfull code in our examples folder.\n\n```\n\n\n```\n\n\n\nTime-series profiling report\n\n## Profiling time-series dataset\n\n\nThe following syntax can be used to generate a profile under the\nassumption that the dataset includes time dependent features:\n\n\n| Setting the configurations for time-series profiling |\n| --- |\n| \n```\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n```\n | \n```\nimport pandas as pd\n\nfrom ydata\\_profiling.utils.cache import cache\\_file\nfrom ydata\\_profiling import ProfileReport\n\nfile\\_name = cache\\_file(\n    \"pollution\\_us\\_2000\\_2016.csv\",\n    \"https://query.data.world/s/mz5ot3l4zrgvldncfgxu34nda45kvb\",\n)\n\ndf = pd.read\\_csv(file\\_name, index\\_col=[0])\n\n\n# Filtering time-series to profile a single site\nsite = df[df[\"Site Num\"] == 3003]\n\n\n#Enable tsmode to True to automatically identify time-series variables\n\n#Provide the column name that provides the chronological order of your time-series\nprofile = ProfileReport(df, tsmode=True, sortby=\"Date Local\", title=\"Time-Series EDA\")\n\nprofile.to\\_file(\"report\\_timeseries.html\")\n\n\nTo enable a time-series report to be generated `ts_mode` needs to be set\nto `True`. If `True` the variables that have temporal dependence\nwill be automatically identified based on the presence of\nautocorrection. The time-series report uses the `sortby` attribute to\norder the dataset. If not provided it is assumed that the dataset is\nalready ordered.\nYou can set up the correlation level to detect to apply the time-series \nvalidation by setting the x configuration. \n\n\n\n### Warnings and validations\n\n\nSpecific to time-series analysis, 2 new warnings were added to the `ydata-profiling`\nwarnings family: **NON\\_STATIONARY** and **SEASONAL**.\n\n\n\n#### Stationarity\n\n\nIn the realm of time-series analysis, a stationary time-series is a dataset \nwhere statistical properties, such as mean, variance, and autocorrelation, \nremain constant over time. This property is essential for many time-series \nforecasting and modeling techniques because they often assume that the underlying \ndata is stationary. Stationarity simplifies the modeling process by making\nit easier to detect patterns and trends.\n\n\n`ydata-profiling` stationary warning is based on an **Augmented Dickey-Fuller(ADF)** test.\nNevertheless, you should always combine the output of this warning with a visual\ninspection to your time-series behaviour and search for variance of the \nrolling statistics analysis. \n\n\n\n#### Seasonality\n\n\nA seasonal time-series is a specific type of time-series data that exhibits\nrecurring patterns or fluctuations at regular intervals. These patterns\nare known as seasonality and are often observed in data associated with yearly,\nmonthly, weekly, or daily cycles. Seasonal time-series data can be challenging\nto model accurately without addressing the underlying seasonality.\n\n\n`ydata-profiling` seasonality warning is based on an **Augmented Dickey-Fuller(ADF)** test.\nNevertheless, you should always combine the output of this warning with a seasonal decomposition\nPACF and ACF plots (also computed in your time-series profiling).\n\n\n### Time-series missing gaps\n\n\n\nTime-series missing data visualization\n\nAs a data scientist, one of the critical aspects of working with time-series data\nis understanding and analyzing time-series gaps. Time-series gaps refer to the\nintervals within your time-series data where observations are missing or incomplete.\nWhile\n\n==================\n Document 3 \n----------------\n Profiling large datasets\n\n\nBy default, `ydata-profiling` comprehensively summarizes the input\ndataset in a way that gives the most insights for data analysis. For\nsmall datasets, these computations can be performed in *quasi*\nreal-time. For larger datasets, deciding upfront which calculations to\nmake might be required. Whether a computation scales to a large datasets\nnot only depends on the exact size of the dataset, but also on its\ncomplexity and on whether fast computations are available. If the\ncomputation time of the profiling becomes a bottleneck,\n`ydata-profiling` offers several alternatives to overcome it.\n\nScale in a fully managed system\n\n\nLooking for an fully managed system that is able to scale the profiling\nfor large datasets? Sign up Fabric\ncommunity for distributed data profiling.\n\n## Pyspark\n\n\nSpark\n\nMinimal mode\n\n\nThis mode was introduced in version v4.0.0\n\n`ydata-profiling` now supports Spark Dataframes profiling. You can find\nan example of the integration\nhere.\n\n\n**Features supported:** - Univariate variables' analysis - Head and Tail\ndataset sample - Correlation matrices: Pearson and Spearman\n\n\n*Coming soon* - Missing values analysis - Interactions - Improved\nhistogram computation\n\n\nKeep an eye on the\nGitHub page to\nfollow the updates on the implementation of Pyspark Dataframes\nsupport.\n\n\n\n## Minimal mode\n\n**Minimal mode**\n\n\nThis mode was introduced in version v2.4.0\n\n`ydata-profiling` includes a minimal configuration file where the most\nexpensive computations are turned off by default. This is the\nrecommended starting point for larger datasets.\n\n```\nprofile = ProfileReport(large\\_dataset, minimal=True)\nprofile.to\\_file(\"output.html\")\n\nThis configuration file can be found here:\nconfig\\_minimal.yaml.\nMore details on settings and configuration are available in\n`../advanced_usage/available_settings`.\n\n\n## Sample the dataset\n\n\nAn alternative way to handle really large datasets is to use a portion\nof it to generate the profiling report. Several users report this is a\ngood way to scale back the computation time while maintaining\nrepresentativity.\n\n> \n> pandas-profiling is"
    },
    {
        "instruction": "Create a Python program that utilizes the \"langchain\" API to generate a prompt template that take to inputs adjective and content to till the model to generate a joke about the content with the given adjective. Then invoke the template with the inputs \"funny\" and \"AI\" to generate a joke about AI.",
        "api": "langchain",
        "output": "#!pip install langchain\n# Import necessary libraries\nfrom langchain.prompts import PromptTemplate\n\n# define the template\nprompt_template = PromptTemplate(\n    input_variables=[\"adjective\", \"content\"],\n    template = \"Tell me a {adjective} joke about {content}.\"\n)\n\n# can also use invoke syntax (applied to following examples as well)\nprompt_value = prompt_template.invoke({\"adjective\": \"funny\", \"content\": \"AI\"})\nprint(prompt_value)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n### Using `LLMChain`​\n\nThe `LLMChain` is most basic building block chain. It takes in a prompt template, formats it with the user input and returns the response from an LLM.\n\nTo use the `LLMChain`, first create a prompt template.\n\n\n```\nfrom langchain.llms import OpenAI  \nfrom langchain.prompts import PromptTemplate  \n  \nllm = OpenAI(temperature=0.9)  \nprompt = PromptTemplate(  \n input\\_variables=[\"product\"],  \n template=\"What is a good name for a company that makes {product}?\",  \n)  \n\n```\nWe can now create a very simple chain that will take user input, format the prompt with it, and then send it to the LLM.\n\n\n```\nfrom langchain.chains import LLMChain  \nchain = LLMChain(llm=llm, prompt=prompt)  \n  # Run the chain only specifying the input variable.  \nprint(chain.run(\"colorful socks\"))  \n\n```\n\n```\n Colorful Toes Co.  \n\n```\nIf there are multiple variables, you can input them all at once using a dictionary.\n\n\n```\nprompt = PromptTemplate(  \n input\\_variables=[\"company\", \"product\"],\n\n==================\n Document 1 \n----------------\n Run the chain only specifying the input variable.  \nprint(chain.run(\"colorful socks\"))  \n\n```\n\n```\n Colorful Toes Co.  \n\n```\nIf there are multiple variables, you can input them all at once using a dictionary.\n\n\n```\nprompt = PromptTemplate(  \n input\\_variables=[\"company\", \"product\"],  \n template=\"What is a good name for {company} that makes {product}?\",  \n)  \nchain = LLMChain(llm=llm, prompt=prompt)  \nprint(chain.run({  \n 'company': \"ABC Startup\",  \n 'product': \"colorful socks\"  \n }))  \n\n```\n Socktopia Colourful Creations.  \n\n```\nYou can use a chat model in an `LLMChain` as well:\n\n\n```\nfrom langchain.chat\\_models import ChatOpenAI  \nfrom langchain.prompts.chat import (  \n ChatPromptTemplate,  \n HumanMessagePromptTemplate,  \n)  \nhuman\\_message\\_prompt = HumanMessagePromptTemplate(  \n prompt=PromptTemplate(  \n template=\"What is a good name for a company that makes {product}?\",  \n input\\_variables=[\"product\"],  \n )  \n )  \nchat\\_prompt\\_template = ChatPromptTemplate.from\\_messages([human\\_message\\_prompt])  \nchat = ChatOpenAI(temperature=0.9)  \nchain = LLMChain(llm=chat, prompt=chat\\_prompt\\_template)  \nprint(chain.run(\"colorful socks\"))  \n\n```\n Rainbow Socks Co.  \n\n\n* \n* Modules\n* Memory\nOn this page# Memory\n\nMost LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation.\nAt bare minimum, a conversational system should be able to access some window of past messages directly.\nA more complex system will need to have a world model that it is constantly updating, which allows it to do things like maintain information about entities and their relationships.\n\nWe call this ability to store information about past interactions \"memory\".\nLangChain provides a lot of utilities for adding memory to a system.\nThese utilities can be used by themselves or incorporated seamlessly into a chain.\n\nA memory system needs to support two basic actions: reading and writing.\nRecall that every chain defines some core execution logic that expects certain inputs.\nSome of these inputs come directly from the user, but some of these inputs can come from memory.\nA chain will interact with its memory system twice in a given run.\n\n1. AFTER receiving the initial user inputs but BEFORE executing the core logic, a chain will READ from its memory system and augment the user inputs.\n2. AFTER executing the core logic but BEFORE returning the answer, a chain will WRITE the inputs and outputs of the current run to memory, so that they can be referred to in future runs.\n\n\n\n## Building memory into a system​\n\nThe two core design decisions in any memory system are:\n\n* How state is stored\n* How state is queried\n\n\n### Storing: List of chat messages​\n\nUnderlying any memory is a history of all chat interactions.\nEven if these are not all used directly, they need to be stored in some form.\nOne of the key parts of the LangChain memory module is a series of integrations for storing these chat messages,\nfrom in-memory lists to persistent databases.\n\n* Chat message storage: How to work with Chat Messages, and the various integrations offered.\n\n### Querying: Data structures and algorithms on top of chat messages​\n\nKeeping a list of chat messages is fairly straight-forward.\nWhat is less straight-forward are the data structures and algorithms built on top of chat messages that serve a view of those\n\n==================\n Document 2 \n----------------\n## Set up the agent​\n\nWe first need to create our agent.\nThis is the chain responsible for determining what action to take next.\n\nIn this example, we will use OpenAI Function Calling to create this agent.\nThis is generally the most reliable way create agents.\nIn this example we will show what it is like to construct this agent from scratch, using LangChain Expression Language.\n\nFor this guide, we will construct a custom agent that has access to a custom tool.\nWe are choosing this example because we think for most use cases you will NEED to customize either the agent or the tools.\nThe tool we will give the agent is a tool to calculate the length of a word.\nThis is useful because this is actually something LLMs can mess up due to tokenization.\nWe will first create it WITHOUT memory, but we will then show how to add memory in.\nMemory is needed to enable conversation.\n\nFirst, let's load the language model we're going to use to control the agent.\n\n\n```\nfrom langchain.chat\\_models import ChatOpenAI  \nllm = ChatOpenAI(temperature=0)  \n\n```\nNext, let's define some tools to use.\nLet's write a really simple Python function to calculate the length of a word that is passed in.\n\n\n```\nfrom langchain.agents import tool  \n  \n@tool  \ndef get\\_word\\_length(word: str) -> int:  \n \"\"\"Returns the length of a word.\"\"\"  \n return len(word)  \n  \ntools = [get\\_word\\_length]  \n\n```\nNow let us create the prompt.\nBecause OpenAI Function Calling is finetuned for tool usage, we hardly need any instructions on how to reason, or how to output format.\nWe will just have two input variables: `input` (for the user question) and `agent_scratchpad` (for any previous steps taken)\n\n\n```\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder  \nprompt = ChatPromptTemplate.from\\_messages([  \n (\"system\", \"You are very powerful assistant, but bad at calculating lengths of words.\"),  \n (\"user\", \"{input}\"),  \n MessagesPlaceholder(variable\\_name=\"agent\\_scratchpad\"),  \n])  \n\n```\nHow does the agent know what tools it can use?\nThose are passed in as a separate argument, so we can bind those as key word arguments to the LLM.\n\n\n```\nfrom langchain.tools.render import format\\_tool\\_to\\_openai\\_function  \nllm\\_with\\_tools = llm.bind(  \n functions=[format\\_tool\\_to\\_openai\\_function(t) for t in tools]  \n)  \n\n```\nPutting those pieces together, we can now create the agent.\nWe will import two last utility functions: a component for formatting intermediate steps to messages, and a component for converting the output message into an agent action/agent finish.\n\n\n```\nfrom langchain.agents.format\\_scratchpad import format\\_to\\_openai\\_functions  \nfrom langchain.agents.output\\_parsers import OpenAIFunctionsAgentOutputParser  \nagent = {  \n \"input\": lambda x: x[\"input\"],  \n \"agent\\_scratchpad\": lambda x: format\\_to\\_openai\\_functions(x['intermediate\\_steps'])  \n} | prompt | llm\\_with\\_tools | OpenAIFunctionsAgentOutputParser()  \n\n```\nNow that we have our agent, let's play around with it!\nLet's pass in a simple question and empty intermediate steps and see what it returns:\n\n\n```\nagent.invoke({  \n \"input\": \"how many letters in the word educa?\",  \n \"intermediate\\_steps\": []  \n})  \n\n```\nWe can see that it responds with an `AgentAction` to take (it's actually an `AgentActionMessageLog` - a subclass of `AgentAction` which also tracks the full message log).\n\nSo this is just the first step - now we need to write a runtime for this.\nThe simplest one is just one that continuously loops, calling the agent, then taking the action, and repeating until an `AgentFinish` is returned.\nLet's code that up below:\n\n\n```\nfrom langchain.schema.agent import AgentFinish  \nintermediate\\_steps = []  \nwhile True:  \n output = agent.invoke({  \n \"input\": \"how many letters in the word educa?\",  \n \"intermediate\\_steps\": intermediate\\_steps  \n })  \n if isinstance(output, AgentFinish):  \n final\\_result = output.return\\_values[\"output\"]  \n break  \n else:  \n print(output.tool, output.tool\\_input)  \n tool = {  \n \"get\\_word\\_length\": get\\_word\\_length  \n }[output.tool]  \n observation = tool.run(output.tool\\_input)  \n intermediate\\_steps.append((output, observation))  \nprint(final\\_result)  \n\n```\nWe can see this prints out the following:\n\n\n```\nget\\_word\\_length {'word': 'educa'}  \nThere are 5 letters in the word \"educa\".  \n\n```\nWoo! It's working.\n\nTo simplify this a bit, we can import and use the `AgentExecutor` class.\nThis bundles up all of the above and adds in error handling, early stopping, tracing, and other quality-of-life improvements that reduce safeguards you need to write.\n\n\n```\nfrom langchain.agents import AgentExecutor  \nagent\\_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)  \n\n```\nNow let's test it out!\n\n\n```\nagent\\_executor.invoke({\"input\": \"how many letters in the word educa?\"})  \n\n```\n   \n   \n > Entering new AgentExecutor chain...  \n  \n Invoking: `get\\_word\\_length` with `{'word': 'educa'}`  \n  \n 5  \n  \n There are 5 letters in the word \"educa\".  \n  \n > Finished chain.  \n  \n 'There are 5 letters in the word \"educa\".'  \n\n```\nThis is great - we have an agent!\nHowever, this agent is stateless - it doesn't remember anything about previous interactions.\nThis means you can't ask follow up questions easily.\nLet's fix that by adding in memory.\n\nIn order to do this, we need to do two things:\n\n1. Add a place for memory variables to go in the prompt\n2. Keep track of the chat history\n\nFirst, let's add a place for memory in the prompt.\nWe do this by adding a placeholder for messages with the key `\"chat_history\"`.\nNotice that we put this ABOVE the new user input (to follow the conversation flow).\n\n\n```\nfrom langchain.prompts import MessagesPlaceholder  \n  \nMEMORY\\_KEY = \"chat\\_history\"  \nprompt = ChatPromptTemplate.from\\_messages([  \n (\"system\", \"You are very powerful assistant, but bad at calculating lengths of words.\"),  \n MessagesPlaceholder(variable\\_name=MEMORY\\_KEY),  \n (\"user\", \"{input}\"),  \n MessagesPlaceholder(variable\\_name=\"agent\\_scratchpad\"),  \n])  \n\n```\nWe can then set up a list to track the chat history\n\n\n```\nfrom langchain.schema.messages import HumanMessage, AIMessage  \nchat\\_history = []  \n\n```\nWe can then put it all together!\n\n\n```\nagent = {  \n \"input\": lambda x: x[\"input\"],  \n \"agent\\_scratchpad\": lambda x: format\\_to\\_openai\\_functions(x['intermediate\\_steps']),  \n \"chat\\_history\": lambda x: x[\"chat\\_history\"]  \n} | prompt | llm\\_with\\_tools | OpenAIFunctionsAgentOutputParser()  \nagent\\_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)  \n\n```\nWhen running, we now need to track the inputs and outputs as chat history\n\n\n```\ninput1 = \"how many letters in the word educa?\"  \nresult = agent\\_executor.invoke({\"input\": input1, \"chat\\_history\": chat\\_history})  \nchat\\_history.append(HumanMessage(content=input1))  \nchat\\_history.append(AIMessage(content=result['output']))  \nagent\\_executor.invoke({\"input\": \"is that a real word?\", \"chat\\_history\": chat\\_history})  \n\n```\n## Next Steps​\n\nAwesome! You've now run your first end-to-end agent.\nTo dive deeper, you can:\n\n* Check out all the different agent types supported\n* Learn all the controls for AgentExecutor\n* See a full list of all the off-the-shelf toolkits we provide\n* Explore all the individual tools supported\n\n\n* \n* Modules\n* Callbacks\n\n# Callbacks\n\ninfoHead to Integrations for documentation on built-in callbacks integrations with 3rd-party tools.\n\nLangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.\n\nYou can subscribe to these events by using the `callbacks` argument available throughout the API. This argument is list of handler objects, which are expected to implement one or more of the methods described below in more detail.\n\n## Callback handlers​\n\n`CallbackHandlers` are objects that implement the `CallbackHandler` interface, which has a method for each event that can be subscribed to. The `CallbackManager` will call the appropriate method on each handler when the event is triggered.\n\n\n```\nclass BaseCallbackHandler:"
    },
    {
        "instruction": "Create a Python program that utilizes the \"langchain\" API to load the text from the Wikipedia page on Germany and print the first 1000 characters of the page content.",
        "api": "langchain",
        "output": "#!pip install langchain\n# Import necessary libraries\nfrom langchain.document_loaders import WebBaseLoader\n\n# Declare the loader\nloader = WebBaseLoader(\n    web_paths=(\"https://en.wikipedia.org/wiki/Germany\",),\n)\n# Use it to load the text data\ndocs = loader.load()\n\n# Let's look at a piece of text from the first document\ndocs[0].page_content[:1000]",
        "documentation": "\n\n==================\n Document 0 \n----------------\n Retrieval\n\nMany LLM applications require user-specific data that is not part of the model's training set.\nThe primary way of accomplishing this is through Retrieval Augmented Generation (RAG).\nIn this process, external data is *retrieved* and then passed to the LLM when doing the *generation* step.\n\nLangChain provides all the building blocks for RAG applications - from simple to complex.\nThis section of the documentation covers everything related to the *retrieval* step - e.g. the fetching of the data.\nAlthough this sounds simple, it can be subtly complex.\nThis encompasses several key modules.\n\n\n\n**Document loaders**\n\nLoad documents from many different sources.\nLangChain provides over 100 different document loaders as well as integrations with other major providers in the space,\nlike AirByte and Unstructured.\nWe provide integrations to load all types of documents (HTML, PDF, code) from all types of locations (private s3 buckets, public websites).\n\n**Document transformers**\n\nA key part of retrieval is fetching only the relevant parts of documents.\nThis involves several transformation steps in order to best prepare the documents for retrieval.\nOne of the primary ones here is splitting (or chunking) a large document into smaller chunks.\nLangChain provides several different algorithms for doing this, as well as logic optimized for specific document types (code, markdown, etc).\n\n**Text embedding models**\n\nAnother key part of retrieval has become creating embeddings for documents.\nEmbeddings capture the semantic meaning of the text, allowing you to quickly and\nefficiently find other pieces of text that are similar.\nLangChain provides integrations with over 25 different embedding providers and methods,\nfrom open-source to proprietary API,\nallowing you to choose the one best suited for your needs.\nLangChain provides a standard interface, allowing you to easily swap between models.\n\n**Vector stores**\n\nWith the rise of embeddings, there has emerged a need for databases to support efficient storage and searching of these embeddings.\nLangChain provides integrations with over 50 different vectorstores, from open-source local ones to cloud-hosted proprietary ones,\nallowing you to choose the one best suited for your needs.\nLangChain exposes a standard interface, allowing you to easily swap between vector stores.\n\n**Retrievers**\n\nOnce the data is in the database, you still need to retrieve it.\nLangChain supports many different retrieval algorithms and is one of the places where we add the most value.\nWe support basic methods that are easy to get started - namely simple semantic search.\nHowever, we have also added a collection of algorithms on top of this to increase performance.\nThese include:\n\n* Parent Document Retriever: This allows you to create multiple embeddings per parent document, allowing you to look up smaller chunks but return larger context.\n* Self Query Retriever: User questions often contain a reference to something that isn't just semantic but rather expresses some logic that can best be represented as a metadata filter. Self-query allows you to parse out the *semantic* part of a query from other *metadata filters* present in the query.\n* Ensemble Retriever: Sometimes you may want to retrieve documents from multiple different sources, or using multiple different algorithms. The ensemble retriever allows you to easily do this.\n* And more!\n\n\n* \n* Modules\n* Chains\nOn this page# Chains\n\nUsing an LLM in isolation is fine for simple applications,\nbut more complex applications require chaining LLMs - either with each other or with other components.\n\nLangChain provides the **Chain** interface for such \"chained\" applications. We define a Chain very generically as a sequence of calls to components, which can include other chains. The base interface is simple:\n\n\n```\nclass Chain(BaseModel, ABC):  \n \"\"\"Base interface that all chains should implement.\"\"\"  \n  \n memory: BaseMemory  \n callbacks: Callbacks  \n  \n def \\_\\_call\\_\\_(  \n self,  \n inputs: Any,  \n return\\_only\\_outputs: bool = False,  \n callbacks: Callbacks = None,  \n ) -> Dict[str, Any]:  \n ...  \n\n```\nThis idea of composing components together in a chain is simple but powerful. It drastically simplifies and makes more modular the implementation of complex applications, which in turn makes it much easier to debug, maintain, and improve your applications.\n\nFor more specifics check out:\n\n* How-to for walkthroughs of different chain features\n* Foundational to get acquainted with core building block chains\n* Document to learn how to incorporate documents into chains\n\n## Why do we need chains?​\n\nChains allow us to combine multiple components together to create a single, coherent application. For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM. We can build more complex chains by combining multiple chains together, or by combining chains with other components.\n\n\n## Get started​\n\n#### Using `LLMChain`​\n\nThe `LLMChain` is most basic building block chain. It takes in a prompt template, formats it with the user input and returns the response from an LLM.\n\nTo use the `LLMChain`, first create a prompt template.\n\n\n```\nfrom langchain.llms import OpenAI\n\n==================\n Document 1 \n----------------\n\n\n* \n* Modules\nOn this page# Modules\n\nLangChain provides standard, extendable interfaces and external integrations for the following modules, listed from least to most complex:\n\n#### Model I/O​\n\nInterface with language models\n\n\n#### Retrieval​\n\nInterface with application-specific data\n\n\n#### Chains​\n\nConstruct sequences of calls\n\n\n#### Agents​\n\nLet chains choose which tools to use given high-level directives\n\n\n#### Memory​\n\nPersist application state between runs of a chain\n\n\n#### Callbacks​\n\nLog and stream intermediate steps of any chain\n\n* \n* Modules\n* Model I/​O\n\n# Model I/O\n\nThe core element of any language model application is...the model. LangChain gives you the building blocks to interface with any language model.\n\n* Prompts: Templatize, dynamically select, and manage model inputs\n* Language models: Make calls to language models through common interfaces\n* Output parsers: Extract information from model outputs\n\n\n\n* \n* Modules\n* Retrieval\n# Retrieval\n\nMany LLM applications require user-specific data that is not part of the model's training set.\nThe primary way of accomplishing this is through Retrieval Augmented Generation (RAG).\nIn this process, external data is *retrieved* and then passed to the LLM when\n\n==================\n Document 2 \n----------------\n## Querying: Data structures and algorithms on top of chat messages​\n\nKeeping a list of chat messages is fairly straight-forward.\nWhat is less straight-forward are the data structures and algorithms built on top of chat messages that serve a view of those messages that is most useful.\n\nA very simply memory system might just return the most recent messages each run. A slightly more complex memory system might return a succinct summary of the past K messages.\nAn even more sophisticated system might extract entities from stored messages and only return information about entities referenced in the current run.\n\nEach application can have different requirements for how memory is queried. The memory module should make it easy to both get started with simple memory systems and write your own custom systems if needed.\n\n* Memory types: The various data structures and algorithms that make up the memory types LangChain supports\n\nLet's take a look at what Memory actually looks like in LangChain.\nHere we'll cover the basics of interacting with an arbitrary memory class.\n\nLet's take a look at how to use `ConversationBufferMemory` in chains.\n`ConversationBufferMemory` is an extremely simple form of memory that just keeps a list of chat messages in a buffer\nand passes those into the prompt template.\n\n\n```\nfrom langchain.memory import ConversationBufferMemory  \n  \nmemory = ConversationBufferMemory()  \nmemory.chat\\_memory.add\\_user\\_message(\"hi!\")  \nmemory.chat\\_memory.add\\_ai\\_message(\"what's up?\")  \n\n```\nWhen using memory in a chain, there are a few key concepts to understand.\nNote that here we cover general concepts that are useful for most types of memory.\nEach individual memory type may very well have its own parameters and concepts that are necessary to understand.\n### What variables get returned from memory​\n\nBefore going into the chain, various variables are read from memory.\nThese have specific names which need to align with the variables the chain expects.\nYou can see what these variables are by calling `memory.load_memory_variables({})`.\nNote that"
    },
    {
        "instruction": "Create a Python program that utilizes the \"langchain\" API to load the text from the Wikipedia page on the Sinai Peninsula and split the text into chunks of 1000 characters with an overlap of 200 characters using Recursive Character Text Splitter.",
        "api": "langchain",
        "output": "#!pip install langchain\n# Import necessary libraries\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.document_loaders import WebBaseLoader\n\n# Declare the loader\nloader = WebBaseLoader(\n    web_paths=(\"https://en.wikipedia.org/wiki/Sinai_Peninsula\",),\n)\n# Use it to load the text data\ndocs = loader.load()\n\n# Declare the loader\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,\n    add_start_index=True\n)\n\n# Use it to split the data\nall_splits = text_splitter.split_documents(docs)",
        "documentation": "\n\n==================\n Document 0 \n----------------\n Retrieval\n\nMany LLM applications require user-specific data that is not part of the model's training set.\nThe primary way of accomplishing this is through Retrieval Augmented Generation (RAG).\nIn this process, external data is *retrieved* and then passed to the LLM when doing the *generation* step.\n\nLangChain provides all the building blocks for RAG applications - from simple to complex.\nThis section of the documentation covers everything related to the *retrieval* step - e.g. the fetching of the data.\nAlthough this sounds simple, it can be subtly complex.\nThis encompasses several key modules.\n\n\n\n**Document loaders**\n\nLoad documents from many different sources.\nLangChain provides over 100 different document loaders as well as integrations with other major providers in the space,\nlike AirByte and Unstructured.\nWe provide integrations to load all types of documents (HTML, PDF, code) from all types of locations (private s3 buckets, public websites).\n\n**Document transformers**\n\nA key part of retrieval is fetching only the relevant parts of documents.\nThis involves several transformation steps in order to best prepare the documents for retrieval.\nOne of the primary ones here is splitting (or chunking) a large document into smaller chunks.\nLangChain provides several different algorithms for doing this, as well as logic optimized for specific document types (code, markdown, etc).\n\n**Text embedding models**\n\nAnother key part of retrieval has become creating embeddings for documents.\nEmbeddings capture the semantic meaning of the text, allowing you to quickly and\nefficiently find other pieces of text that are similar.\nLangChain provides integrations with over 25 different embedding providers and methods,\nfrom open-source to proprietary API,\nallowing you to choose the one best suited for your needs.\nLangChain provides a standard interface, allowing you to easily swap between models.\n\n**Vector stores**\n\nWith the rise of embeddings, there has emerged a need for databases to support efficient storage and searching of these embeddings.\nLangChain provides integrations with over 50 different vectorstores, from open-source local ones to cloud-hosted proprietary ones,\nallowing you to choose the one best suited for your needs.\nLangChain exposes a standard interface, allowing you to easily swap between vector stores.\n\n**Retrievers**\n\nOnce the data is in the database, you still need to retrieve it.\nLangChain supports many different retrieval algorithms and is one of the places where we add the most value.\nWe support basic methods that are easy to get started - namely simple semantic search.\nHowever, we have also added a collection of algorithms on top of this to increase performance.\nThese include:\n\n* Parent Document Retriever: This allows you to create multiple embeddings per parent document, allowing you to look up smaller chunks but return larger context.\n* Self Query Retriever: User questions often contain a reference to something that isn't just semantic but rather expresses some logic that can best be represented as a metadata filter. Self-query allows you to parse out the *semantic* part of a query from other *metadata filters* present in the query.\n* Ensemble Retriever: Sometimes you may want to retrieve documents from multiple different sources, or using multiple different algorithms. The ensemble retriever allows you to easily do this.\n* And more!\n\n\n* \n* Modules\n* Chains\nOn this page# Chains\n\nUsing an LLM in isolation is fine for simple applications,\nbut more complex applications require chaining LLMs - either with each other or with other components.\n\nLangChain provides the **Chain** interface for such \"chained\" applications. We define a Chain very generically as a sequence of calls to components, which can include other chains. The base interface is simple:\n\n\n```\nclass Chain(BaseModel, ABC):  \n \"\"\"Base interface that all chains should implement.\"\"\"  \n  \n memory: BaseMemory  \n callbacks: Callbacks  \n  \n def \\_\\_call\\_\\_(  \n self,  \n inputs: Any,  \n return\\_only\\_outputs: bool = False,  \n callbacks: Callbacks = None,  \n ) -> Dict[str, Any]:  \n ...  \n\n```\nThis idea of composing components together in a chain is simple but powerful. It drastically simplifies and makes more modular the implementation of complex applications, which in turn makes it much easier to debug, maintain, and improve your applications.\n\nFor more specifics check out:\n\n* How-to for walkthroughs of different chain features\n* Foundational to get acquainted with core building block chains\n* Document to learn how to incorporate documents into chains\n\n## Why do we need chains?​\n\nChains allow us to combine multiple components together to create a single, coherent application. For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM. We can build more complex chains by combining multiple chains together, or by combining chains with other components.\n\n\n## Get started​\n\n#### Using `LLMChain`​\n\nThe `LLMChain` is most basic building block chain. It takes in a prompt template, formats it with the user input and returns the response from an LLM.\n\nTo use the `LLMChain`, first create a prompt template.\n\n\n```\nfrom langchain.llms import OpenAI\n\n==================\n Document 1 \n----------------\n\n\n* \n* Modules\nOn this page# Modules\n\nLangChain provides standard, extendable interfaces and external integrations for the following modules, listed from least to most complex:\n\n#### Model I/O​\n\nInterface with language models\n\n\n#### Retrieval​\n\nInterface with application-specific data\n\n\n#### Chains​\n\nConstruct sequences of calls\n\n\n#### Agents​\n\nLet chains choose which tools to use given high-level directives\n\n\n#### Memory​\n\nPersist application state between runs of a chain\n\n\n#### Callbacks​\n\nLog and stream intermediate steps of any chain\n\n* \n* Modules\n* Model I/​O\n\n# Model I/O\n\nThe core element of any language model application is...the model. LangChain gives you the building blocks to interface with any language model.\n\n* Prompts: Templatize, dynamically select, and manage model inputs\n* Language models: Make calls to language models through common interfaces\n* Output parsers: Extract information from model outputs\n\n\n\n* \n* Modules\n* Retrieval\n# Retrieval\n\nMany LLM applications require user-specific data that is not part of the model's training set.\nThe primary way of accomplishing this is through Retrieval Augmented Generation (RAG).\nIn this process, external data is *retrieved* and then passed to the LLM when"
    }
]