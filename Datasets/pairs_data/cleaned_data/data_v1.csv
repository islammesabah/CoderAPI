API,API_CODE_#,INSTRUCT,CODE_PATH,TEST_1,TEST_2,TEST_3,TEST_4,TEST_5,CODE,SPLIT
PyDrive2,39,Create a Python program that empties the trash of a user's Google Drive account using the PyDrive2 API. The program should permanently delete all files and folders that are currently in the trash. Make sure the program is capable of handling cases when there is an authentication issue.,code/PyDrive2/PyDrive2_39.py,Test if the program permanently deletes all files and folders that are currently in the trash of the user\'s Google Drive account.,Test if the program handles authentication errors correctly and displays appropriate error messages.,,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

def empty_trash():
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    drive = GoogleDrive(gauth)
    
    try:
        trash_files = drive.ListFile({'q': 'trashed=true'}).GetList()
        
        for file in trash_files:
            file.Delete()
        
        print(""Trash emptied successfully."")
    except Exception as e:
        print(f""Error emptying trash: {e}"")

if __name__ == ""__main__"":
    empty_trash()",train
PyDrive2,24,Create a Python program that shares a folder on Google Drive with a specific email address using the PyDrive2 API. The program should take the folder ID and the email address as inputs and share the folder with the specified email address. Make sure the program is capable of handling cases where the folder ID provided is invalid or when there is an authentication issue.,code/PyDrive2/PyDrive2_24.py,Test if the program handles the case when the folder with the provided ID does not exist on Google Drive.,Test if the program shares the folder with the specified ID with the provided email address on Google Drive.,Test if the program handles authentication errors correctly and displays appropriate error messages.,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

# Function to authenticate and get the GoogleDrive instance
def authenticate_drive():
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    return GoogleDrive(gauth)

def share_folder_on_drive(folder_id, email_address):
    drive = authenticate_drive()
    
    try:
        folder = drive.CreateFile({'id': folder_id})
        if not folder:
            print(f""Folder with ID '{folder_id}' does not exist on Google Drive."")
            return
        
        folder.InsertPermission({'type': 'user', 'value': email_address, 'role': 'reader'})
        
        print(f""Folder with ID '{folder_id}' shared successfully with email address '{email_address}' on Google Drive."")
    except Exception as e:
        print(f""Error sharing folder: {e}"")

if __name__ == ""__main__"":
    folder_id = input(""Enter the folder ID on Google Drive to share: "")
    email_address = input(""Enter the email address to share the folder with: "")
    
    share_folder_on_drive(folder_id, email_address)",train
PyDrive2,10,"Create a Python program that searches for files on Google Drive using the PyDrive2 API. The program should take a search query as input and display a list of files that match the query, along with their corresponding IDs. Make sure the program is capable of handling cases where no files match the search query or when there is an authentication issue.",code/PyDrive2/PyDrive2_10.py,Test if the program lists the files that match the search query along with their IDs on Google Drive.,Test if the program handles authentication errors correctly and displays appropriate error messages.,Test if the program handles the case when no files match the search query.,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

# Function to authenticate and get the GoogleDrive instance
def authenticate_drive():
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    return GoogleDrive(gauth)

def search_files_on_drive(query):
    drive = authenticate_drive()
    
    try:
        file_list = drive.ListFile({'q': f""title contains '{query}' and trashed=false""}).GetList()
        if not file_list:
            print(f""No files found matching the search query '{query}'."")
            return
        
        print(f""Files matching the search query '{query}':"")
        
        for file in file_list:
            print(f""File Name: {file['title']}, File ID: {file['id']}"")
    except Exception as e:
        print(f""Error searching files: {e}"")

if __name__ == ""__main__"":
    query = input(""Enter the search query to find files on Google Drive: "")
    
    search_files_on_drive(query)",train
PyDrive2,26,Create a Python program that lists all folders in a specified parent folder on Google Drive using the PyDrive2 API. The program should take the parent folder ID as input and display a list of folder names along with their corresponding IDs. Make sure the program is capable of handling cases where the parent folder ID provided is invalid or when there is an authentication issue.,code/PyDrive2/PyDrive2_26.py,Test if the program lists the folders in the specified parent folder along with their IDs on Google Drive.,Test if the program handles authentication errors correctly and displays appropriate error messages.,Test if the program handles the case when the parent folder with the provided ID does not exist on Google Drive.,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

# Function to authenticate and get the GoogleDrive instance
def authenticate_drive():
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    return GoogleDrive(gauth)

def list_folders_in_drive(parent_folder_id):
    drive = authenticate_drive()
    
    try:
        folder_list = drive.ListFile({'q': f""'{parent_folder_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder'""}).GetList()
        if not folder_list:
            print(f""No folders found in the parent folder with ID '{parent_folder_id}'."")
            return
        
        print(f""Folders in the parent folder with ID '{parent_folder_id}':"")
        
        for folder in folder_list:
            print(f""Folder Name: {folder['title']}, Folder ID: {folder['id']}"")
    except Exception as e:
        print(f""Error listing folders: {e}"")

if __name__ == ""__main__"":
    parent_folder_id = input(""Enter the parent folder ID on Google Drive to list folders from: "")
    
    list_folders_in_drive(parent_folder_id)",train
PyDrive2,2,Create a Python program that uploads a file to a specified folder on Google Drive using the PyDrive2 API. The program should take the file path and the folder ID as inputs and upload the file to the specified folder. Make sure the program is capable of handling cases where the file path or folder ID provided is invalid or when there is an authentication issue.,code/PyDrive2/PyDrive2_2.py,Test if the program handles the case when the folder with the provided ID does not exist on Google Drive.,Test if the program handles authentication errors correctly and displays appropriate error messages.,Test if the program handles the case when the file path provided is invalid.,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

# Function to authenticate and get the GoogleDrive instance
def authenticate_drive():
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    return GoogleDrive(gauth)

def upload_file_to_drive(file_path, folder_id):
    drive = authenticate_drive()
    
    try:
        file = drive.CreateFile({'title': file_path.split(""/"")[-1], 'parents': [{'id': folder_id}]})
        if not file:
            print(f""Folder with ID '{folder_id}' does not exist on Google Drive."")
            return
        
        file.SetContentFile(file_path)
        file.Upload()
        
        print(f""File '{file_path}' uploaded successfully to folder with ID '{folder_id}' on Google Drive."")
    except Exception as e:
        print(f""Error uploading file: {e}"")

if __name__ == ""__main__"":
    file_path = input(""Enter the file path to upload: "")
    folder_id = input(""Enter the folder ID on Google Drive to upload the file to: "")
    
    upload_file_to_drive(file_path, folder_id)",train
PyDrive2,38,"Create a Python program that checks the trash of a user's Google Drive account using the PyDrive2 API. The program should display the files and folders that are currently in the trash, including their names and IDs. Make sure the program is capable of handling cases when there is an authentication issue.",code/PyDrive2/PyDrive2_38.py,"Test if the program retrieves and displays the files and folders that are currently in the trash of the user\'s Google Drive account, including their names and IDs.",Test if the program handles authentication errors correctly and displays appropriate error messages.,,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

def check_trash():
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    drive = GoogleDrive(gauth)
    
    try:
        trash_files = drive.ListFile({'q': 'trashed=true'}).GetList()
        
        print(""Files and Folders in the Trash:"")
        
        for file in trash_files:
            print(f""Name: {file['title']}, ID: {file['id']}"")
    except Exception as e:
        print(f""Error checking trash: {e}"")

if __name__ == ""__main__"":
    check_trash()",train
PyDrive2,6,Create a Python program that renames a file on Google Drive using the PyDrive2 API. The program should take the file ID and the new file name as inputs and rename the file on Google Drive. Make sure the program is capable of handling cases where the file ID provided is invalid or when there is an authentication issue.,code/PyDrive2/PyDrive2_6.py,Test if the program renames the file with the specified ID to the new file name on Google Drive.,Test if the program handles authentication errors correctly and displays appropriate error messages.,Test if the program handles the case when the file with the provided ID does not exist on Google Drive.,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

# Function to authenticate and get the GoogleDrive instance
def authenticate_drive():
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    return GoogleDrive(gauth)

def rename_file_on_drive(file_id, new_file_name):
    drive = authenticate_drive()
    
    try:
        file = drive.CreateFile({'id': file_id})
        if not file:
            print(f""File with ID '{file_id}' does not exist on Google Drive."")
            return
        
        file['title'] = new_file_name
        file.Upload()
        
        print(f""File with ID '{file_id}' renamed to '{new_file_name}' successfully on Google Drive."")
    except Exception as e:
        print(f""Error renaming file: {e}"")

if __name__ == ""__main__"":
    file_id = input(""Enter the file ID on Google Drive to rename: "")
    new_file_name = input(""Enter the new file name: "")
    
    rename_file_on_drive(file_id, new_file_name)",train
PyDrive2,3,Create a Python program that downloads a file from Google Drive using the PyDrive2 API. The program should take the file ID and the destination folder path as inputs and download the file to the specified folder. Make sure the program is capable of handling cases where the file ID or destination folder path provided is invalid or when there is an authentication issue.,code/PyDrive2/PyDrive2_3.py,Test if the program handles the case when the destination folder path provided is invalid.,Test if the program handles authentication errors correctly and displays appropriate error messages.,Test if the program handles the case when the file with the provided ID does not exist on Google Drive.,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

# Function to authenticate and get the GoogleDrive instance
def authenticate_drive():
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    return GoogleDrive(gauth)

def download_file_from_drive(file_id, destination_folder):
    drive = authenticate_drive()
    
    try:
        file = drive.CreateFile({'id': file_id})
        if not file:
            print(f""File with ID '{file_id}' does not exist on Google Drive."")
            return
        
        file.GetContentFile(destination_folder + ""/"" + file['title'])
        
        print(f""File with ID '{file_id}' downloaded successfully to '{destination_folder}'"")
    except Exception as e:
        print(f""Error downloading file: {e}"")

if __name__ == ""__main__"":
    file_id = input(""Enter the file ID on Google Drive to download: "")
    destination_folder = input(""Enter the destination folder path to save the downloaded file: "")
    
    download_file_from_drive(file_id, destination_folder)",train
PyDrive2,25,"Create a Python program that retrieves the metadata of a folder on Google Drive using the PyDrive2 API. The program should take the folder ID as input and display the metadata information of the folder, including its title, creation date, and last modified date. Make sure the program is capable of handling cases where the folder ID provided is invalid or when there is an authentication issue.",code/PyDrive2/PyDrive2_25.py,Test if the program handles the case when the folder with the provided ID does not exist on Google Drive.,Test if the program retrieves and displays the metadata information of the folder with the specified ID on Google Drive.,Test if the program handles authentication errors correctly and displays appropriate error messages.,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

# Function to authenticate and get the GoogleDrive instance
def authenticate_drive():
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    return GoogleDrive(gauth)

def get_folder_metadata(folder_id):
    drive = authenticate_drive()
    
    try:
        folder = drive.CreateFile({'id': folder_id})
        if not folder:
            print(f""Folder with ID '{folder_id}' does not exist on Google Drive."")
            return
        
        print(""Folder Metadata:"")
        print(f""Title: {folder['title']}"")
        print(f""Creation Date: {folder['createdDate']}"")
        print(f""Last Modified Date: {folder['modifiedDate']}"")
    except Exception as e:
        print(f""Error retrieving folder metadata: {e}"")

if __name__ == ""__main__"":
    folder_id = input(""Enter the folder ID on Google Drive to retrieve metadata: "")
    
    get_folder_metadata(folder_id)",train
PyDrive2,29,Create a Python program that renames a folder on Google Drive using the PyDrive2 API. The program should take the folder ID and the new folder name as inputs and rename the folder on Google Drive. Make sure the program is capable of handling cases where the folder ID provided is invalid or when there is an authentication issue.,code/PyDrive2/PyDrive2_29.py,Test if the program handles the case when the folder with the provided ID does not exist on Google Drive.,Test if the program handles authentication errors correctly and displays appropriate error messages.,Test if the program renames the folder with the specified ID to the new folder name on Google Drive.,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

# Function to authenticate and get the GoogleDrive instance
def authenticate_drive():
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    return GoogleDrive(gauth)

def rename_folder_on_drive(folder_id, new_folder_name):
    drive = authenticate_drive()
    
    try:
        folder = drive.CreateFile({'id': folder_id})
        if not folder:
            print(f""Folder with ID '{folder_id}' does not exist on Google Drive."")
            return
        
        folder['title'] = new_folder_name
        folder.Upload()
        
        print(f""Folder with ID '{folder_id}' renamed to '{new_folder_name}' successfully on Google Drive."")
    except Exception as e:
        print(f""Error renaming folder: {e}"")

if __name__ == ""__main__"":
    folder_id = input(""Enter the folder ID on Google Drive to rename: "")
    new_folder_name = input(""Enter the new folder name: "")
    
    rename_folder_on_drive(folder_id, new_folder_name)",train
PyDrive2,4,Create a Python program that creates a new folder on Google Drive using the PyDrive2 API. The program should take the folder name and the parent folder ID as inputs and create a new folder with the specified name under the parent folder. Make sure the program is capable of handling cases where the parent folder ID provided is invalid or when there is an authentication issue.,code/PyDrive2/PyDrive2_4.py,Test if the program creates a new folder with the specified name under the parent folder on Google Drive.,Test if the program handles authentication errors correctly and displays appropriate error messages.,Test if the program handles the case when the parent folder with the provided ID does not exist on Google Drive.,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

# Function to authenticate and get the GoogleDrive instance
def authenticate_drive():
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    return GoogleDrive(gauth)

def create_folder_on_drive(folder_name, parent_folder_id):
    drive = authenticate_drive()
    
    try:
        folder = drive.CreateFile({'title': folder_name, 'parents': [{'id': parent_folder_id}], 'mimeType': 'application/vnd.google-apps.folder'})
        if not folder:
            print(f""Parent folder with ID '{parent_folder_id}' does not exist on Google Drive."")
            return
        
        folder.Upload()
        
        print(f""Folder '{folder_name}' created successfully under parent folder with ID '{parent_folder_id}' on Google Drive."")
    except Exception as e:
        print(f""Error creating folder: {e}"")

if __name__ == ""__main__"":
    folder_name = input(""Enter the name of the folder to create: "")
    parent_folder_id = input(""Enter the parent folder ID on Google Drive: "")
    
    create_folder_on_drive(folder_name, parent_folder_id)",train
PyDrive2,5,Create a Python program that deletes a file from Google Drive using the PyDrive2 API. The program should take the file ID as input and delete the file from Google Drive. Make sure the program is capable of handling cases where the file ID provided is invalid or when there is an authentication issue.,code/PyDrive2/PyDrive2_5.py,Test if the program deletes the file with the specified ID from Google Drive.,Test if the program handles authentication errors correctly and displays appropriate error messages.,Test if the program handles the case when the file with the provided ID does not exist on Google Drive.,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

# Function to authenticate and get the GoogleDrive instance
def authenticate_drive():
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    return GoogleDrive(gauth)

def delete_file_from_drive(file_id):
    drive = authenticate_drive()
    
    try:
        file = drive.CreateFile({'id': file_id})
        if not file:
            print(f""File with ID '{file_id}' does not exist on Google Drive."")
            return
        
        file.Delete()
        
        print(f""File with ID '{file_id}' deleted successfully from Google Drive."")
    except Exception as e:
        print(f""Error deleting file: {e}"")

if __name__ == ""__main__"":
    file_id = input(""Enter the file ID on Google Drive to delete: "")
    
    delete_file_from_drive(file_id)",train
PyDrive2,35,"Create a Python program that checks the sharing settings of a file or folder on Google Drive using the PyDrive2 API. The program should take the file or folder ID as input and display the sharing settings, including the email addresses and roles of the users or groups that have access to the file or folder. Make sure the program is capable of handling cases where the file or folder ID provided is invalid or when there is an authentication issue.",code/PyDrive2/PyDrive2_35.py,Test if the program retrieves and displays the sharing settings for the file or folder with the specified ID on Google Drive.,Test if the program handles authentication errors correctly and displays appropriate error messages.,Test if the program handles the case when the file or folder with the provided ID does not exist on Google Drive.,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

def check_sharing_settings(file_or_folder_id):
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    drive = GoogleDrive(gauth)
    
    try:
        file_or_folder = drive.CreateFile({'id': file_or_folder_id})
        if not file_or_folder:
            print(f""File or folder with ID '{file_or_folder_id}' does not exist on Google Drive."")
            return
        
        permissions = file_or_folder.GetPermissions()
        
        print(f""Sharing Settings for file or folder with ID '{file_or_folder_id}':"")
        
        for permission in permissions:
            print(f""Email Address: {permission['emailAddress']}, Role: {permission['role']}"")
    except Exception as e:
        print(f""Error checking sharing settings: {e}"")

if __name__ == ""__main__"":
    file_or_folder_id = input(""Enter the file or folder ID on Google Drive to check sharing settings: "")
    
    check_sharing_settings(file_or_folder_id)",train
PyDrive2,31,"Create a Python program that updates the permissions of a file or folder on Google Drive using the PyDrive2 API. The program should take the file or folder ID, the email address, and the new role as inputs and update the permissions of the file or folder accordingly. Make sure the program is capable of handling cases where the file or folder ID provided is invalid, the email address is not valid, or when there is an authentication issue.",code/PyDrive2/PyDrive2_31.py,Test if the program handles authentication errors correctly and displays appropriate error messages.,Test if the program handles the case when the email address provided is not valid.,Test if the program handles the case when the file or folder with the provided ID does not exist on Google Drive.,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

# Function to authenticate and get the GoogleDrive instance
def authenticate_drive():
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    return GoogleDrive(gauth)

def update_permissions(file_or_folder_id, email_address, new_role):
    drive = authenticate_drive()
    
    try:
        file_or_folder = drive.CreateFile({'id': file_or_folder_id})
        if not file_or_folder:
            print(f""File or folder with ID '{file_or_folder_id}' does not exist on Google Drive."")
            return
        
        permissions = file_or_folder.GetPermissions()
        
        # Find the permission with the specified email address
        permission_to_update = None
        for permission in permissions:
            if permission['emailAddress'] == email_address:
                permission_to_update = permission
                break
        
        if not permission_to_update:
            print(f""No permission found for email address '{email_address}'"")
            return
        
        permission_to_update['role'] = new_role
        permission_to_update.Upload()
        
        print(f""Permission for email address '{email_address}' updated to '{new_role}' successfully."")
    except Exception as e:
        print(f""Error updating permissions: {e}"")

if __name__ == ""__main__"":
    file_or_folder_id = input(""Enter the file or folder ID on Google Drive to update permissions: "")
    email_address = input(""Enter the email address to update permissions for: "")
    new_role = input(""Enter the new role: "")
    
    update_permissions(file_or_folder_id, email_address, new_role)",train
PyDrive2,37,"Create a Python program that checks the revision history of a file on Google Drive using the PyDrive2 API. The program should take the file ID as input and display the revision history, including the revision number, the date and time of the revision, and the user who made the revision. Make sure the program is capable of handling cases where the file ID provided is invalid or when there is an authentication issue.",code/PyDrive2/PyDrive2_37.py,"Test if the program retrieves and displays the revision history of the file with the specified ID on Google Drive, including the revision number, date and time, and user who made the revision.",Test if the program handles authentication errors correctly and displays appropriate error messages.,Test if the program handles the case when the file with the provided ID does not exist on Google Drive.,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

def check_revision_history(file_id):
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    drive = GoogleDrive(gauth)
    
    try:
        file = drive.CreateFile({'id': file_id})
        if not file:
            print(f""File with ID '{file_id}' does not exist on Google Drive."")
            return
        
        revisions = file.GetRevisions()
        
        print(f""Revision History for file with ID '{file_id}':"")
        
        for revision in revisions:
            print(f""Revision Number: {revision['id']}"")
            print(f""Date and Time: {revision['modifiedDate']}"")
            print(f""User: {revision['lastModifyingUserName']}"")
            print()
    except Exception as e:
        print(f""Error checking revision history: {e}"")

if __name__ == ""__main__"":
    file_id = input(""Enter the file ID on Google Drive to check revision history: "")
    
    check_revision_history(file_id)",train
PyDrive2,32,"Create a Python program that revokes the access of a user or group to a file or folder on Google Drive using the PyDrive2 API. The program should take the file or folder ID and the email address as inputs and revoke the access of the specified user or group. Make sure the program is capable of handling cases where the file or folder ID provided is invalid, the email address is not valid, or when there is an authentication issue.",code/PyDrive2/PyDrive2_32.py,Test if the program handles authentication errors correctly and displays appropriate error messages.,Test if the program handles the case when the email address provided is not valid.,Test if the program handles the case when the file or folder with the provided ID does not exist on Google Drive.,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

# Function to authenticate and get the GoogleDrive instance
def authenticate_drive():
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    return GoogleDrive(gauth)

def revoke_access(file_or_folder_id, email_address):
    drive = authenticate_drive()
    
    try:
        file_or_folder = drive.CreateFile({'id': file_or_folder_id})
        if not file_or_folder:
            print(f""File or folder with ID '{file_or_folder_id}' does not exist on Google Drive."")
            return
        
        permissions = file_or_folder.GetPermissions()
        
        # Find the permission with the specified email address
        permission_to_revoke = None
        for permission in permissions:
            if permission['emailAddress'] == email_address:
                permission_to_revoke = permission
                break
        
        if not permission_to_revoke:
            print(f""No permission found for email address '{email_address}'"")
            return
        
        permission_to_revoke.Delete()
        
        print(f""Access revoked for email address '{email_address}' successfully."")
    except Exception as e:
        print(f""Error revoking access: {e}"")

if __name__ == ""__main__"":
    file_or_folder_id = input(""Enter the file or folder ID on Google Drive to revoke access: "")
    email_address = input(""Enter the email address to revoke access for: "")
    
    revoke_access(file_or_folder_id, email_address)",train
PyDrive2,40,"Create a Python program that checks the available storage space of a user's Google Drive account using the PyDrive2 API. The program should display the total storage capacity, the amount of storage used, and the amount of storage available. Make sure the program is capable of handling cases when there is an authentication issue.",code/PyDrive2/PyDrive2_40.py,"Test if the program retrieves and displays the total storage capacity, used storage, and available storage space of the user\'s Google Drive account.",Test if the program handles authentication errors correctly and displays appropriate error messages.,,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

def check_storage_space():
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    drive = GoogleDrive(gauth)
    
    try:
        about = drive.GetAbout()
        total_storage = about['quotaBytesTotal']
        used_storage = about['quotaBytesUsed']
        available_storage = total_storage - used_storage
        
        print(""Storage Space:"")
        print(f""Total Storage: {total_storage} bytes"")
        print(f""Used Storage: {used_storage} bytes"")
        print(f""Available Storage: {available_storage} bytes"")
    except Exception as e:
        print(f""Error checking storage space: {e}"")

if __name__ == ""__main__"":
    check_storage_space()",train
PyDrive2,34,Create a Python program that lists all files and folders in a specified parent folder on Google Drive using the PyDrive2 API. The program should take the parent folder ID as input and display a list of file and folder names along with their corresponding IDs. Make sure the program is capable of handling cases where the parent folder ID provided is invalid or when there is an authentication issue.,code/PyDrive2/PyDrive2_34.py,Test if the program lists the files and folders in the specified parent folder along with their IDs on Google Drive.,Test if the program handles authentication errors correctly and displays appropriate error messages.,Test if the program handles the case when the parent folder with the provided ID does not exist on Google Drive.,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

def list_files_and_folders(parent_folder_id):
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    drive = GoogleDrive(gauth)
    
    try:
        file_list = drive.ListFile({'q': f""'{parent_folder_id}' in parents and trashed=false""}).GetList()
        if not file_list:
            print(f""No files or folders found in the parent folder with ID '{parent_folder_id}'."")
            return
        
        print(f""Files and Folders in the parent folder with ID '{parent_folder_id}':"")
        
        for file in file_list:
            if file['mimeType'] == 'application/vnd.google-apps.folder':
                print(f""Folder Name: {file['title']}, Folder ID: {file['id']}"")
            else:
                print(f""File Name: {file['title']}, File ID: {file['id']}"")
    except Exception as e:
        print(f""Error listing files and folders: {e}"")

if __name__ == ""__main__"":
    parent_folder_id = input(""Enter the parent folder ID on Google Drive to list files and folders from: "")
    
    list_files_and_folders(parent_folder_id)",train
PyDrive2,8,Create a Python program that shares a file on Google Drive with a specific email address using the PyDrive2 API. The program should take the file ID and the email address as inputs and share the file with the specified email address. Make sure the program is capable of handling cases where the file ID provided is invalid or when there is an authentication issue.,code/PyDrive2/PyDrive2_8.py,Test if the program shares the file with the specified ID with the provided email address on Google Drive.,Test if the program handles authentication errors correctly and displays appropriate error messages.,Test if the program handles the case when the file with the provided ID does not exist on Google Drive.,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

# Function to authenticate and get the GoogleDrive instance
def authenticate_drive():
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    return GoogleDrive(gauth)

def share_file_on_drive(file_id, email_address):
    drive = authenticate_drive()
    
    try:
        file = drive.CreateFile({'id': file_id})
        if not file:
            print(f""File with ID '{file_id}' does not exist on Google Drive."")
            return
        
        file.InsertPermission({'type': 'user', 'value': email_address, 'role': 'reader'})
        
        print(f""File with ID '{file_id}' shared successfully with email address '{email_address}' on Google Drive."")
    except Exception as e:
        print(f""Error sharing file: {e}"")

if __name__ == ""__main__"":
    file_id = input(""Enter the file ID on Google Drive to share: "")
    email_address = input(""Enter the email address to share the file with: "")
    
    share_file_on_drive(file_id, email_address)",train
PyDrive2,11,Create a Python program that updates the content of a file on Google Drive using the PyDrive2 API. The program should take the file ID and the new content as inputs and update the content of the file on Google Drive. Make sure the program is capable of handling cases where the file ID provided is invalid or when there is an authentication issue.,code/PyDrive2/PyDrive2_11.py,Test if the program updates the content of the file with the specified ID on Google Drive.,Test if the program handles authentication errors correctly and displays appropriate error messages.,Test if the program handles the case when the file with the provided ID does not exist on Google Drive.,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

# Function to authenticate and get the GoogleDrive instance
def authenticate_drive():
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    return GoogleDrive(gauth)

def update_file_content_on_drive(file_id, new_content):
    drive = authenticate_drive()
    
    try:
        file = drive.CreateFile({'id': file_id})
        if not file:
            print(f""File with ID '{file_id}' does not exist on Google Drive."")
            return
        
        file.SetContentString(new_content)
        file.Upload()
        
        print(f""Content of file with ID '{file_id}' updated successfully on Google Drive."")
    except Exception as e:
        print(f""Error updating file content: {e}"")

if __name__ == ""__main__"":
    file_id = input(""Enter the file ID on Google Drive to update content: "")
    new_content = input(""Enter the new content: "")
    
    update_file_content_on_drive(file_id, new_content)",train
PyDrive2,28,Create a Python program that deletes a folder from Google Drive using the PyDrive2 API. The program should take the folder ID as input and delete the folder from Google Drive. Make sure the program is capable of handling cases where the folder ID provided is invalid or when there is an authentication issue.,code/PyDrive2/PyDrive2_28.py,Test if the program handles the case when the folder with the provided ID does not exist on Google Drive.,Test if the program deletes the folder with the specified ID from Google Drive.,Test if the program handles authentication errors correctly and displays appropriate error messages.,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

# Function to authenticate and get the GoogleDrive instance
def authenticate_drive():
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    return GoogleDrive(gauth)

def delete_folder_from_drive(folder_id):
    drive = authenticate_drive()
    
    try:
        folder = drive.CreateFile({'id': folder_id})
        if not folder:
            print(f""Folder with ID '{folder_id}' does not exist on Google Drive."")
            return
        
        folder.Delete()
        
        print(f""Folder with ID '{folder_id}' deleted successfully from Google Drive."")
    except Exception as e:
        print(f""Error deleting folder: {e}"")

if __name__ == ""__main__"":
    folder_id = input(""Enter the folder ID on Google Drive to delete: "")
    
    delete_folder_from_drive(folder_id)",train
PyDrive2,33,"Create a Python program that checks the storage usage of a user's Google Drive account using the PyDrive2 API. The program should display the total storage capacity, the amount of storage used, and the amount of storage available. Make sure the program is capable of handling cases when there is an authentication issue.",code/PyDrive2/PyDrive2_33.py,"Test if the program retrieves and displays the total storage capacity, used storage, and available storage of the user\'s Google Drive account.",Test if the program handles authentication errors correctly and displays appropriate error messages.,,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

def check_storage_usage():
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    drive = GoogleDrive(gauth)
    
    try:
        about = drive.GetAbout()
        total_storage = about['quotaBytesTotal']
        used_storage = about['quotaBytesUsed']
        available_storage = total_storage - used_storage
        
        print(""Storage Usage:"")
        print(f""Total Storage: {total_storage} bytes"")
        print(f""Used Storage: {used_storage} bytes"")
        print(f""Available Storage: {available_storage} bytes"")
    except Exception as e:
        print(f""Error checking storage usage: {e}"")

if __name__ == ""__main__"":
    check_storage_usage()",train
PyDrive2,7,Create a Python program that moves a file to a different folder on Google Drive using the PyDrive2 API. The program should take the file ID and the destination folder ID as inputs and move the file to the specified destination folder. Make sure the program is capable of handling cases where the file ID or destination folder ID provided is invalid or when there is an authentication issue.,code/PyDrive2/PyDrive2_7.py,Test if the program handles authentication errors correctly and displays appropriate error messages.,Test if the program handles the case when the destination folder with the provided ID does not exist on Google Drive.,Test if the program handles the case when the file with the provided ID does not exist on Google Drive.,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

# Function to authenticate and get the GoogleDrive instance
def authenticate_drive():
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    return GoogleDrive(gauth)

def move_file_on_drive(file_id, destination_folder_id):
    drive = authenticate_drive()
    
    try:
        file = drive.CreateFile({'id': file_id})
        if not file:
            print(f""File with ID '{file_id}' does not exist on Google Drive."")
            return
        
        file['parents'] = [{'id': destination_folder_id}]
        file.Upload()
        
        print(f""File with ID '{file_id}' moved successfully to folder with ID '{destination_folder_id}' on Google Drive."")
    except Exception as e:
        print(f""Error moving file: {e}"")

if __name__ == ""__main__"":
    file_id = input(""Enter the file ID on Google Drive to move: "")
    destination_folder_id = input(""Enter the destination folder ID on Google Drive: "")
    
    move_file_on_drive(file_id, destination_folder_id)",train
PyDrive2,9,"Create a Python program that retrieves the metadata of a file on Google Drive using the PyDrive2 API. The program should take the file ID as input and display the metadata information of the file, including its title, size, creation date, and last modified date. Make sure the program is capable of handling cases where the file ID provided is invalid or when there is an authentication issue.",code/PyDrive2/PyDrive2_9.py,Test if the program retrieves and displays the metadata information of the file with the specified ID on Google Drive.,Test if the program handles authentication errors correctly and displays appropriate error messages.,Test if the program handles the case when the file with the provided ID does not exist on Google Drive.,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

# Function to authenticate and get the GoogleDrive instance
def authenticate_drive():
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    return GoogleDrive(gauth)

def get_file_metadata(file_id):
    drive = authenticate_drive()
    
    try:
        file = drive.CreateFile({'id': file_id})
        if not file:
            print(f""File with ID '{file_id}' does not exist on Google Drive."")
            return
        
        print(""File Metadata:"")
        print(f""Title: {file['title']}"")
        print(f""Size: {file['fileSize']} bytes"")
        print(f""Creation Date: {file['createdDate']}"")
        print(f""Last Modified Date: {file['modifiedDate']}"")
    except Exception as e:
        print(f""Error retrieving file metadata: {e}"")

if __name__ == ""__main__"":
    file_id = input(""Enter the file ID on Google Drive to retrieve metadata: "")
    
    get_file_metadata(file_id)",test
PyDrive2,30,"Create a Python program that retrieves the list of permissions for a file or folder on Google Drive using the PyDrive2 API. The program should take the file or folder ID as input and display the list of permissions, including the email addresses and roles of the users or groups that have access to the file or folder. Make sure the program is capable of handling cases where the file or folder ID provided is invalid or when there is an authentication issue.",code/PyDrive2/PyDrive2_30.py,Test if the program retrieves and displays the list of permissions for the file or folder with the specified ID on Google Drive.,Test if the program handles authentication errors correctly and displays appropriate error messages.,Test if the program handles the case when the file or folder with the provided ID does not exist on Google Drive.,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

# Function to authenticate and get the GoogleDrive instance
def authenticate_drive():
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    return GoogleDrive(gauth)

def get_permissions(file_or_folder_id):
    drive = authenticate_drive()
    
    try:
        file_or_folder = drive.CreateFile({'id': file_or_folder_id})
        if not file_or_folder:
            print(f""File or folder with ID '{file_or_folder_id}' does not exist on Google Drive."")
            return
        
        permissions = file_or_folder.GetPermissions()
        
        print(f""Permissions for file or folder with ID '{file_or_folder_id}':"")
        
        for permission in permissions:
            print(f""Email Address: {permission['emailAddress']}, Role: {permission['role']}"")
    except Exception as e:
        print(f""Error retrieving permissions: {e}"")

if __name__ == ""__main__"":
    file_or_folder_id = input(""Enter the file or folder ID on Google Drive to retrieve permissions: "")
    
    get_permissions(file_or_folder_id)",test
PyDrive2,1,Create a Python program that lists all files in a specified folder on Google Drive using the PyDrive2 API. The program should take the folder's ID as input and display a list of file names along with their corresponding IDs. Make sure the program is capable of handling cases where the folder ID provided does not exist or when there is an authentication issue.,code/PyDrive2/PyDrive2_1.py,Test if the program handles the case when the folder with the provided ID does not exist on Google Drive.,Test if the program handles authentication errors correctly.,Test if the program lists files in the specified folder along with their IDs.,,,"#!pip install pydrive2
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

# Function to authenticate and get the GoogleDrive instance
def authenticate_drive():
    gauth = GoogleAuth()
    gauth.LocalWebserverAuth()  # Authenticate using local webserver
    return GoogleDrive(gauth)

def list_files_in_drive_folder(folder_id):
    drive = authenticate_drive()
    
    try:
        folder = drive.CreateFile({'id': folder_id})
        if not folder:
            print(f""Folder with ID '{folder_id}' does not exist on Google Drive."")
            return
        
        folder_title = folder['title']
        print(f""Listing files in '{folder_title}' folder (ID: {folder_id}):"")
        
        file_list = drive.ListFile({'q': f""'{folder_id}' in parents and trashed=false""}).GetList()
        if not file_list:
            print(""No files found in this folder."")
            return
        
        for file in file_list:
            print(f""File Name: {file['title']}, File ID: {file['id']}"")
    except Exception as e:
        print(f""Error listing files: {e}"")

if __name__ == ""__main__"":
    folder_id = input(""Enter the folder ID on Google Drive to list files from: "")
    
    list_files_in_drive_folder(folder_id)",test
Pygments,41,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as an XML file with line numbers and a dark background color scheme.,code/pygments/Pygments_41.py,Verify that the program correctly determines the lexer based on the file extension.,Test if the program successfully reads the code file.,Check that the program saves the highlighted code as an XML file with line numbers and the dark background color scheme.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import XmlFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom formatter with line numbers and a dark background color scheme
formatter = XmlFormatter(style=""monokai"", linenos=True)

# Apply syntax highlighting to the code with line numbers and the dark background color scheme
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to an XML file with line numbers and the dark background color scheme
with open(""highlighted_code.xml"", ""w"") as xml_file:
    xml_file.write(highlighted_code)",train
Pygments,20,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as an HTML file with a custom CSS style.,code/pygments/Pygments_20.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as an HTML file with the custom CSS style.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import HtmlFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom CSS style for the HTML output
css_style = """"""
.highlight {
    background-color: #f1f1f1;
    padding: 10px;
}
""""""

# Define a custom formatter with the custom CSS style
formatter = HtmlFormatter(style=css_style)

# Apply syntax highlighting to the code with the custom CSS style
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to an HTML file with the custom CSS style
with open(""highlighted_code.html"", ""w"") as html_file:
    html_file.write(highlighted_code)",train
Pygments,40,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a YAML file with line numbers and a dark background color scheme.,code/pygments/Pygments_40.py,Verify that the program correctly determines the lexer based on the file extension.,Test if the program successfully reads the code file.,Check that the program saves the highlighted code as a YAML file with line numbers and the dark background color scheme.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import YamlFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom formatter with line numbers and a dark background color scheme
formatter = YamlFormatter(style=""monokai"", linenos=True)

# Apply syntax highlighting to the code with line numbers and the dark background color scheme
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to a YAML file with line numbers and the dark background color scheme
with open(""highlighted_code.yaml"", ""w"") as yaml_file:
    yaml_file.write(highlighted_code)",train
Pygments,30,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as an XML file with a custom color scheme and line numbers.,code/pygments/Pygments_30.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as an XML file with the custom color scheme and line numbers.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import XmlFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom formatter with a custom color scheme and line numbers
formatter = XmlFormatter(style=""tango"", linenos=True)

# Apply syntax highlighting to the code with the custom color scheme and line numbers
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to an XML file with the custom color scheme and line numbers
with open(""highlighted_code.xml"", ""w"") as xml_file:
    xml_file.write(highlighted_code)",train
Pygments,7,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a DOCX file.,code/pygments/Pygments_7.py,Verify that the program correctly determines the lexer based on the file extension.,Test if the program successfully reads the code file.,Check that the program saves the highlighted code as a DOCX file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import DocxFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Apply syntax highlighting to the code
highlighted_code = highlight(code, lexer, DocxFormatter())

# Save the highlighted code to a DOCX file
with open(""highlighted_code.docx"", ""wb"") as docx_file:
    docx_file.write(highlighted_code)",train
Pygments,27,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a JSON file with a custom color scheme and line numbers.,code/pygments/Pygments_27.py,Verify that the program correctly determines the lexer based on the file extension.,Test if the program successfully reads the code file.,Check that the program saves the highlighted code as a JSON file with the custom color scheme and line numbers.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import JsonFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom formatter with a custom color scheme and line numbers
formatter = JsonFormatter(style=""tango"", linenos=True)

# Apply syntax highlighting to the code with the custom color scheme and line numbers
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to a JSON file with the custom color scheme and line numbers
with open(""highlighted_code.json"", ""w"") as json_file:
    json_file.write(highlighted_code)",train
Pygments,13,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as an XML file.,code/pygments/Pygments_13.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as an XML file.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import XmlFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Apply syntax highlighting to the code
highlighted_code = highlight(code, lexer, XmlFormatter())

# Save the highlighted code to an XML file
with open(""highlighted_code.xml"", ""w"") as xml_file:
    xml_file.write(highlighted_code)",train
Pygments,18,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a plain text file (TXT) with a custom color scheme.,code/pygments/Pygments_18.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as a plain text file (TXT) with the custom color scheme.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import HtmlFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom formatter with a custom color scheme
formatter = HtmlFormatter(style=""colorful"")

# Apply syntax highlighting to the code with the custom color scheme
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to a plain text file (TXT) with the custom color scheme
with open(""highlighted_code.txt"", ""w"") as txt_file:
    txt_file.write(highlighted_code)",train
Pygments,10,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a JSON file.,code/pygments/Pygments_10.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as a JSON file.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import JsonFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Apply syntax highlighting to the code
highlighted_code = highlight(code, lexer, JsonFormatter())

# Save the highlighted code to a JSON file
with open(""highlighted_code.json"", ""w"") as json_file:
    json_file.write(highlighted_code)",train
Pygments,16,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a PowerPoint presentation (PPTX).,code/pygments/Pygments_16.py,Verify that the program correctly determines the lexer based on the file extension.,Test if the program successfully reads the code file.,Check that the program saves the highlighted code as a PowerPoint presentation (PPTX).,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import RtfFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Apply syntax highlighting to the code
highlighted_code = highlight(code, lexer, RtfFormatter())

# Save the highlighted code to a PowerPoint presentation (PPTX)
with open(""highlighted_code.pptx"", ""wb"") as pptx_file:
    pptx_file.write(highlighted_code)",train
Pygments,33,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a PDF file with line numbers and a dark background color scheme.,code/pygments/Pygments_33.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as a PDF file with line numbers and the dark background color scheme.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import PdfFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom formatter with line numbers and a dark background color scheme
formatter = PdfFormatter(style=""monokai"", linenos=True)

# Apply syntax highlighting to the code with line numbers and the dark background color scheme
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to a PDF file with line numbers and the dark background color scheme
with open(""highlighted_code.pdf"", ""wb"") as pdf_file:
    pdf_file.write(highlighted_code)",train
Pygments,17,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a plain text file (TXT) with line numbers.,code/pygments/Pygments_17.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as a plain text file (TXT) with line numbers.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import HtmlFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom formatter with line numbers
formatter = HtmlFormatter(linenos=True)

# Apply syntax highlighting to the code with line numbers
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to a plain text file (TXT) with line numbers
with open(""highlighted_code.txt"", ""w"") as txt_file:
    txt_file.write(highlighted_code)",train
Pygments,36,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as an Excel file (XLSX) with line numbers and a dark background color scheme.,code/pygments/Pygments_36.py,Verify that the program correctly determines the lexer based on the file extension.,Test if the program successfully reads the code file.,Check that the program saves the highlighted code as an Excel file (XLSX) with line numbers and the dark background color scheme.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import ExcelFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom formatter with line numbers and a dark background color scheme
formatter = ExcelFormatter(style=""monokai"", linenos=True)

# Apply syntax highlighting to the code with line numbers and the dark background color scheme
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to an Excel file (XLSX) with line numbers and the dark background color scheme
with open(""highlighted_code.xlsx"", ""wb"") as xlsx_file:
    xlsx_file.write(highlighted_code)",train
Pygments,35,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a Word document (DOC) with line numbers and a dark background color scheme.,code/pygments/Pygments_35.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as a Word document (DOC) with line numbers and the dark background color scheme.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import DocxFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom formatter with line numbers and a dark background color scheme
formatter = DocxFormatter(style=""monokai"", linenos=True)

# Apply syntax highlighting to the code with line numbers and the dark background color scheme
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to a Word document (DOC) with line numbers and the dark background color scheme
with open(""highlighted_code.docx"", ""wb"") as docx_file:
    docx_file.write(highlighted_code)",train
Pygments,1,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code snippet and save the highlighted code as an HTML file.,code/pygments/Pygments_1.py,Check that the HTML file is saved correctly with the highlighted code.,Verify that the generated HTML file contains the highlighted code.,Test if the program successfully applies syntax highlighting to the code snippet using the Python lexer.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import PythonLexer
from pygments.formatters import HtmlFormatter

# Define a code snippet
code_snippet = """"""
def greet(name):
    print(f""Hello, {name}!"")

greet(""Alice"")
""""""

# Apply syntax highlighting with the Python lexer
highlighted_code = highlight(code_snippet, PythonLexer(), HtmlFormatter())

# Save the highlighted code to an HTML file
with open(""highlighted_code.html"", ""w"") as html_file:
    html_file.write(highlighted_code)",train
Pygments,28,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a Markdown file with a custom color scheme and line numbers.,code/pygments/Pygments_28.py,Verify that the program correctly determines the lexer based on the file extension.,Test if the program successfully reads the code file.,Check that the program saves the highlighted code as a Markdown file with the custom color scheme and line numbers.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import MarkdownFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom formatter with a custom color scheme and line numbers
formatter = MarkdownFormatter(style=""colorful"", linenos=True)

# Apply syntax highlighting to the code with the custom color scheme and line numbers
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to a Markdown file with the custom color scheme and line numbers
with open(""highlighted_code.md"", ""w"") as md_file:
    md_file.write(highlighted_code)",train
Pygments,6,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a LaTeX file.,code/pygments/Pygments_6.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as a LaTeX file.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import LatexFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Apply syntax highlighting to the code
highlighted_code = highlight(code, lexer, LatexFormatter())

# Save the highlighted code to a LaTeX file
with open(""highlighted_code.tex"", ""w"") as tex_file:
    tex_file.write(highlighted_code)",train
Pygments,12,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a YAML file.,code/pygments/Pygments_12.py,Verify that the program correctly determines the lexer based on the file extension.,Test if the program successfully reads the code file.,Check that the program saves the highlighted code as a YAML file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import YamlFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Apply syntax highlighting to the code
highlighted_code = highlight(code, lexer, YamlFormatter())

# Save the highlighted code to a YAML file
with open(""highlighted_code.yaml"", ""w"") as yaml_file:
    yaml_file.write(highlighted_code)",train
Pygments,2,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and display the highlighted code in the console.,code/pygments/Pygments_2.py,Verify that the program correctly determines the lexer based on the file extension.,Test if the program successfully reads the code file.,Check that the program displays the highlighted code in the console.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import TerminalFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Apply syntax highlighting to the code
highlighted_code = highlight(code, lexer, TerminalFormatter())

# Display the highlighted code in the console
print(highlighted_code)",train
Pygments,31,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a plain text file (TXT) with a dark background color scheme and line numbers.,code/pygments/Pygments_31.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as a plain text file (TXT) with the dark background color scheme and line numbers.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import HtmlFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom formatter with a dark background color scheme and line numbers
formatter = HtmlFormatter(style=""monokai"", linenos=True)

# Apply syntax highlighting to the code with the dark background color scheme and line numbers
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to a plain text file (TXT) with the dark background color scheme and line numbers
with open(""highlighted_code.txt"", ""w"") as txt_file:
    txt_file.write(highlighted_code)",train
Pygments,22,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a PDF file with a custom color scheme and line numbers.,code/pygments/Pygments_22.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as a PDF file with the custom color scheme and line numbers.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import PdfFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom formatter with a custom color scheme and line numbers
formatter = PdfFormatter(style=""autumn"", linenos=True)

# Apply syntax highlighting to the code with the custom color scheme and line numbers
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to a PDF file with the custom color scheme and line numbers
with open(""highlighted_code.pdf"", ""wb"") as pdf_file:
    pdf_file.write(highlighted_code)",train
Pygments,3,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a PDF file.,code/pygments/Pygments_3.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as a PDF file.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import PdfFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Apply syntax highlighting to the code
highlighted_code = highlight(code, lexer, PdfFormatter())

# Save the highlighted code to a PDF file
with open(""highlighted_code.pdf"", ""wb"") as pdf_file:
    pdf_file.write(highlighted_code)",train
Pygments,32,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as an HTML file with line numbers and a dark background color scheme.,code/pygments/Pygments_32.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as an HTML file with line numbers and the dark background color scheme.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import HtmlFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom formatter with line numbers and a dark background color scheme
formatter = HtmlFormatter(linenos=True, style=""monokai"")

# Apply syntax highlighting to the code with line numbers and the dark background color scheme
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to an HTML file with line numbers and the dark background color scheme
with open(""highlighted_code.html"", ""w"") as html_file:
    html_file.write(highlighted_code)",train
Pygments,38,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a JSON file with line numbers and a dark background color scheme.,code/pygments/Pygments_38.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as a JSON file with line numbers and the dark background color scheme.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import JsonFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom formatter with line numbers and a dark background color scheme
formatter = JsonFormatter(style=""monokai"", linenos=True)

# Apply syntax highlighting to the code with line numbers and the dark background color scheme
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to a JSON file with line numbers and the dark background color scheme
with open(""highlighted_code.json"", ""w"") as json_file:
    json_file.write(highlighted_code)",train
Pygments,4,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a PNG image.,code/pygments/Pygments_4.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as a PNG image file.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import ImageFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Apply syntax highlighting to the code
highlighted_code = highlight(code, lexer, ImageFormatter(format=""png""))

# Save the highlighted code to a PNG image file
with open(""highlighted_code.png"", ""wb"") as png_file:
    png_file.write(highlighted_code)",train
Pygments,37,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a CSV file with line numbers and a dark background color scheme.,code/pygments/Pygments_37.py,Verify that the program correctly determines the lexer based on the file extension.,Test if the program successfully reads the code file.,Check that the program saves the highlighted code as a CSV file with line numbers and the dark background color scheme.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import CsvFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom formatter with line numbers and a dark background color scheme
formatter = CsvFormatter(style=""monokai"", linenos=True)

# Apply syntax highlighting to the code with line numbers and the dark background color scheme
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to a CSV file with line numbers and the dark background color scheme
with open(""highlighted_code.csv"", ""w"") as csv_file:
    csv_file.write(highlighted_code)",train
Pygments,24,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a Word document (DOC) with a custom color scheme and line numbers.,code/pygments/Pygments_24.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as a Word document (DOC) with the custom color scheme and line numbers.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import DocxFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom formatter with a custom color scheme and line numbers
formatter = DocxFormatter(style=""paraiso-dark"", linenos=True)

# Apply syntax highlighting to the code with the custom color scheme and line numbers
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to a Word document (DOC) with the custom color scheme and line numbers
with open(""highlighted_code.docx"", ""wb"") as docx_file:
    docx_file.write(highlighted_code)",train
Pygments,34,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a PNG image with line numbers and a dark background color scheme.,code/pygments/Pygments_34.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as a PNG image file with line numbers and the dark background color scheme.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import ImageFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom formatter with line numbers and a dark background color scheme
formatter = ImageFormatter(style=""monokai"", line_numbers=True)

# Apply syntax highlighting to the code with line numbers and the dark background color scheme
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to a PNG image file with line numbers and the dark background color scheme
with open(""highlighted_code.png"", ""wb"") as png_file:
    png_file.write(highlighted_code)",train
Pygments,11,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a Markdown file.,code/pygments/Pygments_11.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as a Markdown file.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import MarkdownFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Apply syntax highlighting to the code
highlighted_code = highlight(code, lexer, MarkdownFormatter())

# Save the highlighted code to a Markdown file
with open(""highlighted_code.md"", ""w"") as md_file:
    md_file.write(highlighted_code)",train
Pygments,23,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a PNG image with a custom color scheme and line numbers.,code/pygments/Pygments_23.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as a PNG image with the custom color scheme and line numbers.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import ImageFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom formatter with a custom color scheme and line numbers
formatter = ImageFormatter(style=""colorful"", line_numbers=True)

# Apply syntax highlighting to the code with the custom color scheme and line numbers
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to a PNG image file with the custom color scheme and line numbers
with open(""highlighted_code.png"", ""wb"") as png_file:
    png_file.write(highlighted_code)",train
Pygments,19,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a plain text file (TXT) with a dark background color scheme.,code/pygments/Pygments_19.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as a plain text file (TXT) with the dark background color scheme.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import HtmlFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom formatter with a dark background color scheme
formatter = HtmlFormatter(style=""monokai"")

# Apply syntax highlighting to the code with the dark background color scheme
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to a plain text file (TXT) with the dark background color scheme
with open(""highlighted_code.txt"", ""w"") as txt_file:
    txt_file.write(highlighted_code)",train
Pygments,21,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as an HTML file with line numbers and a custom CSS style.,code/pygments/Pygments_21.py,Verify that the program correctly determines the lexer based on the file extension.,Test if the program successfully reads the code file.,Check that the program saves the highlighted code as an HTML file with line numbers and the custom CSS style.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import HtmlFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom CSS style for the HTML output
css_style = """"""
.highlight {
    background-color: #f1f1f1;
    padding: 10px;
}
""""""

# Define a custom formatter with line numbers and the custom CSS style
formatter = HtmlFormatter(linenos=True, style=css_style)

# Apply syntax highlighting to the code with line numbers and the custom CSS style
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to an HTML file with line numbers and the custom CSS style
with open(""highlighted_code.html"", ""w"") as html_file:
    html_file.write(highlighted_code)",train
Pygments,8,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a TXT file.,code/pygments/Pygments_8.py,Verify that the program correctly determines the lexer based on the file extension.,Test if the program successfully reads the code file.,Check that the program saves the highlighted code as a TXT file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import HtmlFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Apply syntax highlighting to the code
highlighted_code = highlight(code, lexer, HtmlFormatter())

# Save the highlighted code to a TXT file
with open(""highlighted_code.txt"", ""w"") as txt_file:
    txt_file.write(highlighted_code)",train
Pygments,15,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a Word document (DOC).,code/pygments/Pygments_15.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as a Word document (DOC).,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import TexFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Apply syntax highlighting to the code
highlighted_code = highlight(code, lexer, TexFormatter())

# Save the highlighted code to a Word document (DOC)
with open(""highlighted_code.doc"", ""w"") as doc_file:
    doc_file.write(highlighted_code)",train
Pygments,29,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a YAML file with a custom color scheme and line numbers.,code/pygments/Pygments_29.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as a YAML file with the custom color scheme and line numbers.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import YamlFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom formatter with a custom color scheme and line numbers
formatter = YamlFormatter(style=""paraiso-light"", linenos=True)

# Apply syntax highlighting to the code with the custom color scheme and line numbers
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to a YAML file with the custom color scheme and line numbers
with open(""highlighted_code.yaml"", ""w"") as yaml_file:
    yaml_file.write(highlighted_code)",train
Pygments,39,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a Markdown file with line numbers and a dark background color scheme.,code/pygments/Pygments_39.py,Verify that the program correctly determines the lexer based on the file extension.,Test if the program successfully reads the code file.,Check that the program saves the highlighted code as a Markdown file with line numbers and the dark background color scheme.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import MarkdownFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom formatter with line numbers and a dark background color scheme
formatter = MarkdownFormatter(style=""monokai"", linenos=True)

# Apply syntax highlighting to the code with line numbers and the dark background color scheme
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to a Markdown file with line numbers and the dark background color scheme
with open(""highlighted_code.md"", ""w"") as md_file:
    md_file.write(highlighted_code)",train
Pygments,25,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as an Excel file (XLSX) with a custom color scheme and line numbers.,code/pygments/Pygments_25.py,Verify that the program correctly determines the lexer based on the file extension.,Test if the program successfully reads the code file.,Check that the program saves the highlighted code as an Excel file (XLSX) with the custom color scheme and line numbers.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import ExcelFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom formatter with a custom color scheme and line numbers
formatter = ExcelFormatter(style=""friendly"", linenos=True)

# Apply syntax highlighting to the code with the custom color scheme and line numbers
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to an Excel file (XLSX) with the custom color scheme and line numbers
with open(""highlighted_code.xlsx"", ""wb"") as xlsx_file:
    xlsx_file.write(highlighted_code)",test
Pygments,14,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as an Excel file (XLSX).,code/pygments/Pygments_14.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as an Excel file (XLSX).,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import ExcelFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Apply syntax highlighting to the code
highlighted_code = highlight(code, lexer, ExcelFormatter())

# Save the highlighted code to an Excel file (XLSX)
with open(""highlighted_code.xlsx"", ""wb"") as xlsx_file:
    xlsx_file.write(highlighted_code)",test
Pygments,9,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a CSV file.,code/pygments/Pygments_9.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as a CSV file.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import CsvFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Apply syntax highlighting to the code
highlighted_code = highlight(code, lexer, CsvFormatter())

# Save the highlighted code to a CSV file
with open(""highlighted_code.csv"", ""w"") as csv_file:
    csv_file.write(highlighted_code)",test
Pygments,26,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a CSV file with a custom color scheme and line numbers.,code/pygments/Pygments_26.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as a CSV file with the custom color scheme and line numbers.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import CsvFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Define a custom formatter with a custom color scheme and line numbers
formatter = CsvFormatter(style=""fruity"", linenos=True)

# Apply syntax highlighting to the code with the custom color scheme and line numbers
highlighted_code = highlight(code, lexer, formatter)

# Save the highlighted code to a CSV file with the custom color scheme and line numbers
with open(""highlighted_code.csv"", ""w"") as csv_file:
    csv_file.write(highlighted_code)",test
Pygments,5,Create a Python program using the 'Pygments' API to apply syntax highlighting to a code file and save the highlighted code as a RTF file.,code/pygments/Pygments_5.py,Verify that the program correctly determines the lexer based on the file extension.,Check that the program saves the highlighted code as a RTF file.,Test if the program successfully reads the code file.,,,"# !pip install Pygments
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import RtfFormatter

# Read the code file
with open(""code.py"", ""r"") as file:
    code = file.read()

# Determine the lexer based on the file extension
lexer = get_lexer_by_name(""python"")

# Apply syntax highlighting to the code
highlighted_code = highlight(code, lexer, RtfFormatter())

# Save the highlighted code to a RTF file
with open(""highlighted_code.rtf"", ""w"") as rtf_file:
    rtf_file.write(highlighted_code)",test
SQLAlchemy,2,"Create a Python program that utilizes the 'SQLAlchemy' API to connect to a PostgreSQL database. Define an 'Article' entity with attributes like 'id,' 'title,' 'content,' and 'published_date.' Ensure that the program can establish a connection to the PostgreSQL database, create the 'articles' table, insert a new article, and retrieve articles from the database.",code/SQLAlchemy/SQLAlchemy_2.py,"Verify the program's ability to handle articles with missing or incomplete data, and check if it raises appropriate exceptions.",Test the program by inserting multiple articles with different titles and content into the database and then retrieving their data to ensure it works correctly.,Evaluate the program's performance with a large number of articles and ensure it properly handles datetime values.,,,"#!pip install SQLAlchemy
from sqlalchemy import create_engine, Column, Integer, String, DateTime
from sqlalchemy.orm import sessionmaker, declarative_base
from datetime import datetime

# Create a PostgreSQL database connection
engine = create_engine('postgresql://username:password@localhost/mydatabase')

# Define the Article class to represent the ""Article"" entity
Base = declarative_base()

class Article(Base):
    __tablename__ = 'articles'
    id = Column(Integer, primary_key=True)
    title = Column(String)
    content = Column(String)
    published_date = Column(DateTime, default=datetime.utcnow)

# Create the table in the database
Base.metadata.create_all(engine)

# Function to add a new article to the database
def add_article(title, content):
    Session = sessionmaker(bind=engine)
    session = Session()

    article = Article(title=title, content=content)
    session.add(article)
    session.commit()
    session.close()

add_article(""Introduction to SQLAlchemy"", ""SQLAlchemy is a powerful and flexible ORM tool for Python."")

Session = sessionmaker(bind=engine)
session = Session()
article = session.query(Article).filter_by(title=""Introduction to SQLAlchemy"").first()
session.close()

print(""Article data: "")
print(article.title)
print(article.content)
print(article.published_date)",train
SQLAlchemy,6,"Create a Python program that uses the 'SQLAlchemy' API to connect to a PostgreSQL database. Define a 'Order' entity with attributes like 'id,' 'order_date,' 'total_amount,' and 'status.' The program should establish a connection to the PostgreSQL database, create the 'orders' table, add a new order, and retrieve order information from the database.",code/SQLAlchemy/SQLAlchemy_6.py,Evaluate the program's performance when dealing with a large number of order records.,"Verify the program's ability to handle orders with missing or invalid data, and check if it raises appropriate exceptions.","Test the program by adding multiple orders with different order dates, total amounts, and statuses to the database, and then retrieve their data to ensure it works correctly.",,,"#!pip install SQLAlchemy
from sqlalchemy import create_engine, Column, Integer, Float, String
from sqlalchemy.orm import sessionmaker, declarative_base

# Create a PostgreSQL database connection
engine = create_engine('postgresql://username:password@localhost/mydatabase')

# Define the Order class to represent the ""Order"" entity
Base = declarative_base()

class Order(Base):
    __tablename__ = 'orders'
    id = Column(Integer, primary_key=True)
    order_date = Column(String)
    total_amount = Column(Float)
    status = Column(String)

# Create the table in the database
Base.metadata.create_all(engine)

# Function to add a new order to the database
def add_order(order_date, total_amount, status):
    Session = sessionmaker(bind=engine)
    session = Session()

    order = Order(order_date=order_date, total_amount=total_amount, status=status)
    session.add(order)
    session.commit()
    session.close()

add_order(""2023-10-15"", 249.99, ""Shipped"")

Session = sessionmaker(bind=engine)
session = Session()
order = session.query(Order).filter_by(order_date=""2023-10-15"").first()
session.close()

print(""Order data: "")
print(order.order_date)
print(order.total_amount)
print(order.status)",train
SQLAlchemy,1,"Create a Python program that uses the 'SQLAlchemy' API to interact with a SQLite database. The program should define an 'User' entity with attributes like 'id,' 'name,' and 'email.' It should set up the database connection, create the 'users' table, add a new user to the table, and retrieve user data from the database.",code/SQLAlchemy/SQLAlchemy_1.py,Test the program by adding multiple users with different names and emails to the database and then retrieving their data to ensure it works correctly.,Evaluate the program's performance with a large number of user records to ensure it remains efficient.,Verify the program's error-handling capabilities by attempting to add a user with invalid data and check if it raises appropriate exceptions.,,,"#!pip install SQLAlchemy
from sqlalchemy import create_engine, Column, Integer, String
from sqlalchemy.orm import sessionmaker, declarative_base

# Create a SQLite database
engine = create_engine('sqlite:///mydatabase.db')

# Define the User class to represent the ""User"" entity
Base = declarative_base()

class User(Base):
    __tablename__ = 'users'
    id = Column(Integer, primary_key=True)
    name = Column(String)
    email = Column(String)

# Create the table in the database
Base.metadata.create_all(engine)

# Function to add a new user to the database
def add_user(name, email):
    Session = sessionmaker(bind=engine)
    session = Session()

    user = User(name=name, email=email)
    session.add(user)
    session.commit()
    session.close()


add_user(""John Doe"", ""john@example.com"")

Session = sessionmaker(bind=engine)
session = Session()
user = session.query(User).filter_by(name=""John Doe"").first()
session.close()

print(""User data: "")
print(user.name)
print(user.email)",train
SQLAlchemy,8,"Create a Python program that uses the 'SQLAlchemy' API to connect to a Microsoft SQL Server database. Define a 'Product' entity with attributes like 'id,' 'name,' 'price,' and 'quantity.' The program should establish a connection to the SQL Server database, create the 'products' table, add a new product, and retrieve product information from the database.",code/SQLAlchemy/SQLAlchemy_8.py,"Test the program by adding multiple products with different names, prices, and quantities to the database, and then retrieve their data to ensure it works correctly.",Evaluate the program's performance when dealing with a large number of product records.,"Verify the program's ability to handle products with missing or invalid price or quantity values, and check if it raises appropriate exceptions.",,,"#!pip install SQLAlchemy
from sqlalchemy import create_engine, Column, Integer, String, Float
from sqlalchemy.orm import sessionmaker, declarative_base

# Create a Microsoft SQL Server database connection
engine = create_engine('mssql+pyodbc://username:password@server/database')

# Define the Product class to represent the ""Product"" entity
Base = declarative_base()

class Product(Base):
    __tablename__ = 'products'
    id = Column(Integer, primary_key=True)
    name = Column(String)
    price = Column(Float)
    quantity = Column(Integer)

# Create the table in the database
Base.metadata.create_all(engine)

# Function to add a new product to the database
def add_product(name, price, quantity):
    Session = sessionmaker(bind=engine)
    session = Session()

    product = Product(name=name, price=price, quantity=quantity)
    session.add(product)
    session.commit()
    session.close()

add_product(""Smartphone"", 499.99, 50)

Session = sessionmaker(bind=engine)
session = Session()
product = session.query(Product).filter_by(name=""Smartphone"").first()
session.close()

print(""Product data: "")
print(product.name)
print(product.price)
print(product.quantity)",train
SQLAlchemy,3,"Create a Python program that utilizes the 'SQLAlchemy' API to connect to a MySQL database. Define a 'Product' entity with attributes like 'id,' 'name,' 'price,' and 'quantity.' The program should establish a connection to the MySQL database, create the 'products' table, add a new product, and retrieve product information from the database.",code/SQLAlchemy/SQLAlchemy_3.py,"Test the program by adding multiple products with different names, prices, and quantities to the database, and then retrieve their data to ensure it works correctly.",Evaluate the program's performance when dealing with a large number of product records.,"Verify the program's ability to handle products with invalid price or quantity values, and check if it raises appropriate exceptions.",,,"#!pip install SQLAlchemy
from sqlalchemy import create_engine, Column, Integer, String, Float
from sqlalchemy.orm import sessionmaker, declarative_base

# Create a MySQL database connection
engine = create_engine('mysql://username:password@localhost/mydatabase')

# Define the Product class to represent the ""Product"" entity
Base = declarative_base()

class Product(Base):
    __tablename__ = 'products'
    id = Column(Integer, primary_key=True)
    name = Column(String)
    price = Column(Float)
    quantity = Column(Integer)

# Create the table in the database
Base.metadata.create_all(engine)

# Function to add a new product to the database
def add_product(name, price, quantity):
    Session = sessionmaker(bind=engine)
    session = Session()

    product = Product(name=name, price=price, quantity=quantity)
    session.add(product)
    session.commit()
    session.close()

add_product(""Laptop"", 999.99, 10)

Session = sessionmaker(bind=engine)
session = Session()
product = session.query(Product).filter_by(name=""Laptop"").first()
session.close()

print(""Product data: "")
print(product.name)
print(product.price)
print(product.quantity)",train
SQLAlchemy,20,"Create a Python program that uses the 'SQLAlchemy' API to connect to a Microsoft SQL Server database. Define a 'Product' entity with attributes like 'id,' 'name,' 'price,' and 'quantity.' The program should establish a connection to the SQL Server database, create the 'products' table, add a new product and retrieve product information from the database.",code/SQLAlchemy/SQLAlchemy_20.py,"Test the program by adding multiple products with different names, prices, and quantities to the database, and then retrieve their data to ensure it works correctly.",Evaluate the program's performance when dealing with a large number of product records.,"Verify the program's ability to handle products with missing or invalid price or quantity values, and check if it raises appropriate exceptions.",,,"#!pip install SQLAlchemy
from sqlalchemy import create_engine, Column, Integer, String, Float
from sqlalchemy.orm import sessionmaker, declarative_base

# Create a Microsoft SQL Server database connection
engine = create_engine('mssql+pyodbc://username:password@server/database')

# Define the Product class to represent the ""Product"" entity
Base = declarative_base()

class Product(Base):
    __tablename__ = 'products'
    id = Column(Integer, primary_key=True)
    name = Column(String)
    price = Column(Float)
    quantity = Column(Integer)

# Create the table in the database
Base.metadata.create_all(engine)

# Function to add a new product to the database
def add_product(name, price, quantity):
    Session = sessionmaker(bind=engine)
    session = Session()

    product = Product(name=name, price=price, quantity=quantity)
    session.add(product)
    session.commit()
    session.close()

add_product(""Smartphone"", 499.99, 50)

Session = sessionmaker(bind=engine)
session = Session()
product = session.query(Product).filter_by(name=""Smartphone"").first()
session.close()

print(""Product data: "")
print(product.name)
print(product.price)
print(product.quantity)",train
SQLAlchemy,5,"Create a Python program that utilizes the 'SQLAlchemy' API to connect to a SQLite database. Define a 'Task' entity with attributes like 'id,' 'description,' 'due_date,' and 'completed.' The program should establish a connection to the SQLite database, create the 'tasks' table, add a new task, and retrieve task information from the database.",code/SQLAlchemy/SQLAlchemy_5.py,Evaluate the program's performance when dealing with a large number of task records.,"Test the program by adding multiple tasks with different descriptions and due dates to the database, and then retrieve their data to ensure it works correctly.","Verify the program's ability to handle tasks with missing or invalid due dates, and check if it raises appropriate exceptions.",,,"#!pip install SQLAlchemy
from sqlalchemy import create_engine, Column, Integer, String, Boolean
from sqlalchemy.orm import sessionmaker, declarative_base

# Create a SQLite database
engine = create_engine('sqlite:///mydatabase.db')

# Define the Task class to represent the ""Task"" entity
Base = declarative_base()

class Task(Base):
    __tablename__ = 'tasks'
    id = Column(Integer, primary_key=True)
    description = Column(String)
    due_date = Column(String)
    completed = Column(Boolean, default=False)

# Create the table in the database
Base.metadata.create_all(engine)

# Function to add a new task to the database
def add_task(description, due_date):
    Session = sessionmaker(bind=engine)
    session = Session()

    task = Task(description=description, due_date=due_date)
    session.add(task)
    session.commit()
    session.close()

add_task(""Complete project report"", ""2023-11-30"")

Session = sessionmaker(bind=engine)
session = Session()
task = session.query(Task).filter_by(description=""Complete project report"").first()
session.close()

print(""Task data: "")
print(task.description)
print(task.due_date)
print(task.completed)",train
SQLAlchemy,4,"Create a Python program that uses the 'SQLAlchemy' API to connect to a Microsoft SQL Server database. Define a 'Customer' entity with attributes like 'id,' 'first_name,' 'last_name,' and 'email.' The program should establish a connection to the SQL Server database, create the 'customers' table, add a new customer, and retrieve customer details from the database.",code/SQLAlchemy/SQLAlchemy_4.py,Evaluate the program's performance with a large number of customer records.,"Verify the program's ability to handle customer data with missing or invalid fields, and check if it raises appropriate exceptions.","Test the program by adding multiple customers with different first names, last names, and emails to the database, and then retrieve their data to ensure it works correctly.",,,"#!pip install SQLAlchemy
from sqlalchemy import create_engine, Column, Integer, String
from sqlalchemy.orm import sessionmaker, declarative_base

# Create a Microsoft SQL Server database connection
engine = create_engine('mssql+pyodbc://username:password@server/database')

# Define the Customer class to represent the ""Customer"" entity
Base = declarative_base()

class Customer(Base):
    __tablename__ = 'customers'
    id = Column(Integer, primary_key=True)
    first_name = Column(String)
    last_name = Column(String)
    email = Column(String)

# Create the table in the database
Base.metadata.create_all(engine)

# Function to add a new customer to the database
def add_customer(first_name, last_name, email):
    Session = sessionmaker(bind=engine)
    session = Session()

    customer = Customer(first_name=first_name, last_name=last_name, email=email)
    session.add(customer)
    session.commit()
    session.close()

add_customer(""Alice"", ""Johnson"", ""alice@example.com"")

Session = sessionmaker(bind=engine)
session = Session()
customer = session.query(Customer).filter_by(first_name=""Alice"").first()
session.close()

print(""Customer data: "")
print(customer.first_name)
print(customer.last_name)
print(customer.email)",train
SQLAlchemy,7,"Create a Python program that uses the 'SQLAlchemy' API to connect to a MySQL database. Define a 'Employee' entity with attributes like 'id,' 'first_name,' 'last_name,' 'position,' and 'salary.' The program should establish a connection to the MySQL database, create the 'employees' table, add a new employee, and retrieve employee details from the database.",code/SQLAlchemy/SQLAlchemy_7.py,Evaluate the program's performance when dealing with a large number of employee records.,"Verify the program's ability to handle employee data with missing or invalid fields, and check if it raises appropriate exceptions.","Test the program by adding multiple employees with different first names, last names, positions, and salaries to the database, and then retrieve their data to ensure it works correctly.",,,"#!pip install SQLAlchemy
from sqlalchemy import create_engine, Column, Integer, String, Float
from sqlalchemy.orm import sessionmaker, declarative_base

# Create a MySQL database connection
engine = create_engine('mysql://username:password@localhost/mydatabase')

# Define the Employee class to represent the ""Employee"" entity
Base = declarative_base()

class Employee(Base):
    __tablename__ = 'employees'
    id = Column(Integer, primary_key=True)
    first_name = Column(String)
    last_name = Column(String)
    position = Column(String)
    salary = Column(Float)

# Create the table in the database
Base.metadata.create_all(engine)

# Function to add a new employee to the database
def add_employee(first_name, last_name, position, salary):
    Session = sessionmaker(bind=engine)
    session = Session()

    employee = Employee(first_name=first_name, last_name=last_name, position=position, salary=salary)
    session.add(employee)
    session.commit()
    session.close()

add_employee(""John"", ""Smith"", ""Manager"", 60000.00)

Session = sessionmaker(bind=engine)
session = Session()
employee = session.query(Employee).filter_by(first_name=""John"").first()
session.close()

print(""Employee data: "")
print(employee.first_name)
print(employee.last_name)
print(employee.position)
print(employee.salary)",train
SQLAlchemy,19,"Create a Python program that uses the 'SQLAlchemy' API to connect to a MySQL database. Define a 'Employee' entity with attributes like 'id,' 'first_name,' 'last_name,' 'position,' and 'salary.' The program should establish a connection to the MySQL database, create the 'employees' table and add a new employee. Then, retrieve employee details from the database.",code/SQLAlchemy/SQLAlchemy_19.py,Evaluate the program's performance when dealing with a large number of employee records.,"Verify the program's ability to handle employee data with missing or invalid fields, and check if it raises appropriate exceptions.","Test the program by adding multiple employees with different first names, last names, positions, and salaries to the database, and then retrieve their data to ensure it works correctly.",,,"#!pip install SQLAlchemy
from sqlalchemy import create_engine, Column, Integer, String, Float
from sqlalchemy.orm import sessionmaker, declarative_base

# Create a MySQL database connection
engine = create_engine('mysql://username:password@localhost/mydatabase')

# Define the Employee class to represent the ""Employee"" entity
Base = declarative_base()

class Employee(Base):
    __tablename__ = 'employees'
    id = Column(Integer, primary_key=True)
    first_name = Column(String)
    last_name = Column(String)
    position = Column(String)
    salary = Column(Float)

# Create the table in the database
Base.metadata.create_all(engine)

# Function to add a new employee to the database
def add_employee(first_name, last_name, position, salary):
    Session = sessionmaker(bind=engine)
    session = Session()

    employee = Employee(first_name=first_name, last_name=last_name, position=position, salary=salary)
    session.add(employee)
    session.commit()
    session.close()

add_employee(""John"", ""Smith"", ""Manager"", 60000.00)

Session = sessionmaker(bind=engine)
session = Session()
employee = session.query(Employee).filter_by(first_name=""John"").first()
session.close()

print(""Employee data: "")
print(employee.first_name)
print(employee.last_name)
print(employee.position)
print(employee.salary)",test
apeye,1,"Develop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the path, domain, and fully qualified domain name (FQDN) information from the URL.",code/apeye/apeye_1.py,Confirm that the program correctly extracts and prints the fully qualified domain name (FQDN) from the URL.,Ensure that the program correctly extracts and prints the path from the URL.,Verify that the program accurately extracts and prints the domain information.,,,"#!pip install apeye
import apeye

base_url = ""https://google.go.com/file.txt""

client = apeye.URL(base_url)

print(""Path:"")
print(client.path)

print(""\nDomain:"")
print(client.domain.registered_domain)

print(""\nFully Qualified Domain Name:"")
print(client.fqdn)",train
apeye,14,"Develop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the status code, headers, and content of the URL response. Additionally, the program should save the content of the response to a file named 'output.txt' and the headers to a file named 'headers.txt'.",code/apeye/apeye_14.py,Verify that the program accurately extracts and prints the headers of the URL response.,"Confirm that the program correctly extracts and prints the content of the URL response and saves it to a file named ""output.txt"". Additionally, confirm that the program correctly saves the headers to a file named ""headers.txt"".",Ensure that the program correctly extracts and prints the status code of the URL response.,,,"#!pip install apeye
import apeye

base_url = ""https://example.com""

client = apeye.URL(base_url)

response = client.get()

print(""Status Code:"")
print(response.status_code)

print(""\nHeaders:"")
print(response.headers)

print(""\nContent:"")
print(response.content)

# Save content to file
with open('output.txt', 'w') as file:
    file.write(response.content)

# Save headers to file
with open('headers.txt', 'w') as file:
    file.write(str(response.headers))",train
apeye,6,"Develop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the IP address, country, and city information of the server hosting the URL.",code/apeye/apeye_6.py,Confirm that the program correctly extracts and prints the city information of the server hosting the URL.,Verify that the program accurately extracts and prints the country information of the server hosting the URL.,Ensure that the program correctly extracts and prints the IP address of the server hosting the URL.,,,"#!pip install apeye
import apeye

base_url = ""https://example.com""

client = apeye.URL(base_url)

ip_address = client.ip_address
country = client.country
city = client.city

print(""IP Address:"")
print(ip_address)

print(""\nCountry:"")
print(country)

print(""\nCity:"")
print(city)",train
apeye,9,"Develop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the content type, content length, and encoding of the URL response.",code/apeye/apeye_9.py,Ensure that the program correctly extracts and prints the content type of the URL response.,Confirm that the program correctly extracts and prints the encoding of the URL response.,Verify that the program accurately extracts and prints the content length of the URL response.,,,"#!pip install apeye
import apeye

base_url = ""https://example.com""

client = apeye.URL(base_url)

response = client.get()

content_type = response.content_type
content_length = response.content_length
encoding = response.encoding

print(""Content Type:"")
print(content_type)

print(""\nContent Length:"")
print(content_length)

print(""\nEncoding:"")
print(encoding)",train
apeye,3,"Develop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the status code, headers, and content of the URL response.",code/apeye/apeye_3.py,Verify that the program accurately extracts and prints the headers of the URL response.,Confirm that the program correctly extracts and prints the content of the URL response.,Ensure that the program correctly extracts and prints the status code of the URL response.,,,"#!pip install apeye
import apeye

base_url = ""https://example.com""

client = apeye.URL(base_url)

response = client.get()

print(""Status Code:"")
print(response.status_code)

print(""\nHeaders:"")
print(response.headers)

print(""\nContent:"")
print(response.content)",train
apeye,2,"Develop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the scheme, netloc, and query parameters from the URL.",code/apeye/apeye_2.py,Verify that the program accurately extracts and prints the netloc information.,Ensure that the program correctly extracts and prints the scheme from the URL.,Confirm that the program correctly extracts and prints the query parameters from the URL.,,,"#!pip install apeye
import apeye

base_url = ""https://example.com/search?query=python""

client = apeye.URL(base_url)

print(""Scheme:"")
print(client.scheme)

print(""\nNetloc:"")
print(client.netloc)

print(""\nQuery Parameters:"")
print(client.query_params)",train
apeye,15,"Develop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the status code, headers, and content of the URL response. Additionally, the program should save the content of the response to a file named 'output.html' and the headers to a file named 'headers.txt'.",code/apeye/apeye_15.py,Verify that the program accurately extracts and prints the headers of the URL response.,"Confirm that the program correctly extracts and prints the content of the URL response and saves it to a file named ""output.html"". Additionally, confirm that the program correctly saves the headers to a file named ""headers.txt"".",Ensure that the program correctly extracts and prints the status code of the URL response.,,,"#!pip install apeye
import apeye

base_url = ""https://example.com""

client = apeye.URL(base_url)

response = client.get()

print(""Status Code:"")
print(response.status_code)

print(""\nHeaders:"")
print(response.headers)

print(""\nContent:"")
print(response.content)

# Save content to file
with open('output.html', 'w') as file:
    file.write(response.content)

# Save headers to file
with open('headers.txt', 'w') as file:
    file.write(str(response.headers))",train
apeye,5,"Develop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the number of links, images, and forms present in the HTML content of the URL.",code/apeye/apeye_5.py,Ensure that the program correctly extracts and prints the number of links from the HTML content of the URL.,Confirm that the program correctly extracts and prints the number of forms from the HTML content of the URL.,Verify that the program accurately extracts and prints the number of images from the HTML content of the URL.,,,"#!pip install apeye
import apeye
from bs4 import BeautifulSoup

base_url = ""https://example.com""

client = apeye.URL(base_url)

response = client.get()

soup = BeautifulSoup(response.content, 'html.parser')

links = soup.find_all('a')
num_links = len(links)

images = soup.find_all('img')
num_images = len(images)

forms = soup.find_all('form')
num_forms = len(forms)

print(""Number of Links:"")
print(num_links)

print(""\nNumber of Images:"")
print(num_images)

print(""\nNumber of Forms:"")
print(num_forms)",train
apeye,8,"Develop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the SSL certificate information of the URL, including the issuer, subject, and expiration date.",code/apeye/apeye_8.py,Confirm that the program correctly extracts and prints the expiration date of the SSL certificate of the URL.,Ensure that the program correctly extracts and prints the issuer of the SSL certificate of the URL.,Verify that the program accurately extracts and prints the subject of the SSL certificate of the URL.,,,"#!pip install apeye
import apeye

base_url = ""https://example.com""

client = apeye.URL(base_url)

ssl_info = client.ssl_info

issuer = ssl_info.issuer
subject = ssl_info.subject
expiration_date = ssl_info.expiration_date

print(""Issuer:"")
print(issuer)

print(""\nSubject:"")
print(subject)

print(""\nExpiration Date:"")
print(expiration_date)",train
apeye,11,"Develop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the response headers, including the content-type, content-length, and server information.",code/apeye/apeye_11.py,Confirm that the program correctly extracts and prints the server header from the URL response.,Ensure that the program correctly extracts and prints the content-type header from the URL response.,Verify that the program accurately extracts and prints the content-length header from the URL response.,,,"#!pip install apeye
import apeye

base_url = ""https://example.com""

client = apeye.URL(base_url)

response = client.get()

headers = response.headers

content_type = headers.get('content-type')
content_length = headers.get('content-length')
server = headers.get('server')

print(""Content Type:"")
print(content_type)

print(""\nContent Length:"")
print(content_length)

print(""\nServer:"")
print(server)",train
apeye,13,"Develop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the response time, content type, and content length of the URL response. Additionally, the program should save the content of the response to a file named 'output.html'.",code/apeye/apeye_13.py,Verify that the program accurately extracts and prints the content type of the URL response.,"Confirm that the program correctly extracts and prints the content length of the URL response and saves it to a file named ""output.html"".",Ensure that the program correctly extracts and prints the response time of the URL response.,,,"#!pip install apeye
import apeye

base_url = ""https://example.com""

client = apeye.URL(base_url)

response = client.get()

response_time = response.response_time
content_type = response.content_type
content_length = response.content_length

print(""Response Time:"")
print(response_time)

print(""\nContent Type:"")
print(content_type)

print(""\nContent Length:"")
print(content_length)

# Save content to file
with open('output.html', 'w') as file:
    file.write(response.content)",train
apeye,4,"Develop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the title, description, and image URL from the HTML content of the URL.",code/apeye/apeye_4.py,Ensure that the program correctly extracts and prints the title from the HTML content of the URL.,Verify that the program accurately extracts and prints the description from the HTML content of the URL.,Confirm that the program correctly extracts and prints the image URL from the HTML content of the URL.,,,"#!pip install apeye
import apeye
from bs4 import BeautifulSoup

base_url = ""https://example.com""

client = apeye.URL(base_url)

response = client.get()

soup = BeautifulSoup(response.content, 'html.parser')

title = soup.title.string
description = soup.find('meta', attrs={'name': 'description'})['content']
image_url = soup.find('meta', attrs={'property': 'og:image'})['content']

print(""Title:"")
print(title)

print(""\nDescription:"")
print(description)

print(""\nImage URL:"")
print(image_url)",train
apeye,7,"Develop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the number of redirects, the final URL after following redirects, and the response time of the URL.",code/apeye/apeye_7.py,Ensure that the program correctly extracts and prints the number of redirects of the URL.,Verify that the program accurately extracts and prints the final URL after following redirects.,Confirm that the program correctly extracts and prints the response time of the URL.,,,"#!pip install apeye
import apeye

base_url = ""https://example.com""

client = apeye.URL(base_url)

response = client.get()

num_redirects = response.num_redirects
final_url = response.final_url
response_time = response.response_time

print(""Number of Redirects:"")
print(num_redirects)

print(""\nFinal URL:"")
print(final_url)

print(""\nResponse Time:"")
print(response_time)",train
apeye,10,"Develop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the cookies, user agent, and referer information from the URL request.",code/apeye/apeye_10.py,Ensure that the program correctly extracts and prints the cookies from the URL request.,Verify that the program accurately extracts and prints the user agent information from the URL request.,Confirm that the program correctly extracts and prints the referer information from the URL request.,,,"#!pip install apeye
import apeye

base_url = ""https://example.com""

client = apeye.URL(base_url)

cookies = client.cookies
user_agent = client.user_agent
referer = client.referer

print(""Cookies:"")
print(cookies)

print(""\nUser Agent:"")
print(user_agent)

print(""\nReferer:"")
print(referer)",test
apeye,12,"Develop a Python program that utilizes the 'apeye' API to interact with a given URL. The program should extract and print the status code, headers, and content of the URL response. Additionally, the program should save the content of the response to a file named 'output.txt'.",code/apeye/apeye_12.py,Verify that the program accurately extracts and prints the headers of the URL response.,"Confirm that the program correctly extracts and prints the content of the URL response and saves it to a file named ""output.txt"".",Ensure that the program correctly extracts and prints the status code of the URL response.,,,"#!pip install apeye
import apeye

base_url = ""https://example.com""

client = apeye.URL(base_url)

response = client.get()

print(""Status Code:"")
print(response.status_code)

print(""\nHeaders:"")
print(response.headers)

print(""\nContent:"")
print(response.content)

# Save content to file
with open('output.txt', 'w') as file:
    file.write(response.content)",test
bitstring,2,Create a Python program using the 'bitstring' API to perform a binary to decimal conversion. The program should take a binary string as input and convert it to its decimal equivalent. The program should then print the decimal value.,code/bitstring/bitstring_2.py,Test the program with a binary string that contains leading zeros.,Test the program with a binary string that represents a large decimal number.,Test the program with a binary string that represents a small decimal number.,,,"#!pip install bitstring
from bitstring import BitArray

binary = input(""Enter a binary string: "")
decimal = BitArray(bin=binary).uint
print(""Decimal value:"", decimal)",train
bitstring,6,Create a Python program using the 'bitstring' API to perform a bitwise NOT operation on a binary string. The program should take a binary string as input and perform the bitwise NOT operation on it. The program should then print the result of the bitwise NOT operation.,code/bitstring/bitstring_6.py,Test the program with a binary string that contains only 0s.,Test the program with a binary string that contains only 1s.,Test the program with a binary string that contains a mix of 0s and 1s.,,,"#!pip install bitstring
from bitstring import BitArray

binary = input(""Enter a binary string: "")

bitarray = BitArray(bin=binary)

result = ~bitarray

print(""Result of bitwise NOT operation:"", result.bin)",train
bitstring,1,"Create a Python program using the 'bitstring' API to perform a prime number check for a given integer and determine if it's an emirp (a prime number whose reverse is also a prime). The program should print whether the input number is prime, emirp, or not prime.",code/bitstring/bitstring_1.py,Verify that the program accurately determines non-prime numbers.,Ensure that the program correctly identifies prime numbers and emirps.,Confirm that the program handles various input numbers and produces the correct output.,,,"#!pip install bitstring
from bitstring import BitArray
import sys

top = 2000000
b = BitArray(top)  
for i in range(2, top):
    if not b[i]:
        b.set(True, range(i * i, top, i))
while True:
    try:
        n = int(input())
    except:
        break
    if n == 0:
      break
    rev = int(str(n)[::-1])
    if not b[n] and not b[rev]:
        print('%d is emirp.' % n)
    elif not b[n]:
        print('%d is prime.' % n)
    else:
        print('%d is not prime.' % n)",train
bitstring,10,Create a Python program using the 'bitstring' API to perform a bitwise inversion operation on a binary string. The program should take a binary string as input and perform the bitwise inversion operation on it. The program should then print the result of the bitwise inversion operation.,code/bitstring/bitstring_10.py,Test the program with a binary string that contains only 0s.,Test the program with a binary string that contains only 1s.,Test the program with a binary string that contains a mix of 0s and 1s.,,,"#!pip install bitstring
from bitstring import BitArray

binary = input(""Enter a binary string: "")

bitarray = BitArray(bin=binary)

result = ~bitarray

print(""Result of bitwise inversion operation:"", result.bin)",train
bitstring,3,Create a Python program using the 'bitstring' API to perform a bitwise AND operation on two binary strings. The program should take two binary strings as input and perform the bitwise AND operation on them. The program should then print the result of the bitwise AND operation.,code/bitstring/bitstring_3.py,Test the program with two binary strings that have different lengths.,Test the program with two binary strings that have the same length and contain only 1s.,Test the program with two binary strings that have the same length and contain only 0s.,,,"#!pip install bitstring
from bitstring import BitArray

binary1 = input(""Enter the first binary string: "")
binary2 = input(""Enter the second binary string: "")

bitarray1 = BitArray(bin=binary1)
bitarray2 = BitArray(bin=binary2)

result = bitarray1 & bitarray2

print(""Result of bitwise AND operation:"", result.bin)",train
bitstring,5,Create a Python program using the 'bitstring' API to perform a bitwise OR operation on two binary strings. The program should take two binary strings as input and perform the bitwise OR operation on them. The program should then print the result of the bitwise OR operation.,code/bitstring/bitstring_5.py,Test the program with two binary strings that have different lengths.,Test the program with two binary strings that have the same length and contain only 1s.,Test the program with two binary strings that have the same length and contain only 0s.,,,"#!pip install bitstring
from bitstring import BitArray

binary1 = input(""Enter the first binary string: "")
binary2 = input(""Enter the second binary string: "")

bitarray1 = BitArray(bin=binary1)
bitarray2 = BitArray(bin=binary2)

result = bitarray1 | bitarray2

print(""Result of bitwise OR operation:"", result.bin)",train
bitstring,4,Create a Python program using the 'bitstring' API to perform a bitwise XOR operation on two binary strings. The program should take two binary strings as input and perform the bitwise XOR operation on them. The program should then print the result of the bitwise XOR operation.,code/bitstring/bitstring_4.py,Test the program with two binary strings that have different lengths.,Test the program with two binary strings that have the same length and contain only 1s.,Test the program with two binary strings that have the same length and contain only 0s.,,,"#!pip install bitstring
from bitstring import BitArray

binary1 = input(""Enter the first binary string: "")
binary2 = input(""Enter the second binary string: "")

bitarray1 = BitArray(bin=binary1)
bitarray2 = BitArray(bin=binary2)

result = bitarray1 ^ bitarray2

print(""Result of bitwise XOR operation:"", result.bin)",train
bitstring,7,Create a Python program using the 'bitstring' API to perform a bitwise shift operation on a binary string. The program should take a binary string and a shift amount as input and perform the bitwise shift operation on the binary string. The program should then print the result of the bitwise shift operation.,code/bitstring/bitstring_7.py,Test the program with a binary string that contains only 1s and a negative shift amount.,Test the program with a binary string that contains a mix of 0s and 1s and a zero shift amount.,Test the program with a binary string that contains only 0s and a positive shift amount.,,,"#!pip install bitstring
from bitstring import BitArray

binary = input(""Enter a binary string: "")
shift = int(input(""Enter the shift amount: ""))

bitarray = BitArray(bin=binary)

result = bitarray << shift

print(""Result of bitwise shift operation:"", result.bin)",train
bitstring,9,Create a Python program using the 'bitstring' API to perform a bitwise concatenation operation on two binary strings. The program should take two binary strings as input and perform the bitwise concatenation operation on them. The program should then print the result of the bitwise concatenation operation.,code/bitstring/bitstring_9.py,Test the program with two binary strings that have different lengths.,Test the program with two binary strings that have the same length and contain only 1s.,Test the program with two binary strings that have the same length and contain only 0s.,,,"#!pip install bitstring
from bitstring import BitArray

binary1 = input(""Enter the first binary string: "")
binary2 = input(""Enter the second binary string: "")

bitarray1 = BitArray(bin=binary1)
bitarray2 = BitArray(bin=binary2)

result = bitarray1 + bitarray2

print(""Result of bitwise concatenation operation:"", result.bin)",test
category-encoders,1,Create a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using binary encoding and print the resulting data frame.,code/category-encoders/category-encoders_1.py,Verify that the program correctly encodes the categorical data in the 'OUTLOOK' and 'TEMPERATURE' columns using binary encoding and displays the transformed data frame.,Test with a different set of categorical data to ensure the program can handle various inputs and still perform binary encoding accurately.,Test the program's ability to handle larger datasets with more categorical columns to validate its scalability and encoding accuracy.,,,"#!pip install category_encoders
import category_encoders as cat_encoder
import pandas as pd

# creating the dictionary
dictionary = {'OUTLOOK': ['Rainy', 'Rainy',
                          'Overcast', 'Sunny',
                          'Sunny', 'Sunny',
                          'Overcast'],
              'TEMPERATURE': ['Hot', 'Hot', 'Hot',
                          'Mild', 'Cool',
                          'Cool', 'Cool']}

# converting the dictionary to DataFrame
df = pd.DataFrame(dictionary)

# creating an object BinaryEncoder
encoder = cat_encoder.BinaryEncoder(cols = df.columns)

# fitting the columns to a data frame
df_category_encoder = encoder.fit_transform( df )

print(df_category_encoder)",train
category-encoders,14,Create a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using target mean encoding and print the resulting data frame.,code/category-encoders/category-encoders_14.py,Test the program's ability to handle larger datasets with more categorical columns to validate its scalability and encoding accuracy.,Test with a different set of categorical data to ensure the program can handle various inputs and still perform target mean encoding accurately.,Verify that the program correctly encodes the categorical data in the 'OUTLOOK' and 'TEMPERATURE' columns using target mean encoding and displays the transformed data frame.,,,"#!pip install category_encoders
import category_encoders as cat_encoder
import pandas as pd

# creating the dictionary
dictionary = {'OUTLOOK': ['Rainy', 'Rainy',
                          'Overcast', 'Sunny',
                          'Sunny', 'Sunny',
                          'Overcast'],
              'TEMPERATURE': ['Hot', 'Hot', 'Hot',
                          'Mild', 'Cool',
                          'Cool', 'Cool'],
              'PLAY': ['No', 'No', 'Yes',
                       'Yes', 'Yes', 'No',
                       'Yes']}

# converting the dictionary to DataFrame
df = pd.DataFrame(dictionary)

# creating an object TargetMeanEncoder
encoder = cat_encoder.TargetMeanEncoder(cols = ['OUTLOOK', 'TEMPERATURE'])

# fitting the columns to a data frame
df_category_encoder = encoder.fit_transform(df[['OUTLOOK', 'TEMPERATURE']], df['PLAY'])

print(df_category_encoder)",train
category-encoders,6,Create a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using count encoding and print the resulting data frame.,code/category-encoders/category-encoders_6.py,Verify that the program correctly encodes the categorical data in the 'OUTLOOK' and 'TEMPERATURE' columns using count encoding and displays the transformed data frame.,Test the program's ability to handle larger datasets with more categorical columns to validate its scalability and encoding accuracy.,Test with a different set of categorical data to ensure the program can handle various inputs and still perform count encoding accurately.,,,"#!pip install category_encoders
import category_encoders as cat_encoder
import pandas as pd

# creating the dictionary
dictionary = {'OUTLOOK': ['Rainy', 'Rainy',
                          'Overcast', 'Sunny',
                          'Sunny', 'Sunny',
                          'Overcast'],
              'TEMPERATURE': ['Hot', 'Hot', 'Hot',
                          'Mild', 'Cool',
                          'Cool', 'Cool'],
              'PLAY': ['No', 'No', 'Yes',
                       'Yes', 'Yes', 'No',
                       'Yes']}

# converting the dictionary to DataFrame
df = pd.DataFrame(dictionary)

# creating an object CountEncoder
encoder = cat_encoder.CountEncoder(cols = ['OUTLOOK', 'TEMPERATURE'])

# fitting the columns to a data frame
df_category_encoder = encoder.fit_transform(df[['OUTLOOK', 'TEMPERATURE']])

print(df_category_encoder)",train
category-encoders,9,Create a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using helmert encoding and print the resulting data frame.,code/category-encoders/category-encoders_9.py,Verify that the program correctly encodes the categorical data in the 'OUTLOOK' and 'TEMPERATURE' columns using helmert encoding and displays the transformed data frame.,Test the program's ability to handle larger datasets with more categorical columns to validate its scalability and encoding accuracy.,Test with a different set of categorical data to ensure the program can handle various inputs and still perform helmert encoding accurately.,,,"#!pip install category_encoders
import category_encoders as cat_encoder
import pandas as pd

# creating the dictionary
dictionary = {'OUTLOOK': ['Rainy', 'Rainy',
                          'Overcast', 'Sunny',
                          'Sunny', 'Sunny',
                          'Overcast'],
              'TEMPERATURE': ['Hot', 'Hot', 'Hot',
                          'Mild', 'Cool',
                          'Cool', 'Cool'],
              'PLAY': ['No', 'No', 'Yes',
                       'Yes', 'Yes', 'No',
                       'Yes']}

# converting the dictionary to DataFrame
df = pd.DataFrame(dictionary)

# creating an object HelmertEncoder
encoder = cat_encoder.HelmertEncoder(cols = ['OUTLOOK', 'TEMPERATURE'])

# fitting the columns to a data frame
df_category_encoder = encoder.fit_transform(df[['OUTLOOK', 'TEMPERATURE']])

print(df_category_encoder)",train
category-encoders,3,Create a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using ordinal encoding and print the resulting data frame.,code/category-encoders/category-encoders_3.py,Verify that the program correctly encodes the categorical data in the 'OUTLOOK' and 'TEMPERATURE' columns using ordinal encoding and displays the transformed data frame.,Test with a different set of categorical data to ensure the program can handle various inputs and still perform ordinal encoding accurately.,Test the program's ability to handle larger datasets with more categorical columns to validate its scalability and encoding accuracy.,,,"#!pip install category_encoders
import category_encoders as cat_encoder
import pandas as pd

# creating the dictionary
dictionary = {'OUTLOOK': ['Rainy', 'Rainy',
                          'Overcast', 'Sunny',
                          'Sunny', 'Sunny',
                          'Overcast'],
              'TEMPERATURE': ['Hot', 'Hot', 'Hot',
                          'Mild', 'Cool',
                          'Cool', 'Cool']}

# converting the dictionary to DataFrame
df = pd.DataFrame(dictionary)

# creating an object OrdinalEncoder
encoder = cat_encoder.OrdinalEncoder(cols = df.columns)

# fitting the columns to a data frame
df_category_encoder = encoder.fit_transform( df )

print(df_category_encoder)",train
category-encoders,2,Create a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using one-hot encoding and print the resulting data frame.,code/category-encoders/category-encoders_2.py,Test with a different set of categorical data to ensure the program can handle various inputs and still perform one-hot encoding accurately.,Test the program's ability to handle larger datasets with more categorical columns to validate its scalability and encoding accuracy.,Verify that the program correctly encodes the categorical data in the 'OUTLOOK' and 'TEMPERATURE' columns using one-hot encoding and displays the transformed data frame.,,,"#!pip install category_encoders
import category_encoders as cat_encoder
import pandas as pd

# creating the dictionary
dictionary = {'OUTLOOK': ['Rainy', 'Rainy',
                          'Overcast', 'Sunny',
                          'Sunny', 'Sunny',
                          'Overcast'],
              'TEMPERATURE': ['Hot', 'Hot', 'Hot',
                          'Mild', 'Cool',
                          'Cool', 'Cool']}

# converting the dictionary to DataFrame
df = pd.DataFrame(dictionary)

# creating an object OneHotEncoder
encoder = cat_encoder.OneHotEncoder(cols = df.columns)

# fitting the columns to a data frame
df_category_encoder = encoder.fit_transform( df )

print(df_category_encoder)",train
category-encoders,15,Create a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using weight of evidence encoding and print the resulting data frame.,code/category-encoders/category-encoders_15.py,Verify that the program correctly encodes the categorical data in the 'OUTLOOK' and 'TEMPERATURE' columns using weight of evidence encoding and displays the transformed data frame.,Test with a different set of categorical data to ensure the program can handle various inputs and still perform weight of evidence encoding accurately.,Test the program's ability to handle larger datasets with more categorical columns to validate its scalability and encoding accuracy.,,,"#!pip install category_encoders
import category_encoders as cat_encoder
import pandas as pd

# creating the dictionary
dictionary = {'OUTLOOK': ['Rainy', 'Rainy',
                          'Overcast', 'Sunny',
                          'Sunny', 'Sunny',
                          'Overcast'],
              'TEMPERATURE': ['Hot', 'Hot', 'Hot',
                          'Mild', 'Cool',
                          'Cool', 'Cool'],
              'PLAY': ['No', 'No', 'Yes',
                       'Yes', 'Yes', 'No',
                       'Yes']}

# converting the dictionary to DataFrame
df = pd.DataFrame(dictionary)

# creating an object WOEEncoder
encoder = cat_encoder.WOEEncoder(cols = ['OUTLOOK', 'TEMPERATURE'])

# fitting the columns to a data frame
df_category_encoder = encoder.fit_transform(df[['OUTLOOK', 'TEMPERATURE']], df['PLAY'])

print(df_category_encoder)",train
category-encoders,5,Create a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using leave-one-out encoding and print the resulting data frame.,code/category-encoders/category-encoders_5.py,Verify that the program correctly encodes the categorical data in the 'OUTLOOK' and 'TEMPERATURE' columns using leave-one-out encoding and displays the transformed data frame.,Test with a different set of categorical data to ensure the program can handle various inputs and still perform leave-one-out encoding accurately.,Test the program's ability to handle larger datasets with more categorical columns to validate its scalability and encoding accuracy.,,,"#!pip install category_encoders
import category_encoders as cat_encoder
import pandas as pd

# creating the dictionary
dictionary = {'OUTLOOK': ['Rainy', 'Rainy',
                          'Overcast', 'Sunny',
                          'Sunny', 'Sunny',
                          'Overcast'],
              'TEMPERATURE': ['Hot', 'Hot', 'Hot',
                          'Mild', 'Cool',
                          'Cool', 'Cool'],
              'PLAY': ['No', 'No', 'Yes',
                       'Yes', 'Yes', 'No',
                       'Yes']}

# converting the dictionary to DataFrame
df = pd.DataFrame(dictionary)

# creating an object LeaveOneOutEncoder
encoder = cat_encoder.LeaveOneOutEncoder(cols = ['OUTLOOK', 'TEMPERATURE'])

# fitting the columns to a data frame
df_category_encoder = encoder.fit_transform(df['PLAY'], df[['OUTLOOK', 'TEMPERATURE']])

print(df_category_encoder)",train
category-encoders,8,Create a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using backward difference encoding and print the resulting data frame.,code/category-encoders/category-encoders_8.py,Test with a different set of categorical data to ensure the program can handle various inputs and still perform backward difference encoding accurately.,Verify that the program correctly encodes the categorical data in the 'OUTLOOK' and 'TEMPERATURE' columns using backward difference encoding and displays the transformed data frame.,Test the program's ability to handle larger datasets with more categorical columns to validate its scalability and encoding accuracy.,,,"#!pip install category_encoders
import category_encoders as cat_encoder
import pandas as pd

# creating the dictionary
dictionary = {'OUTLOOK': ['Rainy', 'Rainy',
                          'Overcast', 'Sunny',
                          'Sunny', 'Sunny',
                          'Overcast'],
              'TEMPERATURE': ['Hot', 'Hot', 'Hot',
                          'Mild', 'Cool',
                          'Cool', 'Cool'],
              'PLAY': ['No', 'No', 'Yes',
                       'Yes', 'Yes', 'No',
                       'Yes']}

# converting the dictionary to DataFrame
df = pd.DataFrame(dictionary)

# creating an object BackwardDifferenceEncoder
encoder = cat_encoder.BackwardDifferenceEncoder(cols = ['OUTLOOK', 'TEMPERATURE'])

# fitting the columns to a data frame
df_category_encoder = encoder.fit_transform(df[['OUTLOOK', 'TEMPERATURE']])

print(df_category_encoder)",train
category-encoders,11,Create a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using base N encoding and print the resulting data frame.,code/category-encoders/category-encoders_11.py,Verify that the program correctly encodes the categorical data in the 'OUTLOOK' and 'TEMPERATURE' columns using base N encoding and displays the transformed data frame.,Test with a different set of categorical data to ensure the program can handle various inputs and still perform base N encoding accurately.,Test the program's ability to handle larger datasets with more categorical columns to validate its scalability and encoding accuracy.,,,"#!pip install category_encoders
import category_encoders as cat_encoder
import pandas as pd

# creating the dictionary
dictionary = {'OUTLOOK': ['Rainy', 'Rainy',
                          'Overcast', 'Sunny',
                          'Sunny', 'Sunny',
                          'Overcast'],
              'TEMPERATURE': ['Hot', 'Hot', 'Hot',
                          'Mild', 'Cool',
                          'Cool', 'Cool'],
              'PLAY': ['No', 'No', 'Yes',
                       'Yes', 'Yes', 'No',
                       'Yes']}

# converting the dictionary to DataFrame
df = pd.DataFrame(dictionary)

# creating an object BaseNEncoder
encoder = cat_encoder.BaseNEncoder(cols = ['OUTLOOK', 'TEMPERATURE'])

# fitting the columns to a data frame
df_category_encoder = encoder.fit_transform(df[['OUTLOOK', 'TEMPERATURE']])

print(df_category_encoder)",train
category-encoders,13,Create a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using M-estimate encoding and print the resulting data frame.,code/category-encoders/category-encoders_13.py,Verify that the program correctly encodes the categorical data in the 'OUTLOOK' and 'TEMPERATURE' columns using M-estimate encoding and displays the transformed data frame.,Test with a different set of categorical data to ensure the program can handle various inputs and still perform M-estimate encoding accurately.,Test the program's ability to handle larger datasets with more categorical columns to validate its scalability and encoding accuracy.,,,"#!pip install category_encoders
import category_encoders as cat_encoder
import pandas as pd

# creating the dictionary
dictionary = {'OUTLOOK': ['Rainy', 'Rainy',
                          'Overcast', 'Sunny',
                          'Sunny', 'Sunny',
                          'Overcast'],
              'TEMPERATURE': ['Hot', 'Hot', 'Hot',
                          'Mild', 'Cool',
                          'Cool', 'Cool'],
              'PLAY': ['No', 'No', 'Yes',
                       'Yes', 'Yes', 'No',
                       'Yes']}

# converting the dictionary to DataFrame
df = pd.DataFrame(dictionary)

# creating an object MEstimateEncoder
encoder = cat_encoder.MEstimateEncoder(cols = ['OUTLOOK', 'TEMPERATURE'])

# fitting the columns to a data frame
df_category_encoder = encoder.fit_transform(df[['OUTLOOK', 'TEMPERATURE']])

print(df_category_encoder)",train
category-encoders,4,Create a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using target encoding and print the resulting data frame.,code/category-encoders/category-encoders_4.py,Test with a different set of categorical data to ensure the program can handle various inputs and still perform target encoding accurately.,Verify that the program correctly encodes the categorical data in the 'OUTLOOK' and 'TEMPERATURE' columns using target encoding and displays the transformed data frame.,Test the program's ability to handle larger datasets with more categorical columns to validate its scalability and encoding accuracy.,,,"#!pip install category_encoders
import category_encoders as cat_encoder
import pandas as pd

# creating the dictionary
dictionary = {'OUTLOOK': ['Rainy', 'Rainy',
                          'Overcast', 'Sunny',
                          'Sunny', 'Sunny',
                          'Overcast'],
              'TEMPERATURE': ['Hot', 'Hot', 'Hot',
                          'Mild', 'Cool',
                          'Cool', 'Cool'],
              'PLAY': ['No', 'No', 'Yes',
                       'Yes', 'Yes', 'No',
                       'Yes']}

# converting the dictionary to DataFrame
df = pd.DataFrame(dictionary)

# creating an object TargetEncoder
encoder = cat_encoder.TargetEncoder(cols = ['OUTLOOK', 'TEMPERATURE'])

# fitting the columns to a data frame
df_category_encoder = encoder.fit_transform(df['PLAY'], df[['OUTLOOK', 'TEMPERATURE']])

print(df_category_encoder)",train
category-encoders,7,Create a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using hash encoding and print the resulting data frame.,code/category-encoders/category-encoders_7.py,Test with a different set of categorical data to ensure the program can handle various inputs and still perform hash encoding accurately.,Verify that the program correctly encodes the categorical data in the 'OUTLOOK' and 'TEMPERATURE' columns using hash encoding and displays the transformed data frame.,Test the program's ability to handle larger datasets with more categorical columns to validate its scalability and encoding accuracy.,,,"#!pip install category_encoders
import category_encoders as cat_encoder
import pandas as pd

# creating the dictionary
dictionary = {'OUTLOOK': ['Rainy', 'Rainy',
                          'Overcast', 'Sunny',
                          'Sunny', 'Sunny',
                          'Overcast'],
              'TEMPERATURE': ['Hot', 'Hot', 'Hot',
                          'Mild', 'Cool',
                          'Cool', 'Cool'],
              'PLAY': ['No', 'No', 'Yes',
                       'Yes', 'Yes', 'No',
                       'Yes']}

# converting the dictionary to DataFrame
df = pd.DataFrame(dictionary)

# creating an object HashingEncoder
encoder = cat_encoder.HashingEncoder(cols = ['OUTLOOK', 'TEMPERATURE'])

# fitting the columns to a data frame
df_category_encoder = encoder.fit_transform(df[['OUTLOOK', 'TEMPERATURE']])

print(df_category_encoder)",train
category-encoders,10,Create a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using polynomial encoding and print the resulting data frame.,code/category-encoders/category-encoders_10.py,Verify that the program correctly encodes the categorical data in the 'OUTLOOK' and 'TEMPERATURE' columns using polynomial encoding and displays the transformed data frame.,Test with a different set of categorical data to ensure the program can handle various inputs and still perform polynomial encoding accurately.,Test the program's ability to handle larger datasets with more categorical columns to validate its scalability and encoding accuracy.,,,"#!pip install category_encoders
import category_encoders as cat_encoder
import pandas as pd

# creating the dictionary
dictionary = {'OUTLOOK': ['Rainy', 'Rainy',
                          'Overcast', 'Sunny',
                          'Sunny', 'Sunny',
                          'Overcast'],
              'TEMPERATURE': ['Hot', 'Hot', 'Hot',
                          'Mild', 'Cool',
                          'Cool', 'Cool'],
              'PLAY': ['No', 'No', 'Yes',
                       'Yes', 'Yes', 'No',
                       'Yes']}

# converting the dictionary to DataFrame
df = pd.DataFrame(dictionary)

# creating an object PolynomialEncoder
encoder = cat_encoder.PolynomialEncoder(cols = ['OUTLOOK', 'TEMPERATURE'])

# fitting the columns to a data frame
df_category_encoder = encoder.fit_transform(df[['OUTLOOK', 'TEMPERATURE']])

print(df_category_encoder)",test
category-encoders,12,Create a Python program using the 'category-encoders' API to demonstrate category encoding. The program should encode categorical data using James-Stein encoding and print the resulting data frame.,code/category-encoders/category-encoders_12.py,Verify that the program correctly encodes the categorical data in the 'OUTLOOK' and 'TEMPERATURE' columns using James-Stein encoding and displays the transformed data frame.,Test with a different set of categorical data to ensure the program can handle various inputs and still perform James-Stein encoding accurately.,Test the program's ability to handle larger datasets with more categorical columns to validate its scalability and encoding accuracy.,,,"#!pip install category_encoders
import category_encoders as cat_encoder
import pandas as pd

# creating the dictionary
dictionary = {'OUTLOOK': ['Rainy', 'Rainy',
                          'Overcast', 'Sunny',
                          'Sunny', 'Sunny',
                          'Overcast'],
              'TEMPERATURE': ['Hot', 'Hot', 'Hot',
                          'Mild', 'Cool',
                          'Cool', 'Cool'],
              'PLAY': ['No', 'No', 'Yes',
                       'Yes', 'Yes', 'No',
                       'Yes']}

# converting the dictionary to DataFrame
df = pd.DataFrame(dictionary)

# creating an object JamesSteinEncoder
encoder = cat_encoder.JamesSteinEncoder(cols = ['OUTLOOK', 'TEMPERATURE'])

# fitting the columns to a data frame
df_category_encoder = encoder.fit_transform(df[['OUTLOOK', 'TEMPERATURE']])

print(df_category_encoder)",test
chardet,40,"Develop a Python program that reads a text file with unknown encoding, uses the 'chardet' API to detect the encoding, and then tokenizes the text before saving it with the detected encoding.",code/chardet/chardet_40.py,Test the program with different text files of unknown encodings to ensure consistent and accurate encoding detection and text tokenization.,Verify that the program correctly utilizes the chardet API for encoding detection and the NLTK library for text tokenization.,"Create a text file with unknown encoding, ensure that the program successfully detects the encoding, tokenizes the text, and saves it with the detected encoding.",,,"#!pip install chardet
import chardet
from nltk import word_tokenize

with open('unknown_encoding.txt', 'rb') as file:
    rawdata = file.read()

result = chardet.detect(rawdata)
encoding = result['encoding']

decoded_text = rawdata.decode(encoding)

tokens = word_tokenize(decoded_text)

with open('tokens_text.txt', 'w', encoding=encoding) as output_file:
    for token in tokens:
        output_file.write(f""{token}\\n"")",train
chardet,12,Develop a Python program that takes a list of byte strings with different encodings and uses the 'chardet' API to detect the encoding of each byte string in the list. The program should then print the encoding detection results for each byte string.,code/chardet/chardet_12.py,Test the program with additional byte strings to ensure consistent and accurate results.,Provide a list of byte strings with different encodings and confirm that the program accurately detects and prints their encodings.,Verify that the program correctly utilizes the chardet API for encoding detection.,,,"#!pip install chardet
from chardet.universaldetector import UniversalDetector

byte_strings = [
    b""This is a byte string in UTF-8."",
    b""Another byte string in ISO-8859-1."",
    b""A third byte string in EUC-JP."",
]

for byte_string in byte_strings:
    detector = UniversalDetector()
    detector.feed(byte_string)
    detector.close()
    result = detector.result
    print(f""Byte String: {byte_string}"")
    print(f""Detected Encoding: {result['encoding']}"")",train
chardet,10,Develop a Python program that takes a list of text strings with mixed encodings and uses the 'chardet' API to detect the encoding of each string in the list. The program should then print the encoding detection results for each string.,code/chardet/chardet_10.py,Provide a list of text strings with mixed encodings and confirm that the program accurately detects and prints their encodings.,Test the program with additional text strings to ensure consistent and accurate results.,Verify that the program correctly utilizes the chardet API for encoding detection.,,,"#!pip install chardet
from chardet.universaldetector import UniversalDetector

text_list = [""This is a sample text in UTF-8."", ""Another text in ISO-8859-1."", ""A third text in EUC-JP."", ""More text in UTF-16LE.""]

for text in text_list:
    detector = UniversalDetector()
    detector.feed(text.encode('utf-8'))
    detector.close()
    result = detector.result
    print(f""Text: {text}"")
    print(f""Detected Encoding: {result['encoding']}"")",train
chardet,14,"Develop a Python program that reads a text file with a known encoding, uses the 'chardet' API to detect the encoding, and verifies that the detected encoding matches the known encoding.",code/chardet/chardet_14.py,Test the program with different text files of known encodings to ensure consistent and accurate encoding detection and verification.,Create a text file with a known encoding and ensure that the program successfully detects and verifies the encoding.,Verify that the program correctly utilizes the chardet API for encoding detection and comparison.,,,"#!pip install chardet
import chardet

with open('known_encoding.txt', 'rb') as file:
    rawdata = file.read()

result = chardet.detect(rawdata)
detected_encoding = result['encoding']
known_encoding = 'utf-8'  # Replace with the known encoding

print(f""Detected Encoding: {detected_encoding}"")
print(f""Known Encoding: {known_encoding}"")
print(f""Match: {detected_encoding == known_encoding}"")",train
chardet,2,Develop a Python program that reads a file containing text with an unknown encoding and uses the 'chardet' API to detect the encoding. The program should then read and print the text using the detected encoding.,code/chardet/chardet_2.py,Create a sample text file with unknown encoding and ensure that the program successfully detects and reads it.,Test the program with various text files to ensure it consistently detects and reads text with unknown encodings.,Verify that the program accurately reads and prints the text using the detected encoding.,,,"#!pip install chardet
import chardet
with open('sample.txt', 'rb') as file:
    rawdata = file.read()

result = chardet.detect(rawdata)
encoding = result['encoding']

with open('sample.txt', 'r', encoding=encoding) as file:
    text = file.read()

print(text)",train
chardet,36,"Develop a Python program that takes a URL as input, fetches the content from the web page, and uses the 'chardet' API to detect the encoding of the web page. The program should then perform entity recognition on the web page content and print the recognized entities.",code/chardet/chardet_36.py,"Provide different URLs as input and ensure that the program successfully fetches web page content, detects the encoding, and performs entity recognition, printing the recognized entities.",Verify that the program correctly utilizes the chardet API for encoding detection and the spaCy library for entity recognition.,Test the program with various web pages to ensure consistent and accurate encoding detection and entity recognition.,,,"#!pip install chardet
import requests
import chardet
from spacy import load

url = 'https://example.com'  # Replace with the desired URL

response = requests.get(url)
rawdata = response.content
result = chardet.detect(rawdata)
encoding = result['encoding']

decoded_text = rawdata.decode(encoding)

nlp = load(""en_core_web_sm"")
doc = nlp(decoded_text)
entities = [(ent.text, ent.label_) for ent in doc.ents]

for entity, label in entities:
    print(f""{entity}: {label}"")",train
chardet,6,Develop a Python program that reads a directory containing multiple text files with various encodings. Use the 'chardet' API to detect the encoding of each file and print the encoding detection results for each file in the directory.,code/chardet/chardet_6.py,Verify that the program correctly processes the text files using the chardet API.,Test the program with different directories containing text files to ensure consistent and accurate encoding detection.,Create a directory with multiple text files in various encodings and ensure that the program successfully detects and prints the encoding of each file.,,,"#!pip install chardet
import os
import chardet

directory_path = '/path/to/text_files'  # Replace with the directory containing text files

for filename in os.listdir(directory_path):
    file_path = os.path.join(directory_path, filename)

    if os.path.isfile(file_path):
        with open(file_path, 'rb') as file:
            rawdata = file.read()
        result = chardet.detect(rawdata)

        print(f""File: {filename}"")
        print(f""Detected Encoding: {result['encoding']}"")",train
chardet,3,Develop a Python program that takes a list of text strings with different encodings and uses the 'chardet' API to detect the encoding of each string in the list. The program should then print the encoding detection results for each string.,code/chardet/chardet_3.py,Provide a list of text strings with different encodings and confirm that the program accurately detects and prints their encodings.,Test the program with additional text strings to ensure consistent and accurate results.,Verify that the program correctly utilizes the chardet API for encoding detection.,,,"#!pip install chardet
from chardet.universaldetector import UniversalDetector

text_list = [""This is a sample text in UTF-8."", ""Another text in ISO-8859-1."", ""A third text in EUC-JP.""]

for text in text_list:
    detector = UniversalDetector()
    detector.feed(text.encode('utf-8'))
    detector.close()
    result = detector.result
    print(f""Text: {text}"")
    print(f""Detected Encoding: {result['encoding']}"")",train
chardet,13,"Develop a Python program that reads a text file with unknown encoding, uses the 'chardet' API to detect the encoding, and then saves the file with the detected encoding.",code/chardet/chardet_13.py,Create a text file with unknown encoding and ensure that the program successfully detects and saves it with the detected encoding.,Verify that the program correctly utilizes the chardet API for encoding detection and file writing.,Test the program with different text files of unknown encodings to ensure consistent and accurate encoding detection and file saving.,,,"#!pip install chardet
import chardet

with open('unknown_encoding.txt', 'rb') as file:
    rawdata = file.read()

result = chardet.detect(rawdata)
encoding = result['encoding']

with open('output.txt', 'w', encoding=encoding) as output_file:
    output_file.write(rawdata.decode(encoding))",train
chardet,16,"Develop a Python program that reads a text file with unknown encoding, uses the 'chardet' API to detect the encoding, and then processes the text by removing any non-ASCII characters before saving it with the detected encoding.",code/chardet/chardet_16.py,"Verify that the program correctly utilizes the chardet API for encoding detection, text processing, and file writing.",Test the program with different text files of unknown encodings to ensure consistent and accurate encoding detection and text cleaning.,"Create a text file with unknown encoding, ensure that the program successfully detects and processes the text by removing non-ASCII characters, and saves it with the detected encoding.",,,"#!pip install chardet
import chardet

with open('unknown_encoding.txt', 'rb') as file:
    rawdata = file.read()

result = chardet.detect(rawdata)
encoding = result['encoding']

decoded_text = rawdata.decode(encoding)
cleaned_text = ''.join(char for char in decoded_text if char.isascii())

with open('output.txt', 'w', encoding=encoding) as output_file:
    output_file.write(cleaned_text)",train
chardet,4,Develop a Python program that reads a CSV file with text data in various encodings. Use the 'chardet' API to detect the encoding of each column and print the encoding detection results for each column in the CSV file.,code/chardet/chardet_4.py,Create a CSV file with columns containing text data in various encodings and ensure that the program successfully detects and prints the encoding of each column.,Test the program with different CSV files to ensure consistent and accurate encoding detection for various columns.,Verify that the program correctly reads and processes CSV data using the chardet API.,,,"#!pip install chardet
import chardet
import csv

with open('data.csv', 'r', newline='') as csvfile:
    reader = csv.reader(csvfile)
    columns = next(reader)

    for column in columns:
        result = chardet.detect(column.encode())
        print(f""Column: {column}"")
        print(f""Detected Encoding: {result['encoding']}"")",train
chardet,5,"Develop a Python program that takes a URL as input, fetches the content from the web page, and uses the 'chardet' API to detect the encoding of the web page. The program should then print the detected encoding.",code/chardet/chardet_5.py,Test the program with various web pages to ensure consistent and accurate encoding detection.,Verify that the program correctly utilizes the chardet API to detect web page encoding.,Provide different URLs as input and ensure that the program successfully fetches web page content and detects the encoding.,,,"#!pip install chardet
import requests
import chardet

url = 'https://example.com'  # Replace with the desired URL

response = requests.get(url)
rawdata = response.content
result = chardet.detect(rawdata)

print(f""Detected Encoding: {result['encoding']}"")",train
chardet,34,"Develop a Python program that reads a text file with unknown encoding, uses the 'chardet' API to detect the encoding, and then performs entity recognition on the text before saving it with the detected encoding.",code/chardet/chardet_34.py,Test the program with different text files of unknown encodings to ensure consistent and accurate encoding detection and entity recognition.,Verify that the program correctly utilizes the chardet API for encoding detection and the spaCy library for entity recognition.,"Create a text file with unknown encoding, ensure that the program successfully detects the encoding, performs entity recognition, and saves the text with the detected encoding.",,,"#!pip install chardet
import chardet
from spacy import load

with open('unknown_encoding.txt', 'rb') as file:
    rawdata = file.read()

result = chardet.detect(rawdata)
encoding = result['encoding']

decoded_text = rawdata.decode(encoding)

nlp = load(""en_core_web_sm"")
doc = nlp(decoded_text)
entities = [(ent.text, ent.label_) for ent in doc.ents]

with open('entities_text.txt', 'w', encoding=encoding) as output_file:
    for entity, label in entities:
        output_file.write(f""{entity}: {label}\\n"")",train
chardet,18,"Develop a Python program that takes a list of text strings with mixed encodings, uses the 'chardet' API to detect the encoding of each string in the list, and then saves the strings with the detected encodings to a text file.",code/chardet/chardet_18.py,Provide a list of text strings with mixed encodings and ensure that the program successfully detects and saves the strings with the detected encodings to a text file.,Test the program with additional text strings to ensure consistent and accurate results.,Verify that the program correctly utilizes the chardet API for encoding detection and text processing.,,,"#!pip install chardet
from chardet.universaldetector import UniversalDetector

text_list = [""This is a sample text in UTF-8."", ""Another text in ISO-8859-1."", ""A third text in EUC-JP."", ""More text in UTF-16LE.""]

detected_text_list = []
for text in text_list:
    detector = UniversalDetector()
    detector.feed(text.encode('utf-8'))
    detector.close()
    result = detector.result
    encoding = result['encoding']
    detected_text = text.encode(encoding).decode(encoding)
    detected_text_list.append(detected_text)

with open('detected_texts.txt', 'w') as output_file:
    for text in detected_text_list:
        output_file.write(text + '\\n')",train
chardet,35,"Develop a Python program that reads a text file with known encoding, uses the 'chardet' API to detect the encoding, verifies that the detected encoding matches the known encoding, performs entity recognition on the text, and saves the entities with the detected encoding.",code/chardet/chardet_35.py,"Test the program with different text files of known encodings to ensure consistent and accurate encoding detection, verification, and entity recognition.","Verify that the program correctly utilizes the chardet API for encoding detection, comparison, entity recognition, and file writing.","Create a text file with a known encoding, ensure that the program successfully detects the encoding, verifies the match with the known encoding, performs entity recognition, and saves the entities with the detected encoding.",,,"#!pip install chardet
import chardet
from spacy import load

with open('known_encoding.txt', 'rb') as file:
    rawdata = file.read()

result = chardet.detect(rawdata)
detected_encoding = result['encoding']
known_encoding = 'utf-8'  # Replace with the known encoding

decoded_text = rawdata.decode(detected_encoding)

if detected_encoding == known_encoding:
    nlp = load(""en_core_web_sm"")
    doc = nlp(decoded_text)
    entities = [(ent.text, ent.label_) for ent in doc.ents]

    with open('entities_text.txt', 'w', encoding=detected_encoding) as output_file:
        for entity, label in entities:
            output_file.write(f""{entity}: {label}\\n"")",train
chardet,31,"Develop a Python program that reads a text file with unknown encoding, uses the 'chardet' API to detect the encoding, and then checks if the text contains specific keywords or phrases before saving it with the detected encoding.",code/chardet/chardet_31.py,"Create a text file with unknown encoding, ensure that the program successfully detects the encoding, checks for specific keywords or phrases, and saves the text with the detected encoding if the keywords are present.",Test the program with different text files of unknown encodings and various keyword combinations to ensure consistent and accurate encoding detection and keyword checking.,"Verify that the program correctly utilizes the chardet API for encoding detection, keyword checking, and file writing.",,,"#!pip install chardet
import chardet

with open('unknown_encoding.txt', 'rb') as file:
    rawdata = file.read()

result = chardet.detect(rawdata)
encoding = result['encoding']

decoded_text = rawdata.decode(encoding)

keywords = ['important', 'urgent', 'confidential']

contains_keywords = any(keyword in decoded_text for keyword in keywords)

with open('filtered_text.txt', 'w', encoding=encoding) as output_file:
    if contains_keywords:
        output_file.write(decoded_text)",train
chardet,41,"Develop a Python program that reads a text file with known encoding, uses the 'chardet' API to detect the encoding, verifies that the detected encoding matches the known encoding, tokenizes the text, and saves the tokens with the detected encoding.",code/chardet/chardet_41.py,"Create a text file with a known encoding, ensure that the program successfully detects the encoding, verifies the match with the known encoding, tokenizes the text, and saves the tokens with the detected encoding.","Test the program with different text files of known encodings to ensure consistent and accurate encoding detection, verification, and text tokenization.","Verify that the program correctly utilizes the chardet API for encoding detection, comparison, text tokenization, and file writing.",,,"#!pip install chardet
import chardet
from nltk import word_tokenize

with open('known_encoding.txt', 'rb') as file:
    rawdata = file.read()

result = chardet.detect(rawdata)
detected_encoding = result['encoding']
known_encoding = 'utf-8'  # Replace with the known encoding

decoded_text = rawdata.decode(detected_encoding)

if detected_encoding == known_encoding:
    tokens = word_tokenize(decoded_text)

    with open('tokens_text.txt', 'w', encoding=detected_encoding) as output_file:
        for token in tokens:
            output_file.write(f""{token}\\n"")",train
chardet,33,"Develop a Python program that takes a URL as input, fetches the content from the web page, and uses the 'chardet' API to detect the encoding of the web page. The program should then check if the web page content contains specific keywords or phrases before printing the result of the keyword check.",code/chardet/chardet_33.py,Test the program with various web pages and keyword combinations to ensure consistent and accurate encoding detection and keyword checking.,Verify that the program correctly utilizes the chardet API for encoding detection and keyword checking.,"Provide different URLs as input and ensure that the program successfully fetches web page content, detects the encoding, and checks if the content contains specific keywords or phrases, printing the result of the keyword check.",,,"#!pip install chardet
import requests
import chardet

url = 'https://example.com'  # Replace with the desired URL

response = requests.get(url)
rawdata = response.content
result = chardet.detect(rawdata)
encoding = result['encoding']

decoded_text = rawdata.decode(encoding)

keywords = ['important', 'urgent', 'confidential']

contains_keywords = any(keyword in decoded_text for keyword in keywords)

print(f""Contains Keywords: {contains_keywords}"")",train
chardet,8,Develop a Python program that reads a JSON file containing text data in various encodings. Use the 'chardet' API to detect the encoding of each JSON value and print the encoding detection results for each value in the JSON file.,code/chardet/chardet_8.py,Create a JSON file with key-value pairs containing text data in various encodings and ensure that the program successfully detects and prints the encoding of each value.,Test the program with different JSON files to ensure consistent and accurate encoding detection for various values.,Verify that the program correctly reads and processes JSON data using the chardet API.,,,"#!pip install chardet
import chardet
import json

with open('data.json', 'r') as jsonfile:
    data = json.load(jsonfile)

for key, value in data.items():
    result = chardet.detect(value.encode())
    print(f""Key: {key}"")
    print(f""Detected Encoding: {result['encoding']}"")",train
chardet,11,Develop a Python program that reads a text file and uses the 'chardet' API to detect the encoding of the entire file. The program should then encode and decode the text to ensure accuracy.,code/chardet/chardet_11.py,"Verify that the program correctly uses the chardet API for encoding detection, encoding, and decoding.",Test the program with different text files of known encodings to ensure consistent and accurate results.,"Create a text file with known encoding and ensure that the program successfully detects, encodes, and decodes the text accurately.",,,"#!pip install chardet
import chardet

with open('text_file.txt', 'rb') as file:
    rawdata = file.read()

result = chardet.detect(rawdata)
encoding = result['encoding']

encoded_text = rawdata.decode(encoding).encode(encoding)
decoded_text = rawdata.decode(encoding)

print(f""Original Text: {decoded_text}"")
print(f""Encoded and Decoded Text: {encoded_text.decode(encoding)}"")",train
chardet,15,"Develop a Python program that takes a URL as input, fetches the content from the web page, and uses the 'chardet' API to detect the encoding of the web page. The program should then save the web page with the detected encoding to a file.",code/chardet/chardet_15.py,Test the program with various web pages to ensure consistent and accurate encoding detection and file saving.,"Provide different URLs as input and ensure that the program successfully fetches web page content, detects the encoding, and saves the web page with the detected encoding to a file.",Verify that the program correctly utilizes the chardet API for encoding detection and file writing.,,,"#!pip install chardet
import requests
import chardet

url = 'https://example.com'  # Replace with the desired URL

response = requests.get(url)
rawdata = response.content
result = chardet.detect(rawdata)
encoding = result['encoding']

with open('web_page.html', 'wb') as file:
    file.write(rawdata)",train
chardet,32,"Develop a Python program that reads a text file with known encoding, uses the 'chardet' API to detect the encoding, verifies that the detected encoding matches the known encoding, checks if the text contains specific keywords or phrases, and saves the text with the detected encoding if the keywords are present.",code/chardet/chardet_32.py,"Verify that the program correctly utilizes the chardet API for encoding detection, comparison, keyword checking, and file writing.","Test the program with different text files of known encodings and various keyword combinations to ensure consistent and accurate encoding detection, verification, and keyword checking.","Create a text file with a known encoding, ensure that the program successfully detects the encoding, verifies the match with the known encoding, checks for specific keywords or phrases, and saves the text with the detected encoding if the keywords are present.",,,"#!pip install chardet
import chardet

with open('known_encoding.txt', 'rb') as file:
    rawdata = file.read()

result = chardet.detect(rawdata)
detected_encoding = result['encoding']
known_encoding = 'utf-8'  # Replace with the known encoding

decoded_text = rawdata.decode(detected_encoding)

keywords = ['important', 'urgent', 'confidential']

contains_keywords = any(keyword in decoded_text for keyword in keywords)

with open('filtered_text.txt', 'w', encoding=detected_encoding) as output_file:
    if detected_encoding == known_encoding and contains_keywords:
        output_file.write(decoded_text)",train
chardet,7,Develop a Python program that takes a user-input text string and uses the 'chardet' API to detect the encoding of the input. The program should then encode the input text using the detected encoding and print the encoded text.,code/chardet/chardet_7.py,Test the program with various user inputs to ensure consistent and accurate results.,Verify that the program correctly utilizes the chardet API for encoding detection and text encoding.,Enter different text strings as user input and ensure that the program successfully detects the encoding and prints the encoded text.,,,"#!pip install chardet
import chardet

user_input = input(""Enter a text string: "")
result = chardet.detect(user_input.encode())
encoding = result['encoding']

encoded_text = user_input.encode(encoding)
print(f""Encoded Text: {encoded_text.decode(encoding)}"")",train
chardet,9,Develop a Python program that reads a text file containing mixed encodings and uses the 'chardet' API to detect the encoding of each line. The program should then print the encoding detection results for each line in the file.,code/chardet/chardet_9.py,Verify that the program correctly reads and processes the lines using the chardet API.,Test the program with different text files containing mixed encodings to ensure consistent and accurate encoding detection for each line.,Create a text file with multiple lines of mixed encodings and ensure that the program successfully detects and prints the encoding of each line.,,,"#!pip install chardet
import chardet

with open('mixed_encodings.txt', 'r') as file:
    for line in file:
        result = chardet.detect(line.encode())
        print(f""Line: {line.strip()}"")
        print(f""Detected Encoding: {result['encoding']}"")",test
chardet,17,"Develop a Python program that reads a JSON file with text data in various encodings, uses the 'chardet' API to detect the encoding of each value, and then saves the JSON data with the detected encodings to a new file.",code/chardet/chardet_17.py,"Verify that the program correctly utilizes the chardet API for encoding detection, data processing, and file writing.",Test the program with different JSON files to ensure consistent and accurate encoding detection and data saving.,Create a JSON file with key-value pairs containing text data in various encodings and ensure that the program successfully detects and saves the data with the detected encodings to a new file.,,,"#!pip install chardet
import chardet
import json

with open('data.json', 'r') as jsonfile:
    data = json.load(jsonfile)

detected_data = {}
for key, value in data.items():
    result = chardet.detect(value.encode())
    encoding = result['encoding']
    detected_data[key] = value.encode(encoding).decode(encoding)

with open('detected_data.json', 'w') as output_file:
    json.dump(detected_data, output_file, ensure_ascii=False)",test
chardet,1,Develop a Python program that uses the 'chardet' API to detect the encoding of a given text. The program should encode the text in ASCII and use the detector to identify the encoding. Print the result of the encoding detection.,code/chardet/chardet_1.py,Verify that the program correctly encodes the text in ASCII.,Confirm that the program provides the correct encoding result.,Ensure that the program accurately detects the encoding of the provided text.,,,"#!pip install chardet
from chardet.universaldetector import UniversalDetector

text = ""Here is a text with numbers 22 and special charcaters ##%&""
text = text.encode(encoding='ascii',errors='strict')

detector = UniversalDetector()
detector.feed(text)
detector.close()

print(detector.result)",test
click,3,Create a Python program using the 'click' API to build a command-line tool. The program should accept two input options: 'number1' and 'number2'. It should then calculate the sum of 'number1' and 'number2' and print the result.,code/click/click_3.py,Enter '5' as the first number and '3' as the second number. Verify that the program correctly calculates and prints 'The sum is: 8'.,Enter '15' as the first number and '20' as the second number. Ensure that the program computes and prints 'The sum is: 35'.,Enter '10' as the first number and '7' as the second number. Confirm that the program accurately calculates and prints 'The sum is: 17'.,,,"#!pip install click
import click

@click.command()
@click.option('--number1', prompt='Enter the first number: ',
              help='First number.')
@click.option('--number2', prompt='Enter the second number: ',
              help='Second number.')
def main(number1, number2):
  number1 = int(number1)
  number2 = int(number2)
  print(""The sum is:"", number1 + number2)

if __name__ == '__main__':
    main()",test
datacompy,1,"Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on the 'employee_id' column, and produce a comparison report. Ensure that it reports matches and does not ignore extra columns.",code/datacompy/datacompy_1.py,Confirm that the program does not ignore extra columns during comparison.,Verify that the program produces a comparison report.,Ensure that the program correctly compares two dataframes for data matching.,,,"#!pip install datacompy
from io import StringIO
import pandas as pd
import datacompy

data1 = """"""employee_id, name
1, islam mesabah
""""""

data2 = """"""employee_id, name
1, islam mesabah
""""""

df1 = pd.read_csv(StringIO(data1))
df2 = pd.read_csv(StringIO(data2))

compare = datacompy.Compare(
  df1,
  df2,
  # You can also specify a list
  # of columns
  join_columns = 'employee_id',

  # Optional, defaults to 'df1'
  df1_name = 'Original',

  # Optional, defaults to 'df2'
  df2_name = 'New'
  )
compare.matches(ignore_extra_columns = False)
print(compare.report())",test
datacompy,2,"Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on multiple columns ('employee_id' and 'department'), and produce a comparison report. Ensure that it reports matches and ignores extra columns.",code/datacompy/datacompy_2.py,Verify that the program produces a comparison report.,Confirm that the program ignores extra columns during comparison.,Ensure that the program correctly compares two dataframes for data matching based on multiple columns.,,,"#!pip install datacompy
from io import StringIO
import pandas as pd
import datacompy

data1 = """"""employee_id, name, department
1, islam mesabah, IT
2, john doe, HR
""""""

data2 = """"""employee_id, name, department
1, islam mesabah, IT
2, john doe, HR
""""""

df1 = pd.read_csv(StringIO(data1))
df2 = pd.read_csv(StringIO(data2))

compare = datacompy.Compare(
  df1,
  df2,
  # You can also specify a list
  # of columns
  join_columns = ['employee_id', 'department'],

  # Optional, defaults to 'df1'
  df1_name = 'Original',

  # Optional, defaults to 'df2'
  df2_name = 'New'
  )
compare.matches(ignore_extra_columns = True)
print(compare.report())",test
datacompy,3,"Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on a specific column ('employee_id'), and produce a comparison report. Ensure that it reports matches and ignores extra columns.",code/datacompy/datacompy_3.py,Verify that the program produces a comparison report.,Confirm that the program ignores extra columns during comparison.,Ensure that the program correctly compares two dataframes for data matching based on a specific column.,,,"#!pip install datacompy
from io import StringIO
import pandas as pd
import datacompy

data1 = """"""employee_id, name, age
1, islam mesabah, 25
2, john doe, 30
""""""

data2 = """"""employee_id, name, age
1, islam mesabah, 25
2, john doe, 30
""""""

df1 = pd.read_csv(StringIO(data1))
df2 = pd.read_csv(StringIO(data2))

compare = datacompy.Compare(
  df1,
  df2,
  # You can also specify a list
  # of columns
  join_columns = 'employee_id',

  # Optional, defaults to 'df1'
  df1_name = 'Original',

  # Optional, defaults to 'df2'
  df2_name = 'New'
  )
compare.matches(ignore_extra_columns = True)
print(compare.report())",test
datacompy,4,"Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on a specific column ('employee_id'), and produce a comparison report. Ensure that it reports matches and includes extra columns in the comparison.",code/datacompy/datacompy_4.py,Confirm that the program includes extra columns in the comparison.,Verify that the program produces a comparison report.,Ensure that the program correctly compares two dataframes for data matching based on a specific column.,,,"#!pip install datacompy
from io import StringIO
import pandas as pd
import datacompy

data1 = """"""employee_id, name, age
1, islam mesabah, 25
2, john doe, 30
""""""

data2 = """"""employee_id, name, age
1, islam mesabah, 25
2, john doe, 30
""""""

df1 = pd.read_csv(StringIO(data1))
df2 = pd.read_csv(StringIO(data2))

compare = datacompy.Compare(
  df1,
  df2,
  # You can also specify a list
  # of columns
  join_columns = 'employee_id',

  # Optional, defaults to 'df1'
  df1_name = 'Original',

  # Optional, defaults to 'df2'
  df2_name = 'New'
  )
compare.matches(ignore_extra_columns = False)
print(compare.report())",test
datacompy,5,"Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on multiple columns ('employee_id' and 'department'), and produce a comparison report. Ensure that it reports matches and does not ignore extra columns.",code/datacompy/datacompy_5.py,Confirm that the program does not ignore extra columns during comparison.,Verify that the program produces a comparison report.,Ensure that the program correctly compares two dataframes for data matching based on multiple columns.,,,"#!pip install datacompy
from io import StringIO
import pandas as pd
import datacompy

data1 = """"""employee_id, name, department
1, islam mesabah, IT
2, john doe, HR
""""""

data2 = """"""employee_id, name, department
1, islam mesabah, IT
2, john doe, HR
""""""

df1 = pd.read_csv(StringIO(data1))
df2 = pd.read_csv(StringIO(data2))

compare = datacompy.Compare(
  df1,
  df2,
  # You can also specify a list
  # of columns
  join_columns = ['employee_id', 'department'],

  # Optional, defaults to 'df1'
  df1_name = 'Original',

  # Optional, defaults to 'df2'
  df2_name = 'New'
  )
compare.matches(ignore_extra_columns = False)
print(compare.report())",test
datacompy,8,"Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on a specific column ('employee_id'), and produce a comparison report. Ensure that it reports matches and does not ignore extra columns.",code/datacompy/datacompy_8.py,Confirm that the program does not ignore extra columns during comparison.,Verify that the program produces a comparison report.,Ensure that the program correctly compares two dataframes for data matching based on a specific column.,,,"#!pip install datacompy
from io import StringIO
import pandas as pd
import datacompy

data1 = """"""employee_id, name, age
1, islam mesabah, 25
2, john doe, 30
""""""

data2 = """"""employee_id, name, age
1, islam mesabah, 25
2, john doe, 30
""""""

df1 = pd.read_csv(StringIO(data1))
df2 = pd.read_csv(StringIO(data2))

compare = datacompy.Compare(
  df1,
  df2,
  # You can also specify a list
  # of columns
  join_columns = 'employee_id',

  # Optional, defaults to 'df1'
  df1_name = 'Original',

  # Optional, defaults to 'df2'
  df2_name = 'New'
  )
compare.matches(ignore_extra_columns = False)
print(compare.report())",test
datacompy,10,"Develop a Python program that utilizes the 'datacompy' API to compare two dataframes for data matching. The program should load two dataframes, compare them based on multiple columns ('employee_id' and 'department'), and produce a comparison report. Ensure that it reports matches and includes extra columns in the comparison.",code/datacompy/datacompy_10.py,Confirm that the program includes extra columns in the comparison.,Verify that the program produces a comparison report.,Ensure that the program correctly compares two dataframes for data matching based on multiple columns.,,,"#!pip install datacompy
from io import StringIO
import pandas as pd
import datacompy

data1 = """"""employee_id, name, department
1, islam mesabah, IT
2, john doe, HR
""""""

data2 = """"""employee_id, name, department
1, islam mesabah, IT
2, john doe, HR
""""""

df1 = pd.read_csv(StringIO(data1))
df2 = pd.read_csv(StringIO(data2))

compare = datacompy.Compare(
  df1,
  df2,
  # You can also specify a list
  # of columns
  join_columns = ['employee_id', 'department'],

  # Optional, defaults to 'df1'
  df1_name = 'Original',

  # Optional, defaults to 'df2'
  df2_name = 'New'
  )
compare.matches(ignore_extra_columns = False)
print(compare.report())",test
datasets,5,"Develop a Python program using the 'datasets' API to load the WMT16 dataset and retrieve a translation pair (source and target language). The program should load the WMT16 dataset, select a random translation pair, and print the source and target language sentences.",code/datasets/datasets_5.py,Confirm that the program retrieves and prints the source and target language sentences for the selected pair.,Verify that the program successfully loads the WMT16 dataset and selects a random translation pair from the training set.,"Ensure that the program handles different translation pairs correctly, providing a consistent output for each run.",,,"#!pip install datasets
from datasets import load_dataset
import random

# Load WMT16 dataset
dataset = load_dataset(""wmt16"")

# Select a random translation pair
random_index = random.choice(range(len(dataset[""train""][""translation""])))

# Get source and target sentences
source_sentence = dataset[""train""][""translation""][random_index][""en""]
target_sentence = dataset[""train""][""translation""][random_index][""de""]

print(""Source Sentence (English):"", source_sentence)
print(""Target Sentence (German):"", target_sentence)",train
datasets,13,"Develop a Python program using the 'datasets' API to load the TIMIT dataset and extract a random speech signal. The program should load the TIMIT dataset, select a random speech signal, and print the signal's waveform.",code/datasets/datasets_13.py,Confirm that the program retrieves and prints the waveform of the selected random speech signal.,Ensure that the program consistently provides different random speech signals for each run.,Verify that the program successfully loads the TIMIT dataset and selects a random speech signal from the training set.,,,"#!pip install datasets
from datasets import load_dataset
import random

# Load TIMIT dataset
dataset = load_dataset(""timit"")

# Select a random speech signal
random_signal = random.choice(dataset[""train""][""file""])

print(""Random Speech Signal:"")
print(random_signal)",train
datasets,39,"Develop a Python program using the 'datasets' API to load the AdversarialNLI dataset and count the number of unique prompt types in the training set. The program should load the AdversarialNLI dataset, access the training prompt types, and count the unique prompt type labels.",code/datasets/datasets_39.py,Confirm that the program accurately counts the number of unique prompt type labels in the training set.,Verify that the program successfully loads the AdversarialNLI dataset and accesses the training prompt types.,Ensure that the program consistently provides the same count of unique prompt types for the AdversarialNLI training set.,,,"#!pip install datasets
from datasets import load_dataset

# Load AdversarialNLI dataset
dataset = load_dataset(""adversarial_nli"")

# Get the training prompt types
train_prompt_types = dataset[""train""][""prompt""]

# Count unique prompt type labels
unique_prompt_types = set(train_prompt_types)

print(""Number of unique prompt types in the AdversarialNLI training set:"", len(unique_prompt_types))",train
datasets,29,"Develop a Python program using the 'datasets' API to load the Paws-X dataset and count the number of positive and negative sentence pairs in the training set. The program should load the Paws-X dataset, separate positive and negative pairs, and count their occurrences.",code/datasets/datasets_29.py,Confirm that the program accurately counts the number of positive and negative pairs.,Verify that the program successfully loads the Paws-X dataset and separates positive and negative sentence pairs in the training set.,Ensure that the program consistently provides the same counts for positive and negative pairs when run multiple times.,,,"#!pip install datasets
from datasets import load_dataset

# Load Paws-X dataset
dataset = load_dataset(""paws-x"")

# Separate positive and negative sentence pairs
positive_pairs = dataset[""train""].filter(lambda example: example[""label""] == 1)
negative_pairs = dataset[""train""].filter(lambda example: example[""label""] == 0)

print(f""Number of positive sentence pairs: {len(positive_pairs)}"")
print(f""Number of negative sentence pairs: {len(negative_pairs)}"")",train
datasets,41,"Develop a Python program using the 'datasets' API to load the GPT-2 dataset and extract a random text prompt. The program should load the GPT-2 dataset, select a random text prompt, and print its content.",code/datasets/datasets_41.py,Ensure that the program consistently provides different random text prompts for each run.,Confirm that the program retrieves and prints the content of the selected random text prompt.,Verify that the program successfully loads the GPT-2 dataset and selects a random text prompt from the training set.,,,"#!pip install datasets
from datasets import load_dataset
import random

# Load GPT-2 dataset
dataset = load_dataset(""gpt2"")

# Select a random text prompt
random_prompt = random.choice(dataset[""train""][""text""])

print(""Random Text Prompt:"")
print(random_prompt)",train
datasets,7,"Develop a Python program using the 'datasets' API to load the WikiText-2 dataset and extract a random article. The program should load the WikiText-2 dataset, select a random article, and print its content.",code/datasets/datasets_7.py,Verify that the program successfully loads the WikiText-2 dataset and selects a random article from the training set.,Confirm that the program retrieves and prints the content of the selected random article.,Ensure that the program consistently provides different random articles for each run.,,,"#!pip install datasets
from datasets import load_dataset
import random

# Load WikiText-2 dataset
dataset = load_dataset(""wikitext"", ""2"")

# Select a random article
random_article = random.choice(dataset[""train""][""text""])

print(""Random Article:"")
print(random_article)",train
datasets,27,"Develop a Python program using the 'datasets' API to load the SNLI dataset and count the number of unique premises in the training set. The program should load the SNLI dataset, access the training premises, and count the unique premise texts.",code/datasets/datasets_27.py,Verify that the program successfully loads the SNLI dataset and accesses the training premises.,Confirm that the program accurately counts the number of unique premise texts in the training set.,Ensure that the program consistently provides the same count of unique premises for the SNLI training set.,,,"#!pip install datasets
from datasets import load_dataset

# Load SNLI dataset
dataset = load_dataset(""snli"")

# Get the training premises
train_premises = dataset[""train""][""premise""]

# Count unique premise texts
unique_premises = set(train_premises)

print(""Number of unique premises in the SNLI training set:"", len(unique_premises))",train
datasets,10,"Develop a Python program using the 'datasets' API to load the StanfordDogs dataset and list the unique dog breed names. The program should load the StanfordDogs dataset, access the labels, and print the unique dog breed names.",code/datasets/datasets_10.py,Verify that the program successfully loads the StanfordDogs dataset and accesses the label names.,Confirm that the program accurately lists the unique dog breed names in the dataset.,Ensure that the program consistently provides the same unique breed names when run multiple times.,,,"#!pip install datasets
from datasets import load_dataset

# Load StanfordDogs dataset
dataset = load_dataset(""stanford_dogs"")

# Get the unique dog breed names
unique_breeds = set(dataset[""train""][""label_name""])

print(""Unique Dog Breed Names:"")
for breed in unique_breeds:
    print(breed)",train
datasets,14,"Develop a Python program using the 'datasets' API to load the GLUE dataset and count the number of unique tasks in the training set. The program should load the GLUE dataset, access the training tasks, and count the unique task names.",code/datasets/datasets_14.py,Verify that the program successfully loads the GLUE dataset and accesses the training tasks.,Ensure that the program consistently provides the same count of unique tasks for the GLUE training set.,Confirm that the program accurately counts the number of unique task names in the training set.,,,"#!pip install datasets
from datasets import load_dataset

# Load GLUE dataset
dataset = load_dataset(""glue"")

# Get the training tasks
train_tasks = dataset[""train""][""task_name""]

# Count unique task names
unique_tasks = set(train_tasks)

print(""Number of unique tasks in the GLUE training set:"", len(unique_tasks))",train
datasets,33,"Develop a Python program using the 'datasets' API to load the ELI5 dataset and extract a random explanation. The program should load the ELI5 dataset, select a random explanation, and print its content.",code/datasets/datasets_33.py,Verify that the program successfully loads the ELI5 dataset and selects a random explanation from the training set.,Confirm that the program retrieves and prints the content of the selected random explanation.,Ensure that the program consistently provides different random explanations for each run.,,,"#!pip install datasets
from datasets import load_dataset
import random

# Load ELI5 dataset
dataset = load_dataset(""eli5"")

# Select a random explanation
random_explanation = random.choice(dataset[""train""][""document""])

print(""Random Explanation:"")
print(random_explanation)",train
datasets,36,"Develop a Python program using the 'datasets' API to load the SQuAD 2.0 dataset and count the number of paragraphs that do not have an answer. The program should load the SQuAD 2.0 dataset, identify paragraphs without answers, and count their occurrences.",code/datasets/datasets_36.py,Verify that the program successfully loads the SQuAD 2.0 dataset and identifies paragraphs without answers in the validation set.,Ensure that the program consistently provides the same count of no-answer paragraphs for the SQuAD 2.0 dataset when run multiple times.,Confirm that the program accurately counts the number of paragraphs without answers.,,,"#!pip install datasets
from datasets import load_dataset

# Load SQuAD 2.0 dataset
dataset = load_dataset(""squad"", split=""validation"")

# Count the number of paragraphs without answers
no_answer_paragraphs = dataset[""context""].filter(lambda context: all(len(answer[""text""]) == 0 for answer in context[""answers""]))

print(""Number of paragraphs without answers in SQuAD 2.0:"", len(no_answer_paragraphs))",train
datasets,9,"Develop a Python program using the 'datasets' API to load the IWSLT dataset and retrieve a random translation pair (source and target language). The program should load the IWSLT dataset, select a random translation pair, and print the source and target language sentences.",code/datasets/datasets_9.py,Confirm that the program retrieves and prints the source and target language sentences for the selected pair.,Verify that the program successfully loads the IWSLT dataset and selects a random translation pair from the training set.,"Ensure that the program handles different translation pairs correctly, providing a consistent output for each run.",,,"#!pip install datasets
from datasets import load_dataset
import random

# Load IWSLT dataset
dataset = load_dataset(""iwslt"")

# Select a random translation pair
random_index = random.choice(range(len(dataset[""train""][""translation""])))

# Get source and target sentences
source_sentence = dataset[""train""][""translation""][random_index][""en""]
target_sentence = dataset[""train""][""translation""][random_index][""de""]

print(""Source Sentence (English):"", source_sentence)
print(""Target Sentence (German):"", target_sentence)",train
datasets,19,"Develop a Python program using the 'datasets' API to load the ParaCrawl dataset and retrieve a random translated sentence pair (source and target language). The program should load the ParaCrawl dataset, select a random translation pair, and print the source and target language sentences.",code/datasets/datasets_19.py,Verify that the program successfully loads the ParaCrawl dataset and selects a random translation pair from the training set.,Confirm that the program retrieves and prints the source and target language sentences for the selected pair.,"Ensure that the program handles different translation pairs correctly, providing a consistent output for each run.",,,"#!pip install datasets
from datasets import load_dataset
import random

# Load ParaCrawl dataset
dataset = load_dataset(""paracrawl"")

# Select a random translation pair
random_index = random.choice(range(len(dataset[""train""][""translation""])))

# Get source and target sentences
source_sentence = dataset[""train""][""translation""][random_index][""en""]
target_sentence = dataset[""train""][""translation""][random_index][""de""]

print(""Source Sentence (English):"", source_sentence)
print(""Target Sentence (German):"", target_sentence)",train
datasets,26,"Develop a Python program using the 'datasets' API to load the Quora Question Pairs dataset and count the number of duplicate question pairs in the training set. The program should load the Quora Question Pairs dataset, identify duplicate question pairs, and count their occurrences.",code/datasets/datasets_26.py,Verify that the program successfully loads the Quora Question Pairs dataset and identifies duplicate question pairs in the training set.,Confirm that the program accurately counts the number of duplicate question pairs.,Ensure that the program consistently provides the same count of duplicate pairs for the Quora Question Pairs training set when run multiple times.,,,"#!pip install datasets
from datasets import load_dataset

# Load Quora Question Pairs dataset
dataset = load_dataset(""quora"")

# Count the number of duplicate question pairs
def count_duplicate_pairs(examples):
    question_pairs = [(example[""question1""], example[""question2""]) for example in examples]
    unique_pairs = set(question_pairs)
    duplicate_count = len(question_pairs) - len(unique_pairs)
    return duplicate_count

duplicate_count = count_duplicate_pairs(dataset[""train""])

print(""Number of duplicate question pairs in the Quora Question Pairs training set:"", duplicate_count)",train
datasets,1,Develop a Python program using the 'datasets' API to load the IMDb dataset and process it with a BERT tokenizer from the 'transformers' API. The program should tokenize both the training and testing data from the IMDb dataset using the BERT tokenizer and print the processed data.,code/datasets/datasets_1.py,"Validate that the program effectively tokenizes the text data using the BERT tokenizer, ensuring that padding and truncation are applied correctly.",Confirm that the program successfully loads the IMDb dataset and retrieves the training and testing splits.,Check that the processed training and testing data is correctly printed and ready for use in natural language processing tasks.,,,"#!pip install datasets transformers
from datasets import load_dataset
from transformers import AutoTokenizer

dataset = load_dataset(""imdb"")

tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")

def tokenize_batch(batch):
    return tokenizer(batch[""text""], padding=""max_length"", truncation=True)

train_data = dataset[""train""].map(tokenize_batch, batched=True)
test_data = dataset[""test""].map(tokenize_batch, batched=True)

print(""Trainig and Testing data: "")
print(train_data)
print(test_data)",train
datasets,35,"Develop a Python program using the 'datasets' API to load the BookCorpus dataset and extract a random book summary. The program should load the BookCorpus dataset, select a random book summary, and print its content.",code/datasets/datasets_35.py,Verify that the program successfully loads the BookCorpus dataset and selects a random book summary from the training set.,Ensure that the program consistently provides different random book summaries for each run.,Confirm that the program retrieves and prints the content of the selected random book summary.,,,"#!pip install datasets
from datasets import load_dataset
import random

# Load BookCorpus dataset
dataset = load_dataset(""bookcorpus"")

# Select a random book summary
random_summary = random.choice(dataset[""train""][""text""])

print(""Random Book Summary:"")
print(random_summary)",train
datasets,6,"Develop a Python program using the 'datasets' API to load the Iris dataset and calculate the mean sepal length for a specific species. The program should load the Iris dataset, select a specific species, and calculate the mean sepal length for that species.",code/datasets/datasets_6.py,Ensure that the program handles different species correctly and provides a consistent mean sepal length for each run.,"Confirm that the program successfully loads the Iris dataset and selects a specific species for analysis (e.g., ""setosa"").",Verify that the program accurately calculates the mean sepal length for the selected species.,,,"#!pip install datasets
from datasets import load_dataset

# Load Iris dataset
dataset = load_dataset(""iris"")

# Select a specific species (e.g., ""setosa"")
species = ""setosa""

# Filter data for the selected species
species_data = dataset[""train""].filter(lambda example: example[""species""] == species)

# Calculate the mean sepal length for the species
sepal_lengths = [example[""sepal_length""] for example in species_data]
mean_sepal_length = sum(sepal_lengths) / len(sepal_lengths)

print(f""Mean Sepal Length for {species}: {mean_sepal_length:.2f}"")",train
datasets,12,"Develop a Python program using the 'datasets' API to load the MultiNLI dataset and count the number of unique genres in the training set. The program should load the MultiNLI dataset, access the training genres, and count the unique genre labels.",code/datasets/datasets_12.py,Verify that the program successfully loads the MultiNLI dataset and accesses the training genres.,Confirm that the program accurately counts the number of unique genre labels in the training set.,Ensure that the program consistently provides the same count of unique genres for the MultiNLI training set.,,,"#!pip install datasets
from datasets import load_dataset

# Load MultiNLI dataset
dataset = load_dataset(""multi_nli"")

# Get the training genres
train_genres = dataset[""train""][""genre""]

# Count unique genre labels
unique_genres = set(train_genres)

print(""Number of unique genres in the MultiNLI training set:"", len(unique_genres))",train
datasets,2,"Develop a Python program using the 'datasets' API to load the COCO dataset and extract image captions for a specific image ID. The program should load the COCO dataset, select a random image ID, and retrieve the corresponding image captions. Finally, it should print the image ID and its associated captions.",code/datasets/datasets_2.py,Verify that the program retrieves and prints the image captions for the selected image ID.,Confirm that the program successfully loads the COCO dataset and selects a random image ID from the training set.,"Ensure that the program handles different image IDs and captions correctly, providing a consistent output for each run.",,,"#!pip install datasets
from datasets import load_dataset
import random

# Load COCO dataset
dataset = load_dataset(""coco"", ""2017"")

# Select a random image ID
random_image_id = random.choice(range(len(dataset[""train""])))

# Get captions for the selected image
captions = dataset[""train""][random_image_id][""captions""]

print(f""Image ID: {random_image_id}"")
print(""Captions:"")
for caption in captions:
    print(caption)",train
datasets,31,"Develop a Python program using the 'datasets' API to load the OpenWebText dataset and extract a random web page document. The program should load the OpenWebText dataset, select a random web page document, and print its content.",code/datasets/datasets_31.py,Verify that the program successfully loads the OpenWebText dataset and selects a random web page document from the training set.,Ensure that the program consistently provides different random web page documents for each run.,Confirm that the program retrieves and prints the content of the selected random web page document.,,,"#!pip install datasets
from datasets import load_dataset
import random

# Load OpenWebText dataset
dataset = load_dataset(""openwebtext"")

# Select a random web page document
random_document = random.choice(dataset[""train""][""text""])

print(""Random Web Page Document:"")
print(random_document)",train
datasets,23,"Develop a Python program using the 'datasets' API to load the TREC dataset and count the number of unique question types in the training set. The program should load the TREC dataset, access the training question types, and count the unique question type labels.",code/datasets/datasets_23.py,Verify that the program successfully loads the TREC dataset and accesses the training question types.,Confirm that the program accurately counts the number of unique question type labels in the training set.,Ensure that the program consistently provides the same count of unique question types for the TREC training set.,,,"#!pip install datasets
from datasets import load_dataset

# Load TREC dataset
dataset = load_dataset(""trec"")

# Get the training question types
train_question_types = dataset[""train""][""question_type""]

# Count unique question type labels
unique_question_types = set(train_question_types)

print(""Number of unique question types in the TREC training set:"", len(unique_question_types))",train
datasets,3,"Develop a Python program using the 'datasets' API to load the SQuAD dataset and extract a sample question and its corresponding answer span. The program should load the SQuAD dataset, select a random question, and print the question text along with its answer span.",code/datasets/datasets_3.py,Verify that the program successfully loads the SQuAD dataset and selects a random question from the training set.,Confirm that the program retrieves and prints the question text along with its corresponding answer span.,"Ensure that the program handles different questions and answer spans correctly, providing a consistent output for each run.",,,"#!pip install datasets
from datasets import load_dataset
import random

# Load SQuAD dataset
dataset = load_dataset(""squad"")

# Select a random question
random_question = random.choice(dataset[""train""][""question""])

# Get the answer span for the selected question
answer_span = dataset[""train""][""answers""][""text""][random_question]

print(""Question: "", random_question)
print(""Answer Span: "", answer_span)",train
datasets,32,"Develop a Python program using the 'datasets' API to load the BERT dataset and count the number of unique token types in the training set. The program should load the BERT dataset, access the training token types, and count the unique token types.",code/datasets/datasets_32.py,Confirm that the program accurately counts the number of unique token types in the training set.,Ensure that the program consistently provides the same count of unique token types for the BERT training set.,Verify that the program successfully loads the BERT dataset and accesses the training token types.,,,"#!pip install datasets
from datasets import load_dataset

# Load BERT dataset
dataset = load_dataset(""bert"")

# Get the training token types
train_token_types = dataset[""train""][""token_type_ids""]

# Count unique token types
unique_token_types = set(train_token_types)

print(""Number of unique token types in the BERT training set:"", len(unique_token_types))",train
datasets,38,"Develop a Python program using the 'datasets' API to load the WMT16 dataset and retrieve a random translated sentence pair (source and target language). The program should load the WMT16 dataset, select a random translation pair, and print the source and target language sentences.",code/datasets/datasets_38.py,Confirm that the program retrieves and prints the source and target language sentences for the selected pair.,Verify that the program successfully loads the WMT16 dataset and selects a random translation pair from the training set.,"Ensure that the program handles different translation pairs correctly, providing a consistent output for each run.",,,"#!pip install datasets
from datasets import load_dataset
import random

# Load WMT16 dataset
dataset = load_dataset(""wmt16"", ""en-de"")

# Select a random translation pair
random_index = random.choice(range(len(dataset[""train""][""translation""])))

# Get source and target sentences
source_sentence = dataset[""train""][""translation""][random_index][""en""]
target_sentence = dataset[""train""][""translation""][random_index][""de""]

print(""Source Sentence (English):"", source_sentence)
print(""Target Sentence (German):"", target_sentence)",train
datasets,4,"Develop a Python program using the 'datasets' API to load the MNLI dataset and count the number of unique labels in the training set. The program should load the MNLI dataset, access the training labels, and count the unique label classes.",code/datasets/datasets_4.py,Ensure that the program consistently provides the same count of unique labels for the MNLI training set.,Verify that the program successfully loads the MNLI dataset and accesses the training labels.,Confirm that the program accurately counts the number of unique label classes in the training set.,,,"#!pip install datasets
from datasets import load_dataset

# Load MNLI dataset
dataset = load_dataset(""mnli"")

# Get the training labels
train_labels = dataset[""train""][""label""]

# Count unique label classes
unique_labels = set(train_labels)

print(""Number of unique labels in the MNLI training set:"", len(unique_labels))",train
datasets,37,"Develop a Python program using the 'datasets' API to load the Waymo Open Dataset and extract a random sensor data record. The program should load the Waymo Open Dataset, select a random sensor data record, and print its content.",code/datasets/datasets_37.py,Verify that the program successfully loads the Waymo Open Dataset and selects a random sensor data record from the training set.,Ensure that the program consistently provides different random sensor data records for each run.,Confirm that the program retrieves and prints the content of the selected random sensor data record.,,,"#!pip install datasets
from datasets import load_dataset
import random

# Load Waymo Open Dataset
dataset = load_dataset(""waymo_open_dataset"")

# Select a random sensor data record


random_sensor_data = random.choice(dataset[""train""][""sample""][""data""])

print(""Random Sensor Data Record:"")
print(random_sensor_data)",train
datasets,25,"Develop a Python program using the 'datasets' API to load the COCO dataset and count the number of images with a specific annotation category. The program should load the COCO dataset, select a specific annotation category (e.g., ""person""), and count the number of images containing that category.",code/datasets/datasets_25.py,Confirm that the program accurately counts the number of images containing the selected category.,"Verify that the program successfully loads the COCO dataset and selects a specific annotation category (e.g., ""person"").",Ensure that the program consistently provides the same count of images with the specified category when run multiple times.,,,"#!pip install datasets
from datasets import load_dataset

# Load COCO dataset
dataset = load_dataset(""coco"", ""2017"")

# Select a specific annotation category
annotation_category = ""person""

# Count images with the category
images_with_category = dataset[""train""].filter(lambda example: annotation_category in example[""annotations""][""category""])

print(f""Number of images with {annotation_category} category:"", len(images_with_category))",train
datasets,34,"Develop a Python program using the 'datasets' API to load the SQAD dataset and extract a random question and its corresponding answer span. The program should load the SQAD dataset, select a random question, and print the question text along with its answer span.",code/datasets/datasets_34.py,Confirm that the program retrieves and prints the question text along with its corresponding answer span.,"Ensure that the program handles different questions and answer spans correctly, providing a consistent output for each run.",Verify that the program successfully loads the SQAD dataset and selects a random question from the training set.,,,"#!pip install datasets
from datasets import load_dataset
import random

# Load SQAD dataset
dataset = load_dataset(""squad"")

# Select a random question
random_question = random.choice(dataset[""train""][""question""])

# Get the answer span for the selected question
answer_span = dataset[""train""][""answers""][""text""][random_question]

print(""Question: "", random_question)
print(""Answer Span: "", answer_span)",train
datasets,11,"Develop a Python program using the 'datasets' API to load the C4 dataset and count the number of unique languages in the training set. The program should load the C4 dataset, access the training languages, and count the unique language codes.",code/datasets/datasets_11.py,Ensure that the program consistently provides the same count of unique languages for the C4 training set.,Confirm that the program accurately counts the number of unique language codes in the training set.,Verify that the program successfully loads the C4 dataset and accesses the training languages.,,,"#!pip install datasets
from datasets import load_dataset

# Load C4 dataset
dataset = load_dataset(""c4"")

# Get the training languages
train_languages = dataset[""train""][""language""]

# Count unique language codes
unique_languages = set(train_languages)

print(""Number of unique languages in the C4 training set:"", len(unique_languages))",train
datasets,24,"Develop a Python program using the 'datasets' API to load the BigPatent dataset and retrieve a random patent document. The program should load the BigPatent dataset, select a random patent document, and print its content.",code/datasets/datasets_24.py,Verify that the program successfully loads the BigPatent dataset and selects a random patent document from the training set.,Confirm that the program retrieves and prints the content of the selected random patent document.,Ensure that the program consistently provides different random patent documents for each run.,,,"#!pip install datasets
from datasets import load_dataset
import random

# Load BigPatent dataset
dataset = load_dataset(""big_patent"")

# Select a random patent document
random_document = random.choice(dataset[""train""][""text""])

print(""Random Patent Document:"")
print(random_document)",train
datasets,20,"Develop a Python program using the 'datasets' API to load the Yelp Polarity dataset and calculate the average length of positive and negative reviews. The program should load the Yelp Polarity dataset, separate positive and negative reviews, and calculate the average review length for each category.",code/datasets/datasets_20.py,Confirm that the program accurately calculates the average review length for positive and negative reviews.,Ensure that the program consistently provides the same average lengths for each category when run multiple times.,Verify that the program successfully loads the Yelp Polarity dataset and separates positive and negative reviews.,,,"#!pip install datasets
from datasets import load_dataset

# Load Yelp Polarity dataset
dataset = load_dataset(""yelp_polarity"")

# Separate positive and negative reviews
positive_reviews = dataset[""train""].filter(lambda example: example[""label""] == 2)
negative_reviews = dataset[""train""].filter(lambda example: example[""label""] == 1)

# Calculate the average review length for each category
def calculate_average_length(reviews):
    total_length = sum(len(example[""content""]) for example in reviews)
    return total_length / len(reviews)

average_length_positive = calculate_average_length(positive_reviews)
average_length_negative = calculate_average_length(negative_reviews)

print(""Average Length of Positive Reviews:"", average_length_positive)
print(""Average Length of Negative Reviews:"", average_length_negative)",train
datasets,22,"Develop a Python program using the 'datasets' API to load the OpenSubtitles dataset and extract a random subtitle. The program should load the OpenSubtitles dataset, select a random subtitle, and print its content.",code/datasets/datasets_22.py,Verify that the program successfully loads the OpenSubtitles dataset and selects a random subtitle from the training set.,Confirm that the program retrieves and prints the content of the selected random subtitle.,Ensure that the program consistently provides different random subtitles for each run.,,,"#!pip install datasets
from datasets import load_dataset
import random

# Load OpenSubtitles dataset
dataset = load_dataset(""opensubtitles"")

# Select a random subtitle
random_subtitle = random.choice(dataset[""train""][""sentence""])

print(""Random Subtitle:"")
print(random_subtitle)",train
datasets,8,"Develop a Python program using the 'datasets' API to load the AmazonPolarity dataset and calculate the average length of positive and negative reviews. The program should load the AmazonPolarity dataset, separate positive and negative reviews, and calculate the average review length for each category.",code/datasets/datasets_8.py,Verify that the program successfully loads the AmazonPolarity dataset and separates positive and negative reviews.,Ensure that the program consistently provides the same average lengths for each category when run multiple times.,Confirm that the program accurately calculates the average review length for positive and negative reviews.,,,"#!pip install datasets
from datasets import load_dataset

# Load AmazonPolarity dataset
dataset = load_dataset(""amazon_polarity"")

# Separate positive and negative reviews
positive_reviews = dataset[""train""].filter(lambda example: example[""label""] == 2)
negative_reviews = dataset[""train""].filter(lambda example: example[""label""] == 1)

# Calculate the average review length for each category
def calculate_average_length(reviews):
    total_length = sum(len(example[""content""]) for example in reviews)
    return total_length / len(reviews)

average_length_positive = calculate_average_length(positive_reviews)
average_length_negative = calculate_average_length(negative_reviews)

print(""Average Length of Positive Reviews:"", average_length_positive)
print(""Average Length of Negative Reviews:"", average_length_negative)",train
datasets,15,"Develop a Python program using the 'datasets' API to load the EuroParl dataset and retrieve a random translated sentence pair (source and target language). The program should load the EuroParl dataset, select a random translation pair, and print the source and target language sentences.",code/datasets/datasets_15.py,Confirm that the program retrieves and prints the source and target language sentences for the selected pair.,Verify that the program successfully loads the EuroParl dataset and selects a random translation pair from the training set.,"Ensure that the program handles different translation pairs correctly, providing a consistent output for each run.",,,"#!pip install datasets
from datasets import load_dataset
import random

# Load EuroParl dataset
dataset = load_dataset(""europarl"", ""fr-en"")

# Select a random translation pair
random_index = random.choice(range(len(dataset[""train""][""translation""])))

# Get source and target sentences
source_sentence = dataset[""train""][""translation""][random_index][""fr""]
target_sentence = dataset[""train""][""translation""][random_index][""en""]

print(""Source Sentence (French):"", source_sentence)
print(""Target Sentence (English):"", target_sentence)",train
datasets,30,"Develop a Python program using the 'datasets' API to load the ChIP-Seq dataset and retrieve a random sequence. The program should load the ChIP-Seq dataset, select a random sequence, and print its content.",code/datasets/datasets_30.py,Confirm that the program retrieves and prints the content of the selected random sequence.,Ensure that the program consistently provides different random sequences for each run.,Verify that the program successfully loads the ChIP-Seq dataset and selects a random sequence from the training set.,,,"#!pip install datasets
from datasets import load_dataset
import random

# Load ChIP-Seq dataset
dataset = load_dataset(""chip_seq"")

# Select a random sequence
random_sequence = random.choice(dataset[""train""][""sequence""])

print(""Random Sequence:"")
print(random_sequence)",train
datasets,40,"Develop a Python program using the 'datasets' API to load the TREC dataset and count the number of unique labels in the training set. The program should load the TREC dataset, access the training labels, and count the unique label names.",code/datasets/datasets_40.py,Ensure that the program consistently provides the same count of unique labels for the TREC training set.,Verify that the program successfully loads the TREC dataset and accesses the training labels.,Confirm that the program accurately counts the number of unique label names in the training set.,,,"#!pip install datasets
from datasets import load_dataset

# Load TREC dataset
dataset = load_dataset(""trec"")

# Get the training labels
train_labels = dataset[""train""][""label""]

# Count unique label names
unique_labels = set(train_labels)

print(""Number of unique labels in the TREC training set:"", len(unique_labels))",train
datasets,21,"Develop a Python program using the 'datasets' API to load the SNLI dataset and count the number of unique genres in the training set. The program should load the SNLI dataset, access the training genres, and count the unique genre labels.",code/datasets/datasets_21.py,Ensure that the program consistently provides the same count of unique genres for the SNLI training set.,Confirm that the program accurately counts the number of unique genre labels in the training set.,Verify that the program successfully loads the SNLI dataset and accesses the training genres.,,,"#!pip install datasets
from datasets import load_dataset

# Load SNLI dataset
dataset = load_dataset(""snli"")

# Get the training genres
train_genres = dataset[""train""][""genre""]

# Count unique genre labels
unique_genres = set(train_genres)

print(""Number of unique genres in the SNLI training set:"", len(unique_genres))",test
datasets,17,"Develop a Python program using the 'datasets' API to load the SAMSum dataset and extract a random conversation. The program should load the SAMSum dataset, select a random conversation, and print its content.",code/datasets/datasets_17.py,Ensure that the program consistently provides different random conversations for each run.,"Confirm that the program retrieves and prints the content of the selected random conversation, including all messages.",Verify that the program successfully loads the SAMSum dataset and selects a random conversation from the training set.,,,"#!pip install datasets
from datasets import load_dataset
import random

# Load SAMSum dataset
dataset = load_dataset(""samsum"")

# Select a random conversation
random_conversation = random.choice(dataset[""train""][""dialog""])

print(""Random Conversation:"")
for message in random_conversation:
    print(message)",test
datasets,16,"Develop a Python program using the 'datasets' API to load the WikiText-103 dataset and extract a random article. The program should load the WikiText-103 dataset, select a random article, and print its content.",code/datasets/datasets_16.py,Verify that the program successfully loads the WikiText-103 dataset and selects a random article from the training set.,Ensure that the program consistently provides different random articles for each run.,Confirm that the program retrieves and prints the content of the selected random article.,,,"#!pip install datasets
from datasets import load_dataset
import random

# Load WikiText-103 dataset
dataset = load_dataset(""wikitext"", ""103"")

# Select a random article
random_article = random.choice(dataset[""train""][""text""])

print(""Random Article:"")
print(random_article)",test
datasets,28,"Develop a Python program using the 'datasets' API to load the TED MultiTranslation dataset and retrieve a random translated speech. The program should load the TED MultiTranslation dataset, select a random translated speech, and print the source and target language transcripts.",code/datasets/datasets_28.py,"Ensure that the program handles different translated speeches correctly, providing a consistent output for each run.",Verify that the program successfully loads the TED MultiTranslation dataset and selects a random translated speech from the training set.,Confirm that the program retrieves and prints the source and target language transcripts for the selected speech.,,,"#!pip install datasets
from datasets import load_dataset
import random

# Load TED MultiTranslation dataset
dataset = load_dataset(""ted_multi"")

# Select a random translated speech
random_speech = random.choice(dataset[""train""][""translation""])

source_language = random_speech[""en""]
target_language = random_speech[""fr""]

print(f""Source Transcript (English): {source_language}"")
print(f""Target Transcript (French): {target_language}"")",test
diffusers,1,"Write a Python program that utilizes the 'diffusers' API to generate an image based on the provided prompt, 'a photo of an astronaut riding a horse on the moon.'",code/diffusers/diffusers_1.py,"Verify that the saved image (""astronaut_rides_horse.png"") exists and can be opened.",Test the execution time of the code to ensure it's within reasonable limits.,Test whether the code successfully generates an image based on the provided prompt.,,,"#!pip install diffusers
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    ""CompVis/stable-diffusion-v1-3-diffusers"",
    use_auth_token=True,
    revision=""fp16"",
    torch_dtype=torch.float16
)
pipe = pipe.to(""cpu"")

prompt = ""a photo of an astronaut riding a horse on moon""
image = pipe(prompt)[""sample""][0]

image.save(""astronaut_rides_horse.png"")",train
diffusers,14,"Write a Python program that utilizes the 'diffusers' API to generate a list of possible completions for the provided prompt, 'What are the benefits of meditation?'",code/diffusers/diffusers_14.py,Test whether the code successfully generates a list of possible completions for the provided prompt.,"Verify that the output is a list of valid completions for the prompt ""What are the benefits of meditation?""",Test the execution time of the code to ensure it's within reasonable limits.,,,"#!pip install diffusers
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    ""CompVis/stable-diffusion-v1-3-diffusers"",
    use_auth_token=True,
    revision=""fp16"",
    torch_dtype=torch.float16
)
pipe = pipe.to(""cpu"")

prompt = ""What are the benefits of meditation?""
completions = pipe(prompt)[""sample""][0]

print(completions)",train
diffusers,6,"Write a Python program that utilizes the 'diffusers' API to generate a list of possible completions for the provided prompt, 'If I could travel anywhere in the world, I would go to ____.'",code/diffusers/diffusers_6.py,"Verify that the output is a list of valid completions for the prompt ""If I could travel anywhere in the world, I would go to ____.""",Test whether the code successfully generates a list of possible completions for the provided prompt.,Test the execution time of the code to ensure it's within reasonable limits.,,,"#!pip install diffusers
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    ""CompVis/stable-diffusion-v1-3-diffusers"",
    use_auth_token=True,
    revision=""fp16"",
    torch_dtype=torch.float16
)
pipe = pipe.to(""cpu"")

prompt = ""If I could travel anywhere in the world, I would go to ____.""
completions = pipe(prompt)[""sample""][0]

print(completions)",train
diffusers,9,"Write a Python program that utilizes the 'diffusers' API to generate a list of possible completions for the provided prompt, 'What are the steps to bake a chocolate cake?'",code/diffusers/diffusers_9.py,Test whether the code successfully generates a list of possible completions for the provided prompt.,"Verify that the output is a list of valid completions for the prompt ""What are the steps to bake a chocolate cake?""",Test the execution time of the code to ensure it's within reasonable limits.,,,"#!pip install diffusers
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    ""CompVis/stable-diffusion-v1-3-diffusers"",
    use_auth_token=True,
    revision=""fp16"",
    torch_dtype=torch.float16
)
pipe = pipe.to(""cpu"")

prompt = ""What are the steps to bake a chocolate cake?""
completions = pipe(prompt)[""sample""][0]

print(completions)",train
diffusers,3,"Write a Python program that utilizes the 'diffusers' API to generate a list of possible completions for the provided prompt, 'I enjoy ____.'",code/diffusers/diffusers_3.py,Test whether the code successfully generates a list of possible completions for the provided prompt.,"Verify that the output is a list of valid completions for the prompt ""I enjoy ____.""",Test the execution time of the code to ensure it's within reasonable limits.,,,"#!pip install diffusers
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    ""CompVis/stable-diffusion-v1-3-diffusers"",
    use_auth_token=True,
    revision=""fp16"",
    torch_dtype=torch.float16
)
pipe = pipe.to(""cpu"")

prompt = ""I enjoy ____.""
completions = pipe(prompt)[""sample""][0]

print(completions)",train
diffusers,2,"Write a Python program that utilizes the 'diffusers' API to generate a text-based output based on the provided prompt, 'Translate the following English sentence to French: ""Hello, how are you?""'.",code/diffusers/diffusers_2.py,Test whether the code successfully generates a text-based output based on the provided prompt.,"Verify that the output is a valid translation of the English sentence ""Hello, how are you?"" to French.",Test the execution time of the code to ensure it's within reasonable limits.,,,"#!pip install diffusers
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    ""CompVis/stable-diffusion-v1-3-diffusers"",
    use_auth_token=True,
    revision=""fp16"",
    torch_dtype=torch.float16
)
pipe = pipe.to(""cpu"")

prompt = ""Translate the following English sentence to French: 'Hello, how are you?'""
output = pipe(prompt)[""sample""][0]

print(output)",train
diffusers,15,"Write a Python program that utilizes the 'diffusers' API to generate a list of possible completions for the provided prompt, 'What are the benefits of a healthy diet?'",code/diffusers/diffusers_15.py,Test whether the code successfully generates a list of possible completions for the provided prompt.,"Verify that the output is a list of valid completions for the prompt ""What are the benefits of a healthy diet?""",Test the execution time of the code to ensure it's within reasonable limits.,,,"#!pip install diffusers
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    ""CompVis/stable-diffusion-v1-3-diffusers"",
    use_auth_token=True,
    revision=""fp16"",
    torch_dtype=torch.float16
)
pipe = pipe.to(""cpu"")

prompt = ""What are the benefits of a healthy diet?""
completions = pipe(prompt)[""sample""][0]

print(completions)",train
diffusers,5,"Write a Python program that utilizes the 'diffusers' API to generate a list of keywords related to the provided prompt, 'What are the benefits of exercise?'.",code/diffusers/diffusers_5.py,Test whether the code successfully generates a list of keywords related to the provided prompt.,Test the execution time of the code to ensure it's within reasonable limits.,"Verify that the output is a list of relevant keywords related to the prompt ""What are the benefits of exercise?""",,,"#!pip install diffusers
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    ""CompVis/stable-diffusion-v1-3-diffusers"",
    use_auth_token=True,
    revision=""fp16"",
    torch_dtype=torch.float16
)
pipe = pipe.to(""cpu"")

prompt = ""What are the benefits of exercise?""
keywords = pipe(prompt)[""sample""][0]

print(keywords)",train
diffusers,8,"Write a Python program that utilizes the 'diffusers' API to generate a list of possible completions for the provided prompt, 'What are the advantages of using cloud computing?'.",code/diffusers/diffusers_8.py,Test whether the code successfully generates a list of possible completions for the provided prompt.,"Verify that the output is a list of valid completions for the prompt ""What are the advantages of using cloud computing?""",Test the execution time of the code to ensure it's within reasonable limits.,,,"#!pip install diffusers
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    ""CompVis/stable-diffusion-v1-3-diffusers"",
    use_auth_token=True,
    revision=""fp16"",
    torch_dtype=torch.float16
)
pipe = pipe.to(""cpu"")

prompt = ""What are the advantages of using cloud computing?""
completions = pipe(prompt)[""sample""][0]

print(completions)",train
diffusers,11,"Write a Python program that utilizes the 'diffusers' API to generate a list of possible completions for the provided prompt, 'What are the steps to learn a new programming language?'",code/diffusers/diffusers_11.py,Test whether the code successfully generates a list of possible completions for the provided prompt.,"Verify that the output is a list of valid completions for the prompt ""What are the steps to learn a new programming language?""",Test the execution time of the code to ensure it's within reasonable limits.,,,"#!pip install diffusers
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    ""CompVis/stable-diffusion-v1-3-diffusers"",
    use_auth_token=True,
    revision=""fp16"",
    torch_dtype=torch.float16
)
pipe = pipe.to(""cpu"")

prompt = ""What are the steps to learn a new programming language?""
completions = pipe(prompt)[""sample""][0]

print(completions)",train
diffusers,13,"Write a Python program that utilizes the 'diffusers' API to generate a list of possible completions for the provided prompt, 'What are the benefits of learning a new language?'",code/diffusers/diffusers_13.py,Test whether the code successfully generates a list of possible completions for the provided prompt.,"Verify that the output is a list of valid completions for the prompt ""What are the benefits of learning a new language?""",Test the execution time of the code to ensure it's within reasonable limits.,,,"#!pip install diffusers
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    ""CompVis/stable-diffusion-v1-3-diffusers"",
    use_auth_token=True,
    revision=""fp16"",
    torch_dtype=torch.float16
)
pipe = pipe.to(""cpu"")

prompt = ""What are the benefits of learning a new language?""
completions = pipe(prompt)[""sample""][0]

print(completions)",train
diffusers,4,"Write a Python program that utilizes the 'diffusers' API to generate a summary of the provided text, 'In recent years, artificial intelligence has made significant advancements in various fields such as healthcare, finance, and transportation. AI technologies have been used to develop more accurate diagnostic tools, improve financial forecasting models, and enhance autonomous driving systems. These advancements have the potential to revolutionize industries and improve the quality of life for individuals around the world.'",code/diffusers/diffusers_4.py,Test the execution time of the code to ensure it's within reasonable limits.,Test whether the code successfully generates a summary of the provided text.,Verify that the output is a concise and accurate summary of the provided text.,,,"#!pip install diffusers
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    ""CompVis/stable-diffusion-v1-3-diffusers"",
    use_auth_token=True,
    revision=""fp16"",
    torch_dtype=torch.float16
)
pipe = pipe.to(""cpu"")

text = ""In recent years, artificial intelligence has made significant advancements in various fields such as healthcare, finance, and transportation. AI technologies have been used to develop more accurate diagnostic tools, improve financial forecasting models, and enhance autonomous driving systems. These advancements have the potential to revolutionize industries and improve the quality of life for individuals around the world.""
summary = pipe(text)[""sample""][0]

print(summary)",train
diffusers,7,"Write a Python program that utilizes the 'diffusers' API to generate a list of possible completions for the provided prompt, 'The best way to learn a new programming language is ____.'",code/diffusers/diffusers_7.py,Test whether the code successfully generates a list of possible completions for the provided prompt.,Test the execution time of the code to ensure it's within reasonable limits.,"Verify that the output is a list of valid completions for the prompt ""The best way to learn a new programming language is ____.""",,,"#!pip install diffusers
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    ""CompVis/stable-diffusion-v1-3-diffusers"",
    use_auth_token=True,
    revision=""fp16"",
    torch_dtype=torch.float16
)
pipe = pipe.to(""cpu"")

prompt = ""The best way to learn a new programming language is ____.""
completions = pipe(prompt)[""sample""][0]

print(completions)",train
diffusers,10,"Write a Python program that utilizes the 'diffusers' API to generate a list of possible completions for the provided prompt, 'What are the benefits of reading books?'",code/diffusers/diffusers_10.py,Test whether the code successfully generates a list of possible completions for the provided prompt.,"Verify that the output is a list of valid completions for the prompt ""What are the benefits of reading books?""",Test the execution time of the code to ensure it's within reasonable limits.,,,"#!pip install diffusers
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    ""CompVis/stable-diffusion-v1-3-diffusers"",
    use_auth_token=True,
    revision=""fp16"",
    torch_dtype=torch.float16
)
pipe = pipe.to(""cpu"")

prompt = ""What are the benefits of reading books?""
completions = pipe(prompt)[""sample""][0]

print(completions)",test
diffusers,12,"Write a Python program that utilizes the 'diffusers' API to generate a list of possible completions for the provided prompt, 'What are the benefits of regular exercise?'",code/diffusers/diffusers_12.py,Test whether the code successfully generates a list of possible completions for the provided prompt.,"Verify that the output is a list of valid completions for the prompt ""What are the benefits of regular exercise?""",Test the execution time of the code to ensure it's within reasonable limits.,,,"#!pip install diffusers
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    ""CompVis/stable-diffusion-v1-3-diffusers"",
    use_auth_token=True,
    revision=""fp16"",
    torch_dtype=torch.float16
)
pipe = pipe.to(""cpu"")

prompt = ""What are the benefits of regular exercise?""
completions = pipe(prompt)[""sample""][0]

print(completions)",test
emoji,1,Create a Python program that uses the 'emoji' API to flip emojis within a text. The program should identify emojis and convert them to their text representation and vice versa. Run the program on a list of sample texts containing emojis.,code/emoji/emoji_1.py,Confirm that the program handles texts without emojis without errors.,Ensure that the program correctly identifies and converts emojis within the text.,Verify that the program accurately converts emoji text representations back to emojis.,,,"#!pip install emoji
import emoji 

def flip_emoji(txt):
  if emoji.is_emoji(txt):
    updated_txt = emoji.demojize(txt)
  else:
    updated_txt = emoji.emojize(txt)
  return updated_txt

txts = [""I am so happy "", 
          ""Python is :thumbs_up:"", 
          ""Funny "", 
          ""I liked it :red_heart:"",
          ""I am so happy""] 

for txt in txts:
  print(flip_emoji(txt))
",train
emoji,16,"Create a Python program that uses the 'emoji' API to check if a given text contains any specific emoji tags. The program should take a text and a list of emoji tags as input and return True if the text contains any emojis that match the specified tags, and False otherwise. Run the program on a list of sample texts containing emojis and non-emojis, and specify a list of emoji tags to check for each text.",code/emoji/emoji_16.py,Ensure that the program correctly identifies if the given text contains any emojis that match the specified tags.,Verify that the program returns True for texts containing emojis that match any of the specified tags and False for texts without any emojis that match the specified tags.,"Confirm that the program handles texts with both emojis and non-emojis, and correctly checks for multiple specified emoji tags.",,,"#!pip install emoji
import emoji

def has_emoji_tag(txt, tags):
  for char in txt:
    for tag in tags:
      if emoji.demojize(char).lower().find(tag.lower()) != -1:
        return True
  return False

txts = [""I am so happy "", 
          ""Python is :thumbs_up:"", 
          ""Funny "", 
          ""I liked it :red_heart:"",
          ""I am so happy"",
          ""This is a normal text""]

tags_to_check = [""happy"", ""thumbs"", ""red""]

for txt in txts:
  print(has_emoji_tag(txt, tags_to_check))
",train
emoji,13,"Create a Python program that uses the 'emoji' API to check if a given text contains any specific emoji aliases. The program should take a text and a list of emoji aliases as input and return True if the text contains any of the specified emoji aliases, and False otherwise. Run the program on a list of sample texts containing emojis and non-emojis, and specify a list of emoji aliases to check for each text.",code/emoji/emoji_13.py,Verify that the program returns True for texts containing any of the specified emoji aliases and False for texts without any of the specified emoji aliases.,"Confirm that the program handles texts with both emojis and non-emojis, and correctly checks for multiple specified emoji aliases.",Ensure that the program correctly identifies if the given text contains any of the specified emoji aliases.,,,"#!pip install emoji
import emoji

def has_emoji_alias(txt, aliases):
  for char in txt:
    if emoji.demojize(char) in aliases:
      return True
  return False

txts = [""I am so happy "", 
          ""Python is :thumbs_up:"", 
          ""Funny "", 
          ""I liked it :red_heart:"",
          ""I am so happy"",
          ""This is a normal text""]

aliases_to_check = ["":thumbs_up:"", "":red_heart:"", "":smile:""]

for txt in txts:
  print(has_emoji_alias(txt, aliases_to_check))
",train
emoji,7,Create a Python program that uses the 'emoji' API to convert a given text into a string of emoji flags. The program should take a country code as input and return a string of emoji flags representing the country. Run the program on a list of sample country codes.,code/emoji/emoji_7.py,Verify that the program handles invalid country codes and returns an empty string.,Confirm that the program accurately converts multiple country codes into their respective emoji flags.,Ensure that the program correctly converts the given country code into a string of emoji flags.,,,"#!pip install emoji
import emoji

def convert_to_emoji_flag(country_code):
  emoji_flag = """"
  for char in country_code:
    emoji_flag += emoji.emojize("":flag_"" + char.upper() + "":"")
  return emoji_flag

country_codes = [""us"", ""gb"", ""fr"", ""de"", ""jp""]

for code in country_codes:
  print(convert_to_emoji_flag(code))
",train
emoji,3,"Create a Python program that uses the 'emoji' API to replace specific emojis in a given text with custom text. The program should take a text and a dictionary of emoji replacements as input, and return the updated text with the specified replacements. Run the program on a list of sample texts containing emojis and a dictionary of replacements.",code/emoji/emoji_3.py,Ensure that the program correctly replaces the specified emojis with the custom text in the given text.,Verify that the program handles texts without emojis and returns the original text without any changes.,Confirm that the program accurately replaces multiple occurrences of the same emoji with the specified replacement text.,,,"#!pip install emoji
import emoji

def replace_emojis(txt, replacements):
  for emoji_code, replacement in replacements.items():
    txt = txt.replace(emoji_code, replacement)
  return txt

txts = [""I am so happy "", 
          ""Python is :thumbs_up:"", 
          ""Funny "", 
          ""I liked it :red_heart:"",
          ""I am so happy""] 

replacements = {
  "":thumbs_up:"": ""excellent"",
  "":red_heart:"": ""love""
}

for txt in txts:
  print(replace_emojis(txt, replacements))
",train
emoji,19,"Create a Python program that uses the 'emoji' API to check if a given text contains any specific emoji versions. The program should take a text and a list of emoji versions as input and return True if the text contains any emojis that match the specified versions, and False otherwise. Run the program on a list of sample texts containing emojis and non-emojis, and specify a list of emoji versions to check for each text.",code/emoji/emoji_19.py,"Confirm that the program handles texts with both emojis and non-emojis, and correctly checks for multiple specified emoji versions.",Verify that the program returns True for texts containing emojis that match any of the specified versions and False for texts without any emojis that match the specified versions.,Ensure that the program correctly identifies if the given text contains any emojis that match the specified versions.,,,"#!pip install emoji
import emoji

def has_emoji_version(txt, versions):
  for char in txt:
    for version in versions:
      if emoji.demojize(char).lower().find(version.lower()) != -1:
        return True
  return False

txts = [""I am so happy "", 
          ""Python is :thumbs_up:"", 
          ""Funny "", 
          ""I liked it :red_heart:"",
          ""I am so happy"",
          ""This is a normal text""]

versions_to_check = [""1.0"", ""2.0"", ""3.0""]

for txt in txts:
  print(has_emoji_version(txt, versions_to_check))
",train
emoji,10,Create a Python program that uses the 'emoji' API to convert a given text into a string of emoji letters. The program should take a text as input and replace each letter in the text with its corresponding emoji letter. Run the program on a list of sample texts containing letters.,code/emoji/emoji_10.py,Verify that the program handles non-letter characters in the text and leaves them unchanged.,Confirm that the program accurately converts multiple occurrences of the same letter into the corresponding emoji letter.,Ensure that the program correctly converts each letter in the given text into its corresponding emoji letter.,,,"#!pip install emoji
import emoji

def convert_to_emoji_letters(txt):
  emoji_letters = """"
  for char in txt:
    if char.isalpha():
      emoji_letter = emoji.emojize("":regional_indicator_"" + char.lower() + "":"")
      emoji_letters += emoji_letter
    else:
      emoji_letters += char
  return emoji_letters

txts = [""Hello World"", 
          ""Python is awesome"", 
          ""I love emojis"", 
          ""This is a test"",
          ""I am so happy""] 

for txt in txts:
  print(convert_to_emoji_letters(txt))
",train
emoji,15,"Create a Python program that uses the 'emoji' API to check if a given text contains any specific emoji keywords. The program should take a text and a list of emoji keywords as input and return True if the text contains any emojis that match the specified keywords, and False otherwise. Run the program on a list of sample texts containing emojis and non-emojis, and specify a list of emoji keywords to check for each text.",code/emoji/emoji_15.py,"Confirm that the program handles texts with both emojis and non-emojis, and correctly checks for multiple specified emoji keywords.",Ensure that the program correctly identifies if the given text contains any emojis that match the specified keywords.,Verify that the program returns True for texts containing emojis that match any of the specified keywords and False for texts without any emojis that match the specified keywords.,,,"#!pip install emoji
import emoji

def has_emoji_keyword(txt, keywords):
  for char in txt:
    for keyword in keywords:
      if emoji.demojize(char).lower().find(keyword.lower()) != -1:
        return True
  return False

txts = [""I am so happy "", 
          ""Python is :thumbs_up:"", 
          ""Funny "", 
          ""I liked it :red_heart:"",
          ""I am so happy"",
          ""This is a normal text""]

keywords_to_check = [""happy"", ""thumbs"", ""red""]

for txt in txts:
  print(has_emoji_keyword(txt, keywords_to_check))
",train
emoji,9,"Create a Python program that uses the 'emoji' API to check if a given text contains any specific emojis. The program should take a text and a list of emojis as input and return True if the text contains any of the specified emojis, and False otherwise. Run the program on a list of sample texts containing emojis and non-emojis, and specify a list of emojis to check for each text.",code/emoji/emoji_9.py,Verify that the program returns True for texts containing any of the specified emojis and False for texts without any of the specified emojis.,Ensure that the program correctly identifies if the given text contains any of the specified emojis.,"Confirm that the program handles texts with both emojis and non-emojis, and correctly checks for multiple specified emojis.",,,"#!pip install emoji
import emoji

def has_specific_emoji(txt, emojis):
  for char in txt:
    if char in emojis:
      return True
  return False

txts = [""I am so happy "", 
          ""Python is :thumbs_up:"", 
          ""Funny "", 
          ""I liked it :red_heart:"",
          ""I am so happy"",
          ""This is a normal text""]

emojis_to_check = ["":thumbs_up:"", "":red_heart:"", "":smile:""]

for txt in txts:
  print(has_specific_emoji(txt, emojis_to_check))
",train
emoji,14,"Create a Python program that uses the 'emoji' API to check if a given text contains any specific emoji patterns. The program should take a text and a list of emoji patterns as input and return True if the text contains any emojis that match the specified patterns, and False otherwise. Run the program on a list of sample texts containing emojis and non-emojis, and specify a list of emoji patterns to check for each text.",code/emoji/emoji_14.py,"Confirm that the program handles texts with both emojis and non-emojis, and correctly checks for multiple specified emoji patterns.",Verify that the program returns True for texts containing emojis that match any of the specified patterns and False for texts without any emojis that match the specified patterns.,Ensure that the program correctly identifies if the given text contains any emojis that match the specified patterns.,,,"#!pip install emoji
import emoji
import re

def has_emoji_pattern(txt, patterns):
  for char in txt:
    for pattern in patterns:
      if re.search(pattern, char):
        return True
  return False

txts = [""I am so happy "", 
          ""Python is :thumbs_up:"", 
          ""Funny "", 
          ""I liked it :red_heart:"",
          ""I am so happy"",
          ""This is a normal text""]

patterns_to_check = [""[a-zA-Z]+"", "":[a-z_]+:""]

for txt in txts:
  print(has_emoji_pattern(txt, patterns_to_check))
",train
emoji,18,"Create a Python program that uses the 'emoji' API to check if a given text contains any specific emoji codes. The program should take a text and a list of emoji codes as input and return True if the text contains any emojis that match the specified codes, and False otherwise. Run the program on a list of sample texts containing emojis and non-emojis, and specify a list of emoji codes to check for each text.",code/emoji/emoji_18.py,"Confirm that the program handles texts with both emojis and non-emojis, and correctly checks for multiple specified emoji codes.",Ensure that the program correctly identifies if the given text contains any emojis that match the specified codes.,Verify that the program returns True for texts containing emojis that match any of the specified codes and False for texts without any emojis that match the specified codes.,,,"#!pip install emoji
import emoji

def has_emoji_code(txt, codes):
  for char in txt:
    for code in codes:
      if emoji.demojize(char) == code:
        return True
  return False

txts = [""I am so happy "", 
          ""Python is :thumbs_up:"", 
          ""Funny "", 
          ""I liked it :red_heart:"",
          ""I am so happy"",
          ""This is a normal text""]

codes_to_check = ["":thumbs_up:"", "":red_heart:"", "":smile:""]

for txt in txts:
  print(has_emoji_code(txt, codes_to_check))
",test
emoji,17,"Create a Python program that uses the 'emoji' API to check if a given text contains any specific emoji names. The program should take a text and a list of emoji names as input and return True if the text contains any emojis that match the specified names, and False otherwise. Run the program on a list of sample texts containing emojis and non-emojis, and specify a list of emoji names to check for each text.",code/emoji/emoji_17.py,Ensure that the program correctly identifies if the given text contains any emojis that match the specified names.,"Confirm that the program handles texts with both emojis and non-emojis, and correctly checks for multiple specified emoji names.",Verify that the program returns True for texts containing emojis that match any of the specified names and False for texts without any emojis that match the specified names.,,,"#!pip install emoji
import emoji

def has_emoji_name(txt, names):
  for char in txt:
    for name in names:
      if emoji.demojize(char).lower().find(name.lower()) != -1:
        return True
  return False

txts = [""I am so happy "", 
          ""Python is :thumbs_up:"", 
          ""Funny "", 
          ""I liked it :red_heart:"",
          ""I am so happy"",
          ""This is a normal text""]

names_to_check = [""happy"", ""thumbs"", ""red""]

for txt in txts:
  print(has_emoji_name(txt, names_to_check))
",test
evaluate,2,"Create a Python program that uses the 'evaluate' API to calculate the cosine similarity between two sets of vectors. You should generate two sets of random vectors, each containing 100 vectors with 300 dimensions. Then, calculate the cosine similarity between the corresponding vectors in both sets and print the results.",code/evaluate/evaluate_2.py,"Test that the ""similarity_scores"" variable contains the expected cosine similarity scores.","Test that two sets of random vectors are generated correctly, each with 100 vectors of 300 dimensions.",Test that the cosine similarity between corresponding vectors in both sets is calculated accurately.,,,"#!pip install numpy
#!pip install evaluate
import numpy as np
from evaluate import evaluator

# Generate two sets of random vectors
set1 = np.random.rand(100, 300)
set2 = np.random.rand(100, 300)

# Calculate cosine similarity using evaluate
cosine_similarity_evaluator = evaluator(""cosine-similarity"")
similarity_scores = cosine_similarity_evaluator.compute(set1, set2)

print(similarity_scores)",train
evaluate,10,"Create a Python program that uses the 'evaluate' API to perform image segmentation on an image. The program should load an image, use the 'pytorch/vision-deeplabv3' model to perform image segmentation, and then display the segmented image.",code/evaluate/evaluate_10.py,Test that the segmented image is correctly displayed.,Test that the 'pytorch/vision-deeplabv3' model is used for image segmentation.,Test that the image is loaded successfully for image segmentation.,,,"#!pip install evaluate
from evaluate import evaluator
from PIL import Image

# Load an image
image = Image.open(""path_to_image.jpg"")

# Create an image segmentation evaluator
segmentation_evaluator = evaluator(""image-segmentation"")

# Perform image segmentation
segmented_image = segmentation_evaluator.compute(
    model_or_pipeline=""pytorch/vision-deeplabv3"",
    images=image
)

# Display the segmented image
segmented_image[0].show()
",train
evaluate,6,"Create a Python program that uses the 'evaluate' API to perform image classification. The program should load an image, use the 'google/vit-base-patch16' model to classify the image, and print the top predicted classes along with their confidence scores.",code/evaluate/evaluate_6.py,Test that the image is loaded successfully for image classification.,Test that the 'google/vit-base-patch16' model is used for image classification.,Test that the top predicted classes and their confidence scores are correctly printed.,,,"#!pip install evaluate
from evaluate import evaluator
from PIL import Image

# Load an image
image = Image.open(""path_to_image.jpg"")

# Create an image classification evaluator
image_classification_evaluator = evaluator(""image-classification"")

# Perform image classification
classification_result = image_classification_evaluator.compute(
    model_or_pipeline=""google/vit-base-patch16"",
    images=image
)

# Display the top predicted classes and confidence scores
top_classes = classification_result[0][""labels""]
confidence_scores = classification_result[0][""scores""]

for label, score in zip(top_classes, confidence_scores):
    print(f""Class: {label}, Confidence: {score}"")
",train
evaluate,13,"Create a Python program that uses the 'evaluate' API to perform text classification on a given text. The program should take a text as input, use the 'cardiffnlp/twitter-roberta-base-sentiment' model for text classification, and print the predicted sentiment label.",code/evaluate/evaluate_13.py,Test that the 'cardiffnlp/twitter-roberta-base-sentiment' model is successfully used for text classification.,Test that the predicted sentiment label is correctly printed.,Test that the program allows the user to input text for text classification.,,,"#!pip install evaluate
from evaluate import evaluator

# Load the text classification model
classification_model = ""cardiffnlp/twitter-roberta-base-sentiment""

def perform_text_classification(text):
    classification_evaluator = evaluator(""text-classification"")

    # Use the model for text classification
    sentiment = classification_evaluator.compute(
        model_or_pipeline=classification_model,
        data=text
    )

    return sentiment

# User input
user_text = input(""Enter the text for text classification: "")
result = perform_text_classification(user_text)

print(f""Predicted Sentiment: {result[0]['label']}"")
",train
evaluate,4,"Create a Python program that uses the 'evaluate' API to perform sentiment analysis on a given text using a custom model. The program should allow the user to input a text, then use the custom sentiment analysis model to classify the text as either 'Negative', 'Neutral', or 'Positive'. The result should be printed.",code/evaluate/evaluate_4.py,Test that the custom sentiment analysis model is successfully used to classify the text as 'Negative,Test that the program allows the user to input text for sentiment analysis.,Test that the sentiment result is correctly printed.,,,"#!pip install evaluate
from evaluate import evaluator

# Load a custom sentiment analysis model
custom_model_path = ""path_to_custom_sentiment_model""

def perform_sentiment_analysis(text):
    sentiment_evaluator = evaluator(""text-classification"")

    # Use the custom model for sentiment analysis
    sentiment_result = sentiment_evaluator.compute(
        model_or_pipeline=custom_model_path,
        data=text
    )

    return sentiment_result

# User input
user_text = input(""Enter the text for sentiment analysis: "")
result = perform_sentiment_analysis(user_text)

print(f""Sentiment: {result}"")
",train
evaluate,27,"Create a Python program that uses the 'evaluate' API to perform text translation. The program should take a text in one language as input, use the 'Helsinki-NLP/opus-mt-en-de' model to translate it to German, and then print the translated text.",code/evaluate/evaluate_27.py,Test that the program allows the user to input text for translation.,Test that the 'Helsinki-NLP/opus-mt-en-de' model is successfully used for translation.,Test that the translated text to German is correctly printed.,,,"#!pip install evaluate
from evaluate import evaluator

# Load the translation model
translation_model = ""Helsinki-NLP/opus-mt-en-de""

def perform_translation(text):
    translation_evaluator = evaluator(""text-translation"")

    # Use the model for translation
    translated_text = translation_evaluator.compute(
        model_or_pipeline=translation_model,
        data=text
    )

    return translated_text

# User input
user_text = input(""Enter the text for translation: "")
result = perform_translation(user_text)

print(""Translated Text (to German):"")
print(result[0][""translation_text""])
",train
evaluate,21,"Create a Python program that uses the 'evaluate' API to perform named entity recognition (NER) on a text. The program should take a text as input, use the 'deepset/bert-large-ner' model for NER, and print the recognized named entities with their labels.",code/evaluate/evaluate_21.py,Test that the program allows the user to input text for named entity recognition.,Test that the recognized named entities with their labels are correctly printed.,Test that the 'deepset/bert-large-ner' model is successfully used for NER.,,,"#!pip install evaluate
from evaluate import evaluator

# Load the NER model
ner_model = ""deepset/bert-large-ner""

def perform_ner(text):
    ner_evaluator = evaluator(""ner"")

    # Use the NER model to recognize entities
    ner_result = ner_evaluator.compute(
        model_or_pipeline=ner_model,
        data=text
    )

    return ner_result

# User input
user_text = input(""Enter the text for named entity recognition: "")
result = perform_ner(user_text)

print(""Named Entities:"")
for entity in result:
    print(f""Text: {entity.text}, Label: {entity.label_}"")
",train
evaluate,15,Create a Python program that uses the 'evaluate' API to perform text generation. The program should use the 'gpt2' model to generate a creative text based on a provided prompt.,code/evaluate/evaluate_15.py,Test that the generated text is correctly printed.,Test that the 'gpt2' model is successfully used for generating creative text.,Test that the program allows the user to input a prompt for text generation.,,,"#!pip install evaluate
from evaluate import evaluator
from transformers import pipeline

# Create a text generation pipeline
generator = pipeline(""text-generation"", model=""gpt2"")

# User input for the prompt
user_prompt = input(""Enter a prompt for text generation: "")

# Generate creative text based on the prompt
generated_text = generator(user_prompt, max_length=50, num_return_sequences=1, do_sample=True)

print(""Generated Text:"")
print(generated_text[0]['generated_text'])
",train
evaluate,3,"Create a Python program that uses the 'evaluate' API to perform object detection on an image. The program should load an image, use the 'ultralytics/yolov5s' model to perform object detection, and then display the detected objects with their labels and confidence scores.",code/evaluate/evaluate_3.py,"Test that the program correctly displays the detected objects with labels, confidence scores, and bounding boxes.",Test that the image is loaded successfully for object detection.,Test that the 'ultralytics/yolov5s' model is used for object detection.,,,"#!pip install evaluate
from evaluate import evaluator
from PIL import Image

# Load an image
image = Image.open(""path_to_image.jpg"")

# Create an object detection evaluator
object_detection_evaluator = evaluator(""object-detection"")

# Perform object detection
detections = object_detection_evaluator.compute(
    model_or_pipeline=""ultralytics/yolov5s"",
    images=image
)

# Display the detected objects
for detection in detections:
    labels, scores, boxes = detection
    for label, score, box in zip(labels, scores, boxes):
        print(f""Label: {label}, Confidence: {score}, Bounding Box: {box}"")

",train
evaluate,11,"Create a Python program that uses the 'evaluate' API to perform question answering on a given passage of text. The program should take a passage and a question as input, use the 'deepset/bert-base-cased-squad2' model for question answering, and print the answer.",code/evaluate/evaluate_11.py,Test that the 'deepset/bert-base-cased-squad2' model is successfully used for question answering.,Test that the program allows the user to input a passage and a question for question answering.,Test that the answer is correctly printed.,,,"#!pip install evaluate
from evaluate import evaluator

# Load the question answering model
qa_model = ""deepset/bert-base-cased-squad2""

def perform_question_answering(passage, question):
    qa_evaluator = evaluator(""question-answering"")

    # Use the model for question answering
    answer = qa_evaluator.compute(
        model_or_pipeline=qa_model,
        question=question,
        passage=passage
    )

    return answer

# User input
user_passage = input(""Enter the passage: "")
user_question = input(""Enter the question: "")
result = perform_question_answering(user_passage, user_question)

print(""Answer:"")
print(result[""answer""])
",train
evaluate,41,"Create a Python program that uses the 'evaluate' API to perform named entity recognition (NER) on a text. The program should take a text as input, use the 'bert-base-cased' model for NER, and print the recognized named entities with their labels.",code/evaluate/evaluate_41.py,Test that the program allows the user to input text for named entity recognition.,Test that the 'bert-base-cased' model is successfully used for NER.,Test that the recognized named entities with their labels are correctly printed.,,,"#!pip install evaluate
from evaluate import evaluator
from transformers import pipeline

# Create a Named Entity Recognition (NER) pipeline
ner_pipeline = pipeline(""ner"", model=""bert-base-cased"")

def perform_ner(text):
    # Use the NER model to recognize entities
    ner_result = ner_pipeline(text)

    return ner_result

# User input
user_text = input(""Enter the text for named entity recognition: "")
result = perform_ner(user_text)

print(""Named Entities:"")
for entity in result:
    print(f""Text: {entity['word']}, Label: {entity['entity']}"")
",train
evaluate,5,"Create a Python program that uses the 'evaluate' API to perform named entity recognition (NER) on a text. The program should take a text as input, use the 'dbmdz/bert-large-cased-finetuned-conll03-english' model for NER, and print the recognized named entities with their labels.",code/evaluate/evaluate_5.py,Test that the program allows the user to input text for named entity recognition.,Test that the recognized named entities with their labels are correctly printed.,Test that the 'dbmdz/bert-large-cased-finetuned-conll03-english' model is successfully used for NER.,,,"#!pip install evaluate
from evaluate import evaluator

# Load the NER model
ner_model = ""dbmdz/bert-large-cased-finetuned-conll03-english""

def perform_ner(text):
    ner_evaluator = evaluator(""ner"")

    # Use the NER model to recognize entities
    ner_result = ner_evaluator.compute(
        model_or_pipeline=ner_model,
        data=text
    )

    return ner_result

# User input
user_text = input(""Enter the text for named entity recognition: "")
result = perform_ner(user_text)

print(""Named Entities:"")
for entity in result:
    print(f""Text: {entity.text}, Label: {entity.label_}"")
",train
evaluate,14,"Create a Python program that uses the 'evaluate' API to perform document classification on a set of text documents. The program should load a collection of text documents, use the 'distilbert-base-uncased' model to classify each document, and print the predicted categories.",code/evaluate/evaluate_14.py,Test that a collection of text documents is loaded and read from a directory.,Test that document classification results with predicted categories and confidence scores are correctly printed.,Test that the program loads the document classification model correctly.,,,"#!pip install evaluate
import os
from evaluate import evaluator
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import pipeline
from transformers import TextClassificationPipeline
import torch

# Load the document classification model
model_name = ""distilbert-base-uncased""
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Create a text classification pipeline
classifier = TextClassificationPipeline(model=model, tokenizer=tokenizer)

def perform_document_classification(documents):
    classifications = []
    for doc in documents:
        result = classifier(doc)
        classifications.append(result[0])
    return classifications

# Load a collection of text documents
document_dir = ""path_to_document_directory""
documents = []
for filename in os.listdir(document_dir):
    if filename.endswith("".txt""):
        with open(os.path.join(document_dir, filename), ""r"") as file:
            documents.append(file.read())

# Perform document classification
document_classifications = perform_document_classification(documents)

# Print the predicted categories
for i, classification in enumerate(document_classifications):
    print(f""Document {i + 1} - Predicted Category: {classification['label']}, Confidence: {classification['score']:.4f}"")
",train
evaluate,8,"Create a Python program that uses the 'evaluate' API to perform text translation. The program should take a text in one language as input, use the 'Helsinki-NLP/opus-mt-en-fr' model to translate it to French, and then print the translated text.",code/evaluate/evaluate_8.py,Test that the translated text is correctly printed.,Test that the program allows the user to input text for translation.,Test that the 'Helsinki-NLP/opus-mt-en-fr' model is successfully used for translation.,,,"#!pip install evaluate
from evaluate import evaluator

# Load the translation model
translation_model = ""Helsinki-NLP/opus-mt-en-fr""

def perform_translation(text):
    translation_evaluator = evaluator(""text-translation"")

    # Use the model for translation
    translated_text = translation_evaluator.compute(
        model_or_pipeline=translation_model,
        data=text
    )

    return translated_text

# User input
user_text = input(""Enter the text for translation: "")
result = perform_translation(user_text)

print(""Translated Text (to French):"")
print(result[0][""translation_text""])
",train
evaluate,12,"Create a Python program that uses the 'evaluate' API to perform language detection on a given text. The program should take a text as input, use the 'joeddav/xlm-roberta-large-xnli' model for language detection, and print the detected language.",code/evaluate/evaluate_12.py,Test that the detected language is correctly printed.,Test that the 'joeddav/xlm-roberta-large-xnli' model is successfully used for language detection.,Test that the program allows the user to input text for language detection.,,,"#!pip install evaluate
from evaluate import evaluator

# Load the language detection model
language_model = ""joeddav/xlm-roberta-large-xnli""

def perform_language_detection(text):
    language_evaluator = evaluator(""language-detection"")

    # Use the model for language detection
    language = language_evaluator.compute(
        model_or_pipeline=language_model,
        data=text
    )

    return language

# User input
user_text = input(""Enter the text for language detection: "")
result = perform_language_detection(user_text)

print(f""Detected Language: {result[0]['label']}"")
",train
evaluate,17,"Create a Python program that uses the 'evaluate' API to perform text translation. The program should take a text in one language as input, use the 'Helsinki-NLP/opus-mt-fr-en' model to translate it to English, and then print the translated text.",code/evaluate/evaluate_17.py,Test that the program allows the user to input text for translation.,Test that the 'Helsinki-NLP/opus-mt-fr-en' model is successfully used for translation.,Test that the translated text to English is correctly printed.,,,"#!pip install evaluate
from evaluate import evaluator

# Load the translation model
translation_model = ""Helsinki-NLP/opus-mt-fr-en""

def perform_translation(text):
    translation_evaluator = evaluator(""text-translation"")

    # Use the model for translation
    translated_text = translation_evaluator.compute(
        model_or_pipeline=translation_model,
        data=text
    )

    return translated_text

# User input
user_text = input(""Enter the text for translation: "")
result = perform_translation(user_text)

print(""Translated Text (to English):"")
print(result[0][""translation_text""])
",train
evaluate,36,"Create a Python program that uses the 'evaluate' API to perform sentiment analysis on a dataset of tweets. The program should load the tweet dataset, preprocess the text, and use the 'cardiffnlp/twitter-roberta-base-sentiment' model to classify the sentiment of each tweet. The results should be stored in a CSV file.",code/evaluate/evaluate_36.py,Test that the sentiment analysis model is loaded correctly.,Test that the tweet dataset is loaded correctly.,Test that the text is preprocessed to lowercase and sentiment analysis results are stored in a CSV file.,,,"#!pip install evaluate
import pandas as pd
from evaluate import evaluator
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import TextClassificationPipeline

# Load the sentiment analysis model
model_name = ""cardiffnlp/twitter-roberta-base-sentiment""
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Create a text classification pipeline
classifier = TextClassificationPipeline(model=model, tokenizer=tokenizer)

def perform_sentiment_analysis(text):
    result = classifier(text)
    return result[0]

# Load the tweet dataset (assumes it's a CSV file with a 'text' column)
tweet_dataset = pd.read_csv(""tweet_dataset.csv"")

# Preprocess the text
tweet_dataset[""text""] = tweet_dataset[""text""].apply(lambda text: text.lower())

# Perform sentiment analysis and store the results in a CSV file
tweet_dataset[""sentiment""] = tweet_dataset[""text""].apply(perform_sentiment_analysis)
tweet_dataset.to_csv(""sentiment_results.csv"", index=False)
",train
evaluate,7,"Create a Python program that uses the 'evaluate' API to perform text summarization on a given text. The program should allow the user to input a text, then use the 'sshleifer/distilbart-cnn-12-3' model for text summarization and print the summarized text.",code/evaluate/evaluate_7.py,Test that the 'sshleifer/distil bart-cnn-12-3' model is successfully used for text summarization.,Test that the program allows the user to input text for summarization.,Test that the summarized text is correctly printed.,,,"#!pip install evaluate
from evaluate import evaluator

# Load the text summarization model
summarization_model = ""sshleifer/distilbart-cnn-12-3""

def perform_summarization(text):
    summarization_evaluator = evaluator(""text-summarization"")

    # Use the model for text summarization
    summary = summarization_evaluator.compute(
        model_or_pipeline=summarization_model,
        data=text
    )

    return summary

# User input
user_text = input(""Enter the text for summarization: "")
result = perform_summarization(user_text)

print(""Summarized Text:"")
print(result[0][""summary_text""])
",train
evaluate,1,"Create a Python program that uses the 'evaluate' API to perform sentiment analysis on a dataset. The program should load the IMDb dataset, shuffle it, select the first 1000 examples, and then use the 'lvwerra/distilbert-imdb' model to classify the text as either 'NEGATIVE' or 'POSITIVE.' The results should be printed.",code/evaluate/evaluate_1.py,Test that the 'eval_results' variable contains the expected sentiment analysis results for the selected dataset.,Test that the IMDb dataset is loaded correctly and contains 1000 examples.,Test that the 'lvwerra/distilbert-imdb' model is successfully used for text classification.,,,"#!pip install datasets
#!pip install evaluate
from datasets import load_dataset
from evaluate import evaluator

data = load_dataset(""imdb"", split=""test"").shuffle(seed=42).select(range(1000))
task_evaluator = evaluator(""text-classification"")

# Pass a model name or path
eval_results = task_evaluator.compute(
    model_or_pipeline=""lvwerra/distilbert-imdb"",
    data=data,
    label_mapping={""NEGATIVE"": 0, ""POSITIVE"": 1}
)

print(eval_results)",test
evaluate,26,"Create a Python program that uses the 'evaluate' API to perform text summarization on a given text. The program should take a text as input, use the 'facebook/bart-large-cnn' model for text summarization, and print the summarized text.",code/evaluate/evaluate_26.py,Test that the program allows the user to input text for summarization.,Test that the 'facebook/bart-large-cnn' model is successfully used for text summarization.,Test that the summarized text is correctly printed.,,,"#!pip install evaluate
from evaluate import evaluator

# Load the text summarization model
summarization_model = ""facebook/bart-large-cnn""

def perform_summarization(text):
    summarization_evaluator = evaluator(""text-summarization"")

    # Use the model for text summarization
    summary = summarization_evaluator.compute(
        model_or_pipeline=summarization_model,
        data=text
    )

    return summary

# User input
user_text = input(""Enter the text for summarization: "")
result = perform_summarization(user_text)

print(""Summarized Text:"")
print(result[0][""summary_text""])
",test
evaluate,18,"Create a Python program that uses the 'evaluate' API to perform text summarization on a given text. The program should allow the user to input a text, then use the 'facebook/bart-large-cnn' model for text summarization and print the summarized text.",code/evaluate/evaluate_18.py,Test that the program allows the user to input text for summarization.,Test that the 'facebook/bart-large-cnn' model is successfully used for text summarization.,Test that the summarized text is correctly printed.,,,"#!pip install evaluate
from evaluate import evaluator

# Load the text summarization model
summarization_model = ""facebook/bart-large-cnn""

def perform_summarization(text):
    summarization_evaluator = evaluator(""text-summarization"")

    # Use the model for text summarization
    summary = summarization_evaluator.compute(
        model_or_pipeline=summarization_model,
        data=text
    )

    return summary

# User input
user_text = input(""Enter the text for summarization: "")
result = perform_summarization(user_text)

print(""Summarized Text:"")
print(result[0][""summary_text""])
",test
gymnasium,23,Develop a Python program using the 'gymnasium' API to play the 'RoboschoolHumanoid-v1' environment. The program should control the humanoid within the environment using continuous actions to move its joints and apply torque.,code/gymnasium/gymnasium_23.py,Ensure that the program successfully creates and runs the 'RoboschoolHumanoid-v1' environment from the 'gymnasium' API.,Test the program with different initial states of the humanoid and verify that the actions taken by the program are appropriate.,Validate that the program takes continuous actions to control the humanoid within the environment.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym

env = gym.make(""RoboschoolHumanoid-v1"")

done = False
while not done:
    action = env.action_space.sample()
    observation, reward, done, info = env.step(action)
    print(""Action:"", action)
    print(""Observation:"", observation)
    print(""Reward:"", reward)
    print(""Done:"", done)
    print(""Info:"", info)",train
gymnasium,1,"Develop a Python program using the 'gymnasium' API to play the 'CarRacing-v2' environment. The program should control a car within the environment using keyboard inputs (W, A, S, D) to accelerate, turn left, reverse, and turn right, respectively.",code/gymnasium/gymnasium_1.py,Test the program with different keyboard inputs and verify that the car's movements are responsive and accurate.,Validate that the program interprets the keyboard inputs correctly and controls the car within the environment as expected.,Ensure that the program successfully creates and runs the 'CarRacing-v2' environment from the 'gymnasium' API.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym
from gymnasium.utils import play
import numpy as np

env = gym.make(""CarRacing-v2"", render_mode = 'rgb_array')

mapping = {""w"": np.array([0, 0.7, 0]),
            ""a"": np.array([-1, 0, 0]),
            ""s"": np.array([0, 0, 1]),
            ""d"": np.array([1, 0, 0]),
            }

default_action = np.array([0,0,0])
play.play(env, keys_to_action=mapping, noop=default_action)",train
gymnasium,13,"Develop a Python program using the 'gymnasium' API to play the 'MsPacman-v0' environment. The program should control Ms. Pacman within the environment using keyboard inputs (A, D, W, S) to move left, move right, move up, and move down, respectively.",code/gymnasium/gymnasium_13.py,Test the program with different keyboard inputs and verify that Ms. Pacman's movements are responsive and accurate.,Ensure that the program successfully creates and runs the 'MsPacman-v0' environment from the 'gymnasium' API.,Validate that the program interprets the keyboard inputs correctly and controls Ms. Pacman within the environment as expected.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym
from gymnasium.utils import play

env = gym.make(""MsPacman-v0"")

mapping = {""a"": 3,
            ""d"": 2,
            ""w"": 1,
            ""s"": 0}

default_action = 0
play.play(env, keys_to_action=mapping, noop=default_action)",train
gymnasium,19,Develop a Python program using the 'gymnasium' API to play the 'Ant-v2' environment. The program should control the ant within the environment using continuous actions to move its joints and apply torque.,code/gymnasium/gymnasium_19.py,Validate that the program takes continuous actions to control the ant within the environment.,Ensure that the program successfully creates and runs the 'Ant-v2' environment from the 'gymnasium' API.,Test the program with different initial states of the ant and verify that the actions taken by the program are appropriate.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym

env = gym.make(""Ant-v2"")

done = False
while not done:
    action = env.action_space.sample()
    observation, reward, done, info = env.step(action)
    print(""Action:"", action)
    print(""Observation:"", observation)
    print(""Reward:"", reward)
    print(""Done:"", done)
    print(""Info:"", info)",train
gymnasium,24,Develop a Python program using the 'gymnasium' API to play the 'RoboschoolWalker2d-v1' environment. The program should control the walker within the environment using continuous actions to move its joints and apply torque.,code/gymnasium/gymnasium_24.py,Test the program with different initial states of the walker and verify that the actions taken by the program are appropriate.,Ensure that the program successfully creates and runs the 'RoboschoolWalker2d-v1' environment from the 'gymnasium' API.,Validate that the program takes continuous actions to control the walker within the environment.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym

env = gym.make(""RoboschoolWalker2d-v1"")

done = False
while not done:
    action = env.action_space.sample()
    observation, reward, done, info = env.step(action)
    print(""Action:"", action)
    print(""Observation:"", observation)
    print(""Reward:"", reward)
    print(""Done:"", done)
    print(""Info:"", info)",train
gymnasium,12,"Develop a Python program using the 'gymnasium' API to play the 'FrozenLake8x8-v0' environment. The program should control the agent within the environment using keyboard inputs (W, A, S, D) to move up, left, down, and right, respectively.",code/gymnasium/gymnasium_12.py,Test the program with different keyboard inputs and verify that the agent's movements are responsive and accurate.,Ensure that the program successfully creates and runs the 'FrozenLake8x8-v0' environment from the 'gymnasium' API.,Validate that the program interprets the keyboard inputs correctly and controls the agent within the environment as expected.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym
from gymnasium.utils import play

env = gym.make(""FrozenLake8x8-v0"")

mapping = {""w"": 0,
            ""a"": 3,
            ""s"": 2,
            ""d"": 1}

default_action = 0
play.play(env, keys_to_action=mapping, noop=default_action)",train
gymnasium,14,"Develop a Python program using the 'gymnasium' API to play the 'SpaceInvaders-v4' environment. The program should control the spaceship within the environment using keyboard inputs (A, D, W, S, Space) to move left, move right, move up, move down, and fire, respectively.",code/gymnasium/gymnasium_14.py,Ensure that the program successfully creates and runs the 'SpaceInvaders-v4' environment from the 'gymnasium' API.,Validate that the program interprets the keyboard inputs correctly and controls the spaceship within the environment as expected.,Test the program with different keyboard inputs and verify that the spaceship's movements and firing are responsive and accurate.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym
from gymnasium.utils import play

env = gym.make(""SpaceInvaders-v4"")

mapping = {""a"": 3,
            ""d"": 2,
            ""w"": 1,
            ""s"": 0,
            ""space"": 4}

default_action = 0
play.play(env, keys_to_action=mapping, noop=default_action)",train
gymnasium,17,"Develop a Python program using the 'gymnasium' API to play the 'Breakout-v4' environment. The program should control the paddle within the environment using keyboard inputs (A, D) to move left and right, respectively.",code/gymnasium/gymnasium_17.py,Ensure that the program successfully creates and runs the 'Breakout-v4' environment from the 'gymnasium' API.,Validate that the program interprets the keyboard inputs correctly and controls the paddle within the environment as expected.,Test the program with different keyboard inputs and verify that the paddle's movements are responsive and accurate.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym
from gymnasium.utils import play

env = gym.make(""Breakout-v4"")

mapping = {""a"": 3,
            ""d"": 2}

default_action = 1
play.play(env, keys_to_action=mapping, noop=default_action)",train
gymnasium,2,"Develop a Python program using the 'gymnasium' API to play the 'Pong-v0' environment. The program should control the paddle within the environment using keyboard inputs (W, S) to move up and down, respectively.",code/gymnasium/gymnasium_2.py,Ensure that the program successfully creates and runs the 'Pong-v0' environment from the 'gymnasium' API.,Validate that the program interprets the keyboard inputs correctly and controls the paddle within the environment as expected.,Test the program with different keyboard inputs and verify that the paddle's movements are responsive and accurate.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym
from gymnasium.utils import play

env = gym.make(""Pong-v0"")

mapping = {""w"": 2,
            ""s"": 3}

default_action = 0
play.play(env, keys_to_action=mapping, noop=default_action)",train
gymnasium,5,"Develop a Python program using the 'gymnasium' API to play the 'CartPole-v1' environment. The program should control the cart within the environment using keyboard inputs (A, D) to move left and right, respectively.",code/gymnasium/gymnasium_5.py,Ensure that the program successfully creates and runs the 'CartPole-v1' environment from the 'gymnasium' API.,Validate that the program interprets the keyboard inputs correctly and controls the cart within the environment as expected.,Test the program with different keyboard inputs and verify that the cart's movements are responsive and accurate.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym
from gymnasium.utils import play

env = gym.make(""CartPole-v1"")

mapping = {""a"": 0,
            ""d"": 1}

default_action = 0
play.play(env, keys_to_action=mapping, noop=default_action)",train
gymnasium,6,"Develop a Python program using the 'gymnasium' API to play the 'Breakout-v0' environment. The program should control the paddle within the environment using keyboard inputs (A, D) to move left and right, respectively.",code/gymnasium/gymnasium_6.py,Validate that the program interprets the keyboard inputs correctly and controls the paddle within the environment as expected.,Ensure that the program successfully creates and runs the 'Breakout-v0' environment from the 'gymnasium' API.,Test the program with different keyboard inputs and verify that the paddle's movements are responsive and accurate.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym
from gymnasium.utils import play

env = gym.make(""Breakout-v0"")

mapping = {""a"": 3,
            ""d"": 2}

default_action = 1
play.play(env, keys_to_action=mapping, noop=default_action)",train
gymnasium,3,"Develop a Python program using the 'gymnasium' API to play the 'LunarLander-v2' environment. The program should control the lunar lander within the environment using keyboard inputs (W, A, S, D) to fire the main engine, rotate left, rotate right, and do nothing, respectively.",code/gymnasium/gymnasium_3.py,Validate that the program interprets the keyboard inputs correctly and controls the lunar lander within the environment as expected.,Test the program with different keyboard inputs and verify that the lunar lander's movements are responsive and accurate.,Ensure that the program successfully creates and runs the 'LunarLander-v2' environment from the 'gymnasium' API.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym
from gymnasium.utils import play

env = gym.make(""LunarLander-v2"")

mapping = {""w"": 2,
            ""a"": 3,
            ""s"": 0,
            ""d"": 1}

default_action = 0
play.play(env, keys_to_action=mapping, noop=default_action)",train
gymnasium,18,Develop a Python program using the 'gymnasium' API to play the 'LunarLanderContinuous-v2' environment. The program should control the lunar lander within the environment using continuous actions to fire the main engine and rotate the lander left or right.,code/gymnasium/gymnasium_18.py,Test the program with different initial states of the lunar lander and verify that the actions taken by the program are appropriate.,Validate that the program takes continuous actions to control the lunar lander within the environment.,Ensure that the program successfully creates and runs the 'LunarLanderContinuous-v2' environment from the 'gymnasium' API.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym

env = gym.make(""LunarLanderContinuous-v2"")

done = False
while not done:
    action = env.action_space.sample()
    observation, reward, done, info = env.step(action)
    print(""Action:"", action)
    print(""Observation:"", observation)
    print(""Reward:"", reward)
    print(""Done:"", done)
    print(""Info:"", info)",train
gymnasium,25,Develop a Python program using the 'gymnasium' API to play the 'RoboschoolHalfCheetah-v1' environment. The program should control the half cheetah within the environment using continuous actions to move its joints and apply torque.,code/gymnasium/gymnasium_25.py,Test the program with different initial states of the half cheetah and verify that the actions taken by the program are appropriate.,Ensure that the program successfully creates and runs the 'RoboschoolHalfCheetah-v1' environment from the 'gymnasium' API.,Validate that the program takes continuous actions to control the half cheetah within the environment.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym

env = gym.make(""RoboschoolHalfCheetah-v1"")

done = False
while not done:
    action = env.action_space.sample()
    observation, reward, done, info = env.step(action)
    print(""Action:"", action)
    print(""Observation:"", observation)
    print(""Reward:"", reward)
    print(""Done:"", done)
    print(""Info:"", info)",train
gymnasium,4,"Develop a Python program using the 'gymnasium' API to play the 'MountainCar-v0' environment. The program should control the car within the environment using keyboard inputs (A, S, D) to move left, do nothing, and move right, respectively.",code/gymnasium/gymnasium_4.py,Test the program with different keyboard inputs and verify that the car's movements are responsive and accurate.,Validate that the program interprets the keyboard inputs correctly and controls the car within the environment as expected.,Ensure that the program successfully creates and runs the 'MountainCar-v0' environment from the 'gymnasium' API.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym
from gymnasium.utils import play

env = gym.make(""MountainCar-v0"")

mapping = {""a"": 0,
            ""s"": 1,
            ""d"": 2}

default_action = 1
play.play(env, keys_to_action=mapping, noop=default_action)",train
gymnasium,28,Develop a Python program using the 'gymnasium' API to play the 'RoboschoolInvertedDoublePendulum-v1' environment. The program should control the inverted double pendulum within the environment using continuous actions to apply torque to the joints.,code/gymnasium/gymnasium_28.py,Test the program with different initial states of the inverted double pendulum and verify that the actions taken by the program are appropriate.,Validate that the program takes continuous actions to control the inverted double pendulum within the environment.,Ensure that the program successfully creates and runs the 'RoboschoolInvertedDoublePendulum-v1' environment from the 'gymnasium' API.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym

env = gym.make(""RoboschoolInvertedDoublePendulum-v1"")

done = False
while not done:
    action = env.action_space.sample()
    observation, reward, done, info = env.step(action)
    print(""Action:"", action)
    print(""Observation:"", observation)
    print(""Reward:"", reward)
    print(""Done:"", done)
    print(""Info:"", info)",train
gymnasium,26,Develop a Python program using the 'gymnasium' API to play the 'RoboschoolHumanoidFlagrun-v1' environment. The program should control the humanoid within the environment using continuous actions to move its joints and apply torque.,code/gymnasium/gymnasium_26.py,Validate that the program takes continuous actions to control the humanoid within the environment.,Test the program with different initial states of the humanoid and verify that the actions taken by the program are appropriate.,Ensure that the program successfully creates and runs the 'RoboschoolHumanoidFlagrun-v1' environment from the 'gymnasium' API.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym

env = gym.make(""RoboschoolHumanoidFlagrun-v1"")

done = False
while not done:
    action = env.action_space.sample()
    observation, reward, done, info = env.step(action)
    print(""Action:"", action)
    print(""Observation:"", observation)
    print(""Reward:"", reward)
    print(""Done:"", done)
    print(""Info:"", info)",train
gymnasium,20,Develop a Python program using the 'gymnasium' API to play the 'Hopper-v2' environment. The program should control the hopper within the environment using continuous actions to move its joints and apply torque.,code/gymnasium/gymnasium_20.py,Ensure that the program successfully creates and runs the 'Hopper-v2' environment from the 'gymnasium' API.,Validate that the program takes continuous actions to control the hopper within the environment.,Test the program with different initial states of the hopper and verify that the actions taken by the program are appropriate.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym

env = gym.make(""Hopper-v2"")

done = False
while not done:
    action = env.action_space.sample()
    observation, reward, done, info = env.step(action)
    print(""Action:"", action)
    print(""Observation:"", observation)
    print(""Reward:"", reward)
    print(""Done:"", done)
    print(""Info:"", info)",train
gymnasium,31,Develop a Python program using the 'gymnasium' API to play the 'RoboschoolHumanoidFlagrunHarder-v1' environment. The program should control the humanoid within the environment using continuous actions to move its joints and apply torque.,code/gymnasium/gymnasium_31.py,Test the program with different initial states of the humanoid and verify that the actions taken by the program are appropriate.,Validate that the program takes continuous actions to control the humanoid within the environment.,Ensure that the program successfully creates and runs the 'RoboschoolHumanoidFlagrunHarder-v1' environment from the 'gymnasium' API.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym

env = gym.make(""RoboschoolHumanoidFlagrunHarder-v1"")

done = False
while not done:
    action = env.action_space.sample()
    observation, reward, done, info = env.step(action)
    print(""Action:"", action)
    print(""Observation:"", observation)
    print(""Reward:"", reward)
    print(""Done:"", done)
    print(""Info:"", info)",train
gymnasium,22,Develop a Python program using the 'gymnasium' API to play the 'RoboschoolAnt-v1' environment. The program should control the ant within the environment using continuous actions to move its joints and apply torque.,code/gymnasium/gymnasium_22.py,Validate that the program takes continuous actions to control the ant within the environment.,Test the program with different initial states of the ant and verify that the actions taken by the program are appropriate.,Ensure that the program successfully creates and runs the 'RoboschoolAnt-v1' environment from the 'gymnasium' API.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym

env = gym.make(""RoboschoolAnt-v1"")

done = False
while not done:
    action = env.action_space.sample()
    observation, reward, done, info = env.step(action)
    print(""Action:"", action)
    print(""Observation:"", observation)
    print(""Reward:"", reward)
    print(""Done:"", done)
    print(""Info:"", info)",train
gymnasium,8,"Develop a Python program using the 'gymnasium' API to play the 'Acrobot-v1' environment. The program should control the two joints of the acrobot within the environment using keyboard inputs (W, A, S, D) to apply torque to the first joint clockwise, counterclockwise, to the second joint clockwise, and counterclockwise, respectively.",code/gymnasium/gymnasium_8.py,Validate that the program interprets the keyboard inputs correctly and controls the acrobot within the environment as expected.,Ensure that the program successfully creates and runs the 'Acrobot-v1' environment from the 'gymnasium' API.,Test the program with different keyboard inputs and verify that the acrobot's movements are responsive and accurate.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym
from gymnasium.utils import play

env = gym.make(""Acrobot-v1"")

mapping = {""w"": 1,
            ""a"": 0,
            ""s"": 3,
            ""d"": 2}

default_action = 0
play.play(env, keys_to_action=mapping, noop=default_action)",train
gymnasium,11,Develop a Python program using the 'gymnasium' API to play the 'Blackjack-v0' environment. The program should play the game of blackjack by taking actions based on the current state of the game.,code/gymnasium/gymnasium_11.py,Validate that the program takes actions based on the current state of the game and interacts with the environment correctly.,Ensure that the program successfully creates and runs the 'Blackjack-v0' environment from the 'gymnasium' API.,Test the program with different initial states of the game and verify that the actions taken by the program are appropriate.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym

env = gym.make(""Blackjack-v0"")

done = False
while not done:
    action = env.action_space.sample()
    observation, reward, done, info = env.step(action)
    print(""Action:"", action)
    print(""Observation:"", observation)
    print(""Reward:"", reward)
    print(""Done:"", done)
    print(""Info:"", info)",train
gymnasium,15,"Develop a Python program using the 'gymnasium' API to play the 'Assault-v0' environment. The program should control the spaceship within the environment using keyboard inputs (A, D, W, S, Space) to move left, move right, move up, move down, and fire, respectively.",code/gymnasium/gymnasium_15.py,Ensure that the program successfully creates and runs the 'Assault-v0' environment from the 'gymnasium' API.,Validate that the program interprets the keyboard inputs correctly and controls the spaceship within the environment as expected.,Test the program with different keyboard inputs and verify that the spaceship's movements and firing are responsive and accurate.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym
from gymnasium.utils import play

env = gym.make(""Assault-v0"")

mapping = {""a"": 3,
            ""d"": 2,
            ""w"": 1,
            ""s"": 0,
            ""space"": 4}

default_action = 0
play.play(env, keys_to_action=mapping, noop=default_action)",train
gymnasium,21,Develop a Python program using the 'gymnasium' API to play the 'Humanoid-v2' environment. The program should control the humanoid within the environment using continuous actions to move its joints and apply torque.,code/gymnasium/gymnasium_21.py,Ensure that the program successfully creates and runs the 'Humanoid-v2' environment from the 'gymnasium' API.,Test the program with different initial states of the humanoid and verify that the actions taken by the program are appropriate.,Validate that the program takes continuous actions to control the humanoid within the environment.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym

env = gym.make(""Humanoid-v2"")

done = False
while not done:
    action = env.action_space.sample()
    observation, reward, done, info = env.step(action)
    print(""Action:"", action)
    print(""Observation:"", observation)
    print(""Reward:"", reward)
    print(""Done:"", done)
    print(""Info:"", info)",train
gymnasium,7,"Develop a Python program using the 'gymnasium' API to play the 'SpaceInvaders-v0' environment. The program should control the spaceship within the environment using keyboard inputs (A, D, W, S, Space) to move left, move right, move up, move down, and fire, respectively.",code/gymnasium/gymnasium_7.py,Ensure that the program successfully creates and runs the 'SpaceInvaders-v0' environment from the 'gymnasium' API.,Validate that the program interprets the keyboard inputs correctly and controls the spaceship within the environment as expected.,Test the program with different keyboard inputs and verify that the spaceship's movements and firing are responsive and accurate.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym
from gymnasium.utils import play

env = gym.make(""SpaceInvaders-v0"")

mapping = {""a"": 3,
            ""d"": 2,
            ""w"": 1,
            ""s"": 0,
            ""space"": 4}

default_action = 0
play.play(env, keys_to_action=mapping, noop=default_action)",train
gymnasium,10,"Develop a Python program using the 'gymnasium' API to play the 'Taxi-v3' environment. The program should control the taxi within the environment using keyboard inputs (W, A, S, D) to move up, left, down, and right, respectively.",code/gymnasium/gymnasium_10.py,Validate that the program interprets the keyboard inputs correctly and controls the taxi within the environment as expected.,Ensure that the program successfully creates and runs the 'Taxi-v3' environment from the 'gymnasium' API.,Test the program with different keyboard inputs and verify that the taxi's movements are responsive and accurate.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym
from gymnasium.utils import play

env = gym.make(""Taxi-v3"")

mapping = {""w"": 0,
            ""a"": 3,
            ""s"": 2,
            ""d"": 1}

default_action = 0
play.play(env, keys_to_action=mapping, noop=default_action)",test
gymnasium,27,Develop a Python program using the 'gymnasium' API to play the 'RoboschoolInvertedPendulum-v1' environment. The program should control the inverted pendulum within the environment using continuous actions to apply torque to the joint.,code/gymnasium/gymnasium_27.py,Ensure that the program successfully creates and runs the 'RoboschoolInvertedPendulum-v1' environment from the 'gymnasium' API.,Validate that the program takes continuous actions to control the inverted pendulum within the environment.,Test the program with different initial states of the inverted pendulum and verify that the actions taken by the program are appropriate.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym

env = gym.make(""RoboschoolInvertedPendulum-v1"")

done = False
while not done:
    action = env.action_space.sample()
    observation, reward, done, info = env.step(action)
    print(""Action:"", action)
    print(""Observation:"", observation)
    print(""Reward:"", reward)
    print(""Done:"", done)
    print(""Info:"", info)",test
gymnasium,9,"Develop a Python program using the 'gymnasium' API to play the 'FrozenLake-v0' environment. The program should control the agent within the environment using keyboard inputs (W, A, S, D) to move up, left, down, and right, respectively.",code/gymnasium/gymnasium_9.py,Ensure that the program successfully creates and runs the 'FrozenLake-v0' environment from the 'gymnasium' API.,Test the program with different keyboard inputs and verify that the agent's movements are responsive and accurate.,Validate that the program interprets the keyboard inputs correctly and controls the agent within the environment as expected.,,,"#!pip install ale_py gymnasium
#!pip install swig
#!pip install gymnasium[box2d]
#!pip install gymnasium[classic-control]
import gymnasium as gym
from gymnasium.utils import play

env = gym.make(""FrozenLake-v0"")

mapping = {""w"": 0,
            ""a"": 3,
            ""s"": 2,
            ""d"": 1}

default_action = 0
play.play(env, keys_to_action=mapping, noop=default_action)",test
holidays,16,Create a Python program using the 'holidays' API to determine the next upcoming public holiday in a specified country and state.,code/holidays/holidays_16.py,Test if the program accurately identifies the next upcoming public holiday in the specified state of the country.,Verify the program\'s behavior when there are no upcoming public holidays in the rest of the year.,Check the program for performance with a state that has many holidays throughout the year.,,,"#!pip install holidays
from datetime import date
import holidays

# Specify the country or region (e.g., 'US' for the United States)
country_code = 'US'

# Specify the state or province code (e.g., 'CA' for California)
state_code = 'CA'

# Get the current date
current_date = date.today()

# Create a dictionary of holidays for the specified state
holiday_dict = holidays.CountryHoliday(country_code, observed=True, years=current_date.year)

# Find the next upcoming public holiday
next_holiday = None
for holiday in sorted(holiday_dict.keys()):
    if holiday > current_date:
        next_holiday = holiday
        break

# Display the result
if next_holiday:
    print(f""The next upcoming public holiday in {state_code}, {country_code} is on {next_holiday}."")
else:
    print(f""There are no upcoming public holidays in {state_code}, {country_code} for the rest of the year."")",train
holidays,2,Create a Python program using the 'holidays' API to list all the public holidays in a specific country or region for a given year.,code/holidays/holidays_2.py,Check the program for performance with a year that has a large number of holidays.,Verify that the program correctly handles a country code that is not supported by the holidays API.,Test if the program accurately lists all known public holidays in the specified country for a given year.,,,"#!pip install holidays
import holidays

# Specify the country or region (e.g., 'US' for the United States)
country_code = 'US'

# Specify the year for which you want to list holidays
year = 2023

# Create a dictionary of holidays for the specified country and year
holiday_dict = holidays.CountryHoliday(country_code, observed=True, years=year)

# List all the public holidays
holiday_list = sorted(holiday_dict.keys())

# Display the list of holidays
print(f""Public holidays in {country_code} for the year {year}:"")
for holiday in holiday_list:
    print(holiday)",train
holidays,9,"Create a Python program using the 'holidays' API to check if a specific date is a public holiday in a given region (e.g., a city or district) within a country.",code/holidays/holidays_9.py,Verify that the program accurately identifies a date that is not a public holiday in the specified region.,Check the program for performance with a region that has a large number of public holidays.,Test if the program correctly identifies a known public holiday in the specified region within the country.,,,"#!pip install holidays
import holidays

# Define the date to check
date_to_check = ""2023-07-04"" 

# Specify the country or region (e.g., 'US' for the United States)
country_code = 'US'

# Specify the region (e.g., 'NYC' for New York City)
region = 'NYC'

# Create a dictionary of holidays for the specified region
holiday_dict = holidays.RegionHoliday(country_code, region, observed=True, years=2023)

# Check if the date is a public holiday in the specified region
is_holiday = date_to_check in holiday_dict

# Display the result
if is_holiday:
    print(f""{date_to_check} is a public holiday in {region}, {country_code}."")
else:
    print(f""{date_to_check} is not a public holiday in {region}, {country_code}."")",train
holidays,6,Create a Python program using the 'holidays' API to list all the public holidays in a specific state or province of a country for a given year.,code/holidays/holidays_6.py,Test if the program accurately lists all known public holidays in the specified state or province for a given year.,Verify that the program correctly handles a state code that is not supported by the holidays API.,Check the program for performance with a state or province that has a large number of public holidays.,,,"#!pip install holidays
import holidays

# Specify the country or region (e.g., 'US' for the United States)
country_code = 'US'

# Specify the state or province code (e.g., 'CA' for California)
state_code = 'CA'

# Specify the year for which you want to list holidays
year = 2023

# Create a dictionary of holidays for the specified state or province and year
holiday_dict = holidays.StateHoliday(country_code, state_code, observed=True, years=year)

# List all the public holidays
holiday_list = sorted(holiday_dict.keys())

# Display the list of holidays
print(f""Public holidays in {state_code}, {country_code} for the year {year}:"")
for holiday in holiday_list:
    print(holiday)",train
holidays,12,"Create a Python program using the 'holidays' API to determine the next upcoming public holiday in a specified region (e.g., a city or district) within a country.",code/holidays/holidays_12.py,Check the program for performance with a region that has many holidays throughout the year.,Test if the program accurately identifies the next upcoming public holiday in the specified region within the country.,Verify the program\'s behavior when there are no upcoming public holidays in the rest of the year.,,,"#!pip install holidays
from datetime import date
import holidays

# Specify the country or region (e.g., 'US' for the United States)
country_code = 'US'

# Specify the region (e.g., 'NYC' for New York City)
region = 'NYC'

# Get the current date
current_date = date.today()

# Create a dictionary of holidays for the specified region
holiday_dict = holidays.RegionHoliday(country_code, region, observed=True, years=current_date.year)

# Find the next upcoming public holiday
next_holiday = None
for holiday in sorted(holiday_dict.keys()):
    if holiday > current_date:
        next_holiday = holiday
        break

# Display the result
if next_holiday:
    print(f""The next upcoming public holiday in {region}, {country_code} is on {next_holiday}."")
else:
    print(f""There are no upcoming public holidays in {region}, {country_code} for the rest of the year."")",train
holidays,4,Create a Python program using the 'holidays' API to determine the next upcoming public holiday in a specified country or region.,code/holidays/holidays_4.py,Verify the program's behavior when there are no upcoming public holidays in the rest of the year.,Test if the program accurately identifies the next upcoming public holiday in the specified country or region.,Check the program for performance with a country that has many holidays throughout the year.,,,"#!pip install holidays
from datetime import date
import holidays

# Specify the country or region (e.g., 'US' for the United States)
country_code = 'US'

# Get the current date
current_date = date.today()

# Create a dictionary of holidays for the specified country
holiday_dict = holidays.CountryHoliday(country_code, observed=True, years=current_date.year)

# Find the next upcoming public holiday
next_holiday = None
for holiday in sorted(holiday_dict.keys()):
    if holiday > current_date:
        next_holiday = holiday
        break

# Display the result
if next_holiday:
    print(f""The next upcoming public holiday in {country_code} is on {next_holiday}."")
else:
    print(f""There are no upcoming public holidays in {country_code} for the rest of the year."")",train
holidays,19,"Create a Python program using the 'holidays' API to calculate the number of working days between two specified dates in a particular region (e.g., a city or district) within a country and state, excluding public holidays.",code/holidays/holidays_19.py,Verify that the program handles cases where the start date is later than the end date.,Check the program with a region that has a large number of public holidays.,Test if the program correctly calculates the number of working days between two specified dates in the specified region within the country and state.,,,"#!pip install holidays
from datetime import date, timedelta
import holidays

# Specify the country or region (e.g., 'US' for the United States)
country_code = 'US'

# Specify the state or province code (e.g., 'CA' for California)
state_code = 'CA'

# Specify the region (e.g., 'LA' for Los Angeles)
region = 'LA'

# Define the start and end dates
start_date = date(2023, 1, 1)
end_date = date(2023, 12, 31)

# Create a dictionary of holidays for the specified region
holiday_dict = holidays.RegionHoliday(country_code, region, observed=True, years=2023)

# Initialize a count for working days
working_days = 0

# Iterate through the date range and count working days
current_date = start_date
while current_date <= end_date:
    if current_date.weekday() < 5 and current_date not in holiday_dict:
        working_days += 1
    current_date += timedelta(days=1)

# Display the result
print(f""Number of working days in {region}, {state_code}, {country_code} between {start_date} and {end_date}: {working_days}"")",train
holidays,17,"Create a Python program using the 'holidays' API to check if a specific date is a public holiday in a given region (e.g., a city or district) within a country and state.",code/holidays/holidays_17.py,Verify that the program accurately identifies a date that is not a public holiday in the specified region.,Test if the program correctly identifies a known public holiday in the specified region within the country and state.,Check the program for performance with a region that has a large number of public holidays.,,,"#!pip install holidays
import holidays



# Define the date to check
date_to_check = ""2023-07-04"" 

# Specify the country or region (e.g., 'US' for the United States)
country_code = 'US'

# Specify the state or province code (e.g., 'CA' for California)
state_code = 'CA'

# Specify the region (e.g., 'LA' for Los Angeles)
region = 'LA'

# Create a dictionary of holidays for the specified region
holiday_dict = holidays.RegionHoliday(country_code, region, observed=True, years=2023)

# Check if the date is a public holiday in the specified region
is_holiday = date_to_check in holiday_dict

# Display the result
if is_holiday:
    print(f""{date_to_check} is a public holiday in {region}, {state_code}, {country_code}."")
else:
    print(f""{date_to_check} is not a public holiday in {region}, {state_code}, {country_code}."")",train
holidays,14,Create a Python program using the 'holidays' API to list all the public holidays in a specific country and state for a given year.,code/holidays/holidays_14.py,Test if the program accurately lists all known public holidays in the specified state of the country for a given year.,Verify that the program correctly handles a state code that is not supported by the holidays API.,Check the program for performance with a state that has a large number of public holidays.,,,"#!pip install holidays
import holidays

# Specify the country or region (e.g., 'US' for the United States)
country_code = 'US'

# Specify the state or province code (e.g., 'CA' for California)
state_code = 'CA'

# Specify the year for which you want to list holidays
year = 2023

# Create a dictionary of holidays for the specified state and year
holiday_dict = holidays.CountryHoliday(country_code, observed=True, years=year)

# List all the public holidays
holiday_list = sorted(holiday_dict.keys())

# Display the list of holidays
print(f""Public holidays in {state_code}, {country_code} for the year {year}:"")
for holiday in holiday_list:
    print(holiday)",train
holidays,3,"Create a Python program using the 'holidays' API to calculate the number of working days between two specified dates in a particular country or region, excluding public holidays.",code/holidays/holidays_3.py,Verify that the program handles cases where the start date is later than the end date.,Test if the program correctly calculates the number of working days between two specified dates in the specified country or region.,Check the program with a country that has a large number of public holidays.,,,"#!pip install holidays
from datetime import date, timedelta
import holidays

# Specify the country or region (e.g., 'US' for the United States)
country_code = 'US'

# Define the start and end dates
start_date = date(2023, 1, 1)
end_date = date(2023, 12, 31)

# Create a dictionary of holidays for the specified country
holiday_dict = holidays.CountryHoliday(country_code, observed=True, years=2023)

# Initialize a count for working days
working_days = 0

# Iterate through the date range and count working days
current_date = start_date
while current_date <= end_date:
    if current_date.weekday() < 5 and current_date not in holiday_dict:
        working_days += 1
    current_date += timedelta(days=1)

# Display the result
print(f""Number of working days in {country_code} between {start_date} and {end_date}: {working_days}"")",train
holidays,10,"Create a Python program using the 'holidays' API to list all the public holidays in a specific region (e.g., a city or district) within a country for a given year.",code/holidays/holidays_10.py,Check the program for performance with a region that has a large number of public holidays.,Verify that the program correctly handles a region code that is not supported by the holidays API.,Test if the program accurately lists all known public holidays in the specified region within the country for a given year.,,,"#!pip install holidays
import holidays

# Specify the country or region (e.g., 'US' for the United States)
country_code = 'US'

# Specify the region (e.g., 'NYC' for New York City)
region = 'NYC'

# Specify the year for which you want to list holidays
year = 2023

# Create a dictionary of holidays for the specified region and year
holiday_dict = holidays.RegionHoliday(country_code, region, observed=True, years=year)

# List all the public holidays
holiday_list = sorted(holiday_dict.keys())

# Display the list of holidays
print(f""Public holidays in {region}, {country_code} for the year {year}:"")
for holiday in holiday_list:
    print(holiday)",train
holidays,20,"Create a Python program using the 'holidays' API to determine the next upcoming public holiday in a specified region (e.g., a city or district) within a country and state.",code/holidays/holidays_20.py,Test if the program accurately identifies the next upcoming public holiday in the specified region within the country and state.,Check the program for performance with a region that has many holidays throughout the year.,Verify the program\'s behavior when there are no upcoming public holidays in the rest of the year.,,,"#!pip install holidays
from datetime import date
import holidays

# Specify the country or region (e.g., 'US' for the United States)
country_code = 'US'

# Specify the state or province code (e.g., 'CA' for California)
state_code = 'CA'

# Specify the region (e.g., 'LA' for Los Angeles)
region = 'LA'

# Get the current date
current_date = date.today()

# Create a dictionary of holidays for the specified region
holiday_dict = holidays.RegionHoliday(country_code, region, observed=True, years=current_date.year)

# Find the next upcoming public holiday
next_holiday = None
for holiday in sorted(holiday_dict.keys()):
    if holiday > current_date:
        next_holiday = holiday
        break

# Display the result
if next_holiday:
    print(f""The next upcoming public holiday in {region}, {state_code}, {country_code} is on {next_holiday}."")
else:
    print(f""There are no upcoming public holidays in {region}, {state_code}, {country_code} for the rest of the year."")",train
holidays,5,Create a Python program using the 'holidays' API to check if a specific date is a public holiday in a given state or province of a country.,code/holidays/holidays_5.py,Verify that the program accurately identifies a date that is not a public holiday in the specified state or province.,Check the program for performance with a state or province that has a large number of public holidays.,Test if the program correctly identifies a known public holiday in the specified state or province of the country.,,,"#!pip install holidays
import holidays

# Define the date to check
date_to_check = ""2023-07-04"" 

# Specify the country or region (e.g., 'US' for the United States)
country_code = 'US'

# Specify the state or province code (e.g., 'CA' for California)
state_code = 'CA'

# Create a dictionary of holidays for the specified state or province
holiday_dict = holidays.StateHoliday(country_code, state_code, observed=True, years=2023)

# Check if the date is a public holiday in the specified state or province
is_holiday = date_to_check in holiday_dict

# Display the result
if is_holiday:
    print(f""{date_to_check} is a public holiday in {state_code}, {country_code}."")
else:
    print(f""{date_to_check} is not a public holiday in {state_code}, {country_code}."")",train
holidays,13,Create a Python program using the 'holidays' API to check if a specific date is a public holiday in a given country and state.,code/holidays/holidays_13.py,Verify that the program accurately identifies a date that is not a public holiday in the specified state.,Test if the program correctly identifies a known public holiday in the specified state of the country.,Check the program for performance with a state that has a large number of public holidays.,,,"#!pip install holidays
import holidays

# Define the date to check
date_to_check = ""2023-07-04"" 

# Specify the country or region (e.g., 'US' for the United States)
country_code = 'US'

# Specify the state or province code (e.g., 'CA' for California)
state_code = 'CA'

# Create a dictionary of holidays for the specified country and state
holiday_dict = holidays.CountryHoliday(country_code, observed=True, years=2023)

# Check if the date is a public holiday in the specified state
is_holiday = date_to_check in holiday_dict

# Display the result
if is_holiday:
    print(f""{date_to_check} is a public holiday in {state_code}, {country_code}."")
else:
    print(f""{date_to_check} is not a public holiday in {state_code}, {country_code}."")",train
holidays,8,Create a Python program using the 'holidays' API to determine the next upcoming public holiday in a specified state or province of a country.,code/holidays/holidays_8.py,Check the program for performance with a state or province that has many holidays throughout the year.,Verify the program\'s behavior when there are no upcoming public holidays in the rest of the year.,Test if the program accurately identifies the next upcoming public holiday in the specified state or province of the country.,,,"#!pip install holidays
from datetime import date
import holidays

# Specify the country or region (e.g., 'US' for the United States)
country_code = 'US'

# Specify the state or province code (e.g., 'CA' for California)
state_code = 'CA'

# Get the current date
current_date = date.today()

# Create a dictionary of holidays for the specified state or province
holiday_dict = holidays.StateHoliday(country_code, state_code, observed=True, years=current_date.year)

# Find the next upcoming public holiday
next_holiday = None
for holiday in sorted(holiday_dict.keys()):
    if holiday > current_date:
        next_holiday = holiday
        break

# Display the result
if next_holiday:
    print(f""The next upcoming public holiday in {state_code}, {country_code} is on {next_holiday}."")
else:
    print(f""There are no upcoming public holidays in {state_code}, {country_code} for the rest of the year."")",train
holidays,11,"Create a Python program using the 'holidays' API to calculate the number of working days between two specified dates in a particular region (e.g., a city or district) within a country, excluding public holidays.",code/holidays/holidays_11.py,Verify that the program handles cases where the start date is later than the end date.,Test if the program correctly calculates the number of working days between two specified dates in the specified region within the country.,Check the program with a region that has a large number of public holidays.,,,"#!pip install holidays
from datetime import date, timedelta
import holidays

# Specify the country or region (e.g., 'US' for the United States)
country_code = 'US'

# Specify the region (e.g., 'NYC' for New York City)
region = 'NYC'

# Define the start and end dates
start_date = date(2023, 1, 1)
end_date = date(2023, 12, 31)

# Create a dictionary of holidays for the specified region
holiday_dict = holidays.RegionHoliday(country_code, region, observed=True, years=2023)

# Initialize a count for working days
working_days = 0

# Iterate through the date range and count working days
current_date = start_date
while current_date <= end_date:
    if current_date.weekday() < 5 and current_date not in holiday_dict:
        working_days += 1
    current_date += timedelta(days=1)

# Display the result
print(f""Number of working days in {region}, {country_code} between {start_date} and {end_date}: {working_days}"")",train
holidays,15,"Create a Python program using the 'holidays' API to calculate the number of working days between two specified dates in a particular country and state, excluding public holidays.",code/holidays/holidays_15.py,Verify that the program handles cases where the start date is later than the end date.,Test if the program correctly calculates the number of working days between two specified dates in the specified state of the country.,Check the program with a state that has a large number of public holidays.,,,"#!pip install holidays
from datetime import date, timedelta
import holidays

# Specify the country or region (e.g., 'US' for the United States)
country_code = 'US'

# Specify the state or province code (e.g., 'CA' for California)
state_code = 'CA'

# Define the start and end dates
start_date = date(2023, 1, 1)
end_date = date(2023, 12, 31)

# Create a dictionary of holidays for the specified state
holiday_dict = holidays.CountryHoliday(country_code, observed=True, years=2023)

# Initialize a count for working days
working_days = 0

# Iterate through the date range and count working days
current_date = start_date
while current_date <= end_date:
    if current_date.weekday() < 5 and current_date not in holiday_dict:
        working_days += 1
    current_date += timedelta(days=1)

# Display the result
print(f""Number of working days in {state_code}, {country_code} between {start_date} and {end_date}: {working_days}"")",train
holidays,7,"Create a Python program using the 'holidays' API to calculate the number of working days between two specified dates in a particular state or province of a country, excluding public holidays.",code/holidays/holidays_7.py,Verify that the program handles cases where the start date is later than the end date.,Test if the program correctly calculates the number of working days between two specified dates in the specified state or province of the country.,Check the program with a state or province that has a large number of public holidays.,,,"#!pip install holidays
from datetime import date, timedelta
import holidays

# Specify the country or region (e.g., 'US' for the United States)
country_code = 'US'

# Specify the state or province code (e.g., 'CA' for California)
state_code = 'CA'

# Define the start and end dates
start_date = date(2023, 1, 1)


end_date = date(2023, 12, 31)

# Create a dictionary of holidays for the specified state or province
holiday_dict = holidays.StateHoliday(country_code, state_code, observed=True, years=2023)

# Initialize a count for working days
working_days = 0

# Iterate through the date range and count working days
current_date = start_date
while current_date <= end_date:
    if current_date.weekday() < 5 and current_date not in holiday_dict:
        working_days += 1
    current_date += timedelta(days=1)

# Display the result
print(f""Number of working days in {state_code}, {country_code} between {start_date} and {end_date}: {working_days}"")",train
holidays,1,Create a Python program using the 'holidays' API to check if a specified date is a public holiday in a given country or region.,code/holidays/holidays_1.py,Verify that the program accurately identifies a date that is not a public holiday in the specified country or region.,Test if the program correctly identifies a known public holiday in the specified country or region.,Check the program's behavior when using an invalid date or an unsupported country code.,,,"#!pip install holidays
import holidays

# Define the date to check
date_to_check = ""2023-07-04"" 

# Specify the country or region (e.g., 'US' for the United States)
country_code = 'US'

# Create a dictionary of holidays for the specified country or region
holiday_dict = holidays.country_holidays(country_code)

# Check if the date is a public holiday
is_holiday = date_to_check in holiday_dict

# Display the result
if is_holiday:
    print(f""{date_to_check} is a public holiday in {country_code}."")
else:
    print(f""{date_to_check} is not a public holiday in {country_code}."")",test
holidays,18,"Create a Python program using the 'holidays' API to list all the public holidays in a specific region (e.g., a city or district) within a country and state for a given year.",code/holidays/holidays_18.py,Check the program for performance with a region that has a large number of public holidays.,Verify that the program correctly handles a region code that is not supported by the holidays API.,Test if the program accurately lists all known public holidays in the specified region within the country and state for a given year.,,,"#!pip install holidays
import holidays

# Specify the country or region (e.g., 'US' for the United States)
country_code = 'US'

# Specify the state or province code (e.g., 'CA' for California)
state_code = 'CA'

# Specify the region (e.g., 'LA' for Los Angeles)
region = 'LA'

# Specify the year for which you want to list holidays
year = 2023

# Create a dictionary of holidays for the specified region and year
holiday_dict = holidays.RegionHoliday(country_code, region, observed=True, years=year)

# List all the public holidays
holiday_list = sorted(holiday_dict.keys())

# Display the list of holidays
print(f""Public holidays in {region}, {state_code}, {country_code} for the year {year}:"")
for holiday in holiday_list:
    print(holiday)",test
hypothesis,18,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of integers contains any prime numbers. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the prime number checking function.,code/hypothesis/hypothesis_18.py,Validate that the 'has_prime_numbers' function correctly identifies whether the generated input lists contain any prime numbers.,Test that the program successfully generates and tests hypothetical input lists for the 'has_prime_numbers' function.,Ensure that the 'has_prime_numbers' function handles edge cases such as empty lists and lists with only non-prime numbers correctly.,,,"#!pip install hypothesis
import math
import unittest
from hypothesis import given, strategies as st

def has_prime_numbers(lst):
    return any(is_prime(num) for num in lst)

def is_prime(n):
    if n < 2:
        return False
    for i in range(2, int(math.sqrt(n))+1):
        if n % i == 0:
            return False
    return True

class HasPrimeNumbersTest(unittest.TestCase):
    @given(st.lists(st.integers()))
    def test_has_prime_numbers(self, lst):
        result = has_prime_numbers(lst)
        self.assertEqual(result, any(is_prime(num) for num in lst))

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,34,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any valid phone numbers. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the phone number checking function.,code/hypothesis/hypothesis_34.py,Test that the program successfully generates and tests hypothetical input lists for the 'has_valid_phone_numbers' function.,Ensure that the 'has_valid_phone_numbers' function handles edge cases such as empty lists and lists with only strings without valid phone numbers correctly.,Validate that the 'has_valid_phone_numbers' function correctly identifies whether the generated input lists contain any valid phone numbers.,,,"#!pip install hypothesis
import re
import unittest
from hypothesis import given, strategies as st

def is_valid_phone_number(phone_number):
    pattern = r""^\+?[1-9]\d{1,14}$""
    return re.match(pattern, phone_number) is not None

def has_valid_phone_numbers(lst):
    return any(is_valid_phone_number(s) for s in lst)

class HasValidPhoneNumbersTest(unittest.TestCase):
    @given(st.lists(st.text(min_size=7, max_size=15, alphabet=st.characters(whitelist_categories=('Nd', 'Pd')))))
    def test_has_valid_phone_numbers(self, lst):
        result = has_valid_phone_numbers(lst)
        self.assertEqual(result, any(is_valid_phone_number(s) for s in lst))

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,23,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of integers contains any odd numbers. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the odd number checking function.,code/hypothesis/hypothesis_23.py,Ensure that the 'has_odd_numbers' function handles edge cases such as empty lists and lists with only even numbers correctly.,Validate that the 'has_odd_numbers' function correctly identifies whether the generated input lists contain any odd numbers.,Test that the program successfully generates and tests hypothetical input lists for the 'has_odd_numbers' function.,,,"#!pip install hypothesis
import unittest
from hypothesis import given, strategies as st

def has_odd_numbers(lst):
    return any(num % 2 != 0 for num in lst)

class HasOddNumbersTest(unittest.TestCase):
    @given(st.lists(st.integers()))
    def test_has_odd_numbers(self, lst):
        result = has_odd_numbers(lst)
        self.assertEqual(result, any(num % 2 != 0 for num in lst))

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,13,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings is sorted in lexicographic order. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the sorting check function.,code/hypothesis/hypothesis_13.py,Validate that the 'is_lexicographically_sorted' function correctly identifies whether the generated input lists are sorted in lexicographic order.,Ensure that the 'is_lexicographically_sorted' function handles edge cases such as empty lists and lists with duplicate elements correctly.,Test that the program successfully generates and tests hypothetical input lists for the 'is_lexicographically_sorted' function.,,,"#!pip install hypothesis
import unittest
from hypothesis import given, strategies as st

def is_lexicographically_sorted(lst):
    return lst == sorted(lst)

class IsLexicographicallySortedTest(unittest.TestCase):
    @given(st.lists(st.text()))
    def test_is_lexicographically_sorted(self, lst):
        result = is_lexicographically_sorted(lst)
        self.assertEqual(result, lst == sorted(lst))

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,9,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any duplicate elements. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the duplicate checking function.,code/hypothesis/hypothesis_9.py,Validate that the 'has_duplicates' function correctly identifies whether the generated input lists contain any duplicate elements.,Ensure that the 'has_duplicates' function handles edge cases such as empty lists and lists with only unique elements correctly.,Test that the program successfully generates and tests hypothetical input lists for the 'has_duplicates' function.,,,"#!pip install hypothesis
import unittest
from hypothesis import given, strategies as st

def has_duplicates(lst):
    return len(lst) != len(set(lst))

class HasDuplicatesTest(unittest.TestCase):
    @given(st.lists(st.text()))
    def test_has_duplicates(self, lst):
        result = has_duplicates(lst)
        self.assertEqual(result, len(lst) != len(set(lst)))

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,19,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given string is a valid phone number. The 'hypothesis' library will be used to generate various input strings and verify the correctness of the phone number validation function.,code/hypothesis/hypothesis_19.py,Test that the program successfully generates and tests hypothetical input strings for the 'is_valid_phone_number' function.,Ensure that the 'is_valid_phone_number' function handles edge cases such as empty strings and strings with special characters correctly.,Validate that the 'is_valid_phone_number' function correctly identifies whether the generated input strings are valid phone numbers.,,,"#!pip install hypothesis
import re
import unittest
from hypothesis import given, strategies as st

def is_valid_phone_number(phone_number):
    pattern = r""^\+?[1-9]\d{1,14}$""
    return re.match(pattern, phone_number) is not None

class ValidPhoneNumberTest(unittest.TestCase):
    @given(st.text(min_size=1, max_size=15, alphabet=st.characters(whitelist_categories=('Nd', 'Pd'))))
    def test_is_valid_phone_number(self, phone_number):
        result = is_valid_phone_number(phone_number)
        self.assertEqual(result, re.match(r""^\+?[1-9]\d{1,14}$"", phone_number) is not None)

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,10,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that calculates the sum of all even numbers up to a given number. The 'hypothesis' library will be used to generate various input numbers and verify the correctness of the sum calculation function.,code/hypothesis/hypothesis_10.py,Test that the program successfully generates and tests hypothetical input numbers for the 'sum_even_numbers' function.,Ensure that the 'sum_even_numbers' function handles edge cases such as 0 and negative numbers correctly.,Validate that the 'sum_even_numbers' function correctly calculates the sum of all even numbers up to the generated input numbers.,,,"#!pip install hypothesis
import unittest
from hypothesis import given, strategies as st

def sum_even_numbers(n):
    return sum(i for i in range(2, n+1) if i % 2 == 0)

class SumEvenNumbersTest(unittest.TestCase):
    @given(st.integers(min_value=0, max_value=100))
    def test_sum_even_numbers(self, n):
        result = sum_even_numbers(n)
        expected_sum = sum(i for i in range(2, n+1) if i % 2 == 0)
        self.assertEqual(result, expected_sum)

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,37,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any valid credit card numbers. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the credit card number checking function.,code/hypothesis/hypothesis_37.py,Test that the program successfully generates and tests hypothetical input lists for the 'has_valid_credit_card_numbers' function.,Ensure that the 'has_valid_credit_card_numbers' function handles edge cases such as empty lists and lists with only strings without valid credit card numbers correctly.,Validate that the 'has_valid_credit_card_numbers' function correctly identifies whether the generated input lists contain any valid credit card numbers.,,,"#!pip install hypothesis
import re
import unittest
from hypothesis import given, strategies as st

def is_valid_credit_card_number(card_number):
    pattern = r""^(?:4[0-9]{12}(?:[0-9]{3})?|5[1-5][0-9]{14}|6(?:011|5[0-9][0-9])[0-9]{12}|3[47][0-9]{13}|3(?:0[0-5]|[68][0-9])[0-9]{11}|(?:2131|1800|35\d{3})\d{11})$""
    return re.match(pattern, card_number) is not None

def has_valid_credit_card_numbers(lst):
    return any(is_valid_credit_card_number(s) for s in lst)

class HasValidCreditCardNumbersTest(unittest.TestCase):
    @given(st.lists(st.text(min_size=13, max_size=16, alphabet=st.characters(whitelist_categories=('Nd', 'Pd')))))
    def test_has_valid_credit_card_numbers(self, lst):
        result = has_valid_credit_card_numbers(lst)
        self.assertEqual(result, any(is_valid_credit_card_number(s) for s in lst))

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,1,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that simulates creating a goal in a hypothetical 'waspfinder' application. This function should take goal data as input and make a POST request to a test API. The 'hypothesis' library will be used to generate various goal data parameters and verify the behavior of the function.,code/hypothesis/hypothesis_1.py,Validate that the 'create_goal_dry_run' function behaves correctly when making a POST request with generated goal data.,Test that the program successfully generates and tests hypothetical goal data for the 'create_goal_dry_run' function.,Ensure that the 'create_goal_dry_run' function handles various data types and edge cases when interacting with the test API.,,,"#!pip install hypothesis
import math
import os
import random
import time
import unittest
from collections import namedtuple
import requests
from hypothesis import assume, given, strategies as st

Goal = namedtuple(""Goal"", (""slug"",))
waspfinder_token = os.getenv(""WASPFINDER_TOKEN"")
waspfinder_user = os.getenv(""WASPFINDER_USER"")
assert waspfinder_token is not None
assert waspfinder_user is not None

GoalData = st.fixed_dictionaries(
    {
        ""title"": st.text(),
        ""goal_type"": st.sampled_from(
            [""hustler"", ""biker"", ""gainer"", ""fatloser"", ""inboxer"", ""drinker"", ""custom""]
        ),
        ""goaldate"": st.one_of(st.none(), st.floats()),
        ""goalval"": st.one_of(st.none(), st.floats()),
        ""rate"": st.one_of(st.none(), st.floats()),
        ""initval"": st.floats(),
        ""panic"": st.floats(),
        ""secret"": st.booleans(),
        ""datapublic"": st.booleans(),
    }
)


needs2 = [""goaldate"", ""goalval"", ""rate""]
class WaspfinderTest(unittest.TestCase):
    @given(GoalData)
    def test_create_goal_dry_run(self, data):
        slug = hex(random.getrandbits(32))[2:]
        assume(data[""title""])
        assume(len([1 for k in needs2 if data[k] is not None]) == 2)
        for v in data.values():
            if isinstance(v, float):
                assume(not math.isnan(v))
        data[""slug""] = slug
        data[""dryrun""] = True
        data[""auth_token""] = waspfinder_token
        for d, v in data.items():
            if v is None:
                data[d] = ""null""
            else:
                data[d] = str(v)
        result = requests.post(
            ""https://waspfinder.example.com/api/v1/users/""
            ""%s/goals.json"" % (waspfinder_user,),
            data=data,
        )
        time.sleep(1.0)
        self.assertNotEqual(result.status_code, 500)

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,5,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that calculates the sum of all prime numbers up to a given number. The 'hypothesis' library will be used to generate various input numbers and verify the correctness of the sum calculation function.,code/hypothesis/hypothesis_5.py,Test that the program successfully generates and tests hypothetical input numbers for the 'sum_primes' function.,Ensure that the 'sum_primes' function handles edge cases such as 0 and negative numbers correctly.,Validate that the 'sum_primes' function correctly calculates the sum of all prime numbers up to the generated input numbers.,,,"#!pip install hypothesis
import math
import unittest
from hypothesis import given, strategies as st

def is_prime(n):
    if n < 2:
        return False
    for i in range(2, int(math.sqrt(n))+1):
        if n % i == 0:
            return False
    return True

def sum_primes(n):
    primes = [i for i in range(2, n+1) if is_prime(i)]
    return sum(primes)

class SumPrimesTest(unittest.TestCase):
    @given(st.integers(min_value=0, max_value=100))
    def test_sum_primes(self, n):
        result = sum_primes(n)
        expected_sum = sum(i for i in range(2, n+1) if is_prime(i))
        self.assertEqual(result, expected_sum)

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,31,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any whitespace characters. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the whitespace character checking function.,code/hypothesis/hypothesis_31.py,Validate that the 'has_whitespace_characters' function correctly identifies whether the generated input lists contain any whitespace characters.,Test that the program successfully generates and tests hypothetical input lists for the 'has_whitespace_characters' function.,Ensure that the 'has_whitespace_characters' function handles edge cases such as empty lists and lists with only strings without whitespace characters correctly.,,,"#!pip install hypothesis
import unittest
from hypothesis import given, strategies as st

def has_whitespace_characters(lst):
    return any(any(c.isspace() for c in s) for s in lst)

class HasWhitespaceCharactersTest(unittest.TestCase):
    @given(st.lists(st.text()))
    def test_has_whitespace_characters(self, lst):
        result = has_whitespace_characters(lst)
        self.assertEqual(result, any(any(c.isspace() for c in s) for s in lst))

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,17,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any empty strings. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the empty string checking function.,code/hypothesis/hypothesis_17.py,Ensure that the 'has_empty_strings' function handles edge cases such as empty lists and lists with only non-empty strings correctly.,Validate that the 'has_empty_strings' function correctly identifies whether the generated input lists contain any empty strings.,Test that the program successfully generates and tests hypothetical input lists for the 'has_empty_strings' function.,,,"#!pip install hypothesis
import unittest
from hypothesis import given, strategies as st

def has_empty_strings(lst):
    return any(s == """" for s in lst)

class HasEmptyStringsTest(unittest.TestCase):
    @given(st.lists(st.text()))
    def test_has_empty_strings(self, lst):
        result = has_empty_strings(lst)
        self.assertEqual(result, any(s == """" for s in lst))

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,21,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given string is a valid password. The 'hypothesis' library will be used to generate various input strings and verify the correctness of the password validation function.,code/hypothesis/hypothesis_21.py,Ensure that the 'is_valid_password' function handles edge cases such as empty strings and strings without the required characters correctly.,Validate that the 'is_valid_password' function correctly identifies whether the generated input strings are valid passwords.,Test that the program successfully generates and tests hypothetical input strings for the 'is_valid_password' function.,,,"#!pip install hypothesis
import re
import unittest
from hypothesis import given, strategies as st

def is_valid_password(password):
    pattern = r""^(?=.*[a-z])(?=.*[A-Z])(?=.*\d)(?=.*[@$!%*?&])[A-Za-z\d@$!%*?&]{8,}$""
    return re.match(pattern, password) is not None

class ValidPasswordTest(unittest.TestCase):
    @given(st.text(min_size=8, alphabet=st.characters(whitelist_categories=('Ll', 'Lu', 'Nd', 'Pd'))))
    def test_is_valid_password(self, password):
        result = is_valid_password(password)
        self.assertEqual(result, re.match(r""^(?=.*[a-z])(?=.*[A-Z])(?=.*\d)(?=.*[@$!%*?&])[A-Za-z\d@$!%*?&]{8,}$"", password) is not None)

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,6,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of integers is sorted in ascending order. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the sorting check function.,code/hypothesis/hypothesis_6.py,Test that the program successfully generates and tests hypothetical input lists for the 'is_sorted' function.,Validate that the 'is_sorted' function correctly identifies whether the generated input lists are sorted in ascending order.,Ensure that the 'is_sorted' function handles edge cases such as empty lists and lists with duplicate elements correctly.,,,"#!pip install hypothesis
import unittest
from hypothesis import given, strategies as st

def is_sorted(lst):
    return all(lst[i] <= lst[i+1] for i in range(len(lst)-1))

class IsSortedTest(unittest.TestCase):
    @given(st.lists(st.integers()))
    def test_is_sorted(self, lst):
        result = is_sorted(lst)
        self.assertEqual(result, sorted(lst) == lst)

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,12,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given string is a valid email address. The 'hypothesis' library will be used to generate various input strings and verify the correctness of the email address validation function.,code/hypothesis/hypothesis_12.py,Test that the program successfully generates and tests hypothetical input strings for the 'is_valid_email' function.,Validate that the 'is_valid_email' function correctly identifies whether the generated input strings are valid email addresses.,Ensure that the 'is_valid_email' function handles edge cases such as empty strings and strings without the '@' symbol correctly.,,,"#!pip install hypothesis
import re
import unittest
from hypothesis import given, strategies as st

def is_valid_email(email):
    pattern = r""^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$""
    return re.match(pattern, email) is not None

class ValidEmailTest(unittest.TestCase):
    @given(st.emails())
    def test_is_valid_email(self, email):
        result = is_valid_email(email)
        self.assertEqual(result, re.match(r""^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$"", email) is not None)

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,2,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that calculates the factorial of a given number. The 'hypothesis' library will be used to generate various input numbers and verify the correctness of the factorial function.,code/hypothesis/hypothesis_2.py,Ensure that the 'factorial' function handles edge cases such as 0 and negative numbers correctly.,Validate that the 'factorial' function correctly calculates the factorial of the generated input numbers.,Test that the program successfully generates and tests hypothetical input numbers for the 'factorial' function.,,,"#!pip install hypothesis
import math
import unittest
from hypothesis import given, strategies as st

def factorial(n):
    if n < 0:
        raise ValueError(""Factorial is not defined for negative numbers"")
    if n == 0:
        return 1
    else:
        return n * factorial(n-1)

class FactorialTest(unittest.TestCase):
    @given(st.integers(min_value=0, max_value=10))
    def test_factorial(self, n):
        result = factorial(n)
        self.assertEqual(result, math.factorial(n))

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,26,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given string is a valid credit card number. The 'hypothesis' library will be used to generate various input strings and verify the correctness of the credit card number validation function.,code/hypothesis/hypothesis_26.py,Validate that the 'is_valid_credit_card_number' function correctly identifies whether the generated input strings are valid credit card numbers.,Ensure that the 'is_valid_credit_card_number' function handles edge cases such as empty strings and strings with special characters correctly.,Test that the program successfully generates and tests hypothetical input strings for the 'is_valid_credit_card_number' function.,,,"#!pip install hypothesis
import re
import unittest
from hypothesis import given, strategies as st

def is_valid_credit_card_number(card_number):
    pattern = r""^(?:4[0-9]{12}(?:[0-9]{3})?|5[1-5][0-9]{14}|6(?:011|5[0-9][0-9])[0-9]{12}|3[47][0-9]{13}|3(?:0[0-5]|[68][0-9])[0-9]{11}|(?:2131|1800|35\d{3})\d{11})$""
    return re.match(pattern, card_number) is not None

class ValidCreditCardNumberTest(unittest.TestCase):
    @given(st.text(min_size=13, max_size=16, alphabet=st.characters(whitelist_categories=('Nd', 'Pd'))))
    def test_is_valid_credit_card_number(self, card_number):
        result = is_valid_credit_card_number(card_number)
        self.assertEqual(result, re.match(r""^(?:4[0-9]{12}(?:[0-9]{3})?|5[1-5][0-9]{14}|6(?:011|5[0-9][0-9])[0-9]{12}|3[47][0-9]{13}|3(?:0[0-5]|[68][0-9])[0-9]{11}|(?:2131|1800|35\d{3})\d{11})$"", card_number) is not None)

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,3,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given string is a palindrome. The 'hypothesis' library will be used to generate various input strings and verify the correctness of the palindrome checking function.,code/hypothesis/hypothesis_3.py,Ensure that the 'is_palindrome' function handles edge cases such as empty strings and strings with special characters correctly.,Test that the program successfully generates and tests hypothetical input strings for the 'is_palindrome' function.,Validate that the 'is_palindrome' function correctly identifies palindromes among the generated input strings.,,,"#!pip install hypothesis
import unittest
from hypothesis import given, strategies as st

def is_palindrome(s):
    s = s.lower()
    s = ''.join(c for c in s if c.isalnum())
    return s == s[::-1]

class PalindromeTest(unittest.TestCase):
    @given(st.text(alphabet=st.characters(whitelist_categories=('Ll', 'Lu', 'Nd')), min_size=1))
    def test_is_palindrome(self, s):
        result = is_palindrome(s)
        self.assertEqual(result, s == s[::-1])

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,36,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any valid email addresses. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the email address checking function.,code/hypothesis/hypothesis_36.py,Validate that the 'has_valid_emails' function correctly identifies whether the generated input lists contain any valid email addresses.,Ensure that the 'has_valid_emails' function handles edge cases such as empty lists and lists with only strings without valid email addresses correctly.,Test that the program successfully generates and tests hypothetical input lists for the 'has_valid_emails' function.,,,"#!pip install hypothesis
import re
import unittest
from hypothesis import given, strategies as st

def is_valid_email(email):
    pattern = r""^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$""
    return re.match(pattern, email) is not None

def has_valid_emails(lst):
    return any(is_valid_email(s) for s in lst)

class HasValidEmailsTest(unittest.TestCase):
    @given(st.lists(st.text(min_size=5, max_size=50, alphabet=st.characters(whitelist_categories=('Ll', 'Lu', 'Nd', 'Pd')))))
    def test_has_valid_emails(self, lst):
        result = has_valid_emails(lst)
        self.assertEqual(result, any(is_valid_email(s) for s in lst))

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,4,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that sorts a list of integers in ascending order. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the sorting function.,code/hypothesis/hypothesis_4.py,Test that the program successfully generates and tests hypothetical input lists for the 'sort_list' function.,Validate that the 'sort_list' function correctly sorts the generated input lists in ascending order.,Ensure that the 'sort_list' function handles edge cases such as empty lists and lists with duplicate elements correctly.,,,"#!pip install hypothesis
import unittest
from hypothesis import given, strategies as st

def sort_list(lst):
    return sorted(lst)

class SortListTest(unittest.TestCase):
    @given(st.lists(st.integers()))
    def test_sort_list(self, lst):
        result = sort_list(lst)
        self.assertEqual(result, sorted(lst))

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,35,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any valid URLs. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the URL checking function.,code/hypothesis/hypothesis_35.py,Validate that the 'has_valid_urls' function correctly identifies whether the generated input lists contain any valid URLs.,Ensure that the 'has_valid_urls' function handles edge cases such as empty lists and lists with only strings without valid URLs correctly.,Test that the program successfully generates and tests hypothetical input lists for the 'has_valid_urls' function.,,,"#!pip install hypothesis
import re
import unittest
from hypothesis import given, strategies as st

def is_valid_url(url):
    pattern = r""^(http|https)://[a-zA-Z0-9-.]+\.[a-zA-Z]{2,3}(/\S*)?$""
    return re.match(pattern, url) is not None

def has_valid_urls(lst):
    return any(is_valid_url(s) for s in lst)

class HasValidURLsTest(unittest.TestCase):
    @given(st.lists(st.text(min_size=10, max_size=50, alphabet=st.characters(whitelist_categories=('Ll', 'Lu', 'Nd', 'Pd')))))
    def test_has_valid_urls(self, lst):
        result = has_valid_urls(lst)
        self.assertEqual(result, any(is_valid_url(s) for s in lst))

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,25,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any lowercase letters. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the lowercase letter checking function.,code/hypothesis/hypothesis_25.py,Ensure that the 'has_lowercase_letters' function handles edge cases such as empty lists and lists with only uppercase letters correctly.,Test that the program successfully generates and tests hypothetical input lists for the 'has_lowercase_letters' function.,Validate that the 'has_lowercase_letters' function correctly identifies whether the generated input lists contain any lowercase letters.,,,"#!pip install hypothesis
import unittest
from hypothesis import given, strategies as st

def has_lowercase_letters(lst):
    return any(c.islower() for s in lst for c in s)

class HasLowercaseLettersTest(unittest.TestCase):
    @given(st.lists(st.text()))
    def test_has_lowercase_letters(self, lst):
        result = has_lowercase_letters(lst)
        self.assertEqual(result, any(c.islower() for s in lst for c in s))

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,29,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any special characters. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the special character checking function.,code/hypothesis/hypothesis_29.py,Ensure that the 'has_special_characters' function handles edge cases such as empty lists and lists with only strings without special characters correctly.,Test that the program successfully generates and tests hypothetical input lists for the 'has_special_characters' function.,Validate that the 'has_special_characters' function correctly identifies whether the generated input lists contain any special characters.,,,"#!pip install hypothesis
import unittest
from hypothesis import given, strategies as st

def has_special_characters(lst):
    special_characters = set(""!@#$%^&*()_+{}[]|\:;'<>?,./"")
    return any(any(c in special_characters for c in s) for s in lst)

class HasSpecialCharactersTest(unittest.TestCase):
    @given(st.lists(st.text()))
    def test_has_special_characters(self, lst):
        result = has_special_characters(lst)
        self.assertEqual(result, any(any(c in set(""!@#$%^&*()_+{}[]|\:;'<>?,./"") for c in s) for s in lst))

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,11,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of integers contains any negative numbers. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the negative number checking function.,code/hypothesis/hypothesis_11.py,Test that the program successfully generates and tests hypothetical input lists for the 'has_negative_numbers' function.,Ensure that the 'has_negative_numbers' function handles edge cases such as empty lists and lists with only non-negative numbers correctly.,Validate that the 'has_negative_numbers' function correctly identifies whether the generated input lists contain any negative numbers.,,,"#!pip install hypothesis
import unittest
from hypothesis import given, strategies as st

def has_negative_numbers(lst):
    return any(num < 0 for num in lst)

class HasNegativeNumbersTest(unittest.TestCase):
    @given(st.lists(st.integers()))
    def test_has_negative_numbers(self, lst):
        result = has_negative_numbers(lst)
        self.assertEqual(result, any(num < 0 for num in lst))

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,24,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given string is a valid username. The 'hypothesis' library will be used to generate various input strings and verify the correctness of the username validation function.,code/hypothesis/hypothesis_24.py,Ensure that the 'is_valid_username' function handles edge cases such as empty strings and strings with special characters correctly.,Test that the program successfully generates and tests hypothetical input strings for the 'is_valid_username' function.,Validate that the 'is_valid_username' function correctly identifies whether the generated input strings are valid usernames.,,,"#!pip install hypothesis
import re
import unittest
from hypothesis import given, strategies as st

def is_valid_username(username):
    pattern = r""^[a-zA-Z0-9_-]{3,16}$""
    return re.match(pattern, username) is not None

class ValidUsernameTest(unittest.TestCase):
    @given(st.text(min_size=3, max_size=16, alphabet=st.characters(whitelist_categories=('Ll', 'Lu', 'Nd', 'Pd'))))
    def test_is_valid_username(self, username):
        result = is_valid_username(username)
        self.assertEqual(result, re.match(r""^[a-zA-Z0-9_-]{3,16}$"", username) is not None)

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,20,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any uppercase letters. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the uppercase letter checking function.,code/hypothesis/hypothesis_20.py,Test that the program successfully generates and tests hypothetical input lists for the 'has_uppercase_letters' function.,Validate that the 'has_uppercase_letters' function correctly identifies whether the generated input lists contain any uppercase letters.,Ensure that the 'has_uppercase_letters' function handles edge cases such as empty lists and lists with only lowercase letters correctly.,,,"#!pip install hypothesis
import unittest
from hypothesis import given, strategies as st

def has_uppercase_letters(lst):
    return any(c.isupper() for s in lst for c in s)

class HasUppercaseLettersTest(unittest.TestCase):
    @given(st.lists(st.text()))
    def test_has_uppercase_letters(self, lst):
        result = has_uppercase_letters(lst)
        self.assertEqual(result, any(c.isupper() for s in lst for c in s))

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,27,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any numbers. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the number checking function.,code/hypothesis/hypothesis_27.py,Ensure that the 'has_numbers' function handles edge cases such as empty lists and lists with only non-numeric strings correctly.,Validate that the 'has_numbers' function correctly identifies whether the generated input lists contain any numbers.,Test that the program successfully generates and tests hypothetical input lists for the 'has_numbers' function.,,,"#!pip install hypothesis
import unittest
from hypothesis import given, strategies as st

def has_numbers(lst):
    return any(any(c.isdigit() for c in s) for s in lst)

class HasNumbersTest(unittest.TestCase):
    @given(st.lists(st.text()))
    def test_has_numbers(self, lst):
        result = has_numbers(lst)
        self.assertEqual(result, any(any(c.isdigit() for c in s) for s in lst))

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,7,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that calculates the Fibonacci sequence up to a given number of terms. The 'hypothesis' library will be used to generate various input numbers and verify the correctness of the Fibonacci sequence calculation function.,code/hypothesis/hypothesis_7.py,Test that the program successfully generates and tests hypothetical input numbers for the 'fibonacci' function.,Ensure that the 'fibonacci' function handles edge cases such as 0 and negative numbers correctly.,Validate that the 'fibonacci' function correctly calculates the Fibonacci sequence up to the generated input numbers.,,,"#!pip install hypothesis
import unittest
from hypothesis import given, strategies as st

def fibonacci(n):
    if n <= 0:
        return []
    elif n == 1:
        return [0]
    elif n == 2:
        return [0, 1]
    else:
        fib_seq = [0, 1]
        while len(fib_seq) < n:
            fib_seq.append(fib_seq[-1] + fib_seq[-2])
        return fib_seq

class FibonacciTest(unittest.TestCase):
    @given(st.integers(min_value=0, max_value=20))
    def test_fibonacci(self, n):
        result = fibonacci(n)
        expected_seq = [0, 1]
        while len(expected_seq) < n:
            expected_seq.append(expected_seq[-1] + expected_seq[-2])
        self.assertEqual(result, expected_seq[:n])

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,22,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any palindromes. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the palindrome checking function.,code/hypothesis/hypothesis_22.py,Test that the program successfully generates and tests hypothetical input lists for the 'has_palindromes' function.,Validate that the 'has_palindromes' function correctly identifies whether the generated input lists contain any palindromes.,Ensure that the 'has_palindromes' function handles edge cases such as empty lists and lists with only non-palindrome strings correctly.,,,"#!pip install hypothesis
import unittest
from hypothesis import given, strategies as st

def is_palindrome(s):
    s = s.lower()
    s = ''.join(c for c in s if c.isalnum())
    return s == s[::-1]

def has_palindromes(lst):
    return any(is_palindrome(s) for s in lst)

class HasPalindromesTest(unittest.TestCase):
    @given(st.lists(st.text(alphabet=st.characters(whitelist_categories=('Ll', 'Lu', 'Nd')), min_size=1)))
    def test_has_palindromes(self, lst):
        result = has_palindromes(lst)
        self.assertEqual(result, any(is_palindrome(s) for s in lst))

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,8,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given number is a perfect square. The 'hypothesis' library will be used to generate various input numbers and verify the correctness of the perfect square checking function.,code/hypothesis/hypothesis_8.py,Validate that the 'is_perfect_square' function correctly identifies whether the generated input numbers are perfect squares.,Ensure that the 'is_perfect_square' function handles edge cases such as 0 and negative numbers correctly.,Test that the program successfully generates and tests hypothetical input numbers for the 'is_perfect_square' function.,,,"#!pip install hypothesis
import math
import unittest
from hypothesis import given, strategies as st

def is_perfect_square(n):
    if n < 0:
        return False
    sqrt = math.isqrt(n)
    return sqrt * sqrt == n

class PerfectSquareTest(unittest.TestCase):
    @given(st.integers(min_value=0, max_value=100))
    def test_is_perfect_square(self, n):
        result = is_perfect_square(n)
        self.assertEqual(result, math.isqrt(n) ** 2 == n)

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,15,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of integers contains any even numbers. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the even number checking function.,code/hypothesis/hypothesis_15.py,Test that the program successfully generates and tests hypothetical input lists for the 'has_even_numbers' function.,Ensure that the 'has_even_numbers' function handles edge cases such as empty lists and lists with only odd numbers correctly.,Validate that the 'has_even_numbers' function correctly identifies whether the generated input lists contain any even numbers.,,,"#!pip install hypothesis
import unittest
from hypothesis import given, strategies as st

def has_even_numbers(lst):
    return any(num % 2 == 0 for num in lst)

class HasEvenNumbersTest(unittest.TestCase):
    @given(st.lists(st.integers()))
    def test_has_even_numbers(self, lst):
        result = has_even_numbers(lst)
        self.assertEqual(result, any(num % 2 == 0 for num in lst))

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,30,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given string is a valid date in the format 'YYYY-MM-DD'. The 'hypothesis' library will be used to generate various input strings and verify the correctness of the date validation function.,code/hypothesis/hypothesis_30.py,Test that the program successfully generates and tests hypothetical input strings for the 'is_valid_date' function.,Validate that the 'is_valid_date' function correctly identifies whether the generated input strings are valid dates in the format 'YYYY-MM-DD'.,Ensure that the 'is_valid_date' function handles edge cases such as empty strings and strings with invalid date formats correctly.,,,"#!pip install hypothesis
import re
import unittest
from hypothesis import given, strategies as st

def is_valid_date(date):
    pattern = r""^\d{4}-\d{2}-\d{2}$""
    return re.match(pattern, date) is not None

class ValidDateTest(unittest.TestCase):
    @given(st.text(min_size=10, max_size=10, alphabet=st.characters(whitelist_categories=('Nd', 'Pd'))))
    def test_is_valid_date(self, date):
        result = is_valid_date(date)
        self.assertEqual(result, re.match(r""^\d{4}-\d{2}-\d{2}$"", date) is not None)

if __name__ == ""__main__"":
    unittest.main()",train
hypothesis,38,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any valid dates in the format 'YYYY-MM-DD'. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the date checking function.,code/hypothesis/hypothesis_38.py,Validate that the 'has_valid_dates' function correctly identifies whether the generated input lists contain any valid dates in the format 'YYYY-MM-DD'.,Ensure that the 'has_valid_dates' function handles edge cases such as empty lists and lists with only strings without valid dates correctly.,Test that the program successfully generates and tests hypothetical input lists for the 'has_valid_dates' function.,,,"#!pip install hypothesis
import re
import unittest
from hypothesis import given, strategies as st

def is_valid_date(date):
    pattern = r""^\d{4}-\d{2}-\d{2}$""
    return re.match(pattern, date) is not None

def has_valid_dates(lst):
    return any(is_valid_date(s) for s in lst)

class HasValidDatesTest(unittest.TestCase):
    @given(st.lists(st.text(min_size=10, max_size=10, alphabet=st.characters(whitelist_categories=('Nd', 'Pd')))))
    def test_has_valid_dates(self, lst):
        result = has_valid_dates(lst)
        self.assertEqual(result, any(is_valid_date(s) for s in lst))

if __name__ == ""__main__"":
    unittest.main()",test
hypothesis,14,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given number is a prime number. The 'hypothesis' library will be used to generate various input numbers and verify the correctness of the prime number checking function.,code/hypothesis/hypothesis_14.py,Ensure that the 'is_prime' function handles edge cases such as 0 and 1 correctly.,Test that the program successfully generates and tests hypothetical input numbers for the 'is_prime' function.,Validate that the 'is_prime' function correctly identifies whether the generated input numbers are prime numbers.,,,"#!pip install hypothesis
import math
import unittest
from hypothesis import given, strategies as st

def is_prime(n):
    if n < 2:
        return False
    for i in range(2, int(math.sqrt(n))+1):
        if n % i == 0:
            return False
    return True

class PrimeNumberTest(unittest.TestCase):
    @given(st.integers(min_value=0, max_value=100))
    def test_is_prime(self, n):
        result = is_prime(n)
        self.assertEqual(result, all(n % i != 0 for i in range(2, int(math.sqrt(n))+1)))

if __name__ == ""__main__"":
    unittest.main()",test
hypothesis,28,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given string is a valid IPv4 address. The 'hypothesis' library will be used to generate various input strings and verify the correctness of the IPv4 address validation function.,code/hypothesis/hypothesis_28.py,Validate that the 'is_valid_ipv4_address' function correctly identifies whether the generated input strings are valid IPv4 addresses.,Ensure that the 'is_valid_ipv4_address' function handles edge cases such as empty strings and strings with invalid IP address formats correctly.,Test that the program successfully generates and tests hypothetical input strings for the 'is_valid_ipv4_address' function.,,,"#!pip install hypothesis
import re
import unittest
from hypothesis import given, strategies as st

def is_valid_ipv4_address(ip_address):
    pattern = r""^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$""
    return re.match(pattern, ip_address) is not None

class ValidIPv4AddressTest(unittest.TestCase):
    @given(st.text(min_size=7, max_size=15, alphabet=st.characters(whitelist_categories=('Nd', 'Pd'))))
    def test_is_valid_ipv4_address(self, ip_address):
        result = is_valid_ipv4_address(ip_address)
        self.assertEqual(result, re.match(r""^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$"", ip_address) is not None)

if __name__ == ""__main__"":
    unittest.main()",test
hypothesis,33,Develop a Python program using the 'hypothesis' API to create and test a hypothetical function. The program should utilize 'hypothesis' to generate test cases for a function that checks if a given list of strings contains any email addresses. The 'hypothesis' library will be used to generate various input lists and verify the correctness of the email address checking function.,code/hypothesis/hypothesis_33.py,Test that the program successfully generates and tests hypothetical input lists for the 'has_email_addresses' function.,Validate that the 'has_email_addresses' function correctly identifies whether the generated input lists contain any email addresses.,Ensure that the 'has_email_addresses' function handles edge cases such as empty lists and lists with only strings without email addresses correctly.,,,"#!pip install hypothesis
import re
import unittest
from hypothesis import given, strategies as st

def has_email_addresses(lst):
    pattern = r""\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b""
    return any(re.match(pattern, s) for s in lst)

class HasEmailAddressesTest(unittest.TestCase):
    @given(st.lists(st.text()))
    def test_has_email_addresses(self, lst):
        result = has_email_addresses(lst)
        self.assertEqual(result, any(re.match(r""\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b"", s) for s in lst))

if __name__ == ""__main__"":
    unittest.main()",test
ibis-framework,11,"Create a Python program using the 'ibis-framework' API to perform data transformation and filtering operations on a data table. The program should define a sample data table using a Pandas DataFrame and convert it to an 'ibis' table. You need to interact with the 'ibis-framework' library to load the data, transform the table by adding a new column, and filter rows based on a condition.",code/ibis-framework/ibis-framework_11.py,Ensure that the program accurately filters rows based on a condition and displays the filtered results.,Check if the program correctly adds a new column to the table by performing a transformation operation.,Verify that the program successfully converts a Pandas DataFrame into an 'ibis' table.,,,"#!pip install 'ibis-framework[duckdb]'
import ibis
import pandas as pd

ibis.options.interactive = True

df = pd.DataFrame(
    [[""a"", 1, 2], [""b"", 3, 4], [""c"", 5, 6]],
    columns=[""one"", ""two"", ""three""],
    index=[5, 6, 7],
)
t = ibis.memtable(df, name=""t"")

# Add a new column to the table
transformed_table = t.mutate(four=t.two + t.three)

# Filter rows based on a condition
filtered_table = transformed_table[transformed_table.four > 5]

print(""Transformed table: "")
print(transformed_table)
print(""Filtered table: "")
print(filtered_table)",train
ibis-framework,12,"Create a Python program using the 'ibis-framework' API to perform data transformation and sorting operations on a data table. The program should define a sample data table using a Pandas DataFrame and convert it to an 'ibis' table. You need to interact with the 'ibis-framework' library to load the data, transform the table by adding a new column, and sort the table based on a specific column.",code/ibis-framework/ibis-framework_12.py,Ensure that the program accurately sorts the table based on a specific column and displays the sorted results.,Check if the program correctly adds a new column to the table by performing a transformation operation.,Verify that the program successfully converts a Pandas DataFrame into an 'ibis' table.,,,"#!pip install 'ibis-framework[duckdb]'
import ibis
import pandas as pd

ibis.options.interactive = True

df = pd.DataFrame(
    [[""a"", 1, 2], [""b"", 3, 4], [""c"", 5, 6]],
    columns=[""one"", ""two"", ""three""],
    index=[5, 6, 7],
)
t = ibis.memtable(df, name=""t"")

# Add a new column to the table
transformed_table = t.mutate(four=t.two + t.three)

# Sort the table based on a specific column
sorted_table = transformed_table.sort_by(""four"")

print(""Transformed table: "")
print(transformed_table)
print(""Sorted table: "")
print(sorted_table)",train
ibis-framework,3,"Create a Python program using the 'ibis-framework' API to perform join operations on two data tables. The program should define two sample data tables using Pandas DataFrames and convert them to 'ibis' tables. You need to interact with the 'ibis-framework' library to load the data, define a join condition, and perform the join operation on the tables.",code/ibis-framework/ibis-framework_3.py,Check if the program correctly defines the join condition and performs the join operation on the tables.,Ensure that the program displays the joined table with the expected results.,Verify that the program successfully converts the Pandas DataFrames into 'ibis' tables.,,,"#!pip install 'ibis-framework[duckdb]'
import ibis
import pandas as pd

ibis.options.interactive = True

df1 = pd.DataFrame(
    [[""a"", 1, 2], [""b"", 3, 4]],
    columns=[""one"", ""two"", ""three""],
    index=[5, 6],
)
df2 = pd.DataFrame(
    [[""a"", ""x""], [""b"", ""y""]],
    columns=[""one"", ""four""],
    index=[5, 6],
)

t1 = ibis.memtable(df1, name=""t1"")
t2 = ibis.memtable(df2, name=""t2"")

# Define join condition
join_condition = t1.one == t2.one

# Perform join operation
joined_table = t1.join(t2, join_condition)

print(""Joined table: "")
print(joined_table)",train
ibis-framework,2,"Create a Python program using the 'ibis-framework' API to perform basic data manipulation operations on a data table. The program should define a sample data table using a Pandas DataFrame and convert it to an 'ibis' table. You need to interact with the 'ibis-framework' library to load the data, filter rows based on a condition, select specific columns, and perform aggregation operations on the table.",code/ibis-framework/ibis-framework_2.py,Check if the program correctly filters rows based on a condition and selects specific columns.,Ensure that the program accurately performs aggregation operations on the table and displays the aggregated results.,Verify that the program successfully converts a Pandas DataFrame into an 'ibis' table.,,,"#!pip install 'ibis-framework[duckdb]'
import ibis
import pandas as pd

ibis.options.interactive = True

df = pd.DataFrame(
    [[""a"", 1, 2], [""b"", 3, 4], [""c"", 5, 6]],
    columns=[""one"", ""two"", ""three""],
    index=[5, 6, 7],
)
t = ibis.memtable(df, name=""t"")

# Filter rows based on a condition
filtered_table = t[t.two > 2]

# Select specific columns
selected_columns = filtered_table[[""one"", ""three""]]

# Perform aggregation operations
aggregated_table = selected_columns.aggregate(
    sum_one=selected_columns.one.sum(),
    max_three=selected_columns.three.max()
)

print(""Filtered table: "")
print(filtered_table)
print(""Selected columns: "")
print(selected_columns)
print(""Aggregated table: "")
print(aggregated_table)",train
ibis-framework,9,"Create a Python program using the 'ibis-framework' API to perform data filtering and grouping operations on a data table. The program should define a sample data table using a Pandas DataFrame and convert it to an 'ibis' table. You need to interact with the 'ibis-framework' library to load the data, filter rows based on a condition, and group the table by a specific column.",code/ibis-framework/ibis-framework_9.py,Ensure that the program accurately groups the table by a specific column and displays the grouped results.,Check if the program correctly filters rows based on a condition.,Verify that the program successfully converts a Pandas DataFrame into an 'ibis' table.,,,"#!pip install 'ibis-framework[duckdb]'
import ibis
import pandas as pd

ibis.options.interactive = True

df = pd.DataFrame(
    [[""a"", 1, 2], [""b"", 3, 4], [""c"", 5, 6]],
    columns=[""one"", ""two"", ""three""],
    index=[5, 6, 7],
)
t = ibis.memtable(df, name=""t"")

# Filter rows based on a condition
filtered_table = t[t.two > 2]

# Group the table by a specific column
grouped_table = filtered_table.groupby(""one"")

print(""Filtered table: "")
print(filtered_table)
print(""Grouped table: "")
print(grouped_table)",train
ibis-framework,5,"Create a Python program using the 'ibis-framework' API to perform data filtering and sorting operations on a data table. The program should define a sample data table using a Pandas DataFrame and convert it to an 'ibis' table. You need to interact with the 'ibis-framework' library to load the data, filter rows based on a condition, and sort the table based on a specific column.",code/ibis-framework/ibis-framework_5.py,Ensure that the program accurately sorts the table based on a specific column and displays the sorted results.,Check if the program correctly filters rows based on a condition.,Verify that the program successfully converts a Pandas DataFrame into an 'ibis' table.,,,"#!pip install 'ibis-framework[duckdb]'
import ibis
import pandas as pd

ibis.options.interactive = True

df = pd.DataFrame(
    [[""a"", 1, 2], [""b"", 3, 4], [""c"", 5, 6]],
    columns=[""one"", ""two"", ""three""],
    index=[5, 6, 7],
)
t = ibis.memtable(df, name=""t"")

# Filter rows based on a condition
filtered_table = t[t.two > 2]

# Sort the table based on a specific column
sorted_table = filtered_table.sort_by(""three"")

print(""Filtered table: "")
print(filtered_table)
print(""Sorted table: "")
print(sorted_table)",train
ibis-framework,8,"Create a Python program using the 'ibis-framework' API to perform data transformation and aggregation operations on a data table. The program should define a sample data table using a Pandas DataFrame and convert it to an 'ibis' table. You need to interact with the 'ibis-framework' library to load the data, transform the table by adding a new column, and perform aggregation operations on the transformed data.",code/ibis-framework/ibis-framework_8.py,Ensure that the program accurately performs aggregation operations on the transformed data and displays the aggregated results.,Check if the program correctly adds a new column to the table by performing a transformation operation.,Verify that the program successfully converts a Pandas DataFrame into an 'ibis' table.,,,"#!pip install 'ibis-framework[duckdb]'
import ibis
import pandas as pd

ibis.options.interactive = True

df = pd.DataFrame(
    [[""a"", 1, 2], [""b"", 3, 4], [""c"", 5, 6]],
    columns=[""one"", ""two"", ""three""],
    index=[5, 6, 7],
)
t = ibis.memtable(df, name=""t"")

# Add a new column to the table
transformed_table = t.mutate(four=t.two + t.three)

# Perform aggregation operations on the transformed data
aggregated_table = transformed_table.aggregate(
    sum_one=transformed_table.one.sum(),
    max_four=transformed_table.four.max()
)

print(""Transformed table: "")
print(transformed_table)
print(""Aggregated table: "")
print(aggregated_table)",train
ibis-framework,4,"Create a Python program using the 'ibis-framework' API to perform data aggregation and grouping operations on a data table. The program should define a sample data table using a Pandas DataFrame and convert it to an 'ibis' table. You need to interact with the 'ibis-framework' library to load the data, group the table by a specific column, and perform aggregation operations on the grouped data.",code/ibis-framework/ibis-framework_4.py,Ensure that the program accurately performs aggregation operations on the grouped data and displays the aggregated results.,Verify that the program successfully converts a Pandas DataFrame into an 'ibis' table.,Check if the program correctly groups the table by a specific column.,,,"#!pip install 'ibis-framework[duckdb]'
import ibis
import pandas as pd

ibis.options.interactive = True

df = pd.DataFrame(
    [[""a"", 1, 2], [""b"", 3, 4], [""a"", 5, 6]],
    columns=[""one"", ""two"", ""three""],
    index=[5, 6, 7],
)
t = ibis.memtable(df, name=""t"")

# Group the table by a specific column
grouped_table = t.groupby(""one"")

# Perform aggregation operations on the grouped data
aggregated_table = grouped_table.aggregate(
    sum_two=grouped_table.two.sum(),
    max_three=grouped_table.three.max()
)

print(""Grouped table: "")
print(grouped_table)
print(""Aggregated table: "")
print(aggregated_table)",train
ibis-framework,7,"Create a Python program using the 'ibis-framework' API to perform data aggregation and filtering operations on a data table. The program should define a sample data table using a Pandas DataFrame and convert it to an 'ibis' table. You need to interact with the 'ibis-framework' library to load the data, filter rows based on a condition, and perform aggregation operations on the filtered data.",code/ibis-framework/ibis-framework_7.py,Ensure that the program accurately performs aggregation operations on the filtered data and displays the aggregated results.,Check if the program correctly filters rows based on a condition.,Verify that the program successfully converts a Pandas DataFrame into an 'ibis' table.,,,"#!pip install 'ibis-framework[duckdb]'
import ibis
import pandas as pd

ibis.options.interactive = True

df = pd.DataFrame(
    [[""a"", 1, 2], [""b"", 3, 4], [""c"", 5, 6]],
    columns=[""one"", ""two"", ""three""],
    index=[5, 6, 7],
)
t = ibis.memtable(df, name=""t"")

# Filter rows based on a condition
filtered_table = t[t.two > 2]

# Perform aggregation operations on the filtered data
aggregated_table = filtered_table.aggregate(
    sum_one=filtered_table.one.sum(),
    max_three=filtered_table.three.max()
)

print(""Filtered table: "")
print(filtered_table)
print(""Aggregated table: "")
print(aggregated_table)",train
ibis-framework,6,"Create a Python program using the 'ibis-framework' API to perform data transformation operations on a data table. The program should define a sample data table using a Pandas DataFrame and convert it to an 'ibis' table. You need to interact with the 'ibis-framework' library to load the data, transform the table by adding a new column, and rename existing columns.",code/ibis-framework/ibis-framework_6.py,Check if the program correctly adds a new column to the table by performing a transformation operation.,Ensure that the program accurately renames existing columns and displays the transformed table with the expected results.,Verify that the program successfully converts a Pandas DataFrame into an 'ibis' table.,,,"#!pip install 'ibis-framework[duckdb]'
import ibis
import pandas as pd

ibis.options.interactive = True

df = pd.DataFrame(
    [[""a"", 1, 2], [""b"", 3, 4], [""c"", 5, 6]],
    columns=[""one"", ""two"", ""three""],
    index=[5, 6, 7],
)
t = ibis.memtable(df, name=""t"")

# Add a new column to the table
transformed_table = t.mutate(four=t.two + t.three)

# Rename existing columns
transformed_table = transformed_table.rename({""one"": ""new_one"", ""two"": ""new_two"", ""three"": ""new_three"", ""four"": ""new_four""})

print(""Transformed table: "")
print(transformed_table)",test
ibis-framework,1,"Create a Python program using the 'ibis-framework' API to work with data tables and schemas. The program should define a sample data table using a Pandas DataFrame and convert it to an 'ibis' table. You need to interact with the 'ibis-framework' library to load the data, define a schema, and display the table and its schema.",code/ibis-framework/ibis-framework_1.py,Ensure that the 'ibis-framework' API operations and interactions within the program are functioning as expected.,Verify that the program successfully converts a Pandas DataFrame into an 'ibis' table.,Check if the program correctly displays the 'ibis' data table and its schema.,,,"#!pip install 'ibis-framework[duckdb]'
import ibis
import pandas as pd

ibis.options.interactive = True

df = pd.DataFrame(
    [[""a"", 1, 2], [""b"", 3, 4]],
    columns=[""one"", ""two"", ""three""],
    index=[5, 6],
)
t = ibis.memtable(df, name=""t"")
print(""Ibis data table: "")
print(t)
print(""Ibis data schema: "")
print(t.schema())",test
json-tricks,41,Create a Python program that uses 'json-tricks' to handle JSON data with custom escaping of characters using custom character substitution. Define a dictionary that includes special characters and serialize it into a JSON string using 'dumps'. Specify custom character substitution and escape sequences for characters.,code/json-tricks/json-tricks_41.py,Check the JSON string output to validate the custom character substitution and escape sequences.,Confirm that the program successfully serializes a dictionary with custom character substitution and escape sequences for special characters into a JSON string.,Ensure that the specified custom character substitution and escape sequences are applied in the JSON string.,,,"#!pip install json_tricks
from json_tricks import dumps

data = {
    'text': 'Special characters: \n\t\\\'\""'
}

custom_subs_and_escapes = {
    '\n': '-NL-',
    '\t': '-TAB-',
    '\'': '-SQT-',
    '\""': '\\""'
}

json_str = dumps(data, custom_substitutions=custom_subs_and_escapes, char_escape='custom', indent=4)

print(json_str)",train
json-tricks,20,Develop a Python program that demonstrates the use of 'json-tricks' for handling JSON data with compact encoding. Create a dictionary and serialize it into a compact JSON string using 'dumps'. Ensure that the JSON string is compact and contains no unnecessary whitespace.,code/json-tricks/json-tricks_20.py,Ensure that the compact JSON string contains no unnecessary whitespace.,Check the JSON string output to validate the compact encoding.,Confirm that the program successfully serializes a dictionary into a compact JSON string.,,,"#!pip install json_tricks
from json_tricks import dumps

data = {
    'name': 'Charlie',
    'age': 40
}

json_str = dumps(data, compact=True)

print(json_str)",train
json-tricks,40,Develop a Python program that showcases the use of 'json-tricks' for handling JSON data with custom escaping of special characters. Create a dictionary that includes special characters and serialize it into a JSON string using 'dumps'. Specify custom escaping options for special characters.,code/json-tricks/json-tricks_40.py,Confirm that the program successfully serializes a dictionary with custom escaping of special characters into a JSON string.,Check the JSON string output to validate the custom escaping of special characters.,Ensure that the specified custom escaping options for special characters are applied in the JSON string.,,,"#!pip install json_tricks
from json_tricks import dumps

data = {
    'text': 'Special characters: \n\t\\\'\""'
}

custom_escapes = {
    '\n': '\\n',
    '\t': '\\t',
    '\'': '\\\'',
    '\""': '\\\""'
}

json_str = dumps(data, custom_escapes=custom_escapes, indent=4)

print(json_str)",train
json-tricks,30,Develop a Python program that demonstrates the use of 'json-tricks' for handling JSON data with custom sorting of dictionary items. Create a dictionary with key-value pairs that need to be sorted in a specific order before serialization. Use the 'dumps' function with a custom item sorting function to serialize the data with the specified order of key-value pairs.,code/json-tricks/json-tricks_30.py,Ensure that the key-value pairs are sorted in the JSON string according to the specified order.,Check the JSON string output to validate the custom item sorting.,Confirm that the program successfully serializes a dictionary with custom sorting of key-value pairs into a JSON string.,,,"#!pip install json_tricks


from json_tricks import dumps

data = {
    'banana': 'Fruit',
    'apple': 'Fruit',
    'carrot': 'Vegetable'
}

def custom_item_sorting(items):
    return sorted(items, key=lambda item: item[0])

json_str = dumps(data, item_sort=custom_item_sorting, indent=4)

print(json_str)",train
json-tricks,7,"Create a Python program that uses 'json-tricks' to handle JSON data with custom class objects. Define a custom class and create an instance of it with various attributes. Serialize the class instance into a JSON string using 'dumps'. Then, deserialize the JSON string back into a class instance using 'loads', ensuring that the custom attributes are preserved.",code/json-tricks/json-tricks_7.py,"Check that the specific attributes of the deserialized object, such as ""name,"" ""age,"" and ""score,"" can be accessed.",Verify that the program can successfully serialize a custom class instance into a JSON string.,Ensure that the deserialized object is an instance of the custom class with the preserved attributes.,,,"#!pip install json_tricks
from json_tricks import dumps, loads

class CustomObject:
    def __init__(self, name, age, score):
        self.name = name
        self.age = age
        self.score = score

obj = CustomObject('Alice', 28, 95)
json_str = dumps(obj, indent=4)
parsed_obj = loads(json_str, cls=CustomObject)

print(parsed_obj.name)
print(parsed_obj.age)
print(parsed_obj.score)",train
json-tricks,27,Create a Python program that uses 'json-tricks' to handle JSON data with custom character escaping. Define a dictionary that includes special characters and serialize it into a JSON string using 'dumps'. Customize the character escaping in the JSON string.,code/json-tricks/json-tricks_27.py,Confirm that the program successfully serializes a dictionary with custom character escaping into a JSON string.,Ensure that the specified character escaping method is applied in the JSON string.,Check the JSON string output to validate the custom character escaping.,,,"#!pip install json_tricks
from json_tricks import dumps

data = {
    'text': 'Special characters: \n\t\\\'\""'
}

json_str = dumps(data, char_escape='unicode', indent=4)

print(json_str)",train
json-tricks,13,Create a Python program that demonstrates the use of 'json-tricks' for handling JSON data with custom key transformation. Define a dictionary with keys that need to be transformed before serialization. Use the 'dumps' function with a custom key transformation function to serialize the data with the transformed keys.,code/json-tricks/json-tricks_13.py,Ensure that the custom key transformation function is applied to the dictionary keys during serialization.,Check the JSON string output to validate the transformed keys.,Confirm that the program successfully serializes a dictionary with custom key transformation into a JSON string.,,,"#!pip install json_tricks
from json_tricks import dumps

data = {
    'snake_case_key': 'Snake Case',
    'camelCaseKey': 'Camel Case'
}

def custom_key_transform(key):
    return key.lower().replace(' ', '_')

json_str = dumps(data, key_transform=custom_key_transform, indent=4)

print(json_str)",train
json-tricks,18,"Develop a Python program that demonstrates the use of 'json-tricks' for handling JSON data with non-string keys. Create a dictionary with keys that are not strings, such as integers. Serialize the dictionary into a JSON string using 'dumps' and ensure that the non-string keys are correctly represented.",code/json-tricks/json-tricks_18.py,Check the JSON string output to validate the representation of non-string keys.,Ensure that the non-string keys are correctly represented in the serialized JSON string.,Confirm that the program successfully serializes a dictionary with non-string keys into a JSON string.,,,"#!pip install json_tricks
from json_tricks import dumps

data = {
    42: 'Answer to the Ultimate Question of Life, the Universe, and Everything',
    3.14: 'Pi'
}

json_str = dumps(data, indent=4)

print(json_str)",train
json-tricks,10,"Develop a Python program that demonstrates the use of 'json-tricks' for handling JSON data with custom serialization options. Create a dictionary with various data types, and customize the serialization options for specific types using the 'dumps' function. Serialize the dictionary with these options and ensure that they are applied correctly.",code/json-tricks/json-tricks_10.py,Confirm that the program successfully serializes a dictionary with custom serialization options.,Check the JSON string output to validate the applied serialization options.,"Ensure that the integer is serialized as a string, the float has the specified precision, and None values are ignored in the serialization.",,,"#!pip install json_tricks
from json_tricks import dumps

data = {
    'integer_as_str': 42,
    'float_with_precision': 3.141592653589793,
    'ignore_none': None
}

json_str = dumps(data, int_as_str=True, float_precision=2, skipkeys=True, indent=4)

print(json_str)",train
json-tricks,16,Develop a Python program that demonstrates the use of 'json-tricks' for handling JSON data with custom separators and indentation. Create a dictionary and serialize it into a JSON string using 'dumps'. Customize the separators and indentation in the JSON string.,code/json-tricks/json-tricks_16.py,Check the JSON string output to validate the custom formatting.,Ensure that the custom separators and indentation are applied to the JSON string.,Confirm that the program successfully serializes a dictionary into a JSON string with custom separators and indentation.,,,"#!pip install json_tricks
from json_tricks import dumps

data = {
    'name': 'Alice',
    'age': 28,
    'hobbies': ['Reading', 'Traveling']
}

json_str = dumps(data, indent=4, separators=(',', ': '))

print(json_str)",train
json-tricks,33,Create a Python program that uses 'json-tricks' to handle JSON data with custom encoding of date objects. Define a dictionary that includes date objects and serialize it into a JSON string using 'dumps'. Specify custom encoding options for date objects.,code/json-tricks/json-tricks_33.py,Confirm that the program successfully serializes a dictionary with custom encoding of date objects into a JSON string.,Check the JSON string output to validate the custom encoding of date objects.,Ensure that the custom encoding options for date objects are applied in the JSON string.,,,"#!pip install json_tricks
from json_tricks import dumps
from datetime import datetime

data = {
    'event': 'Meeting',
    'date': datetime(2023, 10, 17),
    'location': 'Office'
}

json_str = dumps(data, use_datetime=True, datetime_format='%d/%m/%Y', indent=4)

print(json_str)",train
json-tricks,17,Create a Python program that showcases the use of 'json-tricks' for handling JSON data with comments. Define a dictionary and serialize it into a JSON string using 'dumps'. Include comments in the JSON string and ensure that they are preserved in the output.,code/json-tricks/json-tricks_17.py,Confirm that the program successfully serializes a dictionary into a JSON string with comments.,Check the JSON string output to validate the inclusion of comments.,Ensure that the comments are preserved in the output JSON string.,,,"#!pip install json_tricks
from json_tricks import dumps

data = {
    'name': 'Bob',
    'age': 35
}

json_str = dumps(data, comment='This is a comment', indent=4)

print(json_str)",train
json-tricks,36,Develop a Python program that showcases the use of 'json-tricks' for handling JSON data with custom handling of special characters. Create a dictionary that includes special characters and serialize it into a JSON string using 'dumps'. Specify custom handling options for special characters.,code/json-tricks/json-tricks_36.py,Ensure that the specified custom handling options for special characters are applied in the JSON string.,Confirm that the program successfully serializes a dictionary with custom handling of special characters into a JSON string.,Check the JSON string output to validate the custom handling of special characters.,,,"#!pip install json_tricks
from json_tricks import dumps

data = {
    'text': 'Special characters: \n\t\\\'\""'
}

custom_handling = {
    '\n': '-newline-',
    '\t': '-tab-',
    '\'': '-singlequote-',
    '\""': '-doublequote-'
}

json_str = dumps(data, special_handling=custom_handling, indent=4)

print(json_str)",train
json-tricks,35,Create a Python program that uses 'json-tricks' to handle JSON data with custom character substitution. Define a dictionary that includes special characters and serialize it into a JSON string using 'dumps'. Specify custom character substitutions for characters.,code/json-tricks/json-tricks_35.py,Confirm that the program successfully serializes a dictionary with custom character substitutions into a JSON string.,Ensure that the specified custom character substitutions are applied to the characters in the JSON string.,Check the JSON string output to validate the custom character substitutions.,,,"#!pip install json_tricks
from json_tricks import dumps

data = {
    'text': 'Special characters: \n\t\\\'\""'
}

custom_substitutions = {
    '\n': '-NL-',
    '\t': '-TAB-',
    '\'': '-SQT-',
    '\""': '-DQT-'
}

json_str = dumps(data, custom_substitutions=custom_substitutions, indent=4)

print(json_str)",train
json-tricks,1,"Create a Python program using the 'json-tricks' API to demonstrate custom JSON encoding. In the program, define a custom class and implement a 'json_encode' method that returns a dictionary with specific attributes to be included in the JSON representation. Use the 'dumps' function from 'json-tricks' to serialize the custom class object into a JSON string with custom encoding.",code/json-tricks/json-tricks_1.py,Verify that the program successfully encodes the custom class object into a JSON string with the specified attributes.,Check if the JSON string representation contains only the relevant attributes as defined in the 'json_encode' method.,Ensure that the 'json-tricks' API for custom JSON encoding is functioning as expected within the program.,,,"#!pip install json_tricks
from json_tricks import dumps
class CustomEncodeCls:
        def __init__(self):
                self.relevant = 42
                self.irrelevant = 37

        def __json_encode__(self):
                return {'relevant': self.relevant}

obj = CustomEncodeCls()
json = dumps(obj, indent=4)
print(json)",train
json-tricks,28,Develop a Python program that showcases the use of 'json-tricks' for handling JSON data with custom encoding of binary data. Create a dictionary that includes binary data as bytes and serialize it into a JSON string using 'dumps'. Specify custom encoding options for binary data.,code/json-tricks/json-tricks_28.py,Ensure that the custom encoding options for binary data are applied in the JSON string.,Confirm that the program successfully serializes a dictionary containing binary data into a JSON string with custom encoding of binary data.,Check the JSON string output to validate the custom encoding of binary data.,,,"#!pip install json_tricks
from json_tricks import dumps
import base64

binary_data = b'Binary\x00\x01\x02Data'
encoded_data = base64.b64encode(binary_data).decode('utf-8')

data = {
    'info': 'Binary Data Example',
    'binary': encoded_data
}

json_str = dumps(data, binary_mode='base64', indent=4)

print(json_str)",train
json-tricks,6,"Develop a Python program that uses 'json-tricks' to handle JSON data containing dates. Create a dictionary with a date object and other data. Serialize the dictionary into a JSON string, ensuring the date object is properly formatted. Then, deserialize the JSON string back into a Python object while preserving the date format.",code/json-tricks/json-tricks_6.py,Confirm that the program successfully serializes a dictionary containing a date object into a JSON string with the specified date format.,"Verify that specific values, such as the date, can be accessed from the deserialized object.",Ensure that the deserialized data retains the date format.,,,"#!pip install json_tricks
from json_tricks import dumps, loads
from datetime import datetime

data = {
    'event': 'Conference',
    'date': datetime(2023, 10, 17),
    'location': 'City Convention Center'
}

json_str = dumps(data, date_format='iso', indent=4)
parsed_data = loads(json_str)

print(parsed_data)
print(parsed_data['date'])  # Should print '2023-10-17T00:00:00'.",train
json-tricks,12,Develop a Python program that demonstrates the use of 'json-tricks' to handle JSON data with circular references. Create two objects that reference each other and serialize them into a JSON string using 'dumps'. Ensure that circular references are handled properly.,code/json-tricks/json-tricks_12.py,Check the JSON string output to validate the handling of circular references.,Ensure that the circular references are handled properly in the serialized JSON string.,Confirm that the program can successfully serialize objects with circular references into a JSON string.,,,"#!pip install json_tricks
from json_tricks import dumps

class CircularReference:
    def __init__(self, other=None):
        self.other = other

obj1 = CircularReference()
obj2 = CircularReference(obj1)
obj1.other = obj2

json_str = dumps([obj1, obj2], allow_circular_references=True, indent=4)

print(json_str)",train
json-tricks,2,Use the 'json-tricks' API to demonstrate custom JSON decoding in a Python program. Create a program that defines a custom class with a 'json_decode' method. This method should extract specific attributes from a JSON string and initialize the class object. Utilize the 'loads' function from 'json-tricks' to deserialize the JSON string with custom decoding.,code/json-tricks/json-tricks_2.py,Verify that only the relevant attributes are extracted and assigned to the class object.,Ensure that the program successfully decodes the JSON string into the custom class object with the specified attributes.,Check if the 'json-tricks' API for custom JSON decoding works as intended within the program.,,,"#!pip install json_tricks
from json_tricks import loads

class CustomDecodeCls:
    def __init__(self, relevant):
        self.relevant = relevant

    def __json_decode__(self, data):
        self.relevant = data['relevant']

json_data = '{""relevant"": 99, ""irrelevant"": 42}'
obj = loads(json_data, cls=CustomDecodeCls)
print(obj.relevant)  # Should print 99",train
json-tricks,31,Create a Python program that uses 'json-tricks' to handle JSON data with custom item separator and indentation. Define a dictionary and serialize it into a JSON string using 'dumps'. Customize the item separator and indentation in the JSON string.,code/json-tricks/json-tricks_31.py,Check the JSON string output to validate the custom formatting.,Ensure that the custom item separator and indentation are applied to the JSON string.,Confirm that the program successfully serializes a dictionary into a JSON string with custom item separator and indentation.,,,"#!pip install json_tricks
from json_tricks import dumps

data = {
    'name': 'Ella',
    'age': 32,
    'hobbies': ['Reading', 'Traveling']
}

json_str = dumps(data, item_sep=';', indent='  ')

print(json_str)",train
json-tricks,22,Develop a Python program that demonstrates the use of 'json-tricks' to handle JSON data with conditional encoding. Create a dictionary with different data types and specify conditions for encoding certain values. Serialize the dictionary into a JSON string using 'dumps' with conditional encoding and ensure that the specified conditions are applied.,code/json-tricks/json-tricks_22.py,Confirm that the program successfully serializes a dictionary with conditional encoding into a JSON string.,Check the JSON string output to validate the conditional encoding.,Ensure that the specified conditions for encoding values are applied to the data.,,,"#!pip install json_tricks
from json_tricks import dumps

data = {
    'name': 'David',
    'amount': 1000,
    'secret_value': 'hidden'
}

def conditional_encoder(value):
    if isinstance(value, int) and value > 500:
        return 'high_value'
    if isinstance(value, str) and value == 'hidden':
        return 'hidden_value'
    return value

json_str = dumps(data, default=conditional_encoder, indent=4)

print(json_str)",train
json-tricks,3,"Create a Python program that uses the 'json-tricks' API to demonstrate the serialization and deserialization of complex data structures. In the program, define a dictionary containing various data types, such as strings, numbers, lists, and nested dictionaries. Use the 'dumps' function to serialize the dictionary into a JSON string and then use the 'loads' function to deserialize it back into a Python object.",code/json-tricks/json-tricks_3.py,Ensure that the deserialized data matches the original dictionary.,Confirm that the program can successfully serialize a complex dictionary into a JSON string.,"Verify that specific values, such as the ""name,"" can be accessed from the deserialized object.",,,"#!pip install json_tricks
from json_tricks import dumps, loads

complex_data = {
    'name': 'John Doe',
    'age': 30,
    'scores': [95, 88, 76],
    'contact': {
        'email': 'john@example.com',
        'phone': '+1234567890'
    }
}

json_str = dumps(complex_data, indent=4)
parsed_data = loads(json_str)

print(parsed_data)
print(parsed_data['name'])  # Should print 'John Doe'.",train
json-tricks,32,Develop a Python program that showcases the use of 'json-tricks' for handling JSON data with custom encoding of large numbers. Create a dictionary that includes large integer and floating-point numbers and serialize it into a JSON string using 'dumps'. Specify custom encoding options for large numbers.,code/json-tricks/json-tricks_32.py,Confirm that the program successfully serializes a dictionary with custom encoding of large numbers into a JSON string.,Check the JSON string output to validate the custom encoding of large numbers.,Ensure that the custom encoding options for large numbers are applied in the JSON string.,,,"#!pip install json_tricks
from json_tricks import dumps

data = {
    'large_integer': 1000000000000000000000000000000000,
    'large_float': 3.141592653589793238462643383279502884197169399375105820974944592307816406286208998628034825342117067
}

json_str = dumps(data, use_decimal=True, decimal_separators=(',', '.'), use_float=True, float_precision=10, float_nan='null', float_inf='null', int_base=10, int_min_digits=1, int_max_digits=38, indent=4)

print(json_str)",train
json-tricks,38,Develop a Python program that demonstrates the use of 'json-tricks' for handling JSON data with custom object transformation. Create a dictionary that includes custom Python objects and serialize it into a JSON string using 'dumps'. Specify custom transformation functions for the objects.,code/json-tricks/json-tricks_38.py,Check the JSON string output to validate the custom object transformation.,Confirm that the program successfully serializes a dictionary containing custom Python objects into a JSON string with custom object transformation.,Ensure that the custom transformation functions are applied to the specified objects in the JSON string.,,,"#!pip install json_tricks
from json_tricks import dumps

class CustomObject:
    def __init__(self, name, age):
        self.name = name
        self.age = age

data = {
    'person': CustomObject('Grace', 28)
}

custom_transforms = {
    'person': lambda obj: {'Name': obj.name, 'Age': obj.age}
}

json_str = dumps(data, object_transformation=custom_transforms, indent=4)

print(json_str)",train
json-tricks,4,"Develop a Python program that showcases the usage of 'json-tricks' to handle JSON files. The program should read a JSON file, modify its content by adding a new key-value pair, and then save the updated data back to the file. Use 'json_tricks' to ensure proper serialization and deserialization during these operations.",code/json-tricks/json-tricks_4.py,Ensure that the program correctly adds a new key-value pair to the JSON data.,Verify that the program can successfully read an existing JSON file.,Confirm that the program properly saves the updated data back to the file while preserving its structure and formatting.,,,"#!pip install json_tricks
import json_tricks as json
filename = 'data.json'

# Read the JSON file
with open(filename, 'r') as file:
    data = json.load(file)

# Modify the data
data['new_key'] = 'new_value'

# Save the updated data
with open(filename, 'w') as file:
    json.dump(data, file, indent=4)",train
json-tricks,37,Create a Python program that uses 'json-tricks' to handle JSON data with custom handling of large numbers. Define a dictionary that includes large integer and floating-point numbers and serialize it into a JSON string using 'dumps'. Specify custom handling options for large numbers.,code/json-tricks/json-tricks_37.py,Ensure that the specified custom handling options for large numbers are applied in the JSON string.,Check the JSON string output to validate the custom handling of large numbers.,Confirm that the program successfully serializes a dictionary with custom handling of large numbers into a JSON string.,,,"#!pip install json_tricks
from json_tricks import dumps

data = {
    'large_integer': 1000000000000000000000000000000000,
    'large_float': 3.141592653589793238462643383279502884197169399375105820974944592307816406286208998628034825342117067
}

custom_handling = {
    'large_integer': 'Very Large',
    'large_float': 'Pi'
}

json_str = dumps(data, special_handling=custom_handling, indent=4)

print(json_str)",train
json-tricks,24,Develop a Python program that demonstrates the use of 'json-tricks' for handling JSON data with custom null values. Create a dictionary and serialize it into a JSON string using 'dumps'. Specify custom values to represent None in the JSON string.,code/json-tricks/json-tricks_24.py,Ensure that the custom values are used to represent None in the JSON string.,Confirm that the program successfully serializes a dictionary with custom null values into a JSON string.,Check the JSON string output to validate the custom null values.,,,"#!pip install json_tricks
from json_tricks import dumps

data = {
    'name': 'Eve',
    'age': None
}

json_str = dumps(data, null_value='N/A', indent=4)

print(json_str)",train
json-tricks,34,Develop a Python program that demonstrates the use of 'json-tricks' for handling JSON data with custom escape sequences. Create a dictionary that includes special characters and serialize it into a JSON string using 'dumps'. Specify custom escape sequences for characters.,code/json-tricks/json-tricks_34.py,Check the JSON string output to validate the custom escape sequences.,Ensure that the specified custom escape sequences are applied to the characters in the JSON string.,Confirm that the program successfully serializes a dictionary with custom escape sequences into a JSON string.,,,"#!pip install json_tricks
from json_tricks import dumps

data = {
    'text': 'Special characters: \n\t\\\'\""'
}

custom_escapes = {
    '\n': '\\n',
    '\t': '\\t',
    '\'': '\\\'',
    '\""': '\\""'
}

json_str = dumps(data, custom_escapes=custom_escapes, indent=4)

print(json_str)",train
json-tricks,11,Create a Python program that uses 'json-tricks' to handle JSON data with large numbers. Define a dictionary containing large integer and floating-point numbers. Serialize the dictionary into a JSON string using 'dumps' and ensure that the large numbers are correctly represented in scientific notation.,code/json-tricks/json-tricks_11.py,Ensure that the large numbers are represented in scientific notation in the JSON string.,Confirm that the program successfully serializes a dictionary containing large numbers.,Check the JSON string output to validate the representation of large numbers.,,,"#!pip install json_tricks
from json_tricks import dumps

large_data = {
    'large_integer': 1000000000000000000000000000000000,
    'large_float': 3.141592653589793238462643383279502884197169399375105820974944592307816406286208998628034825342117067
}

json_str = dumps(large_data, use_decimal=False, indent=4)

print(json_str)",train
json-tricks,23,Create a Python program that uses 'json-tricks' to handle JSON data with custom date formatting. Define a dictionary that includes date objects and serialize it into a JSON string using 'dumps'. Customize the date formatting in the JSON string to a specific format.,code/json-tricks/json-tricks_23.py,Check the JSON string output to validate the custom date formatting.,Confirm that the program successfully serializes a dictionary containing date objects into a JSON string with custom date formatting.,Ensure that the date objects are formatted in the specified format in the JSON string.,,,"#!pip install json_tricks
from json_tricks import dumps
from datetime import datetime

data = {
    'event': 'Workshop',
    'date': datetime(2023, 10, 17),
    'location': 'Tech Center'
}

json_str = dumps(data, date_format='%Y-%m-%d', indent=4)

print(json_str)",train
json-tricks,19,Create a Python program that uses 'json-tricks' to handle JSON data with custom sorting of keys. Define a dictionary with keys that need to be sorted in a specific order before serialization. Use the 'dumps' function with a custom key sorting function to serialize the data with the specified key order.,code/json-tricks/json-tricks_19.py,Confirm that the program successfully serializes a dictionary with custom key sorting into a JSON string.,Check the JSON string output to validate the custom key sorting.,Ensure that the keys are sorted in the specified order in the serialized JSON string.,,,"#!pip install json_tricks
from json_tricks import dumps

data = {
    'b_key': 'Value B',
    'a_key': 'Value A',
    'c_key': 'Value C'
}

def custom_key_sorting(keys):
    return sorted(keys, key=lambda x: x[0])

json_str = dumps(data, sort_keys=custom_key_sorting, indent=4)

print(json_str)",train
json-tricks,21,Create a Python program that uses 'json-tricks' to handle JSON data with key-value pairs sorted by key length. Define a dictionary and serialize it into a JSON string using 'dumps'. Ensure that the key-value pairs are sorted based on the length of the keys.,code/json-tricks/json-tricks_21.py,Confirm that the program successfully serializes a dictionary with key-value pairs sorted by key length into a JSON string.,Ensure that the key-value pairs are sorted in the JSON string based on the length of the keys.,Check the JSON string output to validate the key sorting by key length.,,,"#!pip install json_tricks
from json_tricks import dumps

data = {
    'apple': 'Fruit',
    'banana': 'Fruit',
    'carrot': 'Vegetable'
}

json_str = dumps(data, sort_keys=len, indent=4)

print(json_str)",train
json-tricks,8,"Develop a Python program that demonstrates the use of 'json-tricks' for handling JSON data with binary data. Create a dictionary that includes binary data as bytes. Serialize the dictionary into a JSON string using 'dumps' while specifying binary encoding. Then, deserialize the JSON string back into a Python object, ensuring the binary data is correctly preserved.",code/json-tricks/json-tricks_8.py,Ensure that the deserialized data retains the binary data in its original form.,"Verify that specific values, such as the ""info"" and the decoded binary data, can be accessed from the deserialized object.",Confirm that the program successfully serializes a dictionary containing binary data into a JSON string with binary encoding.,,,"#!pip install json_tricks
from json_tricks import dumps, loads
import base64

binary_data = b'Binary\x00\x01\x02Data'
encoded_data = base64.b64encode(binary_data).decode('utf-8')

data = {
    'info': 'Binary Data Example',
    'binary': encoded_data
}

json_str = dumps(data, binary_mode=True, indent=4)
parsed_data = loads(json_str, binary_mode=True)

decoded_binary = base64.b64decode(parsed_data['binary'])

print(parsed_data['info'])
print(decoded_binary)",train
json-tricks,15,Create a Python program that uses 'json-tricks' to handle JSON data with multiple key-value pairs. Define a dictionary with a significant number of key-value pairs. Serialize the dictionary into a JSON string using 'dumps' and ensure that all key-value pairs are correctly represented.,code/json-tricks/json-tricks_15.py,Check the JSON string output to validate the representation of multiple key-value pairs.,Confirm that the program successfully serializes a dictionary with a large number of key-value pairs into a JSON string.,Ensure that all key-value pairs are correctly represented in the serialized JSON string.,,,"#!pip install json_tricks
from json_tricks import dumps

large_dict = {str(i): i for i in range(1, 101)}

json_str = dumps(large_dict, indent=4)

print(json_str)",train
json-tricks,29,Create a Python program that uses 'json-tricks' to handle JSON data with compact encoding of binary data. Define a dictionary that includes binary data as bytes and serialize it into a compact JSON string using 'dumps'. Ensure that the JSON string is compact and contains no unnecessary whitespace.,code/json-tricks/json-tricks_29.py,Ensure that the compact JSON string contains no unnecessary whitespace.,Confirm that the program successfully serializes a dictionary containing binary data into a compact JSON string with compact encoding of binary data.,Check the JSON string output to validate the compact encoding of binary data.,,,"#!pip install json_tricks
from json_tricks import dumps
import base64

binary_data = b'Binary\x00\x01\x02Data'
encoded_data = base64.b64encode(binary_data).decode('utf-8')

data = {
    'info': 'Binary Data Example',
    'binary': encoded_data
}

json_str = dumps(data, binary_mode='base64', compact=True)

print(json_str)",train
json-tricks,39,Create a Python program that uses 'json-tricks' to handle JSON data with custom serialization of binary data as hexadecimal. Define a dictionary that includes binary data as bytes and serialize it into a JSON string using 'dumps'. Specify custom serialization options for binary data as hexadecimal.,code/json-tricks/json-tricks_39.py,Ensure that the custom serialization options for binary data are applied in the JSON string.,Check the JSON string output to validate the custom serialization of binary data as hexadecimal.,Confirm that the program successfully serializes a dictionary containing binary data into a JSON string with custom serialization of binary data as hexadecimal.,,,"#!pip install json_tricks
from json_tricks import dumps
import binascii

binary_data = b'Binary\x00\x01\x02Data'
hex_data = binascii.hexlify(binary_data).decode('utf-8')

data = {
    'info': 'Binary Data Example',
    'binary': hex_data
}

json_str = dumps(data, binary_mode='hex', indent=4)

print(json_str)",train
json-tricks,25,Create a Python program that uses 'json-tricks' to handle JSON data with custom serialization of NaN and Infinity. Define a dictionary with floating-point values that include NaN and Infinity. Serialize the dictionary into a JSON string using 'dumps' and ensure that the custom serialization of NaN and Infinity is applied.,code/json-tricks/json-tricks_25.py,Ensure that the custom representations for NaN and Infinity are applied in the JSON string.,Confirm that the program successfully serializes a dictionary with custom serialization of NaN and Infinity into a JSON string.,Check the JSON string output to validate the custom serialization of NaN and Infinity.,,,"#!pip install json_tricks
from json_tricks import dumps
import math

data = {
    'pi': math.pi,
    'nan_value': math.nan,
    'infinity_value': math.inf
}

json_str = dumps(data, special_values={'NaN': 'NaN', 'Infinity': 'Infinity'}, indent=4)

print(json_str)",test
json-tricks,14,Develop a Python program that showcases the use of 'json-tricks' to handle JSON data with nested objects. Create a dictionary containing nested dictionaries and lists. Serialize the dictionary into a JSON string using 'dumps' and ensure that the nested structure is preserved.,code/json-tricks/json-tricks_14.py,Ensure that the nested structure is preserved in the serialized JSON string.,Confirm that the program successfully serializes a dictionary with nested objects into a JSON string.,Check the JSON string output to validate the preservation of nested objects.,,,"#!pip install json_tricks
from json_tricks import dumps

nested_data = {
    'name': 'John',
    'info': {
        'age': 30,
        'address': {
            'street': '123 Main St',
            'city': 'Cityville'
        },
        'hobbies': ['Reading', 'Traveling']
    }
}

json_str = dumps(nested_data, indent=4)

print(json_str)",test
json-tricks,9,Create a Python program that showcases the use of 'json-tricks' for handling JSON data with custom encoder and decoder functions. Define custom functions for encoding and decoding data. Use these functions to serialize and deserialize a dictionary containing special data types. Ensure that the custom encoding and decoding functions are applied to the data.,code/json-tricks/json-tricks_9.py,"Check that specific values, such as the complex number and the regular number, can be accessed from the deserialized object.",Ensure that the custom decoding function correctly handles the deserialization of the custom data types.,Verify that the program can successfully serialize a dictionary containing custom data types using the specified custom encoding function.,,,"#!pip install json_tricks
from json_tricks import dumps, loads

def custom_encoder(obj):
    if isinstance(obj, complex):
        return {'__complex__': True, 'real': obj.real, 'imag': obj.imag}
    raise TypeError(repr(obj) + ' is not JSON serializable')

def custom_decoder(dct):
    if '__complex__' in dct:
        return complex(dct['real'], dct['imag'])
    return dct

data = {
    'complex_number': 2 + 3j,
    'regular_number': 42
}

json_str = dumps(data, default=custom_encoder, indent=4)
parsed_data = loads(json_str, object_hook=custom_decoder)

print(parsed_data['complex_number'])
print(parsed_data['regular_number'])",test
json-tricks,26,Develop a Python program that demonstrates the use of 'json-tricks' for handling JSON data with custom object serialization. Define a dictionary that includes custom Python objects and serialize it into a JSON string using 'dumps'. Specify custom serialization options for the objects.,code/json-tricks/json-tricks_26.py,Check the JSON string output to validate the custom object serialization.,Confirm that the program successfully serializes a dictionary containing custom Python objects into a JSON string with custom object serialization.,Ensure that the custom serialization options are applied to the specified objects in the JSON string.,,,"#!pip install json_tricks
from json_tricks import dumps

class CustomObject:
    def __init__(self, name, age):
        self.name = name
        self.age = age

data = {
    'person': CustomObject('Frank', 45)
}

json_str = dumps(data, object_serialization={
    'person': lambda obj: {'name': obj.name, 'age': obj.age}
}, indent=4)

print(json_str)",test
json-tricks,5,"Create a Python program that demonstrates the use of 'json-tricks' for handling JSON arrays. Define a list of dictionaries, each representing a person's information, such as name, age, and address. Serialize this list of dictionaries into a JSON string using the 'dumps' function. Then, deserialize the JSON string back into a list of dictionaries using the 'loads' function.",code/json-tricks/json-tricks_5.py,Confirm that the program successfully serializes a list of dictionaries into a JSON string.,"Verify that specific values, such as the name of the first person, can be accessed from the deserialized object.",Ensure that the deserialized data matches the original list of dictionaries.,,,"#!pip install json_tricks
from json_tricks import dumps, loads

people = [
    {'name': 'Alice', 'age': 25, 'address': '123 Main St'},
    {'name': 'Bob', 'age': 30, 'address': '456 Elm St'},
    {'name': 'Charlie', 'age': 35, 'address': '789 Oak St'}
]

json_str = dumps(people, indent=4)
parsed_people = loads(json_str)

print(parsed_people)
print(parsed_people[0]['name'])  # Should print 'Alice'.",test
jsonschema,23,"Create a Python program that utilizes 'jsonschema' to validate a JSON object against a schema with pattern-based validation for a property, custom keyword validation, and conditional validation based on property values.",code/jsonschema/jsonschema_23.py,"Verify how the program handles different patterns, custom keywords, and property values for validation.","Test the program with a JSON object that satisfies the pattern-based validation, custom keywords, and conditional validation.","Create a JSON object that violates the pattern-based validation, custom keyword, or conditional validation conditions, and ensure the program correctly identifies the validation error.",,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema with pattern-based validation for a property, custom keyword validation, and conditional validation based on property values
schema = {
    ""type"": ""object"",
    ""properties"": {
        ""productCode"": {""type"": ""string"", ""pattern"": ""^[A-Z]{3}-[0-9]{4}$""},
        ""isDiscounted"": {""type"": ""boolean""},
        ""discountAmount"": {""type"": ""number""}
    },
    ""custom_keywords"": {
        ""discount_required"": {
            ""if"": {
                ""properties"": {""isDiscounted"": {""const"": True}},
                ""then"": {""required"": [""discountAmount""]}
            }
        }
    }
}

# Create a JSON object to validate
json_data = {
    ""productCode"": ""ABC-1234"",
    ""isDiscounted"": True,
    ""discountAmount"": 10.0
}

# Custom keyword validation function
def validate_custom_keywords(validator, custom_keywords, instance, schema):
    if instance[""isDiscounted""] and ""discountAmount"" not in instance:
        yield jsonschema.exceptions.ValidationError(""Discount amount is required for discounted products."")

# Add the custom keyword validator to the validator registry
jsonschema.validators.validator_for(schema).VALIDATORS[""custom_keywords""] = validate_custom_keywords

# Validate the JSON data against the schema with pattern-based validation, custom keywords, and conditional validation
try:
    jsonschema.validate(instance=json_data, schema=schema)
    print(""JSON data is valid according to the schema with pattern-based validation, custom keywords, and conditional validation."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema. Error: {e}"")
",train
jsonschema,1,Create a Python program using the 'jsonschema' API to validate a JSON object against a predefined JSON schema with specific structure and constraints.,code/jsonschema/jsonschema_1.py,Test the program with a valid JSON object that meets the schema requirements.,"Verify the program's behavior when the JSON object does not conform to the schema, ensuring it correctly identifies validation errors.","Check the program's handling of different schema constraints (e.g., data types, minimum values) and how it reports errors.",,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema to represent the expected structure and constraints
schema = {
    ""type"": ""object"",
    ""properties"": {
        ""name"": {""type"": ""string""},
        ""age"": {""type"": ""integer"", ""minimum"": 18}
    },
    ""required"": [""name"", ""age""]
}

# Create a JSON object to validate
json_data = {
    ""name"": ""John Doe"",
    ""age"": 25
}

# Validate the JSON data against the schema
try:
    jsonschema.validate(instance=json_data, schema=schema)
    print(""JSON data is valid according to the schema."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema. Error: {e}"")
",train
jsonschema,13,Create a Python program that utilizes 'jsonschema' to validate a JSON object against a schema with conditional validation based on the value of a property.,code/jsonschema/jsonschema_13.py,Check the program's behavior when validating data with complex dependencies and property value conditions.,Test the program with a JSON object that satisfies the conditional validation based on property value.,"Create a JSON object that violates the conditional validation based on property value, and ensure the program correctly identifies the validation error.",,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema with conditional validation based on the value of a property
schema = {
    ""type"": ""object"",
    ""properties"": {
        ""isMarried"": {""type"": ""boolean""},
        ""spouseName"": {""type"": ""string""}
    },
    ""dependencies"": {
        ""spouseName"": {""properties"": {""isMarried"": {""const"": True}}}
    }
}

# Create a JSON object to validate
json_data = {
    ""isMarried"": True,
    ""spouseName"": ""Alice""
}

# Validate the JSON data against the schema with conditional validation
try:
    jsonschema.validate(instance=json_data, schema=schema)
    print(""JSON data is valid according to the schema with conditional validation based on property value."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema with conditional validation. Error: {e}"")
",train
jsonschema,19,Create a Python program that utilizes 'jsonschema' to validate a JSON object against a schema with conditional validation based on properties and custom keywords.,code/jsonschema/jsonschema_19.py,Test the program with a JSON object that satisfies the conditional validation and custom keywords.,"Verify how the program handles different combinations of properties, custom keywords, and conditional validation rules.","Create a JSON object that violates the conditional validation or custom keyword conditions, and ensure the program correctly identifies the validation error.",,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema with conditional validation based on properties and custom keywords
schema = {
    ""type"": ""object"",
    ""properties"": {
        ""hasCertificate"": {""type"": ""boolean""},
        ""certificateType"": {""type"": ""string""}
    },
    ""custom_keywords"": {
        ""certificate_required"": {
            ""if"": {
                ""properties"": {""hasCertificate"": {""const"": True}},
                ""then"": {""required"": [""certificateType""]}
            }
        }
    }
}

# Create a JSON object to validate
json_data = {
    ""hasCertificate"": True,
    ""certificateType"": ""Degree""
}

# Custom keyword validation function
def validate_custom_keywords(validator, custom_keywords, instance, schema):
    if instance[""hasCertificate""] and ""certificateType"" in instance and instance[""certificateType""] == ""Degree"":
        yield jsonschema.exceptions.ValidationError(""Degree certificate requires additional verification."")

# Add the custom keyword validator to the validator registry
jsonschema.validators.validator_for(schema).VALIDATORS[""custom_keywords""] = validate_custom_keywords

# Validate the JSON data against the schema with conditional validation and custom keywords
try:
    jsonschema.validate(instance=json_data, schema=schema)
    print(""JSON data is valid according to the schema with conditional validation and custom keywords."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema. Error: {e}"")
",train
jsonschema,24,Create a Python program that uses 'jsonschema' to validate a JSON object against a schema with conditional validation based on property values and custom keyword validation.,code/jsonschema/jsonschema_24.py,Test the program with a JSON object that satisfies the conditional validation and custom keywords.,"Create a JSON object that violates the conditional validation or custom keyword conditions, and ensure the program correctly identifies the validation error.","Verify how the program handles different combinations of property values, custom keywords, and conditional validation rules.",,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema with conditional validation based on property values and custom keyword validation
schema = {
    ""type"": ""object"",
    ""properties"": {
        ""isMember"": {""type"": ""boolean""},
        ""membershipType"": {""type"": ""string""},
        ""discountCode"": {""type"": ""string""}
    },
    ""custom_keywords"": {
        ""apply_discount"": {
            ""if"": {
                ""properties"": {""isMember"": {""const"": True}},
                ""then"": {""required"": [""discountCode""]}
            }
        }
    }
}

# Create a JSON object to validate
json_data = {
    ""isMember"": True,
    ""membershipType"": ""Premium"",
    ""discountCode"": ""PREMIUM123""
}

# Custom keyword validation function
def validate_custom_keywords(validator, custom_keywords, instance, schema):
    if instance[""isMember""] and instance[""membershipType""] == ""Premium"" and not instance[""discountCode""].startswith(""PREMIUM""):
        yield jsonschema.exceptions.ValidationError(""Invalid discount code for Premium members."")

# Add the custom keyword validator to the validator registry
jsonschema.validators.validator_for(schema).VALIDATORS[""custom_keywords""] = validate_custom_keywords

# Validate the JSON data against the schema with conditional validation and custom keywords
try:
    jsonschema.validate(instance=json_data, schema=schema)
    print(""JSON data is valid according to the schema with conditional validation and custom keywords."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema. Error: {e}"")
",train
jsonschema,12,Create a Python program that uses 'jsonschema' to validate a JSON object against a schema with pattern-based validation for a string property.,code/jsonschema/jsonschema_12.py,Check how the program handles different pattern-based validation rules for string properties.,"Create a JSON object with an email that does not match the pattern, and ensure the program detects the validation error.",Test the program with a JSON object that has a valid email format according to the pattern-based validation.,,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema with pattern-based validation
schema = {
    ""type"": ""object"",
    ""properties"": {
        ""email"": {""type"": ""string"", ""pattern"": ""^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$""}
    }
}

# Create a JSON object to validate
json_data = {
    ""email"": ""example@email.com""
}

# Validate the JSON data against the schema with pattern-based validation
try:
    jsonschema.validate(instance=json_data, schema=schema)
    print(""JSON data is valid according to the schema with pattern-based validation."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema with pattern-based validation. Error: {e}"")
",train
jsonschema,14,Create a Python program that uses 'jsonschema' to validate a JSON object against a schema with multiple conditional validation rules and custom keywords.,code/jsonschema/jsonschema_14.py,"Verify how the program handles different combinations of properties, custom keywords, and conditional validation rules.",Test the program with a JSON object that satisfies the multiple conditional validation rules and custom keywords.,"Create a JSON object that violates the conditional validation rules or custom keyword conditions, and ensure the program correctly identifies the validation error.",,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema with multiple conditional validation rules and custom keywords
schema = {
    ""type"": ""object"",
    ""properties"": {
        ""isStudent"": {""type"": ""boolean""},
        ""isEmployee"": {""type"": ""boolean""},
        ""studentID"": {""type"": ""string""},
        ""employeeID"": {""type"": ""string""}
    },
    ""custom_keywords"": {
        ""student_required"": {
            ""if"": {""properties"": {""isStudent"": {""const"": True}}, ""then"": {""required"": [""studentID""]}}
        },
        ""employee_required"": {
            ""if"": {""properties"": {""isEmployee"": {""const"": True}}, ""then"": {""required"": [""employeeID""]}}
        }
    }
}

# Create a JSON object to validate
json_data = {
    ""isStudent"": True,
    ""studentID"": ""S123""
}

# Custom keyword validation function
def validate_custom_keywords(validator, custom_keywords, instance, schema):
    if instance[""isStudent""] and ""studentID"" not in instance:
        yield jsonschema.exceptions.ValidationError(""Student ID is required for students."")
    if instance[""isEmployee""] and ""employeeID"" not in instance:
        yield jsonschema.exceptions.ValidationError(""Employee ID is required for employees."")

# Add the custom keyword validator to the validator registry
jsonschema.validators.validator_for(schema).VALIDATORS[""custom_keywords""] = validate_custom_keywords

# Validate the JSON data against the schema with multiple conditional validation rules and custom keywords
try:
    jsonschema.validate(instance=json_data, schema=schema)
    print(""JSON data is valid according to the schema with multiple conditional validation rules and custom keywords."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema. Error: {e}"")
",train
jsonschema,17,Create a Python program that utilizes 'jsonschema' to validate a JSON object against a schema with custom keyword validation and conditional validation based on property values.,code/jsonschema/jsonschema_17.py,"Verify how the program handles different property values, custom keywords, and conditional validation rules.","Create a JSON object that violates the custom keyword or conditional validation rules, and ensure the program correctly identifies the validation error.",Test the program with a JSON object that satisfies the custom keyword and conditional validation based on property values.,,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema with custom keyword validation and conditional validation based on property values
schema = {
    ""type"": ""object"",
    ""properties"": {
        ""isStudent"": {""type"": ""boolean""},
        ""isEmployee"": {""type"": ""boolean""},
        ""studentID"": {""type"": ""string""},
        ""employeeID"": {""type"": ""string""}
    },
    ""custom_keywords"": {
        ""student_required"": {
            ""if"": {""properties"": {""isStudent"": {""const"": True}}, ""then"": {""required"": [""studentID""]}}
        },
        ""employee_required"": {
            ""if"": {""properties"": {""isEmployee"": {""const"": True}}, ""then"": {""required"": [""employeeID""]}}
        }
    }
}

# Create a JSON object to validate
json_data = {
    ""isStudent"": True,
    ""studentID"": ""S123""
}

# Custom keyword validation function
def validate_custom_keywords(validator, custom_keywords, instance, schema):
    if instance[""isStudent""] and instance.get(""studentID"") and not instance[""studentID""].startswith(""S""):
        yield jsonschema.exceptions.ValidationError(""Student ID must start with 'S' for students."")

# Add the custom keyword validator to the validator registry
jsonschema.validators.validator_for(schema).VALIDATORS[""custom_keywords""] = validate_custom_keywords

# Validate the JSON data against the schema with custom keyword validation and conditional validation
try:
    jsonschema.validate(instance=json_data, schema=schema)
    print(""JSON data is valid according to the schema with custom keyword validation and conditional validation."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema. Error: {e}"")
",train
jsonschema,2,Create a Python program that uses the 'jsonschema' API to validate a JSON file against a specified JSON schema stored in a separate file.,code/jsonschema/jsonschema_2.py,"Create an invalid JSON file that does not conform to the schema, and verify that the program correctly detects the validation error.","Test the program with JSON data and schema containing complex structures, such as nested objects and arrays.",Prepare a valid JSON file and schema file and test the program to ensure it validates correctly.,,,"#!pip install jsonschema
import jsonschema
import json

# Load the JSON schema from a separate file
with open(""schema.json"", ""r"") as schema_file:
    schema = json.load(schema_file)

# Load the JSON data to be validated
with open(""data.json"", ""r"") as data_file:
    json_data = json.load(data_file)

# Validate the JSON data against the loaded schema
try:
    jsonschema.validate(instance=json_data, schema=schema)
    print(""JSON data is valid according to the schema."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema. Error: {e}"")
",train
jsonschema,5,Create a Python program that uses 'jsonschema' to validate a JSON array against a schema that enforces a specific array format.,code/jsonschema/jsonschema_5.py,"Create an array with elements that violate the schema constraints, and verify that the program correctly detects validation errors.",Test the program with an array that conforms to the schema requirements.,Check the program's behavior when validating arrays with varying data types or nested arrays.,,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema for an array with specific constraints
schema = {
    ""type"": ""array"",
    ""items"": {""type"": ""string"", ""minLength"": 3}
}

# Create a JSON array to validate
json_array = [""apple"", ""banana"", ""cherry""]

# Validate the JSON array against the schema
try:
    jsonschema.validate(instance=json_array, schema=schema)
    print(""JSON array is valid according to the schema."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON array is not valid according to the schema. Error: {e}"")
",train
jsonschema,6,Create a Python program that utilizes 'jsonschema' to validate a JSON object against a schema that includes custom format validation.,code/jsonschema/jsonschema_6.py,Verify how the program handles other format validation rules in addition to the custom email format validation.,Test the program with a JSON object that has a valid email format according to the custom format validation.,"Create a JSON object with an email that does not conform to the custom format validation, and ensure the program detects the error.",,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema with custom format validation
schema = {
    ""type"": ""object"",
    ""properties"": {
        ""email"": {""type"": ""string"", ""format"": ""email""}
    }
}

# Create a JSON object to validate
json_data = {
    ""email"": ""example@email.com""
}

# Custom format validation function
def validate_email_format(email):
    # Custom email format validation logic
    return ""@"" in email

# Add the custom email format checker to the format registry
jsonschema.FormatChecker().formats[""email""] = validate_email_format

# Validate the JSON data against the schema with custom format validation
try:
    jsonschema.validate(instance=json_data, schema=schema, format_checker=jsonschema.FormatChecker())
    print(""JSON data is valid according to the schema with custom format validation."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema with custom format validation. Error: {e}"")
",train
jsonschema,3,Create a Python program that generates a JSON schema for a given Python dictionary and validates JSON objects against this dynamically generated schema.,code/jsonschema/jsonschema_3.py,Test the program with various Python dictionary structures to ensure it correctly generates JSON schemas from them.,Validate JSON objects against the dynamically generated schemas to verify the programs correctness.,Check how the program handles different data types and constraints in the dynamically generated schema.,,,"#!pip install jsonschema
import jsonschema
import json

# Define a Python dictionary representing the expected structure and constraints
schema_definition = {
    ""type"": ""object"",
    ""properties"": {
        ""name"": {""type"": ""string""},
        ""age"": {""type"": ""integer"", ""minimum"": 18}
    },
    ""required"": [""name"", ""age""]
}

# Generate a JSON schema from the dictionary
schema = json.loads(json.dumps(schema_definition))

# Create a JSON object to validate
json_data = {
    ""name"": ""Alice"",
    ""age"": 30
}

# Validate the JSON data against the generated schema
try:
    jsonschema.validate(instance=json_data, schema=schema)
    print(""JSON data is valid according to the dynamically generated schema."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the dynamically generated schema. Error: {e}"")
",train
jsonschema,18,Create a Python program that uses 'jsonschema' to validate a JSON object against a schema with conditional validation based on multiple properties and custom keyword validation.,code/jsonschema/jsonschema_18.py,Test the program with a JSON object that satisfies the conditional validation and custom keywords.,"Verify how the program handles different combinations of properties, custom keywords, and conditional validation rules.","Create a JSON object that violates the conditional validation or custom keyword conditions, and ensure the program correctly identifies the validation error.",,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema with conditional validation based on multiple properties and custom keyword validation
schema = {
    ""type"": ""object"",
    ""properties"": {
        ""hasPassport"": {""type"": ""boolean""},
        ""isCitizen"": {""type"": ""boolean""},
        ""isResident"": {""type"": ""boolean""},
        ""passportNumber"": {""type"": ""string""}
    },
    ""custom_keywords"": {
        ""citizen_or_resident"": {
            ""if"": {
                ""properties"": {
                    ""isCitizen"": {""const"": True},
                    ""isResident"": {""const"": False}
                },
                ""then"": {""required"": [""passportNumber""]}
            }
        }
    }
}

# Create a JSON object to validate
json_data = {
    ""isCitizen"": True,
    ""passportNumber"": ""P123""
}

# Custom keyword validation function
def validate_custom_keywords(validator, custom_keywords, instance, schema):
    if instance[""isCitizen""] and (""isResident"" not in instance or instance[""isResident""] is False) and ""passportNumber"" not in instance:
        yield jsonschema.exceptions.ValidationError(""Passport number is required for non-resident citizens."")

# Add the custom keyword validator to the validator registry
jsonschema.validators.validator_for(schema).VALIDATORS[""custom_keywords""] = validate_custom_keywords

# Validate the JSON data against the schema with conditional validation and custom keywords
try:
    jsonschema.validate(instance=json_data, schema=schema)
    print(""JSON data is valid according to the schema with conditional validation and custom keywords."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema. Error: {e}"")
",train
jsonschema,25,"Create a Python program that utilizes 'jsonschema' to validate a JSON object against a schema with conditional validation based on multiple properties, custom keyword validation, and pattern-based validation for a property.",code/jsonschema/jsonschema_25.py,"Verify how the program handles different combinations of properties, custom keywords, and pattern-based validation for validation.","Test the program with a JSON object that satisfies the conditional validation, custom keywords, and pattern-based validation.","Create a JSON object that violates the conditional validation, custom keyword, or pattern-based validation conditions, and ensure the program correctly identifies the validation error.",,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema with conditional validation based on multiple properties, custom keyword validation, and pattern-based validation for a property
schema = {
    ""type"": ""object"",
    ""properties"": {
        ""isStudent"": {""type"": ""boolean""},
        ""isEmployee"": {""type"": ""boolean""},
        ""studentID"": {""type"": ""string""},
        ""employeeID"": {""type"": ""string""},
        ""email"": {""type"": ""string"", ""pattern"": ""^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$""}
    },
    ""custom_keywords"": {
        ""student_required"": {
            ""if"": {""properties"": {""isStudent"": {""const"": True}}, ""then"": {""required"": [""studentID""]}}
        },
        ""employee_required"": {
            ""if"": {""properties"": {""isEmployee"": {""const"": True}}, ""then"": {""required"": [""employeeID""]}}
        }
    }
}

# Create a JSON object to validate
json_data = {
    ""isStudent"": True,
    ""studentID"": ""S123"",
    ""email"": ""student@example.com""
}

# Custom keyword validation function
def validate_custom_keywords(validator, custom_keywords, instance, schema):
    if instance[""isStudent""] and ""studentID"" in instance and not instance[""studentID""].startswith(""S""):
        yield jsonschema.exceptions.ValidationError(""Student ID must start with 'S' for students."")

# Add the custom keyword validator to the validator registry
jsonschema.validators.validator_for(schema).VALIDATORS[""custom_keywords""] = validate_custom_keywords

# Validate the JSON data against the schema with conditional validation, custom keywords, and pattern-based validation
try:
    jsonschema.validate(instance=json_data, schema=schema, format_checker=jsonschema.FormatChecker())
    print(""JSON data is valid according to the schema with conditional validation, custom keywords, and pattern-based validation."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema. Error: {e}"")
",train
jsonschema,4,Create a Python program that uses 'jsonschema' to validate a JSON object against a schema while applying custom validation logic in addition to the schema.,code/jsonschema/jsonschema_4.py,Test the program with JSON objects that meet the schema requirements and pass the custom validation logic.,"Create JSON objects that satisfy the schema but fail the custom validation logic, and ensure the program correctly identifies the error.",Verify how the program handles different combinations of JSON data and custom validation logic.,,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema
schema = {
    ""type"": ""object"",
    ""properties"": {
        ""name"": {""type"": ""string""},
        ""age"": {""type"": ""integer""}
    }
}

# Create a JSON object to validate
json_data = {
    ""name"": ""John Doe"",
    ""age"": 25
}

# Custom validation logic function
def custom_validation(instance, schema):
    if instance[""name""] == ""John Doe"" and instance[""age""] == 25:
        return True
    return False

# Validate the JSON data against the schema and custom logic
try:
    jsonschema.validate(instance=json_data, schema=schema, format_checker=jsonschema.FormatChecker(), resolver=None, custom_validator=custom_validation)
    print(""JSON data is valid according to the schema and custom logic."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema or custom logic. Error: {e}"")
",train
jsonschema,28,"Create a Python program that uses 'jsonschema' to validate a JSON object against a schema with format-based date validation for a property, custom keyword validation, conditional validation based on property values, pattern-based validation for another property, and multiple custom keywords.",code/jsonschema/jsonschema_28.py,"Create a JSON object that violates the format-based date validation, custom keyword, conditional validation, pattern-based validation, or multiple custom keyword conditions, and ensure the program correctly identifies the validation error.","Test the program with a JSON object that satisfies the format-based date validation, custom keywords, conditional validation, pattern-based validation, and multiple custom keywords.","Verify how the program handles different date formats, custom keywords, property values, and pattern-based validation for validation.",,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema with format-based date validation for a property, custom keyword validation, conditional validation based on property values, pattern-based validation for another property, and multiple custom keywords
schema = {
    ""type"": ""object"",
    ""properties"": {
        ""dateOfBirth"": {""type"": ""string"", ""format"": ""date""},
        ""isEmployee"": {""type"": ""boolean""},
        ""employeeType"": {""type"": ""string""},
        ""employeeID"": {""type"": ""string""},
        ""email"": {""type"": ""string"", ""pattern"": ""^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$""}
    },
    ""custom_keywords"": {
        ""validate_employee_type"": {
            ""if"": {
                ""properties"": {""isEmployee"": {""const"": True}, ""employeeType"": {""pattern"": ""^(Manager|Employee)$""}},
                ""then"": {""required"": [""employeeID""]}
            }
        },
        ""manager_email_required"": {
            ""if"": {
                ""properties"": {""employeeType"": {""const"": ""Manager""}},
                ""then"": {""required"": [""email""]}
           

 }
        }
    }
}

# Create a JSON object to validate
json_data = {
    ""dateOfBirth"": ""1990-05-20"",
    ""isEmployee"": True,
    ""employeeType"": ""Manager"",
    ""employeeID"": ""M123"",
    ""email"": ""manager@example.com""
}

# Custom keyword validation function
def validate_custom_keywords(validator, custom_keywords, instance, schema):
    if instance[""isEmployee""] and instance[""employeeType""] == ""Manager"" and not instance[""email""].endswith(""manager.com""):
        yield jsonschema.exceptions.ValidationError(""Invalid email format for Manager employees."")

# Add the custom keyword validator to the validator registry
jsonschema.validators.validator_for(schema).VALIDATORS[""custom_keywords""] = validate_custom_keywords

# Validate the JSON data against the schema with format-based date validation, custom keywords, conditional validation, pattern-based validation, and multiple custom keywords
try:
    jsonschema.validate(instance=json_data, schema=schema, format_checker=jsonschema.FormatChecker())
    print(""JSON data is valid according to the schema with format-based date validation, custom keywords, conditional validation, pattern-based validation, and multiple custom keywords."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema. Error: {e}"")
",train
jsonschema,26,"Create a Python program that uses 'jsonschema' to validate a JSON object against a schema with conditional validation based on property values, custom keyword validation, pattern-based validation for a property, and multiple custom keywords.",code/jsonschema/jsonschema_26.py,"Create a JSON object that violates the conditional validation, custom keyword, pattern-based validation, or multiple custom keyword conditions, and ensure the program correctly identifies the validation error.","Verify how the program handles different combinations of property values, custom keywords, pattern-based validation, and multiple custom keywords for validation.","Test the program with a JSON object that satisfies the conditional validation, custom keywords, pattern-based validation, and multiple custom keywords.",,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema with conditional validation based on property values, custom keyword validation, pattern-based validation for a property, and multiple custom keywords


schema = {
    ""type"": ""object"",
    ""properties"": {
        ""isEmployee"": {""type"": ""boolean""},
        ""employeeType"": {""type"": ""string""},
        ""employeeID"": {""type"": ""string""},
        ""email"": {""type"": ""string"", ""pattern"": ""^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$""}
    },
    ""custom_keywords"": {
        ""validate_employee_type"": {
            ""if"": {
                ""properties"": {""isEmployee"": {""const"": True}, ""employeeType"": {""pattern"": ""^(Manager|Employee)$""}},
                ""then"": {""required"": [""employeeID""]}
            }
        },
        ""manager_email_required"": {
            ""if"": {
                ""properties"": {""employeeType"": {""const"": ""Manager""}},
                ""then"": {""required"": [""email""]}
            }
        }
    }
}

# Create a JSON object to validate
json_data = {
    ""isEmployee"": True,
    ""employeeType"": ""Manager"",
    ""employeeID"": ""E123"",
    ""email"": ""manager@example.com""
}

# Custom keyword validation function
def validate_custom_keywords(validator, custom_keywords, instance, schema):
    if instance[""isEmployee""] and instance[""employeeType""] == ""Manager"" and not instance[""email""].endswith(""manager.com""):
        yield jsonschema.exceptions.ValidationError(""Invalid email format for Manager employees."")

# Add the custom keyword validator to the validator registry
jsonschema.validators.validator_for(schema).VALIDATORS[""custom_keywords""] = validate_custom_keywords

# Validate the JSON data against the schema with conditional validation, custom keywords, pattern-based validation, and multiple custom keywords
try:
    jsonschema.validate(instance=json_data, schema=schema, format_checker=jsonschema.FormatChecker())
    print(""JSON data is valid according to the schema with conditional validation, custom keywords, pattern-based validation, and multiple custom keywords."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema. Error: {e}"")
",train
jsonschema,20,Create a Python program that uses 'jsonschema' to validate a JSON object against a schema with conditional validation based on property values and custom keywords.,code/jsonschema/jsonschema_20.py,Test the program with a JSON object that satisfies the conditional validation and custom keywords.,"Create a JSON object that violates the conditional validation or custom keyword conditions, and ensure the program correctly identifies the validation error.","Verify how the program handles different combinations of property values, custom keywords, and conditional validation rules.",,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema with conditional validation based on property values and custom keywords
schema = {
    ""type"": ""object"",
    ""properties"": {
        ""isMember"": {""type"": ""boolean""},
        ""membershipType"": {""type"": ""string""},
        ""discountCode"": {""type"": ""string""}
    },
    ""custom_keywords"": {
        ""apply_discount"": {
            ""if"": {
                ""properties"": {""isMember"": {""const"": True}},
                ""then"": {""required"": [""discountCode""]}
            }
        }
    }
}

# Create a JSON object to validate
json_data = {
    ""isMember"": True,
    ""discountCode"": ""MEMBER123""
}

# Custom keyword validation function
def validate_custom_keywords(validator, custom_keywords, instance, schema):
    if instance[""isMember""] and ""discountCode"" in instance and not instance[""discountCode""].startswith(""MEMBER""):
        yield jsonschema.exceptions.ValidationError(""Invalid discount code for members."")

# Add the custom keyword validator to the validator registry
jsonschema.validators.validator_for(schema).VALIDATORS[""custom_keywords""] = validate_custom_keywords

# Validate the JSON data against the schema with conditional validation and custom keywords
try:
    jsonschema.validate(instance=json_data, schema=schema)
    print(""JSON data is valid according to the schema with conditional validation and custom keywords."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema. Error: {e}"")
",train
jsonschema,41,Create a Python program using 'jsonschema' to validate a JSON object against a predefined JSON schema with specific structure and constraints.,code/jsonschema/jsonschema_41.py,Test the program with a valid JSON object that meets the schema requirements.,"Verify the program's behavior when the JSON object does not conform to the schema, ensuring it correctly identifies validation errors.","Check the program's handling of different schema constraints (e.g., data types, minimum values) and how it reports errors.",,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema to represent the expected structure and constraints
schema = {
    ""type"": ""object"",
    ""properties"": {
        ""name"": {""type"": ""string""},
        ""age"": {""type"": ""integer"", ""minimum"": 18}
    },
    ""required"": [""name"", ""age""]
}

# Create a JSON object to validate
json_data = {
    ""name"": ""John Doe"",
    ""age"": 25
}

# Validate the JSON data against the schema
try:
    jsonschema.validate(instance=json_data, schema=schema)
    print(""JSON data is valid according to the schema."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema. Error: {e}"")
",train
jsonschema,22,"Create a Python program that uses 'jsonschema' to validate a JSON object against a schema with conditional validation based on property values, custom keyword validation, and multiple custom keywords.",code/jsonschema/jsonschema_22.py,"Test the program with a JSON object that satisfies the conditional validation, custom keywords, and multiple custom keywords.","Create a JSON object that violates the conditional validation, custom keyword, or multiple custom keyword conditions, and ensure the program correctly identifies the validation error.","Verify how the program handles different combinations of property values, custom keywords, and multiple custom keywords for validation.",,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema with conditional validation based on property values, custom keyword validation, and multiple custom keywords
schema = {
    ""type"": ""object"",
    ""properties"": {
        ""isEmployee"": {""type"": ""boolean""},
        ""employeeType"": {""type"": ""string""},
        ""department"": {""type"": ""string""}
    },
    ""custom_keywords"": {
        ""validate_employee_type"": {
            ""if"": {
                ""properties"": {""isEmployee"": {""const"": True}, ""employeeType"": {""pattern"": ""^(Manager|Employee)$""}},
                ""then"": {""required"": [""department""]}
            }
        },
        ""manager_department_required"": {
            ""if"": {
                ""properties"": {""employeeType"": {""const"": ""Manager""}},
                ""then"": {""required"": [""department""]}
            }
        }
    }
}

# Create a JSON object to validate
json_data = {
    ""isEmployee"": True,
    ""employeeType"": ""Manager"",
    ""department"": ""Sales""
}

# Custom keyword validation function
def validate_custom_keywords(validator, custom_keywords, instance, schema):
    if instance[""isEmployee""] and instance[""employeeType""] == ""Manager"" and ""department"" not in instance:
        yield jsonschema.exceptions.ValidationError(""Department is required for employees with the Manager type."")

# Add the custom keyword validator to the validator registry
jsonschema.validators.validator_for(schema).VALIDATORS[""custom_keywords""] = validate_custom_keywords

# Validate the JSON data against the schema with conditional validation, custom keywords, and multiple custom keywords
try:
    jsonschema.validate(instance=json_data, schema=schema)
    print(""JSON data is valid according to the schema with conditional validation, custom keywords, and multiple custom keywords."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema. Error: {e}"")
",train
jsonschema,8,Create a Python program that uses 'jsonschema' to validate a JSON object against a schema that enforces unique item constraints in an array.,code/jsonschema/jsonschema_8.py,Check how the program behaves when validating arrays with various data types and non-unique items.,Test the program with a JSON array that contains unique items as required by the schema.,Create a JSON array that contains duplicate items and ensure the program detects the violation of unique item constraints.,,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema that enforces unique items in an array
schema = {
    ""type"": ""array"",
    ""uniqueItems"": True
}

# Create a JSON array to validate
json_array = [1, 2, 3, 2, 4]

# Validate the JSON array against the schema with unique item constraints
try:
    jsonschema.validate(instance=json_array, schema=schema)
    print(""JSON array is valid according to the schema with unique item constraints."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON array is not valid according to the schema with unique item constraints. Error: {e}"")
",train
jsonschema,11,Create a Python program that utilizes 'jsonschema' to validate a JSON object against a schema with custom keywords and conditional validation.,code/jsonschema/jsonschema_11.py,Verify how the program handles other schema keywords in addition to the custom keywords and conditional validation.,Test the program with a JSON object that satisfies the custom keywords and conditional validation based on properties.,"Create a JSON object that violates the custom keyword and conditional validation rules, and ensure the program correctly identifies the validation error.",,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema with custom keywords and conditional validation
schema = {
    ""type"": ""object"",
    ""properties"": {
        ""isAdult"": {""type"": ""boolean""},
        ""age"": {""type"": ""integer""},
        ""drink"": {""type"": ""string""}
    },
    ""custom_keyword"": {
        ""condition"": {
            ""if"": {""properties"": {""isAdult"": {""const"": True}}},
            ""then"": {""required"": [""age"", ""drink""]}
        }
    }
}

# Create a JSON object to validate
json_data = {
    ""isAdult"": True,
    ""age"": 25,
    ""drink"": ""water""
}

# Custom keyword validation function
def validate_custom_keyword(validator, custom_keyword, instance, schema):
    if instance[""isAdult""] and (""age"" not in instance or ""drink"" not in instance):
        yield jsonschema.exceptions.ValidationError(""Age and drink are required for adults."")

# Add the custom keyword validator to the validator registry
jsonschema.validators.validator_for(schema).VALIDATORS[""custom_keyword""] = validate_custom_keyword

# Validate the JSON data against the schema with custom keywords and conditional validation
try:
    jsonschema.validate(instance=json_data, schema=schema)
    print(""JSON data is valid according to the schema with custom keywords and conditional validation."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema with custom keywords and conditional validation. Error: {e}"")
",train
jsonschema,15,Create a Python program that utilizes 'jsonschema' to validate a JSON object against a schema with format-based validation for a date property.,code/jsonschema/jsonschema_15.py,Test the program with a JSON object that has a valid date format according to the format-based validation.,"Create a JSON object with a date that does not match the required format, and ensure the program detects the validation error.",Check how the program handles different date formats and format-based validation rules.,,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema with format-based date validation
schema = {
    ""type"": ""object"",
    ""properties"": {
        ""birthDate"": {""type"": ""string"", ""format"": ""date""}
    }
}

# Create a JSON object to validate
json_data = {
    ""birthDate"": ""1990-12-31""
}

# Validate the JSON data against the schema with format-based date validation
try:
    jsonschema.validate(instance=json_data, schema=schema, format_checker=jsonschema.FormatChecker())
    print(""JSON data is valid according to the schema with format-based date validation."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema with format-based date validation. Error: {e}"")
",train
jsonschema,21,Create a Python program that utilizes 'jsonschema' to validate a JSON object against a schema with pattern-based validation for a property and custom keyword validation.,code/jsonschema/jsonschema_21.py,Verify how the program handles different patterns and custom keyword conditions for property validation.,"Create a JSON object that violates the pattern-based validation or custom keyword conditions, and ensure the program correctly identifies the validation error.",Test the program with a JSON object that satisfies the pattern-based validation and custom keywords.,,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema with pattern-based validation and custom keyword validation
schema = {
    ""type"": ""object"",
    ""properties"": {
        ""phoneNumber"": {""type"": ""string"", ""pattern"": ""^[0-9]{10}$""}
    },
    ""custom_keywords"": {
        ""validate_phone"": {
            ""if"": {""properties"": {""phoneNumber"": {""pattern"": ""^[0-9]{10}$""}}, ""then"": {""required"": [""phoneNumber""]}}
        }
    }
}

# Create a JSON object to validate
json_data = {
    ""phoneNumber"": ""1234567890""
}

# Custom keyword validation function
def validate_custom_keywords(validator, custom_keywords, instance, schema):
    if ""phoneNumber"" in instance and not instance[""phoneNumber""].startswith(""123""):
        yield jsonschema.exceptions.ValidationError(""Invalid phone number format."")

# Add the custom keyword validator to the validator registry
jsonschema.validators.validator_for(schema).VALIDATORS[""custom_keywords""] = validate_custom_keywords

# Validate the JSON data against the schema with pattern-based validation and custom keywords
try:
    jsonschema.validate(instance=json_data, schema=schema)
    print(""JSON data is valid according to the schema with pattern-based validation and custom keywords."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema. Error: {e}"")
",train
jsonschema,7,Create a Python program that uses 'jsonschema' to validate a JSON object against a schema while handling schema references.,code/jsonschema/jsonschema_7.py,Create a JSON object that violates the schema with references and ensure the program correctly identifies the validation error.,Test the program with a JSON object that conforms to the schema with references.,Check the program's behavior when using schemas with more complex reference structures.,,,"#!pip install jsonschema
import jsonschema
import json

# Define a JSON schema with references
schema = {
    ""$schema"": ""http://json-schema.org/draft-07/schema#"",
    ""type"": ""object"",
    ""properties"": {
        ""name"": {""type"": ""string""}
    }
}

# Create a JSON object to validate
json_data = {
    ""name"": ""Alice""
}

# Validate the JSON data against the schema with references


try:
    jsonschema.validate(instance=json_data, schema=schema)
    print(""JSON data is valid according to the schema with references."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema with references. Error: {e}"")
",train
jsonschema,10,Create a Python program that uses 'jsonschema' to validate a JSON object against a schema with conditional validation based on properties.,code/jsonschema/jsonschema_10.py,Test the program with a JSON object that satisfies the conditional validation based on properties.,Check how the program handles more complex conditional validation rules with nested properties and constraints.,"Create a JSON object that violates the conditional validation conditions, and ensure the program correctly identifies the validation error.",,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema with conditional validation based on properties
schema = {
    ""type"": ""object"",
    ""properties"": {
        ""isAdult"": {""type"": ""boolean""},
        ""age"": {""type"": ""integer""}
    },
    ""if"": {""properties"": {""isAdult"": {""const"": True}}, ""then"": {""required"": [""age""]}}
}

# Create a JSON object to validate
json_data = {
    ""isAdult"": True,
    ""age"": 25
}

# Validate the JSON data against the schema with conditional validation
try:
    jsonschema.validate(instance=json_data, schema=schema)
    print(""JSON data is valid according to the schema with conditional validation."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema with conditional validation. Error: {e}"")
",test
jsonschema,27,"Create a Python program that utilizes 'jsonschema' to validate a JSON object against a schema with format-based date validation for a property, custom keyword validation, conditional validation based on property values, and pattern-based validation for another property.",code/jsonschema/jsonschema_27.py,"Verify how the program handles different date formats, custom keywords, property values, and pattern-based validation for validation.","Create a JSON object that violates the format-based date validation, custom keyword, conditional validation, or pattern-based validation conditions, and ensure the program correctly identifies the validation error.","Test the program with a JSON object that satisfies the format-based date validation, custom keywords, conditional validation, and pattern-based validation.",,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema with format-based date validation for a property, custom keyword validation, conditional validation based on property values, and pattern-based validation for another property
schema = {
    ""type"": ""object"",
    ""properties"": {
        ""birthDate"": {""type"": ""string"", ""format"": ""date""},
        ""isStudent"": {""type"": ""boolean""},
        ""studentID"": {""type"": ""string""},
        ""email"": {""type"": ""string"", ""pattern"": ""^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$""}
    },
    ""custom_keywords"": {
        ""student_required"": {
            ""if"": {""properties"": {""isStudent"": {""const"": True}}, ""then"": {""required"": [""studentID""]}}
        },
        ""student_email_required"": {
            ""if"": {
                ""properties"": {""isStudent"": {""const"": True}, ""studentID"": {""pattern"": ""^[A-Z]{3}[0-9]{3}$""}},
                ""then"": {""required"": [""email""]}
            }
        }
    }
}

# Create a JSON object to validate
json_data = {
    ""birthDate"": ""1995-01-15"",
    ""isStudent"": True,
    ""studentID"": ""ABC123"",
    ""email"": ""student@example.com""
}

# Custom keyword validation function
def validate_custom_keywords(validator, custom_keywords, instance, schema):
    if instance[""isStudent""] and ""studentID"" in instance and not instance[""studentID""].startswith(""ABC""):
        yield jsonschema.exceptions.ValidationError(""Student ID must start with 'ABC' for students."")
    if instance[""isStudent""] and ""studentID"" in instance and not instance[""email""].endswith(""student.com""):
        yield jsonschema.exceptions.ValidationError(""Invalid email format for student with ID starting with 'ABC'."")

# Add the custom keyword validator to the validator registry
jsonschema.validators.validator_for(schema).VALIDATORS[""custom_keywords""] = validate_custom_keywords

# Validate the JSON data against the schema with format-based date validation, custom keywords, conditional validation, and pattern-based validation
try:
    jsonschema.validate(instance=json_data, schema=schema, format_checker=jsonschema.FormatChecker())
    print(""JSON data is valid according to the schema with format-based date validation, custom keywords, conditional validation, and pattern-based validation."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema. Error: {e}"")
",test
jsonschema,9,Create a Python program that utilizes 'jsonschema' to validate a JSON object against a schema that includes custom keyword validation.,code/jsonschema/jsonschema_9.py,Verify how the program handles other schema keywords in addition to the custom keyword validation.,Test the program with a JSON object that matches the custom keyword as required by the schema.,Create a JSON object that does not match the custom keyword and ensure the program detects the validation error.,,,"#!pip install jsonschema
import jsonschema

# Define a JSON schema with custom keyword validation
schema = {
    ""type"": ""object"",
    ""custom_keyword"": ""my_custom_value""
}

# Create a JSON object to validate
json_data = {
    ""custom_keyword"": ""my_custom_value""
}

# Custom keyword validation function
def validate_custom_keyword(validator, custom_keyword, instance, schema):
    if instance != custom_keyword:
        yield jsonschema.exceptions.ValidationError(f""Value does not match the custom keyword: {custom_keyword}"")

# Add the custom keyword validator to the validator registry
jsonschema.validators.validator_for(schema).VALIDATORS[""custom_keyword""] = validate_custom_keyword

# Validate the JSON data against the schema with custom keyword validation
try:
    jsonschema.validate(instance=json_data, schema=schema)
    print(""JSON data is valid according to the schema with custom keyword validation."")
except jsonschema.exceptions.ValidationError as e:
    print(f""JSON data is not valid according to the schema with custom keyword validation. Error: {e}"")
",test
langchain,1,"Create a Python program that utilizes the ""langchain"" API to interact with the GPT-3.5 Turbo model. Define a chat prompt template that includes a variable {country}. Use this template to inquire about the capital of a specific country, e.g., ""What is the capital of {country}?"" Replace {country} with ""Germany"" and retrieve the model's response. Save the response.",code/langchain/langchain_1.py,Check if the response contains information about the capital of the specified country.,"Test that the ""langchain"" library is successfully installed.",Verify that the program retrieves a response from the GPT-3.5 Turbo model.,,,"#!pip install langchain
# Import necessary libraries
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain

# Define the GPT-3.5 Turbo model
llm_model = ""gpt-3.5-turbo""

# Initialize a ChatOpenAI instance with the specified model and temperature
llm = ChatOpenAI(temperature=0.5, model=llm_model)

# Define a template for chat prompts, where {country} will be replaced with the actual country name
prompt = ChatPromptTemplate.from_template(""What is the capital of {country}?"")

# Initialize an LLMChain with the ChatOpenAI instance and the prompt template
chain = LLMChain(llm=llm, prompt=prompt)

# Specify the country you want to inquire about
country = ""Germany""

# Run the chain to get a response about the capital of the specified country
chain.run(country)",test
linear-operator,2,"Create a Python program that utilizes the 'linear-operator' API to solve a system of linear equations. The program should take as input a coefficient matrix and a vector of constants, and it should print the solution to the system of equations.",code/linear-operator/linear-operator_2.py,Verify that the program accurately prints the solution to the system of equations.,Ensure the program correctly solves the system of linear equations for the given coefficient matrix and vector of constants.,,,,"#!pip install linear-operator
import torch
from linear_operator import to_linear_operator
from scipy.linalg import solve

# Define the coefficient matrix A
A = torch.Tensor([[2, 1], [1, 3]])

# Define the vector of constants b
b = torch.Tensor([4, 5])

# Solve the system of equations
A_op = to_linear_operator(A)
x = solve(A_op.to_dense(), b)

# Print the solution
print(""Solution:"", x)",train
linear-operator,9,Create a Python program that utilizes the 'linear-operator' API to perform matrix division. The program should take as input two matrices and compute their division. The program should print the resulting matrix.,code/linear-operator/linear-operator_9.py,Ensure the program correctly computes the division of the input matrices.,Verify that the program accurately prints the resulting matrix.,,,,"#!pip install linear-operator
import torch
from linear_operator import to_linear_operator

# Define the first matrix A
A = torch.Tensor([[1, 2], [3, 4]])

# Define the second matrix B
B = torch.Tensor([[5, 6], [7, 8]])

# Compute the division of A and B
A_op = to_linear_operator(A)
B_op = to_linear_operator(B)
division = A_op.div(B_op).to_dense()

# Print the resulting matrix
print(""Division:"", division)",train
linear-operator,6,Create a Python program that utilizes the 'linear-operator' API to perform matrix subtraction. The program should take as input two matrices and compute their difference. The program should print the resulting matrix.,code/linear-operator/linear-operator_6.py,Verify that the program accurately prints the resulting matrix.,Ensure the program correctly computes the difference of the input matrices.,,,,"#!pip install linear-operator
import torch
from linear_operator import to_linear_operator

# Define the first matrix A
A = torch.Tensor([[1, 2], [3, 4]])

# Define the second matrix B
B = torch.Tensor([[5, 6], [7, 8]])

# Compute the difference of A and B
A_op = to_linear_operator(A)
B_op = to_linear_operator(B)
difference = A_op.sub(B_op).to_dense()

# Print the resulting matrix
print(""Difference:"", difference)",train
linear-operator,13,"Create a Python program that utilizes the 'linear-operator' API to compute the singular value decomposition (SVD) of a given matrix. The program should print the computed singular values, left singular vectors, and right singular vectors.",code/linear-operator/linear-operator_13.py,"Verify that the program accurately prints the computed singular values, left singular vectors, and right singular vectors.",Ensure the program correctly computes the singular value decomposition of the input matrix.,,,,"#!pip install linear-operator
import torch
from linear_operator import to_linear_operator
from scipy.linalg import svd

# Define a matrix M
M = torch.Tensor([[1, 2], [3, 4]])

# Compute the singular value decomposition
M_op = to_linear_operator(M)
U, S, V = svd(M_op.to_dense())

# Print the singular values, left singular vectors, and right singular vectors
print(""Singular values:"", S)
print(""Left singular vectors:"", U)
print(""Right singular vectors:"", V)",train
linear-operator,4,Create a Python program that utilizes the 'linear-operator' API to perform matrix inversion. The program should take as input a square matrix and compute its inverse. The program should print the inverse matrix.,code/linear-operator/linear-operator_4.py,Verify that the program accurately prints the inverse matrix.,Ensure the program correctly computes the inverse of the input square matrix.,,,,"#!pip install linear-operator
import torch
from linear_operator import to_linear_operator
from scipy.linalg import inv

# Define the square matrix M
M = torch.Tensor([[1, 2], [3, 4]])

# Compute the inverse of M
M_op = to_linear_operator(M)
inverse = inv(M_op.to_dense())

# Print the inverse matrix
print(""Inverse:"", inverse)",train
linear-operator,21,Create a Python program that utilizes the 'linear-operator' API to compute the matrix hyperbolic sine of a given matrix. The program should print the computed matrix hyperbolic sine.,code/linear-operator/linear-operator_21.py,Verify that the program accurately prints the computed matrix hyperbolic sine.,Ensure the program correctly computes the matrix hyperbolic sine of the input matrix.,,,,"#!pip install linear-operator
import torch
from linear_operator import to_linear_operator
from scipy.linalg import sinhm

# Define a matrix M
M = torch.Tensor([[1, 2], [3, 4]])

# Compute the matrix hyperbolic sine
M_op = to_linear_operator(M)
matrix_sinh = sinhm(M_op.to_dense())

# Print the matrix hyperbolic sine
print(""Matrix Hyperbolic Sine:"", matrix_sinh)",train
linear-operator,19,Create a Python program that utilizes the 'linear-operator' API to compute the matrix cosine of a given matrix. The program should print the computed matrix cosine.,code/linear-operator/linear-operator_19.py,Ensure the program correctly computes the matrix cosine of the input matrix.,Verify that the program accurately prints the computed matrix cosine.,,,,"#!pip install linear-operator
import torch
from linear_operator import to_linear_operator
from scipy.linalg import cosm

# Define a matrix M
M = torch.Tensor([[1, 2], [3, 4]])

# Compute the matrix cosine
M_op = to_linear_operator(M)
matrix_cosine = cosm(M_op.to_dense())

# Print the matrix cosine
print(""Matrix Cosine:"", matrix_cosine)",train
linear-operator,15,Create a Python program that utilizes the 'linear-operator' API to compute the matrix exponential of a given matrix. The program should print the computed matrix exponential.,code/linear-operator/linear-operator_15.py,Verify that the program accurately prints the computed matrix exponential.,Ensure the program correctly computes the matrix exponential of the input matrix.,,,,"#!pip install linear-operator
import torch
from linear_operator import to_linear_operator
from scipy.linalg import expm

# Define a matrix M
M = torch.Tensor([[1, 2], [3, 4]])

# Compute the matrix exponential
M_op = to_linear_operator(M)
matrix_exp = expm(M_op.to_dense())

# Print the matrix exponential
print(""Matrix Exponential:"", matrix_exp)",train
linear-operator,3,Create a Python program that utilizes the 'linear-operator' API to perform matrix multiplication. The program should take as input two matrices and compute their product. The program should print the resulting matrix.,code/linear-operator/linear-operator_3.py,Verify that the program accurately prints the resulting matrix.,Ensure the program correctly computes the product of the input matrices.,,,,"#!pip install linear-operator
import torch
from linear_operator import to_linear_operator

# Define the first matrix A
A = torch.Tensor([[1, 2], [3, 4]])

# Define the second matrix B
B = torch.Tensor([[5, 6], [7, 8]])

# Compute the product of A and B
A_op = to_linear_operator(A)
B_op = to_linear_operator(B)
product = A_op.matmul(B_op).to_dense()

# Print the resulting matrix
print(""Product:"", product)",train
linear-operator,10,Create a Python program that utilizes the 'linear-operator' API to compute the determinant of a given square matrix. The program should print the computed determinant.,code/linear-operator/linear-operator_10.py,Verify that the program accurately prints the computed determinant.,Ensure the program correctly computes the determinant of the input square matrix.,,,,"#!pip install linear-operator
import torch
from linear_operator import to_linear_operator
from scipy.linalg import det

# Define a square matrix M
M = torch.Tensor([[1, 2], [3, 4]])

# Compute the determinant
M_op = to_linear_operator(M)
determinant = det(M_op.to_dense())

# Print the determinant
print(""Determinant:"", determinant)",train
linear-operator,23,Create a Python program that utilizes the 'linear-operator' API to compute the matrix hyperbolic tangent of a given matrix. The program should print the computed matrix hyperbolic tangent.,code/linear-operator/linear-operator_23.py,Verify that the program accurately prints the computed matrix hyperbolic tangent.,Ensure the program correctly computes the matrix hyperbolic tangent of the input matrix.,,,,"#!pip install linear-operator
import torch
from linear_operator import to_linear_operator
from scipy.linalg import tanhm

# Define a matrix M
M = torch.Tensor([[1, 2], [3, 4]])

# Compute the matrix hyperbolic tangent
M_op = to_linear_operator(M)
matrix_tanh = tanhm(M_op.to_dense())

# Print the matrix hyperbolic tangent
print(""Matrix Hyperbolic Tangent:"", matrix_tanh)",train
linear-operator,5,Create a Python program that utilizes the 'linear-operator' API to perform matrix addition. The program should take as input two matrices and compute their sum. The program should print the resulting matrix.,code/linear-operator/linear-operator_5.py,Ensure the program correctly computes the sum of the input matrices.,Verify that the program accurately prints the resulting matrix.,,,,"#!pip install linear-operator
import torch
from linear_operator import to_linear_operator

# Define the first matrix A
A = torch.Tensor([[1, 2], [3, 4]])

# Define the second matrix B
B = torch.Tensor([[5, 6], [7, 8]])

# Compute the sum of A and B
A_op = to_linear_operator(A)
B_op = to_linear_operator(B)
sum = A_op.add(B_op).to_dense()

# Print the resulting matrix
print(""Sum:"", sum)",train
linear-operator,14,Create a Python program that utilizes the 'linear-operator' API to compute the Moore-Penrose pseudoinverse of a given matrix. The program should print the computed pseudoinverse.,code/linear-operator/linear-operator_14.py,Ensure the program correctly computes the Moore-Penrose pseudoinverse of the input matrix.,Verify that the program accurately prints the computed pseudoinverse.,,,,"#!pip install linear-operator
import torch
from linear_operator import to_linear_operator
from scipy.linalg import pinv

# Define a matrix M
M = torch.Tensor([[1, 2], [3, 4]])

# Compute the Moore-Penrose pseudoinverse
M_op = to_linear_operator(M)
pseudoinverse = pinv(M_op.to_dense())

# Print the pseudoinverse
print(""Pseudoinverse:"", pseudoinverse)",train
linear-operator,8,"Create a Python program that utilizes the 'linear-operator' API to perform matrix scaling. The program should take as input a matrix and a scalar value, and it should compute the scaled matrix. The program should print the scaled matrix.",code/linear-operator/linear-operator_8.py,Ensure the program correctly computes the scaled matrix.,Verify that the program accurately prints the scaled matrix.,,,,"#!pip install linear-operator
import torch
from linear_operator import to_linear_operator

# Define the matrix M
M = torch.Tensor([[1, 2], [3, 4]])

# Define the scalar value
scalar = 2

# Compute the scaled matrix
M_op = to_linear_operator(M)
scaled_matrix = M_op.mul(scalar).to_dense()

# Print the scaled matrix
print(""Scaled Matrix:"", scaled_matrix)",train
linear-operator,11,Create a Python program that utilizes the 'linear-operator' API to compute the trace of a given square matrix. The program should print the computed trace.,code/linear-operator/linear-operator_11.py,Verify that the program accurately prints the computed trace.,Ensure the program correctly computes the trace of the input square matrix.,,,,"#!pip install linear-operator
import torch
from linear_operator import to_linear_operator

# Define a square matrix M
M = torch.Tensor([[1, 2], [3, 4]])

# Compute the trace
M_op = to_linear_operator(M)
trace = M_op.trace().to_dense()

# Print the trace
print(""Trace:"", trace)",train
linear-operator,16,Create a Python program that utilizes the 'linear-operator' API to compute the matrix logarithm of a given matrix. The program should print the computed matrix logarithm.,code/linear-operator/linear-operator_16.py,Verify that the program accurately prints the computed matrix logarithm.,Ensure the program correctly computes the matrix logarithm of the input matrix.,,,,"#!pip install linear-operator
import torch
from linear_operator import to_linear_operator
from scipy.linalg import logm

# Define a matrix M
M = torch.Tensor([[1, 2], [3, 4]])

# Compute the matrix logarithm
M_op = to_linear_operator(M)
matrix_log = logm(M_op.to_dense())

# Print the matrix logarithm
print(""Matrix Logarithm:"", matrix_log)",train
linear-operator,22,Create a Python program that utilizes the 'linear-operator' API to compute the matrix hyperbolic cosine of a given matrix. The program should print the computed matrix hyperbolic cosine.,code/linear-operator/linear-operator_22.py,Verify that the program accurately prints the computed matrix hyperbolic cosine.,Ensure the program correctly computes the matrix hyperbolic cosine of the input matrix.,,,,"#!pip install linear-operator
import torch
from linear_operator import to_linear_operator
from scipy.linalg import coshm

# Define a matrix M
M = torch.Tensor([[1, 2], [3, 4]])

# Compute the matrix hyperbolic cosine
M_op = to_linear_operator(M)
matrix_cosh = coshm(M_op.to_dense())

# Print the matrix hyperbolic cosine
print(""Matrix Hyperbolic Cosine:"", matrix_cosh)",train
linear-operator,7,Create a Python program that utilizes the 'linear-operator' API to perform matrix transposition. The program should take as input a matrix and compute its transpose. The program should print the transposed matrix.,code/linear-operator/linear-operator_7.py,Ensure the program correctly computes the transpose of the input matrix.,Verify that the program accurately prints the transposed matrix.,,,,"#!pip install linear-operator
import torch
from linear_operator import to_linear_operator

# Define the matrix M
M = torch.Tensor([[1, 2], [3, 4]])

# Compute the transpose of M
M_op = to_linear_operator(M)
transpose = M_op.transpose().to_dense()

# Print the transposed matrix
print(""Transpose:"", transpose)",train
linear-operator,1,Create a Python program that utilizes the 'linear-operator' API to compute the eigenvalues and eigenvectors of a given square matrix. The program should print the computed eigenvalues and eigenvectors.,code/linear-operator/linear-operator_1.py,Ensure the program correctly computes the eigenvalues of the input square matrix.,Verify that the program accurately computes the eigenvectors of the input matrix.,Confirm that the program correctly prints the computed eigenvalues and eigenvectors.,,,"#!pip install linear-operator
import torch
from linear_operator import to_linear_operator
from scipy.linalg import eigh

# Define a square matrix M
M = torch.Tensor([[4, 2], [2, 5]])

# Compute the eigenvalues and eigenvectors
M_op = to_linear_operator(M)
eigenvalues, eigenvectors = eigh(M_op.to_dense())

# Print the eigenvalues and eigenvectors
print(""Eigenvalues:"", eigenvalues)
print(""Eigenvectors:"", eigenvectors)",test
linear-operator,20,Create a Python program that utilizes the 'linear-operator' API to compute the matrix sine of a given matrix. The program should print the computed matrix sine.,code/linear-operator/linear-operator_20.py,Verify that the program accurately prints the computed matrix sine.,Ensure the program correctly computes the matrix sine of the input matrix.,,,,"#!pip install linear-operator
import torch
from linear_operator import to_linear_operator
from scipy.linalg import sinm

# Define a matrix M
M = torch.Tensor([[1, 2], [3, 4]])

# Compute the matrix sine
M_op = to_linear_operator(M)
matrix_sine = sinm(M_op.to_dense())

# Print the matrix sine
print(""Matrix Sine:"", matrix_sine)",test
linear-operator,17,Create a Python program that utilizes the 'linear-operator' API to compute the matrix square root of a given matrix. The program should print the computed matrix square root.,code/linear-operator/linear-operator_17.py,Ensure the program correctly computes the matrix square root of the input matrix.,Verify that the program accurately prints the computed matrix square root.,,,,"#!pip install linear-operator
import torch
from linear_operator import to_linear_operator
from scipy.linalg import sqrtm

# Define a matrix M
M = torch.Tensor([[1, 2], [3, 4]])

# Compute the matrix square root
M_op = to_linear_operator(M)
matrix_sqrt = sqrtm(M_op.to_dense())

# Print the matrix square root
print(""Matrix Square Root:"", matrix_sqrt)",test
llama-index,1,"Develop a Python program with 'llama-index' to query 'https://en.wikipedia.org/wiki/Ahmed_Zewail' using 'TrafilaturaWebReader,' create 'GPTListIndex,' and answer two questions: Zewail's awards and publications. Print responses.",code/llama-index/llama-index_1.py,Test that the 'TrafilaturaWebReader' correctly loads and processes the content from the specified website.,Test that the program can accurately answer the provided questions based on the indexed data from the webpage.,Test that the 'GPTListIndex' is created and functions as expected to index the webpage content.,,,"#!pip install llama-cpp-python
#!pip install llama-index html2text trafilatura

from pprint import pprint
from llama_index import GPTListIndex
from llama_index import TrafilaturaWebReader

def query_website(url_list, *questions):
    documents = TrafilaturaWebReader().load_data(url_list)
    print(""documents:""); pprint(documents)
    index = GPTListIndex.from_documents(documents)
    engine = index.as_query_engine()
    for question in questions:
        print(f""\n== QUESTION: {question}\n"")
        response = engine.query(question)
        print(f""== RESPONSE: {response}"")

if __name__ == ""__main__"":
  url_list = [""https://en.wikipedia.org/wiki/Ahmed_Zewail""]
  query_website(url_list, ""How many awards does Zewail take?"",
                          ""How many publications have Zewail?"")",train
llama-index,9,Develop a Python program using the 'llama-index' API to scrape and index data from 'https://en.wikipedia.org/wiki/Deep_learning.' Extract information about the concepts and advancements in deep learning. Print the extracted information.,code/llama-index/llama-index_9.py,Test that the program can accurately extract and display information about concepts and advancements in deep learning.,Test that the 'TrafilaturaWebReader' correctly loads and processes the content from the specified website.,Test that the 'GPTListIndex' is created and functions as expected to index the webpage content.,,,"#!pip install llama-cpp-python
#!pip install llama-index html2text trafilatura

from llama_index import GPTListIndex
from llama_index import TrafilaturaWebReader

def scrape_and_index_deep_learning_info(url, *topics):
    documents = TrafilaturaWebReader().load_data([url])
    index = GPTListIndex.from_documents(documents)
    engine = index.as_query_engine()

    results = {}
    for topic in topics:
        results[topic] = engine.query(topic)

    return results

if __name__ == ""__main__"":
    url = ""https://en.wikipedia.org/wiki/Deep_learning""
    topics = [""Deep learning concepts"", ""Advancements in deep learning""]
    extracted_data = scrape_and_index_deep_learning_info(url, *topics)

    for topic, data in extracted_data.items():
        print(f""{topic}:\n{data}"")",train
llama-index,6,Write a Python program using the 'llama-index' API to scrape and index data from 'https://en.wikipedia.org/wiki/Data_science.' Extract information about the key concepts and tools used in data science. Print the extracted information.,code/llama-index/llama-index_6.py,Test that the 'TrafilaturaWebReader' correctly loads and processes the content from the specified website.,Test that the 'GPTListIndex' is created and functions as expected to index the webpage content.,Test that the program can accurately extract and display information about key concepts and tools used in data science.,,,"#!pip install llama-cpp-python
#!pip install llama-index html2text trafilatura

from llama_index import GPTListIndex
from llama_index import TrafilaturaWebReader

def scrape_and_index_data_science_info(url, *topics):
    documents = TrafilaturaWebReader().load_data([url])
    index = GPTListIndex.from_documents(documents)
    engine = index.as_query_engine()

    results = {}
    for topic in topics:
        results[topic] = engine.query(topic)

    return results

if __name__ == ""__main__"":
    url = ""https://en.wikipedia.org/wiki/Data_science""
    topics = [""Key concepts in data science"", ""Tools used in data science""]
    extracted_data = scrape_and_index_data_science_info(url, *topics)

    for topic, data in extracted_data.items():
        print(f""{topic}:\n{data}"")",train
llama-index,3,"Develop a Python program that utilizes the 'llama-index' API to scrape and index content from 'https://en.wikipedia.org/wiki/Machine_learning.' Extract information about the types of machine learning, such as supervised learning, unsupervised learning, and reinforcement learning. Print the extracted information.",code/llama-index/llama-index_3.py,"Test that the program can accurately extract and display information about types of machine learning, including supervised learning, unsupervised learning, and reinforcement learning.",Test that the 'TrafilaturaWebReader' correctly loads and processes the content from the specified website.,Test that the 'GPTListIndex' is created and functions as expected to index the webpage content.,,,"#!pip install llama-cpp-python
#!pip install llama-index html2text trafilatura

from llama_index import GPTListIndex
from llama_index import TrafilaturaWebReader

def scrape_and_index_ml_types(url, *ml_types):
    documents = TrafilaturaWebReader().load_data([url])
    index = GPTListIndex.from_documents(documents)
    engine = index.as_query_engine()

    results = {}
    for ml_type in ml_types:
        results[ml_type] = engine.query(ml_type)

    return results

if __name__ == ""__main__"":
    url = ""https://en.wikipedia.org/wiki/Machine_learning""
    ml_types = [""Supervised learning"", ""Unsupervised learning"", ""Reinforcement learning""]
    extracted_data = scrape_and_index_ml_types(url, *ml_types)

    for ml_type, data in extracted_data.items():
        print(f""{ml_type}:\n{data}"")",train
llama-index,2,Generate a Python program using the 'llama-index' API to scrape and index data from 'https://en.wikipedia.org/wiki/Python_(programming_language).' Extract information about the history and features of the Python programming language. Print the extracted information.,code/llama-index/llama-index_2.py,Test that the program can accurately extract and display information about the history and features of the Python programming language.,Test that the 'TrafilaturaWebReader' correctly loads and processes the content from the specified website.,Test that the 'GPTListIndex' is created and functions as expected to index the webpage content.,,,"#!pip install llama-cpp-python
#!pip install llama-index html2text trafilatura

from llama_index import GPTListIndex
from llama_index import TrafilaturaWebReader

def scrape_and_index(url, *topics):
    documents = TrafilaturaWebReader().load_data([url])
    index = GPTListIndex.from_documents(documents)
    engine = index.as_query_engine()

    results = {}
    for topic in topics:
        results[topic] = engine.query(topic)

    return results

if __name__ == ""__main__"":
    url = ""https://en.wikipedia.org/wiki/Python_(programming_language)""
    topics = [""History of Python"", ""Features of Python""]
    extracted_data = scrape_and_index(url, *topics)

    for topic, data in extracted_data.items():
        print(f""{topic}:\n{data}"")",train
llama-index,13,Develop a Python program using the 'llama-index' API to scrape and index data from 'https://en.wikipedia.org/wiki/Computer_graphics.' Extract information about the history and techniques of computer graphics. Print the extracted information.,code/llama-index/llama-index_13.py,Test that the program can accurately extract and display information about the history and techniques of computer graphics.,Test that the 'TrafilaturaWebReader' correctly loads and processes the content from the specified website.,Test that the 'GPTListIndex' is created and functions as expected to index the webpage content.,,,"#!pip install llama-cpp-python
#!pip install llama-index html2text trafilatura

from llama_index import GPTListIndex
from llama_index import TrafilaturaWebReader

def scrape_and_index_computer_graphics_info(url, *topics):
    documents = TrafilaturaWebReader().load_data([url])
    index = GPTListIndex.from_documents(documents)
    engine = index.as_query_engine()

    results = {}
    for topic in topics:
        results[topic] = engine.query(topic)

    return results

if __name__ == ""__main__"":
    url = ""https://en.wikipedia.org/wiki/Computer_graphics""
    topics = [""History of computer graphics"", ""Techniques of computer graphics""]
    extracted_data = scrape_and_index_computer_graphics_info(url, *topics)

    for topic, data in extracted_data.items():
        print(f""{topic}:\n{data}"")",train
llama-index,5,Develop a Python program using the 'llama-index' API to scrape and index data from 'https://en.wikipedia.org/wiki/Neural_network.' Extract information about the architecture and applications of neural networks. Print the extracted information.,code/llama-index/llama-index_5.py,Test that the program can accurately extract and display information about the architecture and applications of neural networks.,Test that the 'TrafilaturaWebReader' correctly loads and processes the content from the specified website.,Test that the 'GPTListIndex' is created and functions as expected to index the webpage content.,,,"#!pip install llama-cpp-python
#!pip install llama-index html2text trafilatura

from llama_index import GPTListIndex
from llama_index import TrafilaturaWebReader

def scrape_and_index_neural_network_info(url, *topics):
    documents = TrafilaturaWebReader().load_data([url])
    index = GPTListIndex.from_documents(documents)
    engine = index.as_query_engine()

    results = {}
    for topic in topics:
        results[topic] = engine.query(topic)

    return results

if __name__ == ""__main__"":
    url = ""https://en.wikipedia.org/wiki/Neural_network""
    topics = [""Neural network architecture"", ""Applications of neural networks""]
    extracted_data = scrape_and_index_neural_network_info(url, *topics)

    for topic, data in extracted_data.items():
        print(f""{topic}:\n{data}"")",train
llama-index,8,Write a Python program using the 'llama-index' API to scrape and index data from 'https://en.wikipedia.org/wiki/Quantum_computing.' Extract information about the principles and potential applications of quantum computing. Print the extracted information.,code/llama-index/llama-index_8.py,Test that the program can accurately extract and display information about the principles and potential applications of quantum computing.,Test that the 'TrafilaturaWebReader' correctly loads and processes the content from the specified website.,Test that the 'GPTListIndex' is created and functions as expected to index the webpage content.,,,"#!pip install llama-cpp-python
#!pip install llama-index html2text trafilatura

from llama_index import GPTListIndex
from llama_index import TrafilaturaWebReader

def scrape_and_index_quantum_computing_info(url, *topics):
    documents = TrafilaturaWebReader().load_data([url])
    index = GPTListIndex.from_documents(documents)
    engine = index.as_query_engine()

    results = {}
    for topic in topics:
        results[topic] = engine.query(topic)

    return results

if __name__ == ""__main__"":
    url = ""https://en.wikipedia.org/wiki/Quantum_computing""
    topics = [""Principles of quantum computing"", ""Potential applications of quantum computing""]
    extracted_data = scrape_and_index_quantum_computing_info(url, *topics)

    for topic, data in extracted_data.items():
        print(f""{topic}:\n{data}"")",train
llama-index,4,Write a Python program using the 'llama-index' API to scrape and index data from 'https://en.wikipedia.org/wiki/Artificial_intelligence.' Extract information about the history of artificial intelligence and its applications. Print the extracted information.,code/llama-index/llama-index_4.py,Test that the program can accurately extract and display information about the history of artificial intelligence and its applications.,Test that the 'TrafilaturaWebReader' correctly loads and processes the content from the specified website.,Test that the 'GPTListIndex' is created and functions as expected to index the webpage content.,,,"#!pip install llama-cpp-python
#!pip install llama-index html2text trafilatura

from llama_index import GPTListIndex
from llama_index import TrafilaturaWebReader

def scrape_and_index_ai_info(url, *topics):
    documents = TrafilaturaWebReader().load_data([url])
    index = GPTListIndex.from_documents(documents)
    engine = index.as_query_engine()

    results = {}
    for topic in topics:
        results[topic] = engine.query(topic)

    return results

if __name__ == ""__main__"":
    url = ""https://en.wikipedia.org/wiki/Artificial_intelligence""
    topics = [""History of artificial intelligence"", ""Applications of AI""]
    extracted_data = scrape_and_index_ai_info(url, *topics)

    for topic, data in extracted_data.items():
        print(f""{topic}:\n{data}"")",train
llama-index,7,Develop a Python program using the 'llama-index' API to scrape and index data from 'https://en.wikipedia.org/wiki/Blockchain.' Extract information about the technology and applications of blockchain. Print the extracted information.,code/llama-index/llama-index_7.py,Test that the 'TrafilaturaWebReader' correctly loads and processes the content from the specified website.,Test that the 'GPTListIndex' is created and functions as expected to index the webpage content.,Test that the program can accurately extract and display information about the technology and applications of blockchain.,,,"#!pip install llama-cpp-python
#!pip install llama-index html2text trafilatura

from llama_index import GPTListIndex
from llama_index import TrafilaturaWebReader

def scrape_and_index_blockchain_info(url, *topics):
    documents = TrafilaturaWebReader().load_data([url])
    index = GPTListIndex.from_documents(documents)
    engine = index.as_query_engine()

    results = {}
    for topic in topics:
        results[topic] = engine.query(topic)

    return results

if __name__ == ""__main__"":
    url = ""https://en.wikipedia.org/wiki/Blockchain""
    topics = [""Blockchain technology"", ""Applications of blockchain""]
    extracted_data = scrape_and_index_blockchain_info(url, *topics)

    for topic, data in extracted_data.items():
        print(f""{topic}:\n{data}"")",train
llama-index,11,Develop a Python program using the 'llama-index' API to scrape and index data from 'https://en.wikipedia.org/wiki/Robotics.' Extract information about the history and types of robotics. Print the extracted information.,code/llama-index/llama-index_11.py,Test that the 'TrafilaturaWebReader' correctly loads and processes the content from the specified website.,Test that the program can accurately extract and display information about the history and types of robotics.,Test that the 'GPTListIndex' is created and functions as expected to index the webpage content.,,,"#!pip install llama-cpp-python
#!pip install llama-index html2text trafilatura

from llama_index import GPTListIndex
from llama_index import TrafilaturaWebReader

def scrape_and_index_robotics_info(url, *topics):
    documents = TrafilaturaWebReader().load_data([url])
    index = GPTListIndex.from_documents(documents)
    engine = index.as_query_engine()

    results = {}
    for topic in topics:
        results[topic] = engine.query(topic)

    return results

if __name__ == ""__main__"":
    url = ""https://en.wikipedia.org/wiki/Robotics""
    topics = [""History of robotics"", ""Types of robotics""]
    extracted_data = scrape_and_index_robotics_info(url, *topics)
    
    for topic, data in extracted_data.items():
        print(f""{topic}:\n{data}"")",test
llama-index,10,Write a Python program using the 'llama-index' API to scrape and index data from 'https://en.wikipedia.org/wiki/Machine_vision.' Extract information about the history and applications of machine vision. Print the extracted information.,code/llama-index/llama-index_10.py,Test that the 'TrafilaturaWebReader' correctly loads and processes the content from the specified website.,Test that the program can accurately extract and display information about the history and applications of machine vision.,Test that the 'GPTListIndex' is created and functions as expected to index the webpage content.,,,"#!pip install llama-cpp-python
#!pip install llama-index html2text trafilatura

from llama_index import GPTListIndex
from llama_index import TrafilaturaWebReader

def scrape_and_index_machine_vision_info(url, *topics):
    documents = TrafilaturaWebReader().load_data([url])
    index = GPTListIndex.from_documents(documents)
    engine = index.as_query_engine()

    results = {}
    for topic in topics:
        results[topic] = engine.query(topic)

    return results

if __name__ == ""__main__"":
    url = ""https://en.wikipedia.org/wiki/Machine_vision""
    topics = [""History of machine vision"", ""Applications of machine vision""]
    extracted_data = scrape_and_index_machine_vision_info(url, *topics)

    for topic, data in extracted_data.items():
        print(f""{topic}:\n{data}"")",test
mlflow,9,Write a program that utilizes the MLflow API to train a K-means clustering model on the famous iris dataset. Log the cluster centers as artifacts.,code/mlflow/mlflow_9.py,Test if the K-means clustering model is correctly trained.,Test if cluster centers are logged as artifacts.,,,,"#!pip install mlflow
import mlflow
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans

# Load the Iris dataset
data = load_iris()
X = data.data

# Start an MLflow run
with mlflow.start_run() as run:
    # Train a K-means clustering model
    kmeans = KMeans(n_clusters=3, random_state=42)
    kmeans.fit(X)
    
    # Log cluster centers as an artifact
    cluster_centers = kmeans.cluster_centers_
    mlflow.log_artifact(""cluster_centers.txt"")
",train
mlflow,18,"Develop a program that uses the MLflow API to train a logistic regression classifier on a synthetic binary classification dataset. Log the model, its hyperparameters, and accuracy as a metric.",code/mlflow/mlflow_18.py,Test if the accuracy metric is logged correctly.,Test if hyperparameters are logged correctly.,Test if the logistic regression classifier is correctly trained.,,,"#!pip install mlflow
import mlflow
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Generate a synthetic binary classification dataset
X = np.random.rand(100, 10)
y = np.random.randint(2, size=100)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Start an MLflow run
with mlflow.start_run() as run:
    # Train a logistic regression classifier
    logreg = LogisticRegression(random_state=42)
    logreg.fit(X_train, y_train)
    
    # Log hyperparameters
    mlflow.log_params(logreg.get_params())
    
    # Make predictions on the test set
    y_pred = logreg.predict(X_test)
    
    # Calculate and log accuracy as a metric
    accuracy = accuracy_score(y_test, y_pred)
    mlflow.log_metric(""accuracy"", accuracy)
    
    # Log the trained model
    mlflow.sklearn.log_model(logreg, ""logistic_regression_model"")
",train
mlflow,13,"Write a program that uses the MLflow API to train a linear regression model on a synthetic dataset. Log the model, its coefficients, and the mean absolute error as metrics.",code/mlflow/mlflow_13.py,Test if the mean absolute error metric is logged correctly.,Test if model coefficients are logged correctly.,Test if the linear regression model is correctly trained.,,,"#!pip install mlflow
import mlflow
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error

# Generate a synthetic dataset
X = np.random.rand(200, 5)
y = 2 * X[:, 0] + 3 * X[:, 1] - 4 * X[:, 2] + np.random.randn(200)

# Start an MLflow run
with mlflow.start_run() as run:
    # Train a linear regression model
    reg = LinearRegression()
    reg.fit(X, y)
    
    # Log model coefficients
    coefs = reg.coef_
    mlflow.log_params({""coefficients"": coefs})
    
    # Make predictions
    y_pred = reg.predict(X)
    
    # Calculate and log mean absolute error as a metric
    mae = mean_absolute_error(y, y_pred)
    mlflow.log_metric(""mean_absolute_error"", mae)
    
    # Log the trained model
    mlflow.sklearn.log_model(reg, ""linear_regression_model"")
",train
mlflow,2,"Create a program that utilizes the MLflow API to track and log hyperparameters, metrics, and artifacts for a random forest classifier trained on the breast cancer dataset. Save the trained model as an artifact.",code/mlflow/mlflow_2.py,Test if the Random Forest model is correctly trained.,Test if the accuracy metric is logged correctly.,Test if hyperparameters are logged correctly.,,,"#!pip install mlflow
import mlflow
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Load the breast cancer dataset
data = load_breast_cancer()
X, y = data.data, data.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Start an MLflow run
with mlflow.start_run() as run:
    # Train a Random Forest classifier
    clf = RandomForestClassifier(n_estimators=100, random_state=42)
    clf.fit(X_train, y_train)
    
    # Log hyperparameters
    mlflow.log_params(clf.get_params())
    
    # Make predictions on the test set
    y_pred = clf.predict(X_test)
    
    # Calculate and log accuracy as a metric
    accuracy = accuracy_score(y_test, y_pred)
    mlflow.log_metric(""accuracy"", accuracy)
    
    # Save the model as an artifact
    mlflow.sklearn.log_model(clf, ""random_forest_model"")",train
mlflow,14,"Develop a program that uses the MLflow API to train a simple decision tree classifier on a synthetic binary classification dataset. Log the model, its hyperparameters, and accuracy as a metric.",code/mlflow/mlflow_14.py,Test if the Decision Tree classifier is correctly trained.,Test if hyperparameters are logged correctly.,Test if the accuracy metric is logged correctly.,,,"#!pip install mlflow
import mlflow
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Generate a synthetic binary classification dataset
X = np.random.rand(100, 5)
y = np.random.randint(2, size=100)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Start an MLflow run
with mlflow.start_run() as run:
    # Train a Decision Tree classifier
    clf = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)
    clf.fit(X_train, y_train)
    
    # Log hyperparameters
    mlflow.log_params(clf.get_params())
    
    # Make predictions on the test set
    y_pred = clf.predict(X_test)
    
    # Calculate and log accuracy as a metric
    accuracy = accuracy_score(y_test, y_pred)
    mlflow.log_metric(""accuracy"", accuracy)
    
    # Log the trained model
    mlflow.sklearn.log_model(clf, ""decision_tree_classifier_model"")
",train
mlflow,6,"Develop a program that uses the MLflow API to train a neural network classifier (e.g., a simple feedforward network) on a synthetic dataset. Log the trained model and the loss curve as metrics.",code/mlflow/mlflow_6.py,Test if the neural network model is correctly trained.,Test if the loss curve is logged as an artifact.,,,,"#!pip install mlflow
import mlflow
import numpy as np
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
import matplotlib.pyplot as plt

# Generate a synthetic dataset
X = np.random.rand(100, 10)
y = np.random.randint(2, size=100)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Start an MLflow run
with mlflow.start_run() as run:
    # Create a simple feedforward neural network
    model = Sequential()
    model.add(Dense(32, input_dim=10, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    
    # Compile the model


    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])
    
    # Train the model
    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))
    
    # Log the trained model
    mlflow.keras.log_model(model, ""neural_network_model"")
    
    # Log the loss curve
    plt.plot(history.history['loss'], label='train_loss')
    plt.plot(history.history['val_loss'], label='val_loss')
    plt.legend()
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Loss Curve')
    plt.savefig('loss_curve.png')
    mlflow.log_artifact('loss_curve.png')
",train
mlflow,3,Develop a program that uses the MLflow API to track the training of a K-means clustering model on a synthetic dataset. Log the cluster centers as artifacts.,code/mlflow/mlflow_3.py,Test if the K-means clustering model is correctly trained.,Test if cluster centers are logged as artifacts.,,,,"#!pip install mlflow
import mlflow
import numpy as np
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

# Create a synthetic dataset
X, _ = make_blobs(n_samples=300, centers=3, random_state=42)

# Start an MLflow run
with mlflow.start_run() as run:
    # Train a K-means clustering model
    kmeans = KMeans(n_clusters=3, random_state=42)
    kmeans.fit(X)
    
    # Log cluster centers as an artifact
    centers = kmeans.cluster_centers_
    np.savetxt(""cluster_centers.txt"", centers)
    mlflow.log_artifact(""cluster_centers.txt"")
",train
mlflow,12,"Develop a program that utilizes the MLflow API to train a random forest regressor on a synthetic dataset. Log the model, its hyperparameters, and mean squared error as a metric.",code/mlflow/mlflow_12.py,Test if the mean squared error metric is logged correctly.,Test if hyperparameters are logged correctly.,Test if the Random Forest regressor is correctly trained.,,,"#!pip install mlflow
import mlflow
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Generate a synthetic dataset
X = np.random.rand(200, 5)
y = np.random.rand(200)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Start an MLflow run
with mlflow.start_run() as run:
    # Train a Random Forest regressor
    rf = RandomForestRegressor(n_estimators=100, random_state=42)
    rf.fit(X_train, y_train)
    
    # Log hyperparameters
    mlflow.log_params(rf.get_params())
    
    # Make predictions on the test set
    y_pred = rf.predict(X_test)
    
    # Calculate and log mean squared error as a metric
    mse = mean_squared_error(y_test, y_pred)
    mlflow.log_metric(""mean_squared_error"", mse)
    
    # Log the trained model
    mlflow.sklearn.log_model(rf, ""random_forest_regressor_model"")
",train
mlflow,21,"Write a program that uses the MLflow API to train a decision tree regressor on a synthetic dataset. Log the model, its hyperparameters, and mean squared error as a metric.",code/mlflow/mlflow_21.py,Test if the mean squared error metric is logged correctly.,Test if hyperparameters are logged correctly.,Test if the Decision Tree regressor is correctly trained.,,,"#!pip install mlflow
import mlflow
import numpy as np
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Generate a synthetic dataset
X = np.random.rand(200, 5)
y = 3 * X[:, 0] - 2 * X[:, 1] + 4 * X[:, 2] + np.random.randn(200)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Start an MLflow run
with mlflow.start_run() as run:
    # Train a Decision Tree regressor
    clf = DecisionTreeRegressor(criterion='mse', max_depth=5, random_state=42)
    clf.fit(X_train, y_train)
    
    # Log hyperparameters
    mlflow.log_params(clf.get_params())
    
    # Make predictions on the test set
    y_pred = clf.predict(X_test)
    
    # Calculate and log mean squared error as a metric
    mse = mean_squared_error(y_test, y_pred)
    mlflow.log_metric(""mean_squared_error"", mse)
    
    # Log the trained model
    mlflow.sklearn.log_model(clf, ""decision_tree_regressor_model"")
",train
mlflow,4,"Write a program that uses the MLflow API to train a linear regression model on a sample housing dataset. Log the model's coefficients, mean squared error, and R-squared score as metrics.",code/mlflow/mlflow_4.py,Test if model coefficients are logged correctly.,Test if mean squared error and R-squared score are logged correctly.,Test if the linear regression model is correctly trained.,,,"#!pip install mlflow
import mlflow
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load a sample housing dataset
data_url = ""https://raw.githubusercontent.com/openai/openai-cookbook/master/data/housing.csv""
data = pd.read_csv(data_url)

# Split the dataset into features and target
X = data.drop(columns=['MEDV'])
y = data['MEDV']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Start an MLflow run
with mlflow.start_run() as run:
    # Train a linear regression model
    reg = LinearRegression()
    reg.fit(X_train, y_train)
    
    # Log model coefficients
    coefs = reg.coef_
    mlflow.log_params({""coefficients"": coefs})
    
    # Make predictions on the test set
    y_pred = reg.predict(X_test)
    
    # Calculate and log mean squared error
    mse = mean_squared_error(y_test, y_pred)
    mlflow.log_metric(""mean_squared_error"", mse)
    
    # Calculate and log R-squared score
    r2 = r2_score(y_test, y_pred)
    mlflow.log_metric(""r2_score"", r2)
",train
mlflow,5,"Create a program that leverages the MLflow API to train a support vector machine (SVM) classifier on the famous iris dataset. Log the trained model, and its hyperparameters.",code/mlflow/mlflow_5.py,Test if the accuracy metric is logged correctly.,Test if hyperparameters are logged correctly.,Test if the SVM classifier is correctly trained.,,,"#!pip install mlflow
import mlflow
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load the Iris dataset
data = load_iris()
X, y = data.data, data.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Start an MLflow run
with mlflow.start_run() as run:
    # Train a Support Vector Machine (SVM) classifier
    svc = SVC(C=1.0, kernel='rbf', random_state=42)
    svc.fit(X_train, y_train)
    
    # Log hyperparameters
    mlflow.log_params(svc.get_params())
    
    # Make predictions on the test set
    y_pred = svc.predict(X_test)
    
    # Calculate and log accuracy as a metric
    accuracy = accuracy_score(y_test, y_pred)
    mlflow.log_metric(""accuracy"", accuracy)
    
    # Save the model as an artifact
    mlflow.sklearn.log_model(svc, ""svm_model"")",train
mlflow,19,"Write a program that utilizes the MLflow API to train a neural network regressor (e.g., a simple feedforward network) on a synthetic dataset. Log the trained model and the loss curve as metrics.",code/mlflow/mlflow_19.py,Test if the neural network regressor is correctly trained.,Test if the loss curve is logged as an artifact.,,,,"#!pip install mlflow
import mlflow
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
import matplotlib.pyplot as plt

# Generate a synthetic dataset
X = np.random.rand(200, 10)
y = np.random.rand(200)

# Start an MLflow run
with mlflow.start_run() as run:
    # Create a simple feedforward neural network
    model = Sequential()
    model.add(Dense(32, input_dim=10, activation='relu'))
    model.add(Dense(1, activation='linear'))
    
    # Compile the model
    model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001))
    
    # Train the model
    history = model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)
    
    # Log the trained model
    mlflow.keras.log_model(model, ""neural_network_regressor_model"")
    
    # Log the loss curve
    plt.plot(history.history['loss'], label='train_loss')
    plt.plot(history.history['val_loss'], label='val_loss')
    plt.legend()
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Loss Curve')
    plt.savefig('loss_curve.png')
    mlflow.log_artifact('loss_curve.png')
",train
mlflow,17,"Write a program that uses the MLflow API to train a decision tree regressor on a synthetic dataset. Log the model, its hyperparameters, and the mean squared error as a metric.",code/mlflow/mlflow_17.py,Test if the mean squared error metric is logged correctly.,Test if hyperparameters are logged correctly.,Test if the Decision Tree regressor is correctly trained.,,,"#!pip install mlflow
import mlflow
import numpy as np
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Generate a synthetic dataset
X = np.random.rand(200, 5)
y = 3 * X[:, 0] - 2 * X[:, 1] + 4 * X[:, 2] + np.random.randn(200)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Start an MLflow run
with mlflow.start_run() as run:
    # Train a Decision Tree regressor
    clf = DecisionTreeRegressor(criterion='mse', max_depth=5, random_state=42)
    clf.fit(X_train, y_train)
    
    # Log hyperparameters
    mlflow.log_params(clf.get_params())
    
    # Make predictions on the test set
    y_pred = clf.predict(X_test)
    
    # Calculate and log mean squared error as a metric
    mse = mean_squared_error(y_test, y_pred)
    mlflow.log_metric(""mean_squared_error"", mse)
    
    # Log the trained model
    mlflow.sklearn.log_model(clf, ""decision_tree_regressor_model"")
",train
mlflow,23,"Write a program that uses the MLflow API to train a logistic regression classifier on a synthetic binary classification dataset. Log the model, its hyperparameters, and accuracy as a metric.",code/mlflow/mlflow_23.py,Test if the accuracy metric is logged correctly.,Test if hyperparameters are logged correctly.,Test if the logistic regression classifier is correctly trained.,,,"#!pip install mlflow
import mlflow
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Generate a synthetic binary classification dataset
X = np.random.rand(100, 10)
y = np.random.randint(2, size=100)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Start an MLflow run
with mlflow.start_run() as run:
    # Train a logistic regression classifier
    logreg = LogisticRegression(random_state=42)
    logreg.fit(X_train, y_train)
    
    # Log hyperparameters
    mlflow.log_params(logreg.get_params())
    
    # Make predictions on the test set
    y_pred = logreg.predict(X_test)
    
    # Calculate and log accuracy as a metric
    accuracy = accuracy_score(y_test, y_pred)
    mlflow.log_metric(""accuracy"", accuracy)
    
    # Log the trained model
    mlflow.sklearn.log_model(logreg, ""logistic_regression_model"")
",train
mlflow,25,"Write a program that utilizes the MLflow API to train a random forest regressor on a synthetic dataset. Log the model, its hyperparameters, and mean squared error as a metric.",code/mlflow/mlflow_25.py,Test if the mean squared error metric is logged correctly.,Test if hyperparameters are logged correctly.,Test if the Random Forest regressor is correctly trained.,,,"#!pip install mlflow
import mlflow
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Generate a synthetic dataset
X = np.random.rand(200, 5)
y = np.random.rand(200)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Start an MLflow run
with mlflow.start_run() as run:
    # Train a Random Forest regressor
    rf = RandomForestRegressor(n_estimators=100, random_state=42)
    rf.fit(X_train, y_train)
    
    # Log hyperparameters
    mlflow.log_params(rf.get_params())
    
    # Make predictions on the test set
    y_pred = rf.predict(X_test)
    
    # Calculate and log mean squared error as a metric
    mse = mean_squared_error(y_test, y_pred)
    mlflow.log_metric(""mean_squared_error"", mse)
    
    # Log the trained model
    mlflow.sklearn.log_model(rf, ""random_forest_regressor_model"")
",train
mlflow,8,"Develop a program that uses the MLflow API to train a random forest regressor on a synthetic dataset. Log the model, its hyperparameters, and the mean absolute error as a metric.",code/mlflow/mlflow_8.py,Test if the mean absolute error metric is logged correctly.,Test if hyperparameters are logged correctly.,Test if the Random Forest regressor is correctly trained.,,,"#!pip install mlflow
import mlflow
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error

# Generate a synthetic dataset
X = np.random.rand(200, 5)
y = np.random.rand(200)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Start an MLflow run
with mlflow.start_run() as run:
    # Train a Random Forest regressor
    rf = RandomForestRegressor(n_estimators=100, random_state=42)
    rf.fit(X_train, y_train)
    
    # Log hyperparameters
    mlflow.log_params(rf.get_params())
    
    # Make predictions on the test set
    y_pred = rf.predict(X_test)
    
    # Calculate and log mean absolute error as a metric
    mae = mean_absolute_error(y_test, y_pred)
    mlflow.log_metric(""mean_absolute_error"", mae)
    
    # Save the model as an artifact
    mlflow.sklearn.log_model(rf, ""random_forest_regressor_model"")",train
mlflow,11,"Write a program that uses the MLflow API to train a support vector machine (SVM) classifier on a synthetic dataset. Log the model, its hyperparameters, and accuracy as a metric.",code/mlflow/mlflow_11.py,Test if the accuracy metric is logged correctly.,Test if hyperparameters are logged correctly.,Test if the SVM classifier is correctly trained.,,,"#!pip install mlflow
import mlflow
import numpy as np
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Generate a synthetic dataset
X = np.random.rand(100, 5)
y = np.random.randint(2, size=100)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Start an MLflow run
with mlflow.start_run() as run:
    # Train a Support Vector Machine (SVM) classifier
    svc = SVC(C=1.0, kernel='rbf',

 random_state=42)
    svc.fit(X_train, y_train)
    
    # Log hyperparameters
    mlflow.log_params(svc.get_params())
    
    # Make predictions on the test set
    y_pred = svc.predict(X_test)
    
    # Calculate and log accuracy as a metric
    accuracy = accuracy_score(y_test, y_pred)
    mlflow.log_metric(""accuracy"", accuracy)
    
    # Log the trained model
    mlflow.sklearn.log_model(svc, ""svm_classifier_model"")
",train
mlflow,15,"Write a program that leverages the MLflow API to train a random forest regressor on a synthetic dataset. Log the model, its hyperparameters, and the mean squared error as a metric.",code/mlflow/mlflow_15.py,Test if the mean squared error metric is logged correctly.,Test if hyperparameters are logged correctly.,Test if the Random Forest regressor is correctly trained.,,,"#!pip install mlflow
import mlflow
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Generate a synthetic dataset
X = np.random.rand(200, 5)
y = np.random.rand(200)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Start an MLflow run
with mlflow.start_run() as run:
    # Train a Random Forest regressor
    rf = RandomForestRegressor(n_estimators=100, random_state=42)
    rf.fit(X_train, y_train)
    
    # Log hyperparameters
    mlflow.log_params(rf.get_params())
    
    # Make predictions on the test set
    y_pred = rf.predict(X_test)
    
    # Calculate and log mean squared error as a metric
    mse = mean_squared_error(y_test, y_pred)
    mlflow.log_metric(""mean_squared_error"", mse)
    
    # Log the trained model
    mlflow.sklearn.log_model(rf, ""random_forest_regressor_model"")
",train
mlflow,20,"Develop a program that uses the MLflow API to train a random forest classifier on a synthetic binary classification dataset. Log the model, its hyperparameters, and accuracy as a metric.",code/mlflow/mlflow_20.py,Test if the accuracy metric is logged correctly.,Test if hyperparameters are logged correctly.,Test if the Random Forest classifier is correctly trained.,,,"#!pip install mlflow
import mlflow
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Generate a synthetic binary classification dataset
X = np.random.rand(100, 5)
y = np.random.randint(2, size=100)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Start an MLflow run
with mlflow.start_run() as run:
    # Train a Random Forest classifier
    clf = RandomForestClassifier(n_estimators=100, random_state=42)
    clf.fit(X_train, y_train)
    
    # Log hyperparameters
    mlflow.log_params(clf.get_params())
    
    # Make predictions on the test set
    y_pred = clf.predict(X_test)
    
    # Calculate and log accuracy as a metric
    accuracy = accuracy_score(y_test, y_pred)
    mlflow.log_metric(""accuracy"", accuracy)
    
    # Log the trained model
    mlflow.sklearn.log_model(clf

, ""random_forest_classifier_model"")
",train
mlflow,7,Write a program that utilizes the MLflow API to train a Naive Bayes classifier on the well-known iris dataset. Log the model and its hyperparameters. Save the model as an artifact.,code/mlflow/mlflow_7.py,Test if the Naive Bayes classifier is correctly trained.,Test if the accuracy metric is logged correctly.,Test if hyperparameters are logged correctly.,,,"#!pip install mlflow
import mlflow
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# Load the Iris dataset
data = load_iris()
X, y = data.data, data.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Start an MLflow run
with mlflow.start_run() as run:
    # Train a Naive Bayes classifier
    nb = GaussianNB()
    nb.fit(X_train, y_train)
    
    # Log hyperparameters
    mlflow.log_params(nb.get_params())
    
    # Make predictions on the test set
    y_pred = nb.predict(X_test)
    
    # Calculate and log accuracy as a metric
    accuracy = accuracy_score(y_test, y_pred)
    mlflow.log_metric(""accuracy"", accuracy)
    
    # Save the model as an artifact
    mlflow.sklearn.log_model(nb, ""naive_bayes_model"")",train
mlflow,16,Develop a program that uses the MLflow API to train a K-means clustering model on a synthetic dataset. Log the model and its hyperparameters.,code/mlflow/mlflow_16.py,Test if the K-means clustering model is correctly trained.,Test if hyperparameters are logged correctly.,,,,"#!pip install mlflow
import mlflow
import numpy as np
from sklearn.cluster import KMeans

# Generate a synthetic dataset
X = np.random.rand(300, 2)

# Start an MLflow run
with mlflow.start_run() as run:
    # Train a K-means clustering model
    kmeans = KMeans(n_clusters=3, random_state=42)
    kmeans.fit(X)
    
    # Log hyperparameters
    mlflow.log_params(kmeans.get_params())
    
    # Log the trained model
    mlflow.sklearn.log_model(kmeans, ""kmeans_model"")
",test
mlflow,10,Develop a program that uses the MLflow API to train a logistic regression classifier on a synthetic binary classification dataset. Log the model and its hyperparameters.,code/mlflow/mlflow_10.py,Test if hyperparameters are logged correctly.,Test if the logistic regression classifier is correctly trained.,,,,"#!pip install mlflow
import mlflow
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

# Generate a synthetic binary classification dataset
X = np.random.rand(100, 10)
y = np.random.randint(2, size=100)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Start an MLflow run
with mlflow.start_run() as run:
    # Train a logistic regression classifier
    logreg = LogisticRegression(random_state=42)
    logreg.fit(X_train, y_train)
    
    # Log hyperparameters
    mlflow.log_params(logreg.get_params())
    
    # Log the trained model
    mlflow.sklearn.log_model(logreg, ""logistic_regression_model"")
",test
mlflow,1,"Create a program that uses the MLflow API to train a simple decision tree classifier on the Iris dataset. Log the model, its hyperparameters, and accuracy as a metric. Save the model as an artifact.",code/mlflow/mlflow_1.py,Test if hyperparameters 'criterion' and 'max_depth' are logged correctly.,Test if the Decision Tree model is correctly trained.,Test if the accuracy metric is logged correctly.,,,"#!pip install mlflow
import mlflow
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Load the Iris dataset
data = load_iris()
X, y = data.data, data.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Start an MLflow run
with mlflow.start_run() as run:
    # Train a Decision Tree classifier
    clf = DecisionTreeClassifier()
    clf.fit(X_train, y_train)
    
    # Log hyperparameters
    mlflow.log_param(""criterion"", clf.criterion)
    mlflow.log_param(""max_depth"", clf.max_depth)
    
    # Make predictions on the test set
    y_pred = clf.predict(X_test)
    
    # Calculate and log accuracy as a metric
    accuracy = accuracy_score(y_test, y_pred)
    mlflow.log_metric(""accuracy"", accuracy)
    
    # Save the model as an artifact
    mlflow.sklearn.log_model(clf, ""decision_tree_model"")",test
more-itertools,1,"Create a Python program using the 'more-itertools' API to demonstrate grouping and transforming pairs of integers. The program should take a list of integers, group them into pairs, and then double each integer in the pair. Finally, calculate the sum of the transformed pairs and print the grouped pairs and the sum square.",code/more-itertools/more-itertools_1.py,Test if the program correctly groups the list of integers into pairs using 'grouper' from 'more-itertools'.,Ensure that the program calculates and prints the sum of the transformed pairs without errors.,Verify that the program doubles each integer in the pairs correctly and stores the transformed pairs.,,,"#!pip install more-itertools
from more_itertools import grouper
from itertools import starmap

integers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
grouped_pairs = grouper(integers, 2)
transformed_pairs = list(starmap(lambda a, b: (a * 2, b * 2), grouped_pairs))
sum_pairs = list(starmap(lambda a, b: a+b, transformed_pairs))

print(""Grouped Pairs:"", list( grouper(integers, 2)))
print(""Sum Square: "")
print(sum_pairs)",train
more-itertools,15,"Create a Python program using the 'more-itertools' API to demonstrate the grouping and filtering of elements in a list based on a condition. The program should take a list of strings, group them into consecutive sublists using the 'consecutive_groups' function from 'more-itertools', and filter out sublists that contain only strings with all lowercase letters. Finally, print the filtered sublists and calculate the length of each sublist and print the lengths.",code/more-itertools/more-itertools_15.py,Ensure that the program prints the filtered sublists and calculates and prints the lengths of each sublist without errors.,Verify that the program filters out sublists that contain only strings with all lowercase letters correctly.,Test if the program correctly partitions the list of strings into consecutive sublists using 'consecutive_groups' from 'more-itertools'.,,,"#!pip install more-itertools
from more_itertools import consecutive_groups

strings = [""apple"", ""banana"", ""cherry"", ""date"", ""elderberry"", ""fig"", ""grape""]
partitioned_sublists = list(consecutive_groups(strings))
filtered_sublists = [sublist for sublist in partitioned_sublists if all(word.islower() for word in sublist)]
lengths = [len(sublist) for sublist in filtered_sublists]

print(""Filtered Sublists:"")
for sublist in filtered_sublists:
    print(list(sublist))

print(""Lengths:"")
print(lengths)",train
more-itertools,9,"Create a Python program using the 'more-itertools' API to demonstrate the grouping and filtering of elements in a list based on a condition. The program should take a list of strings, group them into consecutive sublists using the 'consecutive_groups' function from 'more-itertools', and filter out sublists that have a length greater than 3. Finally, print the filtered sublists and calculate the length of each sublist and print the lengths.",code/more-itertools/more-itertools_9.py,Ensure that the program prints the filtered sublists and calculates and prints the lengths of each sublist without errors.,Verify that the program filters out sublists that have a length greater than 3 correctly.,Test if the program correctly partitions the list of strings into consecutive sublists using 'consecutive_groups' from 'more-itertools'.,,,"#!pip install more-itertools
from more_itertools import consecutive_groups

strings = [""apple"", ""banana"", ""cherry"", ""date"", ""elderberry"", ""fig"", ""grape""]
partitioned_sublists = list(consecutive_groups(strings))
filtered_sublists = [sublist for sublist in partitioned_sublists if len(sublist) > 3]
lengths = [len(sublist) for sublist in filtered_sublists]

print(""Filtered Sublists:"")
for sublist in filtered_sublists:
    print(list(sublist))

print(""Lengths:"")
print(lengths)",train
more-itertools,3,"Create a Python program using the 'more-itertools' API to demonstrate the partitioning of a list into consecutive sublists. The program should take a list of integers, partition it into sublists of consecutive integers using the 'consecutive_groups' function from 'more-itertools', and print the partitioned sublists. Finally, calculate the sum of each sublist and print the sums.",code/more-itertools/more-itertools_3.py,Verify that the program prints the partitioned sublists without errors.,Test if the program correctly partitions the list of integers into consecutive sublists using 'consecutive_groups' from 'more-itertools'.,Ensure that the program calculates and prints the sums of each sublist without errors.,,,"#!pip install more-itertools
from more_itertools import consecutive_groups

integers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
partitioned_sublists = list(consecutive_groups(integers))
sums = [sum(sublist) for sublist in partitioned_sublists]

print(""Partitioned Sublists:"")
for sublist in partitioned_sublists:
    print(list(sublist))

print(""Sums:"")
print(sums)",train
more-itertools,2,"Create a Python program using the 'more-itertools' API to demonstrate the combination of multiple iterables. The program should take two lists of integers, combine them using the 'roundrobin' function from 'more-itertools', and print the combined list. Finally, calculate the sum of the combined list and print the sum.",code/more-itertools/more-itertools_2.py,Ensure that the program calculates and prints the sum of the combined list without errors.,Test if the program correctly combines the two lists using 'roundrobin' from 'more-itertools'.,Verify that the program prints the combined list without errors.,,,"#!pip install more-itertools
from more_itertools import roundrobin

list1 = [1, 2, 3, 4, 5]
list2 = [6, 7, 8, 9, 10]
combined_list = list(roundrobin(list1, list2))
sum_combined = sum(combined_list)

print(""Combined List:"", combined_list)
print(""Sum:"", sum_combined)",train
more-itertools,23,"Create a Python program using the 'more-itertools' API to demonstrate the grouping and filtering of elements in a list based on a condition. The program should take a list of strings, group them into consecutive sublists using the 'consecutive_groups' function from 'more-itertools', and filter out sublists that contain only strings with all alphabetic characters. Finally, print the filtered sublists and calculate the length of each sublist and print the lengths.",code/more-itertools/more-itertools_23.py,Ensure that the program prints the filtered sublists and calculates and prints the lengths of each sublist without errors.,Verify that the program filters out sublists that contain only strings with all alphabetic characters correctly.,Test if the program correctly partitions the list of strings into consecutive sublists using 'consecutive_groups' from 'more-itertools'.,,,"#!pip install more-itertools
from more_itertools import consecutive_groups

strings = [""apple"", ""banana"", ""cherry"", ""date"", ""elderberry"", ""fig"", ""grape""]
partitioned_sublists = list(consecutive_groups(strings))
filtered_sublists = [sublist for sublist in partitioned_sublists if all(word.isalpha() for word in sublist)]
lengths = [len(sublist) for sublist in filtered_sublists]

print(""Filtered Sublists:"")
for sublist in filtered_sublists:
    print(list(sublist))

print(""Lengths:"")
print(lengths)",train
more-itertools,7,"Create a Python program using the 'more-itertools' API to demonstrate the grouping and filtering of elements in a list based on a condition. The program should take a list of integers, group them into consecutive sublists using the 'consecutive_groups' function from 'more-itertools', and filter out sublists that have a sum greater than 10. Finally, print the filtered sublists and calculate the sum of each sublist and print the sums.",code/more-itertools/more-itertools_7.py,Test if the program correctly partitions the list of integers into consecutive sublists using 'consecutive_groups' from 'more-itertools'.,Ensure that the program prints the filtered sublists and calculates and prints the sums of each sublist without errors.,Verify that the program filters out sublists that have a sum greater than 10 correctly.,,,"#!pip install more-itertools
from more_itertools import consecutive_groups

integers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
partitioned_sublists = list(consecutive_groups(integers))
filtered_sublists = [sublist for sublist in partitioned_sublists if sum(sublist) > 10]
sums = [sum(sublist) for sublist in filtered_sublists]

print(""Filtered Sublists:"")
for sublist in filtered_sublists:
    print(list(sublist))

print(""Sums:"")
print(sums)",train
more-itertools,13,"Create a Python program using the 'more-itertools' API to demonstrate the grouping and filtering of elements in a list based on a condition. The program should take a list of strings, group them into consecutive sublists using the 'consecutive_groups' function from 'more-itertools', and filter out sublists that contain only strings with a length greater than 5. Finally, print the filtered sublists and calculate the length of each sublist and print the lengths.",code/more-itertools/more-itertools_13.py,Verify that the program filters out sublists that contain only strings with a length greater than 5 correctly.,Ensure that the program prints the filtered sublists and calculates and prints the lengths of each sublist without errors.,Test if the program correctly partitions the list of strings into consecutive sublists using 'consecutive_groups' from 'more-itertools'.,,,"#!pip install more-itertools
from more_itertools import consecutive_groups

strings = [""apple"", ""banana"", ""cherry"", ""date"", ""elderberry"", ""fig"", ""grape""]
partitioned_sublists = list(consecutive_groups(strings))
filtered_sublists = [sublist for sublist in partitioned_sublists if any(len(word) > 5 for word in sublist)]
lengths = [len(sublist) for sublist in filtered_sublists]

print(""Filtered Sublists:"")
for sublist in filtered_sublists:
    print(list(sublist))

print(""Lengths:"")
print(lengths)",train
more-itertools,19,"Create a Python program using the 'more-itertools' API to demonstrate the grouping and filtering of elements in a list based on a condition. The program should take a list of strings, group them into consecutive sublists using the 'consecutive_groups' function from 'more-itertools', and filter out sublists that contain only strings with a length less than 5. Finally, print the filtered sublists and calculate the length of each sublist and print the lengths.",code/more-itertools/more-itertools_19.py,Ensure that the program prints the filtered sublists and calculates and prints the lengths of each sublist without errors.,Verify that the program filters out sublists that contain only strings with a length less than 5 correctly.,Test if the program correctly partitions the list of strings into consecutive sublists using 'consecutive_groups' from 'more-itertools'.,,,"#!pip install more-itertools
from more_itertools import consecutive_groups

strings = [""apple"", ""banana"", ""cherry"", ""date"", ""elderberry"", ""fig"", ""grape""]
partitioned_sublists = list(consecutive_groups(strings))
filtered_sublists = [sublist for sublist in partitioned_sublists if any(len(word) < 5 for word in sublist)]
lengths = [len(sublist) for sublist in filtered_sublists]

print(""Filtered Sublists:"")
for sublist in filtered_sublists:
    print(list(sublist))

print(""Lengths:"")
print(lengths)",train
more-itertools,5,"Create a Python program using the 'more-itertools' API to demonstrate the grouping and filtering of elements in a list. The program should take a list of integers, group them into consecutive sublists using the 'consecutive_groups' function from 'more-itertools', and filter out sublists that contain only even numbers. Finally, print the filtered sublists and calculate the sum of each sublist and print the sums.",code/more-itertools/more-itertools_5.py,Verify that the program filters out sublists that contain only even numbers correctly.,Test if the program correctly partitions the list of integers into consecutive sublists using 'consecutive_groups' from 'more-itertools'.,Ensure that the program prints the filtered sublists and calculates and prints the sums of each sublist without errors.,,,"#!pip install more-itertools
from more_itertools import consecutive_groups

integers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
partitioned_sublists = list(consecutive_groups(integers))
filtered_sublists = [sublist for sublist in partitioned_sublists if any(num % 2 != 0 for num in sublist)]
sums = [sum(sublist) for sublist in filtered_sublists]

print(""Filtered Sublists:"")
for sublist in filtered_sublists:
    print(list(sublist))

print(""Sums:"")
print(sums)",train
more-itertools,11,"Create a Python program using the 'more-itertools' API to demonstrate the grouping and filtering of elements in a list based on a condition. The program should take a list of strings, group them into consecutive sublists using the 'consecutive_groups' function from 'more-itertools', and filter out sublists that contain only strings starting with a vowel. Finally, print the filtered sublists and calculate the length of each sublist and print the lengths.",code/more-itertools/more-itertools_11.py,Ensure that the program prints the filtered sublists and calculates and prints the lengths of each sublist without errors.,Verify that the program filters out sublists that contain only strings starting with a vowel correctly.,Test if the program correctly partitions the list of strings into consecutive sublists using 'consecutive_groups' from 'more-itertools'.,,,"#!pip install more-itertools
from more_itertools import consecutive_groups

strings = [""apple"", ""banana"", ""cherry"", ""date"", ""elderberry"", ""fig"", ""grape""]
partitioned_sublists = list(consecutive_groups(strings))
filtered_sublists = [sublist for sublist in partitioned_sublists if any(word[0].lower() in ['a', 'e', 'i', 'o', 'u'] for word in sublist)]
lengths = [len(sublist) for sublist in filtered_sublists]

print(""Filtered Sublists:"")
for sublist in filtered_sublists:
    print(list(sublist))

print(""Lengths:"")
print(lengths)",train
more-itertools,21,"Create a Python program using the 'more-itertools' API to demonstrate the grouping and filtering of elements in a list based on a condition. The program should take a list of strings, group them into consecutive sublists using the 'consecutive_groups' function from 'more-itertools', and filter out sublists that contain only strings with all numeric characters. Finally, print the filtered sublists and calculate the length of each sublist and print the lengths.",code/more-itertools/more-itertools_21.py,Verify that the program filters out sublists that contain only strings with all numeric characters correctly.,Ensure that the program prints the filtered sublists and calculates and prints the lengths of each sublist without errors.,Test if the program correctly partitions the list of strings into consecutive sublists using 'consecutive_groups' from 'more-itertools'.,,,"#!pip install more-itertools
from more_itertools import consecutive_groups

strings = [""123"", ""456"", ""789"", ""abc"", ""def"", ""ghi"", ""123456789""]
partitioned_sublists = list(consecutive_groups(strings))
filtered_sublists = [sublist for sublist in partitioned_sublists if all(word.isdigit() for word in sublist)]
lengths = [len(sublist) for sublist in filtered_sublists]

print(""Filtered Sublists:"")
for sublist in filtered_sublists:
    print(list(sublist))

print(""Lengths:"")
print(lengths)",test
more-itertools,17,"Create a Python program using the 'more-itertools' API to demonstrate the grouping and filtering of elements in a list based on a condition. The program should take a list of strings, group them into consecutive sublists using the 'consecutive_groups' function from 'more-itertools', and filter out sublists that contain only strings with all uppercase letters. Finally, print the filtered sublists and calculate the length of each sublist and print the lengths.",code/more-itertools/more-itertools_17.py,Ensure that the program prints the filtered sublists and calculates and prints the lengths of each sublist without errors.,Verify that the program filters out sublists that contain only strings with all uppercase letters correctly.,Test if the program correctly partitions the list of strings into consecutive sublists using 'consecutive_groups' from 'more-itertools'.,,,"#!pip install more-itertools
from more_itertools import consecutive_groups

strings = [""APPLE"", ""BANANA"", ""CHERRY"", ""DATE"", ""ELDERBERRY"", ""FIG"", ""GRAPE""]
partitioned_sublists = list(consecutive_groups(strings))
filtered_sublists = [sublist for sublist in partitioned_sublists if all(word.isupper() for word in sublist)]
lengths = [len(sublist) for sublist in filtered_sublists]

print(""Filtered Sublists:"")
for sublist in filtered_sublists:
    print(list(sublist))

print(""Lengths:"")
print(lengths)",test
numpy,6,"Create a Python program that uses the 'numpy' API to perform matrix operations, including generating a random matrix, computing its eigenvalues and eigenvectors.",code/numpy/numpy_6.py,Verify the correctness of the eigenvalues and eigenvectors by manually calculating them for smaller matrices.,Test the program's performance by generating and processing large matrices.,Test the program with matrices of different sizes and verify that the eigenvalues and eigenvectors are calculated correctly.,,,"#!pip install numpy
import numpy as np

# Generate a random matrix of size (3x3)
original_matrix = np.random.rand(3, 3)
print(""Original Matrix: "", original_matrix)

# Compute the eigenvalues and eigenvectors of the matrix
eigenvalues, eigenvectors = np.linalg.eig(original_matrix)
print(""Eigenvalues: "", eigenvalues)
print(""Eigenvectors: "", eigenvectors)",train
numpy,15,"Create a Python program that uses the 'numpy' API to perform matrix operations, including generating a random matrix, computing its covariance matrix, and calculating the correlation coefficient between two columns of the matrix.",code/numpy/numpy_15.py,Test the program with matrices of different sizes and verify that the covariance matrix and correlation coefficient are calculated correctly.,Test the program's performance by generating and processing large matrices.,Verify the correctness of the covariance matrix and correlation coefficient by comparing them to manual calculations for smaller matrices.,,,"#!pip install numpy
import numpy as np

# Generate a random matrix of size (3x3)
original_matrix = np.random.rand(3, 3)
print(""Original Matrix: "", original_matrix)

# Compute the covariance matrix of the matrix
covariance_matrix = np.cov(original_matrix)
print(""Covariance Matrix: "", covariance_matrix)

# Calculate the correlation coefficient between two columns of the matrix
correlation_coefficient = np.corrcoef(original_matrix[:, 0], original_matrix[:, 1])
print(""Correlation Coefficient: "", correlation_coefficient)",train
numpy,14,"Create a Python program that uses the 'numpy' API to perform matrix operations, including generating a random matrix, computing its mean, and calculating the sum of its elements.",code/numpy/numpy_14.py,Test the program with matrices of different sizes and verify that the mean and sum are calculated correctly.,Test the program's performance by generating and processing large matrices.,Verify the correctness of the mean calculation by manually calculating it for smaller matrices.,,,"#!pip install numpy
import numpy as np

# Generate a random matrix of size (3x3)
original_matrix = np.random.rand(3, 3)
print(""Original Matrix: "", original_matrix)

# Compute the mean of the matrix
mean = np.mean(original_matrix)
print(""Mean: "", mean)

# Calculate the sum of the matrix elements
sum = np.sum(original_matrix)
print(""Sum: "", sum)",train
numpy,12,"Create a Python program that uses the 'numpy' API to perform matrix operations, including generating a random matrix, computing its trace, and calculating the sum of its diagonal elements.",code/numpy/numpy_12.py,Test the program's performance by generating and processing large matrices.,Verify the correctness of the trace calculation by manually calculating it for smaller matrices.,Test the program with matrices of different sizes and verify that the trace and diagonal sum are calculated correctly.,,,"#!pip install numpy
import numpy as np

# Generate a random matrix of size (3x3)
original_matrix = np.random.rand(3, 3)
print(""Original Matrix: "", original_matrix)

# Compute the trace of the matrix
trace = np.trace(original_matrix)
print(""Trace: "", trace)

# Calculate the sum of the diagonal elements
diagonal_sum = np.sum(np.diagonal(original_matrix))
print(""Diagonal Sum: "", diagonal_sum)",train
numpy,9,"Create a Python program that uses the 'numpy' API to perform matrix operations, including generating a random matrix, computing its LU decomposition, and reconstructing the original matrix using the LU decomposition components.",code/numpy/numpy_9.py,Verify the correctness of the reconstruction by comparing the reconstructed matrix to the original matrix for smaller matrices.,Test the program with matrices of different sizes and verify that the LU decomposition and reconstruction are performed correctly.,Test the program's performance by generating and processing large matrices.,,,"#!pip install numpy
import numpy as np

# Generate a random matrix of size (3x3)
original_matrix = np.random.rand(3, 3)
print(""Original Matrix: "", original_matrix)

# Compute the LU decomposition of the matrix
P, L, U = np.linalg.lu(original_matrix)
print(""P: "", P)
print(""L: "", L)
print(""U: "", U)

# Reconstruct the original matrix using the LU decomposition components
reconstructed_matrix = np.dot(P, np.dot(L, U))
print(""Reconstructed Matrix: "", reconstructed_matrix)",train
numpy,10,"Create a Python program that uses the 'numpy' API to perform matrix operations, including generating a random matrix, computing its Cholesky decomposition, and reconstructing the original matrix using the Cholesky decomposition components.",code/numpy/numpy_10.py,Verify the correctness of the reconstruction by comparing the reconstructed matrix to the original matrix for smaller matrices.,Test the program's performance by generating and processing large positive definite matrices.,Test the program with positive definite matrices of different sizes and verify that the Cholesky decomposition and reconstruction are performed correctly.,,,"#!pip install numpy
import numpy as np

# Generate a random positive definite matrix of size (3x3)
original_matrix = np.random.rand(3, 3)
positive_definite_matrix = np.dot(original_matrix, original_matrix.T)
print(""Original Matrix: "", positive_definite_matrix)

# Compute the Cholesky decomposition of the matrix
L = np.linalg.cholesky(positive_definite_matrix)
print(""L: "", L)

# Reconstruct the original matrix using the Cholesky decomposition components
reconstructed_matrix = np.dot(L, L.T)
print(""Reconstructed Matrix: "", reconstructed_matrix)",train
numpy,3,"Create a Python program that uses the 'numpy' API to perform element-wise mathematical operations on a given array. The program should read an array from a text file, calculate the square root, exponential, and logarithm of each element, and display the results.",code/numpy/numpy_3.py,Test the program's performance by processing large text files with a large number of elements.,"Test the program with different text files containing arrays of different sizes and verify that the square root, exponential, and logarithm are calculated correctly for each element.","Verify the correctness of the calculations by manually calculating the square root, exponential, and logarithm for smaller arrays.",,,"#!pip install numpy
import numpy as np

# Read the array from a text file
array = np.loadtxt('array.txt')

# Calculate the square root of each element
sqrt_array = np.sqrt(array)
print(""Square Root: "", sqrt_array)

# Calculate the exponential of each element
exp_array = np.exp(array)
print(""Exponential: "", exp_array)

# Calculate the logarithm of each element
log_array = np.log(array)
print(""Logarithm: "", log_array)",train
numpy,16,"Create a Python program that uses the 'numpy' API to perform matrix operations, including generating a random matrix, computing its mean along a specified axis, and calculating the sum of its elements along a specified axis.",code/numpy/numpy_16.py,Verify the correctness of the mean and sum calculations by manually calculating them for smaller matrices.,Test the program with matrices of different sizes and verify that the mean and sum along the specified axis are calculated correctly.,Test the program's performance by generating and processing large matrices.,,,"#!pip install numpy
import numpy as np

# Generate a random matrix of size (3x3)
original_matrix = np.random.rand(3, 3)
print(""Original Matrix: "", original_matrix)

# Compute the mean along the rows (axis=0) of the matrix
mean_rows = np.mean(original_matrix, axis=0)
print(""Mean along Rows: "", mean_rows)

# Compute the mean along the columns (axis=1) of the matrix
mean_columns = np.mean(original_matrix, axis=1)
print(""Mean along Columns: "", mean_columns)

# Calculate the sum of the matrix elements along the rows (axis=0)
sum_rows = np.sum(original_matrix, axis=0)
print(""Sum along Rows: "", sum_rows)

# Calculate the sum of the matrix elements along the columns (axis=1)
sum_columns = np.sum(original_matrix, axis=1)
print(""Sum along Columns: "", sum_columns)",train
numpy,5,"Create a Python program that uses the 'numpy' API to perform matrix operations, including generating a random matrix, computing its inverse, and calculating the determinant.",code/numpy/numpy_5.py,Verify the correctness of the determinant calculation by comparing it to a manual calculation for smaller matrices.,Test the program's performance by generating and processing large matrices.,Test the program with matrices of different sizes and verify that the inverse operation correctly calculates the inverse matrix.,,,"#!pip install numpy
import numpy as np

# Generate a random matrix of size (3x3)
original_matrix = np.random.rand(3, 3)
print(""Original Matrix: "", original_matrix)

# Compute the inverse of the matrix
inverse_matrix = np.linalg.inv(original_matrix)
print(""Inverse Matrix: "", inverse_matrix)

# Calculate the determinant of the matrix
determinant = np.linalg.det(original_matrix)
print(""Determinant: "", determinant)",train
numpy,8,"Create a Python program that uses the 'numpy' API to perform matrix operations, including generating a random matrix, computing its QR decomposition, and reconstructing the original matrix using the QR decomposition components.",code/numpy/numpy_8.py,Verify the correctness of the reconstruction by comparing the reconstructed matrix to the original matrix for smaller matrices.,Test the program with matrices of different sizes and verify that the QR decomposition and reconstruction are performed correctly.,Test the program's performance by generating and processing large matrices.,,,"#!pip install numpy
import numpy as np

# Generate a random matrix of size (3x3)
original_matrix = np.random.rand(3, 3)
print(""Original Matrix: "", original_matrix)

# Compute the QR decomposition of the matrix
Q, R = np.linalg.qr(original_matrix)
print(""Q: "", Q)
print(""R: "", R)

# Reconstruct the original matrix using the QR decomposition components
reconstructed_matrix = np.dot(Q, R)
print(""Reconstructed Matrix: "", reconstructed_matrix)",train
numpy,11,"Create a Python program that uses the 'numpy' API to perform matrix operations, including generating a random matrix, computing its singular value decomposition (SVD), and calculating the rank of the matrix.",code/numpy/numpy_11.py,Test the program with matrices of different sizes and verify that the singular value decomposition and rank calculation are performed correctly.,Test the program's performance by generating and processing large matrices.,Verify the correctness of the rank calculation by comparing it to a manual calculation for smaller matrices.,,,"#!pip install numpy
import numpy as np

# Generate a random matrix of size (3x3)
original_matrix = np.random.rand(3, 3)
print(""Original Matrix: "", original_matrix)

# Compute the singular value decomposition (SVD) of the matrix
U, S, V = np.linalg.svd(original_matrix)
print(""U: "", U)
print(""S: "", S)
print(""V: "", V)

# Calculate the rank of the matrix
rank = np.linalg.matrix_rank(original_matrix)
print(""Rank: "", rank)",train
numpy,13,"Create a Python program that uses the 'numpy' API to perform matrix operations, including generating a random matrix, computing its norm, and calculating the maximum and minimum values of its elements.",code/numpy/numpy_13.py,Verify the correctness of the norm calculation by manually calculating it for smaller matrices.,"Test the program with matrices of different sizes and verify that the norm, maximum value, and minimum value are calculated correctly.",Test the program's performance by generating and processing large matrices.,,,"#!pip install numpy
import numpy as np

# Generate a random matrix of size (3x3)
original_matrix = np.random.rand(3, 3)
print(""Original Matrix: "", original_matrix)

# Compute the norm of the matrix
norm = np.linalg.norm(original_matrix)
print(""Norm: "", norm)

# Calculate the maximum value of the matrix elements
max_value = np.max(original_matrix)
print(""Maximum Value: "", max_value)

# Calculate the minimum value of the matrix elements
min_value = np.min(original_matrix)
print(""Minimum Value: "", min_value)",train
numpy,4,"Create a Python program that uses the 'numpy' API to perform element-wise mathematical operations on a given array. The program should read an array from a text file, calculate the sine, cosine, and tangent of each element, and display the results.",code/numpy/numpy_4.py,Test the program's performance by processing large text files with a large number of elements.,"Verify the correctness of the calculations by manually calculating the sine, cosine, and tangent for smaller arrays.","Test the program with different text files containing arrays of different sizes and verify that the sine, cosine, and tangent are calculated correctly for each element.",,,"#!pip install numpy
import numpy as np

# Read the array from a text file
array = np.loadtxt('array.txt')

# Calculate the sine of each element
sin_array = np.sin(array)
print(""Sine: "", sin_array)

# Calculate the cosine of each element
cos_array = np.cos(array)
print(""Cosine: "", cos_array)

# Calculate the tangent of each element
tan_array = np.tan(array)
print(""Tangent: "", tan_array)",train
numpy,7,"Create a Python program that uses the 'numpy' API to perform matrix operations, including generating a random matrix, computing its singular value decomposition (SVD), and reconstructing the original matrix using the SVD components.",code/numpy/numpy_7.py,Test the program with matrices of different sizes and verify that the singular value decomposition and reconstruction are performed correctly.,Verify the correctness of the reconstruction by comparing the reconstructed matrix to the original matrix for smaller matrices.,Test the program's performance by generating and processing large matrices.,,,"#!pip install numpy
import numpy as np

# Generate a random matrix of size (3x3)
original_matrix = np.random.rand(3, 3)
print(""Original Matrix: "", original_matrix)

# Compute the singular value decomposition (SVD) of the matrix
U, S, V = np.linalg.svd(original_matrix)
print(""U: "", U)
print(""S: "", S)
print(""V: "", V)

# Reconstruct the original matrix using the SVD components
reconstructed_matrix = np.dot(U, np.dot(np.diag(S), V))
print(""Reconstructed Matrix: "", reconstructed_matrix)",train
numpy,1,"Create a Python program that uses the 'numpy' API to perform matrix operations, including generating a random matrix, computing its transpose, and calculating the matrix product.",code/numpy/numpy_1.py,Test the program with matrices of different sizes and verify that the transpose operation correctly swaps rows and columns.,Verify the correctness of the matrix product by comparing it to a manual calculation for smaller matrices.,Test the program's performance by generating and processing large matrices.,,,"#!pip install numpy
import numpy as np

# Generate a random matrix of size (3x3)
original_matrix = np.random.rand(3, 3)
print(""Original Matrix: "", original_matrix)

# Compute the matrix's transpose
transposed_matrix = np.transpose(original_matrix)
print(""Transposed Matrix: "", transposed_matrix)

# Calculate the matrix product
matrix_product = np.dot(original_matrix, transposed_matrix)
print(""Matrix Product: "", matrix_product)",test
numpy,2,"Create a Python program that uses the 'numpy' API to perform statistical operations on a given dataset. The program should read a CSV file containing numerical data, calculate the mean, median, and standard deviation of each column, and display the results.",code/numpy/numpy_2.py,"Test the program with different CSV files containing numerical data and verify that the mean, median, and standard deviation are calculated correctly for each column.",Test the program's performance by processing large CSV files with a large number of columns.,"Verify the correctness of the calculations by manually calculating the mean, median, and standard deviation for smaller datasets.",,,"#!pip install numpy
import numpy as np
import csv

# Read the CSV file
data = []
with open('data.csv', 'r') as file:
    reader = csv.reader(file)
    for row in reader:
        data.append(row)

# Convert the data to a numpy array
data_array = np.array(data, dtype=float)

# Calculate the mean of each column
mean = np.mean(data_array, axis=0)
print(""Mean: "", mean)

# Calculate the median of each column
median = np.median(data_array, axis=0)
print(""Median: "", median)

# Calculate the standard deviation of each column
std_dev = np.std(data_array, axis=0)
print(""Standard Deviation: "", std_dev)",test
optuna,3,Create a Python program using the 'optuna' API to optimize the hyperparameters of a gradient boosting classifier. The program should search for the best combination of 'n_estimators' and 'max_depth' hyperparameters for a gradient boosting classifier on a custom dataset and print the cross-validated F1 score.,code/optuna/optuna_3.py,"Verify that the program performs hyperparameter optimization using ""optuna"" and returns the best hyperparameters.",Test if the program correctly generates a custom dataset for the machine learning task.,"Ensure that the program creates and evaluates the gradient boosting classifier with the best hyperparameters, printing the cross-validated F1 score without errors.",,,"#!pip install optuna scikit-learn
import optuna
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.datasets import make_classification
from sklearn.metrics import f1_score

# Generate a custom dataset
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Define the machine learning task: Gradient Boosting Classifier
def objective(trial):
    # Define hyperparameters to search
    n_estimators = trial.suggest_int(""n_estimators"", 50, 200)
    max_depth = trial.suggest_int(""max_depth"", 3, 10)

    # Create and evaluate the gradient boosting classifier with the chosen hyperparameters
    clf = GradientBoostingClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)
    f1_scores = cross_val_score(clf, X, y, cv=5, scoring='f1')

    return np.mean(f1_scores)

# Create a study and optimize hyperparameters
if __name__ == ""__main__"":
    study = optuna.create_study(direction=""maximize"")
    study.optimize(objective, n_trials=50)

    # Get the best hyperparameters
    best_params = study.best_params
    n_estimators = best_params[""n_estimators""]
    max_depth = best_params[""max_depth""]

    print(""Best Hyperparameters:"", best_params)

    # Create and evaluate the gradient boosting classifier with the best hyperparameters
    clf = GradientBoostingClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)
    f1_scores = cross_val_score(clf, X, y, cv=5, scoring='f1')
    mean_f1_score = np.mean(f1_scores)

    print(""Cross-Validated F1 Score with Best Hyperparameters:"",

 mean_f1_score)
",train
optuna,11,Create a Python program using the 'optuna' API to optimize the hyperparameters of a principal component analysis (PCA) model. The program should search for the best combination of 'n_components' and 'svd_solver' hyperparameters for a PCA model on a custom dataset and print the explained variance ratio.,code/optuna/optuna_11.py,"Verify that the program performs hyperparameter optimization using ""optuna"" and returns the best hyperparameters.",Test if the program correctly generates a custom dataset for the machine learning task.,"Ensure that the program creates and evaluates the PCA model with the best hyperparameters, printing the explained variance ratio without errors.",,,"#!pip install optuna scikit-learn
import optuna
import numpy as np
from sklearn.decomposition import PCA
from sklearn.datasets import make_classification

# Generate a custom dataset
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Define the machine learning task: Principal Component Analysis (PCA)
def objective(trial):
    # Define hyperparameters to search
    n_components = trial.suggest_int(""n_components"", 1, 20)
    svd_solver = trial.suggest_categorical(""svd_solver"", [""auto"", ""full"", ""arpack"", ""randomized""])

    # Create and evaluate the PCA model with the chosen hyperparameters
    pca = PCA(n_components=n_components, svd_solver=svd_solver)
    pca.fit(X)
    explained_variance = np.sum(pca.explained_variance_ratio_)

    return explained_variance

# Create a study and optimize hyperparameters
if __name__ == ""__main__"":
    study = optuna.create_study(direction=""maximize"")
    study.optimize(objective, n_trials=50)

    # Get the best hyperparameters
    best_params = study.best_params
    n_components = best_params[""n_components""]
    svd_solver = best_params[""svd_solver""]

    print(""Best Hyperparameters:"", best_params)

    # Create and evaluate the PCA model with the best hyperparameters
    pca = PCA(n_components=n_components, svd_solver=svd_solver)
    pca.fit(X)
    explained_variance = np.sum(pca.explained_variance_ratio_)

    print(""Explained Variance Ratio with Best Hyperparameters:"", explained_variance)
",train
optuna,8,Create a Python program using the 'optuna' API to optimize the hyperparameters of a random forest regressor. The program should search for the best combination of 'n_estimators' and 'max_depth' hyperparameters for a random forest regressor on a custom dataset and print the cross-validated mean squared error.,code/optuna/optuna_8.py,"Ensure that the program creates and evaluates the random forest regressor with the best hyperparameters, printing the cross-validated mean squared error without errors.","Verify that the program performs hyperparameter optimization using ""optuna"" and returns the best hyperparameters.",Test if the program correctly generates a custom dataset for the machine learning task.,,,"#!pip install optuna scikit-learn
import optuna
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression

# Generate a custom dataset
X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)

# Define the machine learning task: Random Forest Regressor
def objective(trial):
    # Define hyperparameters to search
    n_estimators = trial.suggest_int(""n_estimators"", 50, 200)
    max_depth = trial.suggest_int(""max_depth"", 3, 20)

    # Create and evaluate the random forest regressor with the chosen hyperparameters
    model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=42)
    mse_scores = -cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')

    return np.mean(mse_scores)

# Create a study and optimize hyperparameters
if __name__ == ""__main__"":
    study = optuna.create_study(direction=""minimize"")
    study.optimize(objective, n_trials=50)

    # Get the best hyperparameters
    best_params = study.best_params
    n_estimators = best_params[""n_estimators""]
    max_depth = best_params[""max_depth""]

    print(""Best Hyperparameters:"", best_params)

    # Create and evaluate the random forest regressor with the best hyperparameters
    model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=42)
    mse_scores = -cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')
    mean_mse = np.mean(mse_scores)

    print(""Cross-Validated Mean Squared Error with Best Hyperparameters:"", mean_mse)
",train
optuna,5,Create a Python program using the 'optuna' API to optimize the hyperparameters of a K-nearest neighbors (K-NN) classifier. The program should search for the best combination of 'n_neighbors' and 'p' hyperparameters for a K-NN classifier on a custom dataset and print the cross-validated accuracy.,code/optuna/optuna_5.py,"Ensure that the program creates and evaluates the K-NN classifier with the best hyperparameters, printing the cross-validated accuracy without errors.","Verify that the program performs hyperparameter optimization using ""optuna"" and returns the best hyperparameters.",Test if the program correctly generates a custom dataset for the machine learning task.,,,"#!pip install optuna scikit-learn
import optuna
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import make_classification

# Generate a custom dataset
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Define the machine learning task: K-Nearest Neighbors (K-NN) Classifier
def objective(trial):
    # Define hyperparameters to search
    n_neighbors = trial.suggest_int(""n_neighbors"", 3, 15)
    p = trial.suggest_categorical(""p"", [1, 2])

    # Create and evaluate the K-NN classifier with the chosen hyperparameters
    clf = KNeighborsClassifier(n_neighbors=n_neighbors, p=p)
    accuracy_scores = cross_val_score(clf, X, y, cv=5)

    return np.mean(accuracy_scores)

# Create a study and optimize hyperparameters
if __name__ == ""__main__"":
    study = optuna.create_study(direction=""maximize"")
    study.optimize(objective, n_trials=50)

    # Get the best hyperparameters
    best_params = study.best_params
    n_neighbors = best_params[""n_neighbors""]
    p = best_params[""p""]

    print(""Best Hyperparameters:"", best_params)

    # Create and evaluate the K-NN classifier with the best hyperparameters
    clf = KNeighborsClassifier(n_neighbors=n_neighbors, p=p)
    accuracy_scores = cross_val_score(clf, X, y, cv=5)
    mean_accuracy = np.mean(accuracy_scores)

    print(""Cross-Validated Accuracy with Best Hyperparameters:"", mean_accuracy)
",train
optuna,4,Create a Python program using the 'optuna' API to optimize the hyperparameters of a decision tree classifier. The program should search for the best combination of 'criterion' and 'max_depth' hyperparameters for a decision tree classifier on a custom dataset and print the cross-validated accuracy.,code/optuna/optuna_4.py,"Ensure that the program creates and evaluates the decision tree classifier with the best hyperparameters, printing the cross-validated accuracy without errors.","Verify that the program performs hyperparameter optimization using ""optuna"" and returns the best hyperparameters.",Test if the program correctly generates a custom dataset for the machine learning task.,,,"#!pip install optuna scikit-learn
import optuna
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_classification

# Generate a custom dataset
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Define the machine learning task: Decision Tree Classifier
def objective(trial):
    # Define hyperparameters to search
    criterion = trial.suggest_categorical(""criterion"", [""gini"", ""entropy""])
    max_depth = trial.suggest_int(""max_depth"", 3, 20)

    # Create and evaluate the decision tree classifier with the chosen hyperparameters
    clf = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth, random_state=42)
    accuracy_scores = cross_val_score(clf, X, y, cv=5)

    return np.mean(accuracy_scores)

# Create a study and optimize hyperparameters
if __name__ == ""__main__"":
    study = optuna.create_study(direction=""maximize"")
    study.optimize(objective, n_trials=50)

    # Get the best hyperparameters
    best_params = study.best_params
    criterion = best_params[""criterion""]
    max_depth = best_params[""max_depth""]

    print(""Best Hyperparameters:"", best_params)

    # Create and evaluate the decision tree classifier with the best hyperparameters
    clf = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth, random_state=42)
    accuracy_scores = cross_val_score(clf, X, y, cv=5)
    mean_accuracy = np.mean(accuracy_scores)

    print(""Cross-Validated Accuracy with Best Hyperparameters:"", mean_accuracy)
",train
optuna,17,Create a Python program using the 'optuna' API to optimize the hyperparameters of a logistic regression model. The program should search for the best combination of 'C' and 'penalty' hyperparameters for a logistic regression model on a custom dataset and print the cross-validated accuracy.,code/optuna/optuna_17.py,"Ensure that the program creates and evaluates the logistic regression model with the best hyperparameters, printing the cross-validated accuracy without errors.","Verify that the program performs hyperparameter optimization using ""optuna"" and returns the best hyperparameters.",Test if the program correctly generates a custom dataset for the machine learning task.,,,"#!pip install optuna scikit-learn
import optuna
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.datasets import make_classification

# Generate a custom dataset
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Define the machine learning task: Logistic Regression
def objective(trial):
    # Define hyperparameters to search
    C = trial.suggest_float(""C"", 1e-5, 1e5, log=True)
    penalty = trial.suggest_categorical(""penalty"", [""l1"", ""l2""])

    # Create and evaluate the logistic regression model with the chosen hyperparameters
    model = LogisticRegression(C=C, penalty=penalty, random_state=42, solver='liblinear')
    accuracy_scores = cross_val_score(model, X, y, cv=5)

    return np.mean(accuracy_scores)

# Create a study and optimize hyperparameters
if __name__ == ""__main__"":
    study = optuna.create_study(direction=""maximize"")
    study.optimize(objective, n_trials=50)

    # Get the best hyperparameters
    best_params = study.best_params
    C = best_params[""C""]
    penalty = best_params[""penalty""]

    print(""Best Hyperparameters:"", best_params)

    # Create and evaluate the logistic regression model with the best hyperparameters
    model = LogisticRegression(C=C, penalty=penalty, random_state=42, solver='liblinear')
    accuracy_scores = cross_val_score(model, X, y, cv=5)
    mean_accuracy = np.mean(accuracy_scores)

    print(""Cross-Validated Accuracy with Best Hyperparameters:"", mean_accuracy)
",train
optuna,7,Create a Python program using the 'optuna' API to optimize the hyperparameters of a linear regression model. The program should search for the best combination of 'alpha' and 'fit_intercept' hyperparameters for a Lasso regression model on a custom dataset and print the cross-validated R-squared score.,code/optuna/optuna_7.py,"Ensure that the program creates and evaluates the Lasso regression model with the best hyperparameters, printing the cross-validated R-squared score without errors.","Verify that the program performs hyperparameter optimization using ""optuna"" and returns the best hyperparameters.",Test if the program correctly generates a custom dataset for the machine learning task.,,,"#!pip install optuna scikit-learn
import optuna
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import Lasso
from sklearn.datasets import make_regression

# Generate a custom dataset
X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)

# Define the machine learning task: Lasso Regression
def objective(trial):
    # Define hyperparameters to search
    alpha = trial.suggest_float(""alpha"", 0.01, 1.0, log=True)
    fit_intercept = trial.suggest_categorical(""fit_intercept"", [True, False])

    # Create and evaluate the Lasso regression model with the chosen hyperparameters
    model = Lasso(alpha=alpha, fit_intercept=fit_intercept)
    r2_scores = cross_val_score(model, X, y, cv=5, scoring='r2')

    return np.mean(r2_scores)

# Create a study and optimize hyperparameters
if __name__ == ""__main__"":
    study = optuna.create_study(direction=""maximize"")
    study.optimize(objective, n_trials=50)

    # Get the best hyperparameters
    best_params = study.best_params
    alpha = best_params[""alpha""]
    fit_intercept = best_params[""fit_intercept""]

    print(""Best Hyperparameters:"", best_params)

    # Create and evaluate the Lasso regression model with the best hyperparameters
    model = Lasso(alpha=alpha, fit_intercept=fit_intercept)
    r2_scores = cross_val_score(model, X, y, cv=5, scoring='r2')
    mean_r2_score = np.mean(r2_scores)

    print(""Cross-Validated R-squared Score with Best Hyperparameters:"", mean_r2_score)
",train
optuna,10,Create a Python program using the 'optuna' API to optimize the hyperparameters of a K-means clustering model. The program should search for the best combination of 'n_clusters' and 'max_iter' hyperparameters for a K-means clustering model on a custom dataset and print the silhouette score.,code/optuna/optuna_10.py,"Ensure that the program creates and evaluates the K-means clustering model with the best hyperparameters, printing the silhouette score without errors.","Verify that the program performs hyperparameter optimization using ""optuna"" and returns the best hyperparameters.",Test if the program correctly generates a custom dataset for the machine learning task.,,,"#!pip install optuna scikit-learn
import optuna
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.datasets import make_blobs

# Generate a custom dataset
X, _ = make_blobs(n_samples=1000, n_features=2, centers=4, cluster_std=1.0, random_state=42)

# Define the machine learning task: K-means Clustering
def objective(trial):
    # Define hyperparameters to search
    n_clusters = trial.suggest_int(""n_clusters"", 2, 10)
    max_iter = trial.suggest_int(""max_iter"", 100, 1000)

    # Create and evaluate the K-means clustering model with the chosen hyperparameters
    model = KMeans(n_clusters=n_clusters, max_iter=max_iter, random_state=42)
    labels = model.fit_predict(X)
    silhouette = silhouette_score(X, labels)

    return silhouette

# Create a study and optimize hyperparameters
if __name__ == ""__main__"":
    study = optuna.create_study(direction=""maximize"")
    study.optimize(objective, n_trials=50)

    # Get the best hyperparameters
    best_params = study.best_params
    n_clusters = best_params[""n_clusters""]
    max_iter = best_params[""max_iter""]

    print(""Best Hyperparameters:"", best_params)

    # Create and evaluate the K-means clustering model with the best hyperparameters
    model = KMeans(n_clusters=n_clusters, max_iter=max_iter, random_state=42)
    labels = model.fit_predict(X)
    silhouette = silhouette_score(X, labels)

    print(""Silhouette Score with Best Hyperparameters:"", silhouette)
",train
optuna,13,Create a Python program using the 'optuna' API to optimize the hyperparameters of a support vector machine (SVM) classifier. The program should search for the best combination of 'C' and 'kernel' hyperparameters for an SVM classifier on a custom dataset and print the cross-validated accuracy.,code/optuna/optuna_13.py,"Ensure that the program creates and evaluates the SVM classifier with the best hyperparameters, printing the cross-validated accuracy without errors.","Verify that the program performs hyperparameter optimization using ""optuna"" and returns the best hyperparameters.",Test if the program correctly generates a custom dataset for the machine learning task.,,,"#!pip install optuna scikit-learn
import optuna
import numpy as np
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score
from sklearn.datasets import make_classification

# Generate a custom dataset
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Define the machine learning task: Support Vector Machine (SVM) Classifier
def objective(trial):
    # Define hyperparameters to search
    C = trial.suggest_float(""C"", 1e-5, 1e5, log=True)
    kernel = trial.suggest_categorical(""kernel"", [""linear"", ""poly"", ""rbf"", ""sigmoid""])

    # Create and evaluate the SVM classifier with the chosen hyperparameters
    clf = SVC(C=C, kernel=kernel)
    accuracy_scores = cross_val_score(clf, X, y, cv=5)

    return np.mean(accuracy_scores)

# Create a study and optimize hyperparameters
if __name__ == ""__main__"":
    study = optuna.create_study(direction=""maximize"")
    study.optimize(objective, n_trials=50)

    # Get the best hyperparameters
    best_params = study.best_params
    C = best_params[""C""]
    kernel = best_params[""kernel""]

    print(""Best Hyperparameters:"", best_params)

    # Create and evaluate the SVM classifier with the best hyperparameters
    clf = SVC(C=C, kernel=kernel)
    accuracy_scores = cross_val_score(clf, X, y, cv=5)
    mean_accuracy = np.mean(accuracy_scores)

    print(""Cross-Validated Accuracy with Best Hyperparameters:"", mean_accuracy)
",train
optuna,6,Create a Python program using the 'optuna' API to optimize the hyperparameters of a random forest classifier. The program should search for the best combination of 'n_estimators' and 'max_depth' hyperparameters for a random forest classifier on a custom dataset and print the cross-validated accuracy.,code/optuna/optuna_6.py,"Verify that the program performs hyperparameter optimization using ""optuna"" and returns the best hyperparameters.",Test if the program correctly generates a custom dataset for the machine learning task.,"Ensure that the program creates and evaluates the random forest classifier with the best hyperparameters, printing the cross-validated accuracy without errors.",,,"#!pip install optuna scikit-learn
import optuna
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

# Generate a custom dataset
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Define the machine learning task: Random Forest Classifier
def objective(trial):
    # Define hyperparameters to search
    n_estimators = trial.suggest_int(""n_estimators"", 50, 200)
    max_depth = trial.suggest_int(""max_depth"", 3, 20)

    # Create and evaluate the random forest classifier with the chosen hyperparameters
    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)
    accuracy_scores = cross_val_score(clf, X, y, cv=5)

    return np.mean(accuracy_scores)

# Create a study and optimize hyperparameters
if __name__ == ""__main__"":
    study = optuna.create_study(direction=""maximize"")
    study.optimize(objective, n_trials=50)

    # Get the best hyperparameters
    best_params = study.best_params
    n_estimators = best_params[""n_estimators""]
    max_depth = best_params[""max_depth""]

    print(""Best Hyperparameters:"", best_params)

    # Create and evaluate the random forest classifier with the best hyperparameters
    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)
    accuracy_scores = cross_val_score(clf, X, y, cv=5)
    mean_accuracy = np.mean(accuracy_scores)

    print(""Cross-Validated Accuracy with Best Hyperparameters:"", mean_accuracy)
",train
optuna,9,Create a Python program using the 'optuna' API to optimize the hyperparameters of a gradient boosting regressor. The program should search for the best combination of 'n_estimators' and 'max_depth' hyperparameters for a gradient boosting regressor on a custom dataset and print the cross-validated mean absolute error.,code/optuna/optuna_9.py,"Ensure that the program creates and evaluates the gradient boosting regressor with the best hyperparameters, printing the cross-validated mean absolute error without errors.","Verify that the program performs hyperparameter optimization using ""optuna"" and returns the best hyperparameters.",Test if the program correctly generates a custom dataset for the machine learning task.,,,"#!pip install optuna scikit-learn
import optuna
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.datasets import make_regression

# Generate a custom dataset
X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)

# Define the machine learning task: Gradient Boosting Regressor
def objective(trial):
    # Define hyperparameters to search
    n_estimators = trial.suggest_int(""n_estimators"", 50, 200)
    max_depth = trial.suggest_int(""max_depth"", 3, 20)

    # Create and evaluate the gradient boosting regressor with the chosen hyperparameters
    model = GradientBoostingRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=42)
    mae_scores = -cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error')

    return np.mean(mae_scores)

# Create a study and optimize hyperparameters
if __name__ == ""__main__"":
    study = optuna.create_study(direction=""minimize"")
    study.optimize(objective, n_trials=50)

    # Get the best hyperparameters
    best_params = study.best_params
    n_estimators = best_params[""n_estimators""]
    max_depth = best_params[""max_depth""]

    print(""Best Hyperparameters:"", best_params)

    # Create and evaluate the gradient boosting regressor with the best hyperparameters
    model = GradientBoostingRegressor(n_estimators=n_estimators, max_depth=max_depth,

 random_state=42)
    mae_scores = -cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error')
    mean_mae = np.mean(mae_scores)

    print(""Cross-Validated Mean Absolute Error with Best Hyperparameters:"", mean_mae)
",train
optuna,14,Create a Python program using the 'optuna' API to optimize the hyperparameters of a random forest classifier. The program should search for the best combination of 'n_estimators' and 'max_depth' hyperparameters for a random forest classifier on a custom dataset and print the cross-validated F1 score.,code/optuna/optuna_14.py,"Ensure that the program creates and evaluates the random forest classifier with the best hyperparameters, printing the cross-validated F1 score without errors.","Verify that the program performs hyperparameter optimization using ""optuna"" and returns the best hyperparameters.",Test if the program correctly generates a custom dataset for the machine learning task.,,,"#!pip install optuna scikit-learn
import optuna
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.metrics import f1_score

# Generate a custom dataset
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Define the machine learning task: Random Forest Classifier
def objective(trial):
    # Define hyperparameters to search
    n_estimators = trial.suggest_int(""n_estimators"", 50, 200)
    max_depth = trial.suggest_int(""max_depth"", 3, 20)

    # Create and evaluate the random forest classifier with the chosen hyperparameters
    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)
    f1_scores = cross_val_score(clf, X, y, cv=5, scoring='f1')

    return np.mean(f1_scores)

# Create a study and optimize hyperparameters
if __name__ == ""__main__"":
    study = optuna.create_study(direction=""maximize"")
    study.optimize(objective, n_trials=50)

    # Get the best hyperparameters
    best_params = study.best_params
    n_estimators = best_params[""n_estimators""]
    max_depth = best_params[""max_depth""]

    print(""Best Hyperparameters:"", best_params)

    # Create and evaluate the random forest classifier with the best hyperparameters
    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)
    f1_scores = cross_val_score(clf, X, y, cv=5, scoring='f1')
    mean_f1_score = np.mean(f1_scores)

    print(""Cross-Validated F1 Score with Best Hyperparameters:"", mean_f1_score)
",test
optuna,12,Create a Python program using the 'optuna' API to optimize the hyperparameters of a linear discriminant analysis (LDA) model. The program should search for the best combination of 'n_components' and 'solver' hyperparameters for an LDA model on a custom dataset and print the explained variance ratio.,code/optuna/optuna_12.py,"Verify that the program performs hyperparameter optimization using ""optuna"" and returns the best hyperparameters.",Test if the program correctly generates a custom dataset for the machine learning task.,"Ensure that the program creates and evaluates the LDA model with the best hyperparameters, printing the explained variance ratio without errors.",,,"#!pip install optuna scikit-learn
import optuna
import numpy as np
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.datasets import make_classification

# Generate a custom dataset
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Define the machine learning task: Linear Discriminant Analysis (LDA)
def objective(trial):
    # Define hyperparameters to search
    n_components = trial.suggest_int(""n_components"", 1, 20)
    solver = trial.suggest_categorical(""solver"", [""svd"", ""lsqr"", ""eigen""])

    # Create and evaluate the LDA model with the chosen hyperparameters
    lda = LinearDiscriminantAnalysis(n_components=n_components, solver=solver)
    lda.fit(X, y)
    explained_variance = np.sum(lda.explained_variance_ratio_)

    return explained_variance

# Create a study and optimize hyperparameters
if __name__ == ""__main__"":
    study = optuna.create_study(direction=""maximize"")
    study.optimize(objective, n_trials=50)

    # Get the best hyperparameters
    best_params = study.best_params
    n_components = best_params[""n_components""]
    solver = best_params[""solver""]

    print(""Best Hyperparameters:"", best_params)

    # Create and evaluate the LDA model with the best hyperparameters
    lda = LinearDiscriminantAnalysis(n_components=n_components, solver=solver)
    lda.fit(X, y)
    explained_variance = np.sum(lda.explained_variance_ratio_

)

    print(""Explained Variance Ratio with Best Hyperparameters:"", explained_variance)
",test
pandas,2,"Create a Python program that uses the 'pandas' API to read a CSV file containing student data, calculate the average score for each subject, identify the subject with the highest average score, and create a summary DataFrame.",code/pandas/pandas_2.py,Implement unit tests to verify the correctness of average score calculations and the identification of the subject with the highest average score.,Check the performance of the program with a large student data file and record the execution time for performance testing.,"Test the program with different student data files, including edge cases with missing or incomplete data, to ensure robustness.",,,"#!pip install pandas
import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv('student_data.csv')

# Calculate the average score for each subject
subject_avg_score = df.groupby('Subject')['Score'].mean()
subject_avg_score = subject_avg_score.reset_index()
subject_avg_score.columns = ['Subject', 'Average Score']

# Identify the subject with the highest average score
highest_avg_score_subject = subject_avg_score[subject_avg_score['Average Score'] == subject_avg_score['Average Score'].max()]

# Create a summary DataFrame
summary_df = pd.concat([highest_avg_score_subject, subject_avg_score], axis=1)

print(""Summary Dataframe: "")
print(summary_df)",train
pandas,9,"Create a Python program that uses the 'pandas' API to read a CSV file containing employee data, calculate the average salary for each department, identify the department with the highest average salary, and create a summary DataFrame.",code/pandas/pandas_9.py,Implement unit tests to verify the correctness of average salary calculations and the identification of the department with the highest average salary.,"Test the program with different employee data files, including edge cases with missing or incomplete data, to ensure robustness.",Check the performance of the program with a large employee data file and record the execution time for performance testing.,,,"#!pip install pandas
import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv('employee_data.csv')

# Calculate the average salary for each department
department_avg_salary = df.groupby('Department')['Salary'].mean()
department_avg_salary = department_avg_salary.reset_index()
department_avg_salary.columns = ['Department', 'Average Salary']

# Identify the department with the highest average salary
highest_avg_salary_department = department_avg_salary[department_avg_salary['Average Salary'] == department_avg_salary['Average Salary'].max()]

# Create a summary DataFrame
summary_df = pd.concat([highest_avg_salary_department, department_avg_salary], axis=1)

print(""Summary Dataframe: "")
print(summary_df)",train
pandas,6,"Create a Python program that uses the 'pandas' API to read a CSV file containing employee data, calculate the average salary for each job title, identify the job title with the highest average salary, and create a summary DataFrame.",code/pandas/pandas_6.py,Implement unit tests to verify the correctness of average salary calculations and the identification of the job title with the highest average salary.,"Test the program with different employee data files, including edge cases with missing or incomplete data, to ensure robustness.",Check the performance of the program with a large employee data file and record the execution time for performance testing.,,,"#!pip install pandas
import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv('employee_data.csv')

# Calculate the average salary for each job title
job_title_avg_salary = df.groupby('Job Title')['Salary'].mean()
job_title_avg_salary = job_title_avg_salary.reset_index()
job_title_avg_salary.columns = ['Job Title', 'Average Salary']

# Identify the job title with the highest average salary
highest_avg_salary_job_title = job_title_avg_salary[job_title_avg_salary['Average Salary'] == job_title_avg_salary['Average Salary'].max()]

# Create a summary DataFrame
summary_df = pd.concat([highest_avg_salary_job_title, job_title_avg_salary], axis=1)

print(""Summary Dataframe: "")
print(summary_df)",train
pandas,12,"Create a Python program that uses the 'pandas' API to read a CSV file containing employee data, calculate the average age for each department, identify the department with the highest average age, and create a summary DataFrame.",code/pandas/pandas_12.py,Implement unit tests to verify the correctness of average age calculations and the identification of the department with the highest average age.,"Test the program with different employee data files, including edge cases with missing or incomplete data, to ensure robustness.",Check the performance of the program with a large employee data file and record the execution time for performance testing.,,,"#!pip install pandas
import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv('employee_data.csv')

# Calculate the average age for each department
department_avg_age = df.groupby('Department')['Age'].mean()
department_avg_age = department_avg_age.reset_index()
department_avg_age.columns = ['Department', 'Average Age']

# Identify the department with the highest average age
highest_avg_age_department = department_avg_age[department_avg_age['Average Age'] == department_avg_age['Average Age'].max()]

# Create a summary DataFrame
summary_df = pd.concat([highest_avg_age_department, department_avg_age], axis=1)

print(""Summary Dataframe: "")
print(summary_df)",train
pandas,4,"Create a Python program that uses the 'pandas' API to read a CSV file containing stock data, calculate the average closing price for each month, identify the month with the highest average closing price, and create a summary DataFrame.",code/pandas/pandas_4.py,Implement unit tests to verify the correctness of average closing price calculations and the identification of the month with the highest average closing price.,"Test the program with different stock data files, including edge cases with missing or incomplete data, to ensure robustness.",Check the performance of the program with a large stock data file and record the execution time for performance testing.,,,"#!pip install pandas
import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv('stock_data.csv')

# Convert the 'Date' column to datetime format
df['Date'] = pd.to_datetime(df['Date'])

# Extract the month from the 'Date' column
df['Month'] = df['Date'].dt.month

# Calculate the average closing price for each month
month_avg_closing_price = df.groupby('Month')['Close'].mean()
month_avg_closing_price = month_avg_closing_price.reset_index()
month_avg_closing_price.columns = ['Month', 'Average Closing Price']

# Identify the month with the highest average closing price
highest_avg_closing_price_month = month_avg_closing_price[month_avg_closing_price['Average Closing Price'] == month_avg_closing_price['Average Closing Price'].max()]

# Create a summary DataFrame
summary_df = pd.concat([highest_avg_closing_price_month, month_avg_closing_price], axis=1)

print(""Summary Dataframe: "")
print(summary_df)",train
pandas,21,"Create a Python program that uses the 'pandas' API to read a CSV file containing stock data, calculate the average closing price for each year, identify the year with the highest average closing price, and create a summary DataFrame.",code/pandas/pandas_21.py,"Test the program with different stock data files, including edge cases with missing or incomplete data, to ensure robustness.",Check the performance of the program with a large stock data file and record the execution time for performance testing.,Implement unit tests to verify the correctness of average closing price calculations and the identification of the year with the highest average closing price.,,,"#!pip install pandas
import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv('stock_data.csv')

# Convert the 'Date' column to datetime format
df['Date'] = pd.to_datetime(df['Date'])

# Extract the year from the 'Date' column
df['Year'] = df['Date'].dt.year

# Calculate the average closing price for each year
year_avg_closing_price = df.groupby('Year')['Close'].mean()
year_avg_closing_price = year_avg_closing_price.reset_index()
year_avg_closing_price.columns = ['Year', 'Average Closing Price']

# Identify the year with the highest average closing price
highest_avg_closing_price_year = year_avg_closing_price[year_avg_closing_price['Average Closing Price'] == year_avg_closing_price['Average Closing Price'].max()]

# Create a summary DataFrame
summary_df = pd.concat([highest_avg_closing_price_year, year_avg_closing_price], axis=1)

print(""Summary Dataframe: "")
print(summary_df)",train
pandas,17,"Create a Python program that uses the 'pandas' API to read a CSV file containing employee data, calculate the total number of employees for each department, identify the department with the highest number of employees, and create a summary DataFrame.",code/pandas/pandas_17.py,Implement unit tests to verify the correctness of total number of employees calculations and the identification of the department with the highest number of employees.,"Test the program with different employee data files, including edge cases with missing or incomplete data, to ensure robustness.",Check the performance of the program with a large employee data file and record the execution time for performance testing.,,,"#!pip install pandas
import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv('employee_data.csv')

# Calculate the total number of employees for each department
department_total_employees = df.groupby('Department')['Employee ID'].count()
department_total_employees = department_total_employees.reset_index()
department_total_employees.columns = ['Department', 'Total Employees']

# Identify the department with the highest number of employees
highest_total_employees_department = department_total_employees[department_total_employees['Total Employees'] == department_total_employees['Total Employees'].max()]

# Create a summary DataFrame
summary_df = pd.concat([highest_total_employees_department, department_total_employees], axis=1)

print(""Summary Dataframe: "")
print(summary_df)",train
pandas,14,"Create a Python program that uses the 'pandas' API to read a CSV file containing stock data, calculate the average volume for each month, identify the month with the highest average volume, and create a summary DataFrame.",code/pandas/pandas_14.py,"Test the program with different stock data files, including edge cases with missing or incomplete data, to ensure robustness.",Implement unit tests to verify the correctness of average volume calculations and the identification of the month with the highest average volume.,Check the performance of the program with a large stock data file and record the execution time for performance testing.,,,"#!pip install pandas
import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv('stock_data.csv')

# Convert the 'Date' column to datetime format
df['Date'] = pd.to_datetime(df['Date'])

# Extract the month from the 'Date' column
df['Month'] = df['Date'].dt.month

# Calculate the average volume for each month
month_avg_volume = df.groupby('Month')['Volume'].mean()
month_avg_volume = month_avg_volume.reset_index()
month_avg_volume.columns = ['Month', 'Average Volume']

# Identify the month with the highest average volume
highest_avg_volume_month = month_avg_volume[month_avg_volume['Average Volume'] == month_avg_volume['Average Volume'].max()]

# Create a summary DataFrame
summary_df = pd.concat([highest_avg_volume_month, month_avg_volume], axis=1)

print(""Summary Dataframe: "")
print(summary_df)",train
pandas,3,"Create a Python program that uses the 'pandas' API to read a CSV file containing employee data, calculate the total salary for each department, identify the department with the highest total salary, and create a summary DataFrame.",code/pandas/pandas_3.py,Implement unit tests to verify the correctness of total salary calculations and the identification of the department with the highest total salary.,"Test the program with different employee data files, including edge cases with missing or incomplete data, to ensure robustness.",Check the performance of the program with a large employee data file and record the execution time for performance testing.,,,"#!pip install pandas
import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv('employee_data.csv')

# Calculate the total salary for each department
department_total_salary = df.groupby('Department')['Salary'].sum()
department_total_salary = department_total_salary.reset_index()
department_total_salary.columns = ['Department', 'Total Salary']

# Identify the department with the highest total salary
highest_total_salary_department = department_total_salary[department_total_salary['Total Salary'] == department_total_salary['Total Salary'].max()]

# Create a summary DataFrame
summary_df = pd.concat([highest_total_salary_department, department_total_salary], axis=1)

print(""Summary Dataframe: "")
print(summary_df)",train
pandas,10,"Create a Python program that uses the 'pandas' API to read a CSV file containing customer data, calculate the average purchase amount for each customer, identify the customer with the highest average purchase amount, and create a summary DataFrame.",code/pandas/pandas_10.py,"Test the program with different customer data files, including edge cases with missing or incomplete data, to ensure robustness.",Implement unit tests to verify the correctness of average purchase amount calculations and the identification of the customer with the highest average purchase amount.,Check the performance of the program with a large customer data file and record the execution time for performance testing.,,,"#!pip install pandas
import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv('customer_data.csv')

# Calculate the average purchase amount for each customer
customer_avg_purchase = df.groupby('Customer')['Purchase Amount'].mean()
customer_avg_purchase = customer_avg_purchase.reset_index()
customer_avg_purchase.columns = ['Customer', 'Average Purchase Amount']

# Identify the customer with the highest average purchase amount
highest_avg_purchase_customer = customer_avg_purchase[customer_avg_purchase['Average Purchase Amount'] == customer_avg_purchase['Average Purchase Amount'].max()]

# Create a summary DataFrame
summary_df = pd.concat([highest_avg_purchase_customer, customer_avg_purchase], axis=1)

print(""Summary Dataframe: "")
print(summary_df)",train
pandas,28,"Create a Python program that uses the 'pandas' API to read a CSV file containing employee data, calculate the total salary for each job title, identify the job title with the highest total salary, and create a summary DataFrame.",code/pandas/pandas_28.py,Implement unit tests to verify the correctness of total salary calculations and the identification of the job title with the highest total salary.,"Test the program with different employee data files, including edge cases with missing or incomplete data, to ensure robustness.",Check the performance of the program with a large employee data file and record the execution time for performance testing.,,,"#!pip install pandas
import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv('employee_data.csv')

# Calculate the total salary for each job title
job_title_total_salary = df.groupby('Job Title')['Salary'].sum()
job_title_total_salary = job_title_total_salary.reset_index()
job_title_total_salary.columns = ['Job Title', 'Total Salary']

# Identify the job title with the highest total salary
highest_total_salary_job_title = job_title_total_salary[job_title_total_salary['Total Salary'] == job_title_total_salary['Total Salary'].max()]

# Create a summary DataFrame
summary_df = pd.concat([highest_total_salary_job_title, job_title_total_salary], axis=1)

print(""Summary Dataframe: "")
print(summary_df)",train
pandas,5,"Create a Python program that uses the 'pandas' API to read a CSV file containing customer data, calculate the total revenue for each customer, identify the customer with the highest total revenue, and create a summary DataFrame.",code/pandas/pandas_5.py,"Test the program with different customer data files, including edge cases with missing or incomplete data, to ensure robustness.",Implement unit tests to verify the correctness of total revenue calculations and the identification of the customer with the highest total revenue.,Check the performance of the program with a large customer data file and record the execution time for performance testing.,,,"#!pip install pandas
import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv('customer_data.csv')

# Calculate the total revenue for each customer
customer_total_revenue = df.groupby('Customer')['Revenue'].sum()
customer_total_revenue = customer_total_revenue.reset_index()
customer_total_revenue.columns = ['Customer', 'Total Revenue']

# Identify the customer with the highest total revenue
highest_total_revenue_customer = customer_total_revenue[customer_total_revenue['Total Revenue'] == customer_total_revenue['Total Revenue'].max()]

# Create a summary DataFrame
summary_df = pd.concat([highest_total_revenue_customer, customer_total_revenue], axis=1)

print(""Summary Dataframe: "")
print(summary_df)",train
pandas,13,"Create a Python program that uses the 'pandas' API to read a CSV file containing customer data, calculate the total number of purchases for each customer, identify the customer with the highest number of purchases, and create a summary DataFrame.",code/pandas/pandas_13.py,Implement unit tests to verify the correctness of total number of purchases calculations and the identification of the customer with the highest number of purchases.,"Test the program with different customer data files, including edge cases with missing or incomplete data, to ensure robustness.",Check the performance of the program with a large customer data file and record the execution time for performance testing.,,,"#!pip install pandas
import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv('customer_data.csv')

# Calculate the total number of purchases for each customer
customer_total_purchases = df.groupby('Customer')['Purchase'].count()
customer_total_purchases = customer_total_purchases.reset_index()
customer_total_purchases.columns = ['Customer', 'Total Purchases']

# Identify the customer with the highest number of purchases
highest_total_purchases_customer = customer_total_purchases[customer_total_purchases['Total Purchases'] == customer_total_purchases['Total Purchases'].max()]

# Create a summary DataFrame
summary_df = pd.concat([highest_total_purchases_customer, customer_total_purchases], axis=1)

print(""Summary Dataframe: "")
print(summary_df)",train
pandas,8,"Create a Python program that uses the 'pandas' API to read a CSV file containing customer data, calculate the average age for each gender, identify the gender with the highest average age, and create a summary DataFrame.",code/pandas/pandas_8.py,"Test the program with different customer data files, including edge cases with missing or incomplete data, to ensure robustness.",Implement unit tests to verify the correctness of average age calculations and the identification of the gender with the highest average age.,Check the performance of the program with a large customer data file and record the execution time for performance testing.,,,"#!pip install pandas
import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv('customer_data.csv')

# Calculate the average age for each gender
gender_avg_age = df.groupby('Gender')['Age'].mean()
gender_avg_age = gender_avg_age.reset_index()
gender_avg_age.columns = ['Gender', 'Average Age']

# Identify the gender with the highest average age
highest_avg_age_gender = gender_avg_age[gender_avg_age['Average Age'] == gender_avg_age['Average Age'].max()]

# Create a summary DataFrame
summary_df = pd.concat([highest_avg_age_gender, gender_avg_age], axis=1)

print(""Summary Dataframe: "")
print(summary_df)",train
pandas,11,"Create a Python program that uses the 'pandas' API to read a CSV file containing sales data, calculate the total sales revenue for each category, identify the category with the highest sales revenue, and create a summary DataFrame.",code/pandas/pandas_11.py,Check the performance of the program with a large sales data file and record the execution time for performance testing.,Implement unit tests to verify the correctness of total sales revenue calculations and the identification of the category with the highest sales revenue.,"Test the program with different sales data files, including edge cases with missing or incomplete data, to ensure robustness.",,,"#!pip install pandas
import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv('sales_data.csv')

# Calculate the total sales revenue for each category
category_total_revenue = df.groupby('Category')['Revenue'].sum()
category_total_revenue = category_total_revenue.reset_index()
category_total_revenue.columns = ['Category', 'Total Revenue']

# Identify the category with the highest sales revenue
highest_total_revenue_category = category_total_revenue[category_total_revenue['Total Revenue'] == category_total_revenue['Total Revenue'].max()]

# Create a summary DataFrame
summary_df = pd.concat([highest_total_revenue_category, category_total_revenue], axis=1)

print(""Summary Dataframe: "")
print(summary_df)",train
pandas,15,"Create a Python program that uses the 'pandas' API to read a CSV file containing sales data, calculate the total sales revenue for each region, identify the region with the highest sales revenue, and create a summary DataFrame.",code/pandas/pandas_15.py,Check the performance of the program with a large sales data file and record the execution time for performance testing.,Implement unit tests to verify the correctness of total sales revenue calculations and the identification of the region with the highest sales revenue.,"Test the program with different sales data files, including edge cases with missing or incomplete data, to ensure robustness.",,,"#!pip install pandas
import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv('sales_data.csv')

# Calculate the total sales revenue for each region
region_total_revenue = df.groupby('Region')['Revenue'].sum()
region_total_revenue = region_total_revenue.reset_index()
region_total_revenue.columns = ['Region', 'Total Revenue']

# Identify the region with the highest sales revenue
highest_total_revenue_region = region_total_revenue[region_total_revenue['Total Revenue'] == region_total_revenue['Total Revenue'].max()]

# Create a summary DataFrame
summary_df = pd.concat([highest_total_revenue_region, region_total_revenue], axis=1)

print(""Summary Dataframe: "")
print(summary_df)",train
pandas,25,"Create a Python program that uses the 'pandas' API to read a CSV file containing sales data, calculate the total sales revenue for each product category, identify the category with the highest sales revenue, and create a summary DataFrame.",code/pandas/pandas_25.py,Check the performance of the program with a large sales data file and record the execution time for performance testing.,Implement unit tests to verify the correctness of total sales revenue calculations and the identification of the category with the highest sales revenue.,"Test the program with different sales data files, including edge cases with missing or incomplete data, to ensure robustness.",,,"#!pip install pandas
import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv('sales_data.csv')

# Calculate the total sales revenue for each product category
category_total_revenue = df.groupby('Category')['Revenue'].sum()
category_total_revenue = category_total_revenue.reset_index()
category_total_revenue.columns = ['Category', 'Total Revenue']

# Identify the category with the highest sales revenue
highest_total_revenue_category = category_total_revenue[category_total_revenue['Total Revenue'] == category_total_revenue['Total Revenue'].max()]

# Create a summary DataFrame
summary_df = pd.concat([highest_total_revenue_category, category_total_revenue], axis=1)

print(""Summary Dataframe: "")
print(summary_df)",train
pandas,7,"Create a Python program that uses the 'pandas' API to read a CSV file containing sales data, calculate the total sales revenue for each month, identify the month with the highest sales revenue, and create a summary DataFrame.",code/pandas/pandas_7.py,Check the performance of the program with a large sales data file and record the execution time for performance testing.,"Test the program with different sales data files, including edge cases with missing or incomplete data, to ensure robustness.",Implement unit tests to verify the correctness of total sales revenue calculations and the identification of the month with the highest sales revenue.,,,"#!pip install pandas
import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv('sales_data.csv')

# Convert the 'Date' column to datetime format
df['Date'] = pd.to_datetime(df['Date'])

# Extract the month from the 'Date' column
df['Month'] = df['Date'].dt.month

# Calculate the total sales revenue for each month
month_total_revenue = df.groupby('Month')['Revenue'].sum()
month_total_revenue = month_total_revenue.reset_index()
month_total_revenue.columns = ['Month', 'Total Revenue']

# Identify the month with the highest sales revenue
highest_total_revenue_month = month_total_revenue[month_total_revenue['Total Revenue'] == month_total_revenue['Total Revenue'].max()]

# Create a summary DataFrame
summary_df = pd.concat([highest_total_revenue_month, month_total_revenue], axis=1)

print(""Summary Dataframe: "")
print(summary_df)",train
pandas,1,"Create a Python program that uses the 'pandas' API to read a JSON file containing sales data, calculate total sales revenue for each product, identify the product with the highest revenue, and create a summary DataFrame.",code/pandas/pandas_1.py,Implement unit tests to verify the correctness of total sales revenue calculations and the identification of the highest revenue product.,Check the performance of the program with a large sales data file and record the execution time for performance testing.,"Test the program with different sales data files, including edge cases with missing or incomplete data, to ensure robustness.",,,"#!pip install pandas
import pandas as pd

# Read the JSON file into a DataFrame
df = pd.read_json('sales_data.json')

# Calculate total sales revenue for each product
product_sales = df.groupby('Product')['Quantity'].sum() * df.groupby('Product')['Price'].sum()
product_sales = product_sales.reset_index()
product_sales.columns = ['Product', 'Total Revenue']

# Identify the product with the highest revenue
highest_revenue_product = product_sales[product_sales['Total Revenue'] == product_sales['Total Revenue'].max()]

# Create a summary DataFrame
summary_df = pd.concat([highest_revenue_product, product_sales], axis=1)

print(""Summary Dataframe: "")
print(summary_df)",test
pandas,18,"Create a Python program that uses the 'pandas' API to read a CSV file containing sales data, calculate the total sales revenue for each customer, identify the customer with the highest sales revenue, and create a summary DataFrame.",code/pandas/pandas_18.py,Check the performance of the program with a large sales data file and record the execution time for performance testing.,"Test the program with different sales data files, including edge cases with missing or incomplete data, to ensure robustness.",Implement unit tests to verify the correctness of total sales revenue calculations and the identification of the customer with the highest sales revenue.,,,"#!pip install pandas
import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv('sales_data.csv')

# Calculate the total sales revenue for each customer
customer_total_revenue = df.groupby('Customer')['Revenue'].sum()
customer_total_revenue = customer_total_revenue.reset_index()
customer_total_revenue.columns = ['Customer', 'Total Revenue']

# Identify the customer with the highest sales revenue
highest_total_revenue_customer = customer_total_revenue[customer_total_revenue['Total Revenue'] == customer_total_revenue['Total Revenue'].max()]

# Create a summary DataFrame
summary_df = pd.concat([highest_total_revenue_customer, customer_total_revenue], axis=1)

print(""Summary Dataframe: "")
print(summary_df)",test
pandas,16,"Create a Python program that uses the 'pandas' API to read a CSV file containing customer data, calculate the average age for each occupation, identify the occupation with the highest average age, and create a summary DataFrame.",code/pandas/pandas_16.py,"Test the program with different customer data files, including edge cases with missing or incomplete data, to ensure robustness.",Check the performance of the program with a large customer data file and record the execution time for performance testing.,Implement unit tests to verify the correctness of average age calculations and the identification of the occupation with the highest average age.,,,"#!pip install pandas
import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv('customer_data.csv')

# Calculate the average age for each occupation
occupation_avg_age = df.groupby('Occupation')['Age'].mean()
occupation_avg_age = occupation_avg_age.reset_index()
occupation_avg_age.columns = ['Occupation', 'Average Age']

# Identify the occupation with the highest average age
highest_avg_age_occupation = occupation_avg_age[occupation_avg_age['Average Age'] == occupation_avg_age['Average Age'].max()]

# Create a summary DataFrame
summary_df = pd.concat([highest_avg_age_occupation, occupation_avg_age], axis=1)

print(""Summary Dataframe: "")
print(summary_df)",test
peft,41,"Create a Python program that uses the 'peft' API to generate code for a basic e-commerce application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for an e-commerce application that allows users to browse products, add items to a cart, and complete purchases.",code/peft/peft_41.py,Test that the generated e-commerce application code enables users to shop for products and make online purchases.,Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,"Test that the program can generate code for an e-commerce application that allows users to browse products, add items to a cart, and complete purchases.",,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for an e-commerce application
ecommerce_code = model.generate_ecommerce_code(max_length=500)

# Print generated e-commerce application code
print(ecommerce_code)",train
peft,20,Develop a Python program using the 'peft' API to generate code for a basic image recognition application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for an image recognition application that can identify objects and scenes in images.,code/peft/peft_20.py,Test that the program can generate code for an image recognition application that can identify objects and scenes in images.,Test that the generated image recognition application code accurately recognizes objects and scenes in images.,Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for an image recognition application
image_recognition_app_code = model.generate_image_recognition_app_code(max_length=500)

# Print generated image recognition application code
print(image_recognition_app_code)",train
peft,40,"Develop a Python program using the 'peft' API to generate code for a basic social media application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a social media application that allows users to create posts, interact with other users, and view a feed of posts from their network.",code/peft/peft_40.py,"Test that the program can generate code for a social media application that allows users to create posts, interact with other users, and view a feed of posts from their network.",Test that the generated social media application code enables users to participate in social interactions and share content.,Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a social media application
social_media_code = model.generate_social_media_code(max_length=500)

# Print generated social media application code
print(social_media_code)",train
peft,30,"Develop a Python program using the 'peft' API to generate code for a basic time management application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a time management application that allows users to schedule events, set reminders, and manage their time effectively.",code/peft/peft_30.py,Test that the generated time management application code enables users to organize their schedules and stay on top of tasks.,Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,"Test that the program can generate code for a time management application that allows users to schedule events, set reminders, and manage their time effectively.",,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a time management application
time_management_code = model.generate_time_management_code(max_length=500)

# Print generated time management application code
print(time_management_code)",train
peft,7,Create a Python program that employs the 'peft' API to generate code in the Python programming language. Use the 'gpt2' model for code generation. The program should take a high-level description of a task as input and generate Python code that accomplishes the task.,code/peft/peft_7.py,Test that the 'gpt2' model and peft model are successfully loaded.,Test that the program can generate Python code based on different high-level task descriptions.,Test that the program produces functional Python code that accomplishes the specified tasks.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt2""
peft_model = ""gpt2""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Task description
task_description = ""Write a Python program that calculates the sum of all even numbers from 1 to 100.""

# Generate Python code
generated_code = model.generate_code(task_description, max_length=200)

# Print generated code
print(generated_code)",train
peft,27,"Create a Python program that uses the 'peft' API to generate code for a basic personal finance management application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a personal finance management application that allows users to track income, expenses, and budgeting.",code/peft/peft_27.py,"Test that the program can generate code for a personal finance management application that allows users to track income, expenses, and budgeting.",Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,Test that the generated finance management application code enables users to manage their finances effectively.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a personal finance management application
finance_app_code = model.generate_finance_app_code(max_length=500)

# Print generated personal finance management application code
print(finance_app_code)",train
peft,13,Create a Python program that uses the 'peft' API to generate code for a weather forecasting application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code that takes a location as input and provides weather forecasts for that location.,code/peft/peft_13.py,Test that the program can generate code for a weather forecasting application that takes a location as input and provides weather forecasts.,Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,Test that the generated code allows users to input a location and receive accurate weather forecasts.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a weather forecasting application
weather_app_code = model.generate_weather_app_code(max_length=500)

# Print generated weather app code
print(weather_app_code)",train
peft,18,"Develop a Python program using the 'peft' API to generate code for a basic e-learning platform. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for an e-learning platform that allows users to access courses, view content, and take quizzes.",code/peft/peft_18.py,Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,"Test that the program can generate code for a basic e-learning platform that allows users to access courses, view content, and take quizzes.",Test that the generated e-learning platform code enables users to access educational materials and complete quizzes.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a basic e-learning platform
elearning_platform_code = model.generate_elearning_platform_code(max_length=500)

# Print generated e-learning platform code
print(elearning_platform_code)",train
peft,10,Develop a Python program using the 'peft' API to perform text classification on a set of documents. Utilize the 'distilbert-base-uncased' model for text classification. The program should accept a list of documents as input and classify them into predefined categories.,code/peft/peft_10.py,Test that the program handles various types of documents and assigns appropriate classifications.,Test that the 'distilbert-base-uncased' model and peft model are successfully loaded.,Test that the program can accurately classify different documents into predefined categories.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForSequenceClassification
from peft import PeftModel

# Load Models
base_model = ""distilbert-base-uncased""
peft_model = ""distilbert-base-uncased""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForSequenceClassification.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# List of documents
documents = [""This is an article about climate change and its impact on the environment."", ""The latest technology trends in artificial intelligence."", ""A recipe for a delicious chocolate cake.""]

# Perform text classification
classifications = []
for document in documents:
    encoded_document = tokenizer(document, return_tensors=""pt"")
    classification = model.classify_text(encoded_document)
    classifications.append(classification)

# Print text classifications
for i, document in enumerate(documents):
    print(f""Document: {document}"")
    print(f""Classification: {classifications[i]}"")",train
peft,16,Develop a Python program using the 'peft' API to generate code for a basic chat application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a chat application that allows users to send and receive messages in real-time.,code/peft/peft_16.py,Test that the generated chat application code enables real-time communication between users.,Test that the program can generate code for a basic chat application that allows users to send and receive messages in real-time.,Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a basic chat application
chat_app_code = model.generate_chat_app_code(max_length=500)

# Print generated chat application code
print(chat_app_code)",train
peft,33,"Create a Python program that uses the 'peft' API to generate code for a basic task scheduling application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a task scheduling application that allows users to create, edit, and prioritize tasks.",code/peft/peft_33.py,Test that the generated task scheduling application code enables users to manage their tasks and schedules effectively.,"Test that the program can generate code for a task scheduling application that allows users to create, edit, and prioritize tasks.",Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a task scheduling application
task_scheduling_code = model.generate_task_scheduling_code(max_length=500)

# Print generated task scheduling application code
print(task_scheduling_code)",train
peft,17,Create a Python program that utilizes the 'peft' API to generate code for a content recommendation system. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a recommendation system that suggests content based on user preferences and behavior.,code/peft/peft_17.py,Test that the program can generate code for a content recommendation system that suggests content based on user preferences and behavior.,Test that the generated recommendation system code provides relevant content recommendations to users.,Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a content recommendation system
recommendation_system_code = model.generate_recommendation_system_code(max_length=500)

# Print generated recommendation system code
print(recommendation_system_code)",train
peft,36,Develop a Python program using the 'peft' API to generate code for a basic chatbot application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a chatbot that can engage in conversations and answer user queries on various topics.,code/peft/peft_36.py,Test that the generated chatbot code effectively interacts with users and provides relevant responses.,Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,Test that the program can generate code for a chatbot application that can engage in conversations and answer user queries on various topics.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a chatbot application
chatbot_code = model.generate_chatbot_code(max_length=500)

# Print generated chatbot application code
print(chatbot_code)",train
peft,35,"Create a Python program that utilizes the 'peft' API to generate code for a basic travel booking application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a travel booking application that allows users to search for and book flights, hotels, and travel packages.",code/peft/peft_35.py,"Test that the program can generate code for a travel booking application that allows users to search for and book flights, hotels, and travel packages.",Test that the generated travel booking application code enables users to plan and book their travel arrangements.,Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a travel booking application
travel_booking_code = model.generate_travel_booking_code(max_length=500)

# Print generated travel booking application code
print(travel_booking_code)",train
peft,1,"Develop a Python program using the 'peft' API that assesses the sentiment of a provided news snippet. Load the necessary models, including 'THUDM/chatglm2-6b' as the base model and 'oliverwang15/FinGPT_ChatGLM2_Sentiment_Instruction_LoRA_FT' as the peft model. Create a prompt that instructs the model to determine the sentiment of the news snippet, which should be provided as input. The program should generate the sentiment result and print it.",code/peft/peft_1.py,Test that the program correctly handles different news snippets and returns the corresponding sentiment.,Test that the program can generate sentiment results for the provided news snippet.,Test that the base model ('THUDM/chatglm2-6b') and sentiment model ('oliverwang15/FinGPT_ChatGLM2_Sentiment_Instruction_LoRA_FT') are loaded successfully.,,,"#!pip install transformers
#!pip install peft
from transformers import AutoModel, AutoTokenizer
from peft import PeftModel

# Load Models
base_model = ""THUDM/chatglm2-6b""
peft_model = ""oliverwang15/FinGPT_ChatGLM2_Sentiment_Instruction_LoRA_FT""
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
model = AutoModel.from_pretrained(base_model, trust_remote_code=True,  device_map = ""auto"")
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Make prompt
prompt = [
'''Instruction: What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}
Input: FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is aggressively pursuing its growth strategy by increasingly focusing on technologically more demanding HDI printed circuit boards PCBs .
Answer: '''
]

# Generate results
tokens = tokenizer(prompt, return_tensors='pt', padding=True, max_length=512)
res = model.generate(**tokens, max_length=512)
res_sentences = [tokenizer.decode(i) for i in res]
out_text = [o.split(""Answer: "")[1] for o in res_sentences]

# show results
for sentiment in out_text:
    print(sentiment)",train
peft,28,Develop a Python program using the 'peft' API to generate code for a basic news aggregation application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a news aggregation application that collects and displays news articles from various sources.,code/peft/peft_28.py,Test that the program can generate code for a news aggregation application that collects and displays news articles from various sources.,Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,Test that the generated news aggregation application code allows users to access and read news articles.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a news aggregation application
news_app_code = model.generate_news_app_code(max_length=500)

# Print generated news aggregation application code
print(news_app_code)",train
peft,6,Develop a Python program using the 'peft' API to perform sentiment analysis on a given set of user reviews. Utilize the 'nlptown/bert-base-multilingual-uncased-sentiment' model for sentiment analysis. The program should accept a list of user reviews as input and provide sentiment scores for each review.,code/peft/peft_6.py,Test that the program can accurately perform sentiment analysis on different user reviews.,Test that the program handles a variety of user reviews and provides appropriate sentiment scores.,Test that the 'nlptown/bert-base-multilingual-uncased-sentiment' model and peft model are successfully loaded.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForSequenceClassification
from peft import PeftModel

# Load Models
base_model = ""nlptown/bert-base-multilingual-uncased-sentiment""
peft_model = ""nlptown/bert-base-multilingual-uncased-sentiment""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForSequenceClassification.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# List of user reviews
user_reviews = [""I love this product! It's amazing."", ""The quality is terrible. I wouldn't recommend it to anyone."", ""It's okay, not great but not terrible either.""]

# Perform sentiment analysis
sentiments = []
for review in user_reviews:
    encoded_review = tokenizer(review, return_tensors=""pt"")
    sentiment = model.get_sentiment(encoded_review)
    sentiments.append(sentiment)

# Print sentiment scores
for i, review in enumerate(user_reviews):
    print(f""Review: {review}"")
    print(f""Sentiment Score: {sentiments[i]}"")",train
peft,12,Develop a Python program using the 'peft' API to generate code for a simple game. Utilize the 'gpt-3.5-turbo' model for game code generation. The program should generate code for a basic text-based game with a defined objective and rules.,code/peft/peft_12.py,Test that the program can generate code for a simple text-based game with defined objectives and rules.,Test that the generated game code allows users to play and interact with the game.,Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a simple text-based game
game_code = model.generate_game_code(max_length=500)

# Print generated game code
print(game_code)",train
peft,2,Develop a Python program that utilizes the 'peft' API to summarize a given text. Use the 'google/pegasus-newsroom' model for summarization. The program should take an input text and produce a concise summary. Ensure that the program is capable of handling long texts and generating meaningful summaries.,code/peft/peft_2.py,Test that the program handles long input texts and produces concise summaries.,Test that the program can generate meaningful summaries for a variety of input texts.,Test that the 'google/pegasus-newsroom' model and peft model are successfully loaded.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from peft import PeftModel

# Load Models
base_model = ""google/pegasus-newsroom""
peft_model = ""google/pegasus-newsroom""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForSeq2SeqLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Input text
input_text = ""Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla vehicula, purus sit amet vulputate iaculis, quam arcu dictum urna, in tincidunt erat arcu vel tortor. Nullam sit amet malesuada metus. Suspendisse potenti. Aenean euismod in urna et auctor. Nullam id massa lectus. Aliquam dapibus ex ut arcu feugiat, id vehicula dolor elementum. Fusce aliquam mi at augue finibus, eu gravida nisl vulputate.""

# Generate summary
summary = model.summarize(input_text, max_length=150)

# Print summary
print(summary)",train
peft,31,"Create a Python program that uses the 'peft' API to generate code for a basic ride-sharing application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a ride-sharing application that allows users to request rides, view driver information, and track ride progress.",code/peft/peft_31.py,Test that the generated ride-sharing application code enables users to request and complete rides.,"Test that the program can generate code for a ride-sharing application that allows users to request rides, view driver information, and track ride progress.",Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a ride-sharing application
ride_sharing_code = model.generate_ride_sharing_code(max_length=500)

# Print generated ride-sharing application code
print(ride_sharing_code)",train
peft,22,"Develop a Python program using the 'peft' API to generate code for a basic music player application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a music player application that allows users to play, pause, and skip songs.",code/peft/peft_22.py,"Test that the program can generate code for a music player application that allows users to play, pause, and skip songs.",Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,Test that the generated music player application code enables users to control music playback.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a music player application
music_player_app_code = model.generate_music_player_app_code(max_length=500)

# Print generated music player application code
print(music_player_app_code)",train
peft,3,Create a Python program that uses the 'peft' API to translate a given English text to French. Employ the 'facebook/mbart-large-cc25' model for translation. The program should be capable of translating a range of English texts into French.,code/peft/peft_3.py,Test that the program handles both short and long English texts for translation.,Test that the program can accurately translate various English texts into French.,Test that the 'facebook/mbart-large-cc25' model and peft model are successfully loaded.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from peft import PeftModel

# Load Models
base_model = ""facebook/mbart-large-cc25""
peft_model = ""facebook/mbart-large-cc25""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForSeq2SeqLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Input English text
english_text = ""The quick brown fox jumps over the lazy dog.""

# Translate to French
translated_text = model.translate(english_text, max_length=150, target_language=""fr"")

# Print translated text
print(translated_text)",train
peft,32,"Develop a Python program using the 'peft' API to generate code for a basic recipe management application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a recipe management application that allows users to store, organize, and search for recipes.",code/peft/peft_32.py,"Test that the program can generate code for a recipe management application that allows users to store, organize, and search for recipes.",Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,Test that the generated recipe management application code enables users to manage their recipe collections effectively.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a recipe management application
recipe_management_code = model.generate_recipe_management_code(max_length=500)

# Print generated recipe management application code
print(recipe_management_code)",train
peft,38,"Develop a Python program using the 'peft' API to generate code for a basic file management application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a file management application that allows users to upload, organize, and share files and documents.",code/peft/peft_38.py,Test that the generated file management application code enables users to manage their digital files effectively.,"Test that the program can generate code for a file management application that allows users to upload, organize, and share files and documents.",Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a file management application
file_management_code = model.generate_file_management_code(max_length=500)

# Print generated file management application code
print(file_management_code)",train
peft,4,Develop a Python program that uses the 'peft' API to answer questions based on a given passage. Utilize the 'ahotrod/eleutherai-turboqa' model for question-answering. The program should take a passage and a set of questions as input and provide corresponding answers.,code/peft/peft_4.py,Test that the program can accurately answer a variety of questions based on the provided passage.,Test that the 'ahotrod/eleutherai-turboqa' model and peft model are successfully loaded.,"Test that the program handles different passages and questions, providing correct answers.",,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForQuestionAnswering
from peft import PeftModel

# Load Models
base_model = ""ahotrod/eleutherai-turboqa""
peft_model = ""ahotrod/eleutherai-turboqa""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForQuestionAnswering.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Input passage and questions
passage = ""The Eiffel Tower is a famous landmark in Paris, France. It was built in the late 19th century and has become an iconic symbol of the city. The tower stands at 324 meters and was the tallest man-made structure in the world when it was completed.""
questions = [""When was the Eiffel Tower built?"", ""How tall is the Eiffel Tower?""]

# Answer questions
answers = []
for question in questions:
    encoded_question = tokenizer(question, return_tensors=""pt"")
    answer = model.answer_question(encoded_question, passage)
    answers.append(answer[""answer""])

# Print answers
for i, question in enumerate(questions):
    print(f""Question: {question}"")
    print(f""Answer: {answers[i]}"")",train
peft,37,"Create a Python program that uses the 'peft' API to generate code for a basic event management application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for an event management application that allows users to create, organize, and promote events.",code/peft/peft_37.py,Test that the generated event management application code enables users to manage and publicize their events.,Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,"Test that the program can generate code for an event management application that allows users to create, organize, and promote events.",,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for an event management application
event_management_code = model.generate_event_management_code(max_length=500)

# Print generated event management application code
print(event_management_code)",train
peft,24,Develop a Python program using the 'peft' API to generate code for a basic recipe recommendation application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a recipe recommendation application that suggests recipes based on user preferences and dietary restrictions.,code/peft/peft_24.py,Test that the generated recipe recommendation application code provides relevant recipe suggestions to users.,Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,Test that the program can generate code for a recipe recommendation application that suggests recipes based on user preferences and dietary restrictions.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a recipe recommendation application
recipe_app_code = model.generate_recipe_app_code(max_length=500)

# Print generated recipe recommendation application code
print(recipe_app_code)",train
peft,34,Develop a Python program using the 'peft' API to generate code for a basic personal journal application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a personal journal application that allows users to create and organize journal entries.,code/peft/peft_34.py,Test that the program can generate code for a personal journal application that allows users to create and organize journal entries.,Test that the generated personal journal application code enables users to maintain their digital journals effectively.,Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a personal journal application
journal_app_code = model.generate_personal_journal_code(max_length=500)

# Print generated personal journal application code
print(journal_app_code)",train
peft,11,Create a Python program that uses the 'peft' API to generate code for a simple chatbot. Utilize the 'gpt-3.5-turbo' model for chatbot code generation. The program should generate code that enables a chatbot to answer basic questions and engage in a conversation.,code/peft/peft_11.py,Test that the program can generate code for a chatbot capable of answering basic questions and engaging in conversation.,Test that the generated code allows the chatbot to provide meaningful responses in a conversation.,Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a chatbot
chatbot_code = model.generate_chatbot_code(max_length=500)

# Print generated code
print(chatbot_code)",train
peft,23,Create a Python program that uses the 'peft' API to generate code for a basic fitness tracking application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a fitness tracking application that allows users to log workouts and track their fitness progress.,code/peft/peft_23.py,Test that the program can generate code for a fitness tracking application that allows users to log workouts and track their fitness progress.,Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,Test that the generated fitness tracking application code enables users to record their fitness activities and monitor progress.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a fitness tracking application
fitness_app_code = model.generate_fitness_app_code(max_length=500)

# Print generated fitness tracking application code
print(fitness_app_code)",train
peft,19,"Create a Python program that uses the 'peft' API to generate code for a task management application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a task management application that allows users to create, assign, and track tasks.",code/peft/peft_19.py,"Test that the program can generate code for a task management application that allows users to create, assign, and track tasks.",Test that the generated task management application code enables users to manage tasks effectively.,Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a task management application
task_management_app_code = model.generate_task_management_app_code(max_length=500)

# Print generated task management application code
print(task_management_app_code)",train
peft,21,Create a Python program that uses the 'peft' API to generate code for a basic calendar application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a calendar application that allows users to schedule events and view their calendars.,code/peft/peft_21.py,Test that the program can generate code for a calendar application that allows users to schedule events and view their calendars.,Test that the generated calendar application code enables users to manage their schedules.,Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a calendar application
calendar_app_code = model.generate_calendar_app_code(max_length=500)

# Print generated calendar application code
print(calendar_app_code)",train
peft,8,"Develop a Python program using the 'peft' API to extract key information from a set of text documents. Utilize the 'dbmdz/bert-large-cased-finetuned-conll03-english' model for named entity recognition. The program should accept a list of text documents as input and identify and extract entities like names, organizations, and dates.",code/peft/peft_8.py,Test that the 'dbmdz/bert-large-cased-finetuned-conll03-english' model and peft model are successfully loaded.,Test that the program handles various text documents and identifies relevant entities.,Test that the program can accurately extract entities from different text documents.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForTokenClassification
from peft import PeftModel

# Load Models
base_model = ""dbmdz/bert-large-cased-finetuned-conll03-english""
peft_model = ""dbmdz/bert-large-cased-finetuned-conll03-english""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForTokenClassification.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# List of text documents
text_documents = [""Apple Inc. is planning to open a new store in Paris on January 15, 2023."", ""John Smith, the CEO of XYZ Corporation, will be attending the conference in London next week.""]

# Extract entities
entities = []
for document in text_documents:
    encoded_document = tokenizer(document, return_tensors=""pt"")
    entity = model.extract_entities(encoded_document)
    entities.append(entity)

# Print extracted entities
for i, document in enumerate(text_documents):
    print(f""Document: {document}"")
    print(f""Extracted Entities: {entities[i]}"")",train
peft,15,"Create a Python program that uses the 'peft' API to generate code for a social media platform. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a simplified social media platform with user profiles, posts, likes, and comments functionality.",code/peft/peft_15.py,"Test that the generated social media code allows users to create profiles, make posts, and interact with other users.","Test that the program can generate code for a simplified social media platform with user profiles, posts, likes, and comments functionality.",Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a simplified social media platform
social_media_code = model.generate_social_media_code(max_length=500)

# Print generated social media platform code
print(social_media_code)",train
peft,29,Create a Python program that utilizes the 'peft' API to generate code for a basic weather forecast application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a weather forecast application that provides weather information for a specific location.,code/peft/peft_29.py,Test that the program can generate code for a weather forecast application that provides weather information for a specific location.,Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,Test that the generated weather forecast application code delivers accurate weather forecasts.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a weather forecast application
weather_forecast_code = model.generate_weather_forecast_code(max_length=500)

# Print generated weather forecast application code
print(weather_forecast_code)",train
peft,39,Create a Python program that utilizes the 'peft' API to generate code for a basic home automation application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a home automation application that allows users to control smart devices and automate home routines.,code/peft/peft_39.py,Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded .,Test that the generated home automation application code effectively manages and automates home devices.,Test that the program can generate code for a home automation application that allows users to control smart devices and automate home routines.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a home automation application
home_automation_code = model.generate_home_automation_code(max_length=500)

# Print generated home automation application code
print(home_automation_code)",train
peft,25,Create a Python program that utilizes the 'peft' API to generate code for a basic language translation application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a language translation application that can translate text between different languages.,code/peft/peft_25.py,Test that the program can generate code for a language translation application that can translate text between different languages.,Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,Test that the generated translation application code accurately translates text between various languages.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a language translation application
translation_app_code = model.generate_translation_app_code(max_length=500)

# Print generated language translation application code
print(translation_app_code)",test
peft,14,"Develop a Python program using the 'peft' API to generate code for a basic e-commerce website. Utilize the 'gpt-3.5-turbo' model for website code generation. The program should generate code for a simple e-commerce website with product listings, a shopping cart, and checkout functionality.",code/peft/peft_14.py,"Test that the program can generate code for a basic e-commerce website with product listings, a shopping cart, and checkout functionality.",Test that the generated website code allows users to browse products and make purchases.,Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a basic e-commerce website
ecommerce_website_code = model.generate_ecommerce_website_code(max_length=500)

# Print generated e-commerce website code
print(ecommerce_website_code)",test
peft,9,Create a Python program that utilizes the 'peft' API to generate SQL queries based on a given description of a database query. Use the 'gpt2' model for SQL query generation. The program should accept high-level descriptions of queries and produce SQL queries that retrieve the requested data from a database.,code/peft/peft_9.py,Test that the 'gpt2' model and peft model are successfully loaded.,Test that the program produces SQL queries that retrieve the requested data from a database.,Test that the program can generate SQL queries based on different high-level descriptions of queries.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt2""
peft_model = ""gpt2""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Query description
query_description = ""Retrieve all orders placed by customer 'John Smith' in the year 2022.""

# Generate SQL query
generated_query = model.generate_sql_query(query_description, max_length=200)

# Print generated SQL query
print(generated_query)",test
peft,26,"Develop a Python program using the 'peft' API to generate code for a basic note-taking application. Utilize the 'gpt-3.5-turbo' model for code generation. The program should generate code for a note-taking application that allows users to create, edit, and organize notes.",code/peft/peft_26.py,"Test that the program can generate code for a note-taking application that allows users to create, edit, and organize notes.",Test that the 'gpt-3.5-turbo' model and peft model are successfully loaded.,Test that the generated note-taking application code enables users to manage their notes effectively.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt-3.5-turbo""
peft_model = ""gpt-3.5-turbo""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Generate code for a note-taking application
note_app_code = model.generate_note_app_code(max_length=500)

# Print generated note-taking application code
print(note_app_code)",test
peft,5,Create a Python program using the 'peft' API to generate text in the style of William Shakespeare. Utilize the 'gpt2' model for text generation. The program should take a starting phrase and generate a coherent and Shakespearean-style text.,code/peft/peft_5.py,Test that the program can generate coherent and Shakespearean-style text based on different starting phrases.,Test that the 'gpt2' model and peft model are successfully loaded.,Test that the program can handle various starting phrases and produce meaningful text.,,,"# Install necessary libraries
#!pip install transformers
#!pip install peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load Models
base_model = ""gpt2""
peft_model = ""gpt2""
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# Load peft
model = PeftModel.from_pretrained(model, peft_model)
model = model.eval()

# Starting phrase
starting_phrase = ""To be or not to be, that is the""

# Generate text
generated_text = model.generate_text(starting_phrase, max_length=200)

# Print generated text
print(generated_text)",test
plotly,16,"Create a Python program using the 'plotly' API to generate a polar chart. The program should define data points for the theta and r axes, create the chart, customize the chart title, and display the data points on the chart. Finally, save the chart as a PDF file and display it on the screen.",code/plotly/plotly_16.py,Ensure that the program saves the chart as a PDF file and displays it on the screen without errors.,Verify that the program creates a polar chart with the specified title and displays the data points on the chart.,Test if the program correctly defines data points for the theta and r axes.,,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the theta and r axes
theta = [0, 45, 90, 135, 180, 225, 270, 315]
r = [1, 2, 3, 4, 5, 4, 3, 2]

# Create a polar chart using Plotly Graph Objects
fig = go.Figure(data=go.Scatterpolar(r=r, theta=theta, fill='toself'))
fig.update_layout(title=""Polar Chart"")

# Save the chart as a PDF file
fig.write_image(""polar_chart.pdf"")

# Display the chart on the screen
fig.show()
",train
plotly,31,"Create a Python program using the 'plotly' API to generate a donut chart. The program should define data points for the labels and values, create the chart, customize the chart title, and display the percentage values on the chart. Finally, save the chart as a PNG image file and display it on the screen.",code/plotly/plotly_31.py,Ensure that the program saves the chart as a PNG image file and displays it on the screen without errors.,Test if the program correctly defines data points for the labels and values.,Verify that the program creates a donut chart with the specified title and displays the percentage values on the chart.,,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the labels and values
labels = ['A', 'B', 'C', 'D', 'E']
values = [10, 16, 5, 11, 8]

# Create a donut chart using Plotly Graph Objects
fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3, title=""Donut Chart"")])

# Customize the chart title and display the percentage values on the chart
fig.update_layout(title=""Donut Chart"", showlegend=False)
fig.update_traces(textposition='inside', textinfo='percent+label')

# Save the chart as a PNG image file
fig.write_image(""donut_chart.png"")

# Display the chart on the screen
fig.show()
",train
plotly,21,"Create a Python program using the 'plotly' API to generate a treemap. The program should define data points for the labels, parents, and values, create the chart, customize the chart title, and display the data points on the chart. Finally, save the chart as a PNG image file and display it on the screen.",code/plotly/plotly_21.py,Ensure that the program saves the chart as a PNG image file and displays it on the screen without errors.,Verify that the program creates a treemap with the specified title and displays the data points on the chart.,"Test if the program correctly defines data points for the labels, parents, and values.",,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the labels, parents, and values
labels = [""A"", ""B"", ""C"", ""D"", ""E"", ""F"", ""G"", ""H""]
parents = ["""", """", """", """", """", ""A"", ""A"", ""B""]
values = [10, 5, 7, 3, 9, 2, 4, 6]

# Create a treemap using Plotly Graph Objects
fig = go.Figure(go.Treemap(labels=labels, parents=parents, values=values))
fig.update_layout(title=""Treemap"")

# Save the chart as a PNG image file
fig.write_image(""treemap.png"")

# Display the chart on the screen
fig.show()
",train
plotly,13,"Create a Python program using the 'plotly' API to generate a radar chart. The program should define data points for the categories and values, create the chart, customize the chart title, and display the data points on the chart. Finally, save the chart as a PDF file and display it on the screen.",code/plotly/plotly_13.py,Test if the program correctly defines data points for the categories and values.,Ensure that the program saves the chart as a PDF file and displays it on the screen without errors.,Verify that the program creates a radar chart with the specified title and displays the data points on the chart.,,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the categories and values
categories = ['A', 'B', 'C', 'D', 'E']
values = [10, 16, 5, 11, 8]

# Create a radar chart using Plotly Graph Objects
fig = go.Figure(data=go.Scatterpolar(r=values, theta=categories, fill='toself'))
fig.update_layout(title=""Radar Chart"")

# Save the chart as a PDF file
fig.write_image(""radar_chart.pdf"")

# Display the chart on the screen
fig.show()
",train
plotly,9,"Create a Python program using the 'plotly' API to generate a histogram. The program should define data points for the x axis, create the histogram, customize the histogram title, X-axis label, and Y-axis label. Finally, save the histogram as a JPEG image file and display it on the screen.",code/plotly/plotly_9.py,Ensure that the program saves the histogram as a JPEG image file and displays it on the screen without errors.,Test if the program correctly defines data points for the x axis.,"Verify that the program creates a histogram with the specified title, X-axis label, and Y-axis label.",,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x axis
x = [1, 2, 3, 4, 5, 5, 4, 3, 2, 1]

# Create a histogram using Plotly Graph Objects
fig = go.Figure(data=[go.Histogram(x=x)])
fig.update_layout(title=""Histogram"", xaxis_title=""X-axis"", yaxis_title=""Y-axis"")

# Save the histogram as a JPEG image file
fig.write_image(""histogram.jpeg"")

# Display the histogram on the screen
fig.show()
",train
plotly,17,"Create a Python program using the 'plotly' API to generate a waterfall chart. The program should define data points for the x axis and multiple sets of data for the y axis, create the chart, customize the chart title, X-axis label, and Y-axis label. Finally, save the chart as a PNG image file and display it on the screen.",code/plotly/plotly_17.py,"Verify that the program creates a waterfall chart with the specified title, X-axis label, and Y-axis label.",Ensure that the program saves the chart as a PNG image file and displays it on the screen without errors.,Test if the program correctly defines data points for the x axis.,,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x axis
x = ['A', 'B', 'C', 'D', 'E']

# Define multiple sets of data for the y axis
y1 = [10, 16, 5, 11, 8]
y2 = [5, 8, 3, 6, 9]
y3 = [7, 12, 4, 9, 6]

# Create a waterfall chart using Plotly Graph Objects
fig = go.Figure()
fig.add_trace(go.Waterfall(x=x, y=y1, name='Category 1'))
fig.add_trace(go.Waterfall(x=x, y=y2, name='Category 2'))
fig.add_trace(go.Waterfall(x=x, y=y3, name='Category 3'))
fig.update_layout(title=""Waterfall Chart"", xaxis_title=""X-axis"", yaxis_title=""Y-axis"")

# Save the chart as a PNG image file
fig.write_image(""waterfall_chart.png"")

# Display the chart on the screen
fig.show()
",train
plotly,10,"Create a Python program using the 'plotly' API to generate a line chart with error bars. The program should define data points for the x and y axes, as well as the error values for each data point. The program should create the chart, customize the chart title, X-axis label, and Y-axis label, and display the error bars on the chart. Finally, save the chart as a PDF file and display it on the screen.",code/plotly/plotly_10.py,Ensure that the program saves the chart as a PDF file and displays it on the screen without errors.,Test if the program correctly defines data points for the x and y axes.,"Verify that the program creates a line chart with error bars and displays the specified title, X-axis label, and Y-axis label.",,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x and y axes
x = [1, 2, 3, 4, 5]
y = [10, 16, 5, 11, 8]

# Define error values for each data point
error = [1, 2, 0.5, 1.5, 1]

# Create a line chart with error bars using Plotly Graph Objects
fig = go.Figure(data=go.Scatter(x=x, y=y, mode='lines+markers', error_y=dict(type='data', array=error, visible=True)))
fig.update_layout(title=""Line Chart with Error Bars"", xaxis_title=""X-axis"", yaxis_title=""Y-axis"")

# Save the chart as a PDF file
fig.write_image(""line_chart_with_error_bars.pdf"")

# Display the chart on the screen
fig.show()
",train
plotly,35,"Create a Python program using the 'plotly' API to generate a scatter plot with a trendline and error bars. The program should define data points for the x and y axes, as well as the error values for each data point. The program should create the scatter plot with a trendline and error bars, customize the chart title, X-axis label, and Y-axis label, and display the trendline and error bars on the chart. Finally, save the chart as a PDF file and display it on the screen.",code/plotly/plotly_35.py,Ensure that the program saves the chart as a PDF file and displays it on the screen without errors.,Test if the program correctly defines data points for the x and y axes.,"Verify that the program creates a scatter plot with a trendline and error bars and displays the specified title, X-axis label, and Y-axis label.",,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x and y axes
x = [1, 2, 3, 4, 5]
y = [10, 16, 5, 11, 8]

# Define error values for each data point
error = [1, 2, 0.5, 1.5, 1]

# Create a scatter plot with a trendline and error bars using Plotly Graph Objects
fig = go.Figure()
fig.add_trace(go.Scatter(x=x, y=y, mode='markers', name='Data'))
fig.add_trace(go.Scatter(x=x, y=y, mode='lines', name='Trendline'))
fig.add_trace(go.Scatter(x=x, y=y, mode='lines', name='Upper Error', line=dict(color='red', dash='dash')))
fig.add_trace(go.Scatter(x=x, y=y, mode='lines', name='Lower Error', line=dict(color='red', dash='dash')))
fig.update_layout(title=""Scatter Plot with Trendline and Error Bars"", xaxis_title=""X-axis"", yaxis_title=""Y-axis"")

# Save the chart as a PDF file
fig.write_image(""scatter_plot_with_trendline_and_error_bars.pdf"")

# Display the chart on the screen
fig.show()
",train
plotly,1,"Create a Python program using the 'plotly' API to generate an interactive line chart. The program should define data points for the x and y axes, create the chart, customize the chart title, X-axis label, and Y-axis label. Finally, save the chart as an HTML file and display it on the screen.",code/plotly/plotly_1.py,"Verify that the program creates an interactive line chart with the specified title, X-axis label, and Y-axis label.",Ensure that the program saves the chart as an HTML file and displays it on the screen without errors.,Test if the program correctly defines data points for the x and y axes.,,,"#!pip install plotly
import plotly.express as px

# Define data points for the x and y axes
x = [1, 2, 3, 4, 5]
y = [10, 16, 5, 11, 8]

# Create an interactive line chart using Plotly Express
fig = px.line(x=x, y=y, title=""Interactive Line Chart"")
fig.update_xaxes(title=""X-axis"")
fig.update_yaxes(title=""Y-axis"")

# Save the chart to an HTML file
fig.write_html(""interactive_line_chart.html"")

# Display the chart on the screen
fig.show()
",train
plotly,5,"Create a Python program using the 'plotly' API to generate a 3D scatter plot. The program should define data points for the x, y, and z axes, create the plot, customize the plot title, X-axis label, Y-axis label, and Z-axis label. Finally, save the plot as an SVG file and display it on the screen.",code/plotly/plotly_5.py,"Verify that the program creates a 3D scatter plot with the specified title, X-axis label, Y-axis label, and Z-axis label.",Ensure that the program saves the plot as an SVG file and displays it on the screen without errors.,"Test if the program correctly defines data points for the x, y, and z axes.",,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x, y, and z axes
x = [1, 2, 3, 4, 5]
y = [10, 16, 5, 11, 8]
z = [7, 3, 9, 5, 2]

# Create a 3D scatter plot using Plotly Graph Objects
fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z, mode='markers')])
fig.update_layout(title=""3D Scatter Plot"", scene=dict(xaxis_title=""X-axis"", yaxis_title=""Y-axis"", zaxis_title=""Z-axis""))

# Save the plot as an SVG file
fig.write_image(""3d_scatter_plot.svg"")

# Display the plot on the screen
fig.show()
",train
plotly,18,"Create a Python program using the 'plotly' API to generate a sunburst chart. The program should define data points for the labels, parents, and values, create the chart, customize the chart title, and display the data points on the chart. Finally, save the chart as an SVG file and display it on the screen.",code/plotly/plotly_18.py,Ensure that the program saves the chart as an SVG file and displays it on the screen without errors.,Verify that the program creates a sunburst chart with the specified title and displays the data points on the chart.,"Test if the program correctly defines data points for the labels, parents, and values.",,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the labels, parents, and values
labels = [""A"", ""B"", ""C"", ""D"", ""E"", ""F"", ""G"", ""H""]
parents = ["""", """", """", """", """", ""A"", ""A"", ""B""]
values = [10, 5, 7, 3, 9, 2, 4, 6]

# Create a sunburst chart using Plotly Graph Objects
fig = go.Figure(go.Sunburst(labels=labels, parents=parents, values=values))
fig.update_layout(title=""Sunburst Chart"")

# Save the chart as an SVG file
fig.write_image(""sunburst_chart.svg"")

# Display the chart on the screen
fig.show()
",train
plotly,29,"Create a Python program using the 'plotly' API to generate a line chart with multiple lines. The program should define data points for the x axis, create multiple sets of data for the y axis, create the chart with multiple lines, customize the chart title, X-axis label, and Y-axis label. Finally, save the chart as a JPEG image file and display it on the screen.",code/plotly/plotly_29.py,Ensure that the program saves the chart as a JPEG image file and displays it on the screen without errors.,"Verify that the program creates a line chart with multiple lines and displays the specified title, X-axis label, and Y-axis label.",Test if the program correctly defines data points for the x axis.,,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x axis
x = ['A', 'B', 'C', 'D', 'E']

# Define multiple sets of data for the y axis
y1 = [10, 16, 5, 11, 8]
y2 = [5, 8, 3, 6, 9]
y3 = [7, 12, 4, 9, 6]

# Create a line chart with multiple lines using Plotly Graph Objects
fig = go.Figure()
fig.add_trace(go.Scatter(x=x, y=y1, mode='lines', name='Line 1'))
fig.add_trace(go.Scatter(x=x, y=y2, mode='lines', name='Line 2'))
fig.add_trace(go.Scatter(x=x, y=y3, mode='lines', name='Line 3'))
fig.update_layout(title=""Line Chart with Multiple Lines"", xaxis_title=""X-axis"", yaxis_title=""Y-axis"")

# Save the chart as a JPEG image file
fig.write_image(""line_chart_with_multiple_lines.jpeg"")

# Display the chart on the screen
fig.show()
",train
plotly,6,"Create a Python program using the 'plotly' API to generate a stacked bar chart. The program should define data points for the x axis, create multiple sets of data for the y axis, create the chart, customize the chart title, X-axis label, and Y-axis label. Finally, save the chart as a JPEG image file and display it on the screen.",code/plotly/plotly_6.py,"Verify that the program creates a stacked bar chart with the specified title, X-axis label, and Y-axis label.",Ensure that the program saves the chart as a JPEG image file and displays it on the screen without errors.,Test if the program correctly defines data points for the x axis.,,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x axis
x = ['A', 'B', 'C', 'D', 'E']

# Define multiple sets of data for the y axis
y1 = [10, 16, 5, 11, 8]
y2 = [5, 8, 3, 6, 9]
y3 = [7, 12, 4, 9, 6]

# Create a stacked bar chart using Plotly Graph Objects
fig = go.Figure()
fig.add_trace(go.Bar(x=x, y=y1, name='Category 1'))
fig.add_trace(go.Bar(x=x, y=y2, name='Category 2'))
fig.add_trace(go.Bar(x=x, y=y3, name='Category 3'))
fig.update_layout(barmode='stack', title=""Stacked Bar Chart"", xaxis_title=""X-axis"", yaxis_title=""Y-axis"")

# Save the chart as a JPEG image file
fig.write_image(""stacked_bar_chart.jpeg"")

# Display the chart on the screen
fig.show()
",train
plotly,12,"Create a Python program using the 'plotly' API to generate a violin plot. The program should define data points for the x axis and multiple sets of data for the y axis, create the plot, customize the plot title, X-axis label, and Y-axis label. Finally, save the plot as a PNG image file and display it on the screen.",code/plotly/plotly_12.py,Test if the program correctly defines data points for the x axis.,Ensure that the program saves the plot as a PNG image file and displays it on the screen without errors.,"Verify that the program creates a violin plot with the specified title, X-axis label, and Y-axis label.",,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x axis
x = ['A', 'B', 'C', 'D', 'E']

# Define multiple sets of data for the y axis
y1 = [10, 16, 5, 11, 8]
y2 = [5, 8, 3, 6, 9]
y3 = [7, 12, 4, 9, 6]

# Create a violin plot using Plotly Graph Objects
fig = go.Figure()
fig.add_trace(go.Violin(y=y1, name='Category 1'))
fig.add_trace(go.Violin(y=y2, name='Category 2'))
fig.add_trace(go.Violin(y=y3, name='Category 3'))
fig.update_layout(title=""Violin Plot"", xaxis_title=""X-axis"", yaxis_title=""Y-axis"")

# Save the plot as a PNG image file
fig.write_image(""violin_plot.png"")

# Display the plot on the screen
fig.show()
",train
plotly,2,"Create a Python program using the 'plotly' API to generate a bar chart. The program should define data points for the x and y axes, create the chart, customize the chart title, X-axis label, and Y-axis label. Finally, save the chart as an image file and display it on the screen.",code/plotly/plotly_2.py,"Verify that the program creates a bar chart with the specified title, X-axis label, and Y-axis label.",Test if the program correctly defines data points for the x and y axes.,Ensure that the program saves the chart as an image file and displays it on the screen without errors.,,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x and y axes
x = ['A', 'B', 'C', 'D', 'E']
y = [10, 16, 5, 11, 8]

# Create a bar chart using Plotly Graph Objects
fig = go.Figure(data=[go.Bar(x=x, y=y)])
fig.update_layout(title=""Bar Chart"", xaxis_title=""X-axis"", yaxis_title=""Y-axis"")

# Save the chart as an image file
fig.write_image(""bar_chart.png"")

# Display the chart on the screen
fig.show()
",train
plotly,3,"Create a Python program using the 'plotly' API to generate a scatter plot. The program should define data points for the x and y axes, create the plot, customize the plot title, X-axis label, and Y-axis label. Finally, save the plot as a PDF file and display it on the screen.",code/plotly/plotly_3.py,"Verify that the program creates a scatter plot with the specified title, X-axis label, and Y-axis label.",Ensure that the program saves the plot as a PDF file and displays it on the screen without errors.,Test if the program correctly defines data points for the x and y axes.,,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x and y axes
x = [1, 2, 3, 4, 5]
y = [10, 16, 5, 11, 8]

# Create a scatter plot using Plotly Graph Objects
fig = go.Figure(data=go.Scatter(x=x, y=y, mode='markers'))
fig.update_layout(title=""Scatter Plot"", xaxis_title=""X-axis"", yaxis_title=""Y-axis"")

# Save the plot as a PDF file
fig.write_image(""scatter_plot.pdf"")

# Display the plot on the screen
fig.show()
",train
plotly,34,"Create a Python program using the 'plotly' API to generate a 3D surface plot with color scale. The program should define data points for the x, y, and z axes, as well as the color values for each data point. The program should create the 3D surface plot with a color scale, customize the chart title, X-axis label, Y-axis label, and Z-axis label, and display the color scale on the chart. Finally, save the chart as an SVG file and display it on the screen.",code/plotly/plotly_34.py,Ensure that the program saves the chart as an SVG file and displays it on the screen without errors.,"Verify that the program creates a 3D surface plot with a color scale and displays the specified title, X-axis label, Y-axis label, and Z-axis label.","Test if the program correctly defines data points for the x, y, and z axes.",,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x, y, and z axes
x = [1, 2, 3, 4, 5]
y = [10, 16, 5, 11, 8]
z = [[7, 3, 9, 5, 2],
     [4, 8, 2, 6, 10],
     [9, 6, 11, 7, 4],
     [5, 3, 8, 4, 9],
     [2, 7, 4, 10, 6]]

# Define color values for each data point
color = [[0, 0, 0, 0, 0],
         [1, 1, 1, 1, 1],
         [2, 2, 2, 2, 2],
         [3, 3, 3, 3, 3],
         [4, 4, 4, 4, 4]]

# Create a 3D surface plot with color scale using Plotly Graph Objects
fig = go.Figure(data=go.Surface(x=x, y=y, z=z, colorscale='Viridis', surfacecolor=color))
fig.update_layout(title=""3D Surface Plot with Color Scale"", scene=dict(xaxis_title=""X-axis"", yaxis_title=""Y-axis"", zaxis_title=""Z-axis""))

# Save the chart as an SVG file
fig.write_image(""3d_surface_plot_with_color_scale.svg"")

# Display the chart on the screen
fig.show()
",train
plotly,4,"Create a Python program using the 'plotly' API to generate a pie chart. The program should define data points for the labels and values, create the chart, customize the chart title, and display the percentage values on the chart. Finally, save the chart as a PNG image file and display it on the screen.",code/plotly/plotly_4.py,Ensure that the program saves the chart as a PNG image file and displays it on the screen without errors.,Test if the program correctly defines data points for the labels and values.,Verify that the program creates a pie chart with the specified title and displays the percentage values on the chart.,,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the labels and values
labels = ['A', 'B', 'C', 'D', 'E']
values = [10, 16, 5, 11, 8]

# Create a pie chart using Plotly Graph Objects
fig = go.Figure(data=[go.Pie(labels=labels, values=values, textinfo='percent', title=""Pie Chart"")])

# Customize the chart title and display the percentage values on the chart
fig.update_layout(title=""Pie Chart"", showlegend=False)
fig.update_traces(textposition='inside', textinfo='percent+label')

# Save the chart as a PNG image file
fig.write_image(""pie_chart.png"")

# Display the chart on the screen
fig.show()
",train
plotly,32,"Create a Python program using the 'plotly' API to generate a scatter plot with a color scale and size scale. The program should define data points for the x and y axes, as well as the color and size values for each data point. The program should create the scatter plot with a color scale and size scale, customize the chart title, X-axis label, and Y-axis label, and display the color scale and size scale on the chart. Finally, save the chart as a PDF file and display it on the screen.",code/plotly/plotly_32.py,Ensure that the program saves the chart as a PDF file and displays it on the screen without errors.,Test if the program correctly defines data points for the x and y axes.,"Verify that the program creates a scatter plot with a color scale and size scale and displays the specified title, X-axis label, and Y-axis label.",,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x and y axes
x = [1, 2, 3, 4, 5]
y = [10, 16, 5, 11, 8]

# Define color and size values for each data point
color = [1, 2, 3, 4, 5]
size = [10, 20, 30, 40, 50]

# Create a scatter plot with a color scale and size scale using Plotly Graph Objects
fig = go.Figure(data=go.Scatter(x=x, y=y, mode='markers', marker=dict(color=color, colorscale='Viridis', showscale=True, size=size, sizemode='diameter')))
fig.update_layout(title=""Scatter Plot with Color Scale and Size Scale"", xaxis_title=""X-axis"", yaxis_title=""Y-axis"")

# Save the chart as a PDF file
fig.write_image(""scatter_plot_with_color_and_size_scale.pdf"")

# Display the chart on the screen
fig.show()
",train
plotly,25,"Create a Python program using the 'plotly' API to generate a stacked area chart with a logarithmic y-axis. The program should define data points for the x axis, create multiple sets of data for the y axis, create the chart with a logarithmic y-axis, customize the chart title, X-axis label, and Y-axis label. Finally, save the chart as a JPEG image file and display it on the screen.",code/plotly/plotly_25.py,"Verify that the program creates a stacked area chart with a logarithmic y-axis and displays the specified title, X-axis label, and Y-axis label.",Ensure that the program saves the chart as a JPEG image file and displays it on the screen without errors.,Test if the program correctly defines data points for the x axis.,,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x axis
x = ['A', 'B', 'C', 'D', 'E']

# Define multiple sets of data for the y axis
y1 = [10, 16, 5, 11, 8]
y2 = [5, 8, 3, 6, 9]
y3 = [7, 12, 4, 9, 6]

# Create a stacked area chart with a logarithmic y-axis using Plotly Graph Objects
fig = go.Figure()
fig.add_trace(go.Scatter(x=x, y=y1, mode='lines', stackgroup='one', name='Category 1'))
fig.add_trace(go.Scatter(x=x, y=y2, mode='lines', stackgroup='one', name='Category 2'))
fig.add_trace(go.Scatter(x=x, y=y3, mode='lines', stackgroup='one', name='Category 3'))
fig.update_layout(title=""Stacked Area Chart with Logarithmic Y-axis"", xaxis_title=""X-axis"", yaxis_title=""Y-axis"", yaxis_type=""log"")

# Save the chart as a JPEG image file
fig.write_image(""stacked_area_chart_log_y.jpeg"")

# Display the chart on the screen
fig.show()
",train
plotly,33,"Create a Python program using the 'plotly' API to generate a stacked scatter plot. The program should define data points for the x and y axes, as well as the z values for each data point. The program should create the scatter plot with stacked markers, customize the chart title, X-axis label, and Y-axis label, and display the z values on the markers. Finally, save the chart as a JPEG image file and display it on the screen.",code/plotly/plotly_33.py,"Verify that the program creates a stacked scatter plot with the specified title, X-axis label, and Y-axis label, and displays the z values on the markers.",Ensure that the program saves the chart as a JPEG image file and displays it on the screen without errors.,Test if the program correctly defines data points for the x and y axes.,,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x and y axes
x = [1, 2, 3, 4, 5]
y = [10, 16, 5, 11, 8]

# Define z values for each data point
z = [1, 2, 3, 4, 5]

# Create a stacked scatter plot using Plotly Graph Objects
fig = go.Figure(data=go.Scatter(x=x, y=y, mode='markers', marker=dict(size=z, sizemode='area', sizeref=0.1, line=dict(width=1, color='black')), hovertemplate='x: %{x}<br>y: %{y}<br>z: %{marker.size}'))

# Customize the chart title, X-axis label, and Y-axis label
fig.update_layout(title=""Stacked Scatter Plot"", xaxis_title=""X-axis"", yaxis_title=""Y-axis"")

# Save the chart as a JPEG image file
fig.write_image(""stacked_scatter_plot.jpeg"")

# Display the chart on the screen
fig.show()
",train
plotly,11,"Create a Python program using the 'plotly' API to generate a stacked area chart. The program should define data points for the x axis, create multiple sets of data for the y axis, create the chart, customize the chart title, X-axis label, and Y-axis label. Finally, save the chart as an SVG file and display it on the screen.",code/plotly/plotly_11.py,Ensure that the program saves the chart as an SVG file and displays it on the screen without errors.,"Verify that the program creates a stacked area chart with the specified title, X-axis label, and Y-axis label.",Test if the program correctly defines data points for the x axis.,,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x axis
x = ['A', 'B', 'C', 'D', 'E']

# Define multiple sets of data for the y axis
y1 = [10, 16, 5, 11, 8]
y2 = [5, 8, 3, 6, 9]
y3 = [7, 12, 4, 9, 6]

# Create a stacked area chart using Plotly Graph Objects
fig = go.Figure()
fig.add_trace(go.Scatter(x=x, y=y1, mode='lines', stackgroup='one', name='Category 1'))
fig.add_trace(go.Scatter(x=x, y=y2, mode='lines', stackgroup='one', name='Category 2'))
fig.add_trace(go.Scatter(x=x, y=y3, mode='lines', stackgroup='one', name='Category 3'))
fig.update_layout(title=""Stacked Area Chart"", xaxis_title=""X-axis"", yaxis_title=""Y-axis"")

# Save the chart as an SVG file
fig.write_image(""stacked_area_chart.svg"")

# Display the chart on the screen
fig.show()
",train
plotly,24,"Create a Python program using the 'plotly' API to generate a scatter plot with a trendline. The program should define data points for the x and y axes, create the scatter plot with a trendline, customize the chart title, X-axis label, and Y-axis label. Finally, save the chart as a JPEG image file and display it on the screen.",code/plotly/plotly_24.py,"Verify that the program creates a scatter plot with a trendline and displays the specified title, X-axis label, and Y-axis label.",Ensure that the program saves the chart as a JPEG image file and displays it on the screen without errors.,Test if the program correctly defines data points for the x and y axes.,,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x and y axes
x = [1, 2, 3, 4, 5]
y = [10, 16, 5, 11, 8]

# Create a scatter plot with a trendline using Plotly Graph Objects
fig = go.Figure()
fig.add_trace(go.Scatter(x=x, y=y, mode='markers', name='Data'))
fig.add_trace(go.Scatter(x=x, y=y, mode='lines', name='Trendline'))
fig.update_layout(title=""Scatter Plot with Trendline"", xaxis_title=""X-axis"", yaxis_title=""Y-axis"")

# Save the chart as a JPEG image file
fig.write_image(""scatter_plot_with_trendline.jpeg"")

# Display the chart on the screen
fig.show()
",train
plotly,20,"Create a Python program using the 'plotly' API to generate a 3D surface plot. The program should define data points for the x, y, and z axes, create the plot, customize the plot title, X-axis label, Y-axis label, and Z-axis label. Finally, save the plot as an SVG file and display it on the screen.",code/plotly/plotly_20.py,Ensure that the program saves the plot as an SVG file and displays it on the screen without errors.,"Verify that the program creates a 3D surface plot with the specified title, X-axis label, Y-axis label, and Z-axis label.","Test if the program correctly defines data points for the x, y, and z axes.",,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x, y, and z axes
x = [1, 2, 3, 4, 5]
y = [10, 16, 5, 11, 8]
z = [[7, 3, 9, 5, 2],
     [4, 8, 2, 6, 10],
     [9, 6, 11, 7, 4],
     [5, 3, 8, 4, 9],
     [2, 7, 4, 10, 6]]

# Create a 3D surface plot using Plotly Graph Objects
fig = go.Figure(data=[go.Surface(x=x, y=y, z=z)])
fig.update_layout(title=""3D Surface Plot"", scene=dict(xaxis_title=""X-axis"", yaxis_title=""Y-axis"", zaxis_title=""Z-axis""))

# Save the plot as an SVG file
fig.write_image(""3d_surface_plot.svg"")

# Display the plot on the screen
fig.show()
",train
plotly,27,"Create a Python program using the 'plotly' API to generate a bar chart with grouped bars. The program should define data points for the x axis, create multiple sets of data for the y axis, create the chart with grouped bars, customize the chart title, X-axis label, and Y-axis label. Finally, save the chart as a PNG image file and display it on the screen.",code/plotly/plotly_27.py,"Verify that the program creates a bar chart with grouped bars and displays the specified title, X-axis label, and Y-axis label.",Ensure that the program saves the chart as a PNG image file and displays it on the screen without errors.,Test if the program correctly defines data points for the x axis.,,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x axis
x = ['A', 'B', 'C', 'D', 'E']

# Define multiple sets of data for the y axis
y1 = [10, 16, 5, 11, 8]
y2 = [5, 8, 3, 6, 9]
y3 = [7, 12, 4, 9, 6]

# Create a bar chart with grouped bars using Plotly Graph Objects
fig = go.Figure()
fig.add_trace(go.Bar(x=x, y=y1, name='Category 1'))
fig.add_trace(go.Bar(x=x, y=y2, name='Category 2'))
fig.add_trace(go.Bar(x=x, y=y3, name='Category 3'))
fig.update_layout(barmode='group', title=""Grouped Bar Chart"", xaxis_title=""X-axis"", yaxis_title=""Y-axis"")

# Save the chart as a PNG image file
fig.write_image(""grouped_bar_chart.png"")

# Display the chart on the screen
fig.show()
",train
plotly,7,"Create a Python program using the 'plotly' API to generate a box plot. The program should define data points for the x axis and multiple sets of data for the y axis, create the plot, customize the plot title, X-axis label, and Y-axis label. Finally, save the plot as a PDF file and display it on the screen.",code/plotly/plotly_7.py,"Verify that the program creates a box plot with the specified title, X-axis label, and Y-axis label.",Test if the program correctly defines data points for the x axis.,Ensure that the program saves the plot as a PDF file and displays it on the screen without errors.,,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x axis
x = ['A', 'B', 'C', 'D', 'E']

# Define multiple sets of data for the y axis
y1 = [10, 16, 5, 11, 8]
y2 = [5, 8, 3, 6, 9]
y3 = [7, 12, 4, 9, 6]

# Create a box plot using Plotly Graph Objects
fig = go.Figure()
fig.add_trace(go.Box(y=y1, name='Category 1'))
fig.add_trace(go.Box(y=y2, name='Category 2'))
fig.add_trace(go.Box(y=y3, name='Category 3'))
fig.update_layout(title=""Box Plot"", xaxis_title=""X-axis"", yaxis_title=""Y-axis"")

# Save the plot as a PDF file
fig.write_image(""box_plot.pdf"")

# Display the plot on the screen
fig.show()
",train
plotly,22,"Create a Python program using the 'plotly' API to generate a gauge chart. The program should define data points for the value and range of the gauge, create the chart, customize the chart title, and display the value on the chart. Finally, save the chart as a PDF file and display it on the screen.",code/plotly/plotly_22.py,Verify that the program creates a gauge chart with the specified title and displays the value on the chart.,Ensure that the program saves the chart as a PDF file and displays it on the screen without errors.,Test if the program correctly defines data points for the value and range of the gauge.,,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the value and range of the gauge
value = 75
range_min = 0
range_max = 100

# Create a gauge chart using Plotly Graph Objects
fig = go.Figure(go.Indicator(
    mode = ""gauge+number"",
    value = value,
    domain = {'x': [0, 1], 'y': [0, 1]},
    gauge = {'axis': {'range': [range_min, range_max]},
             'bar': {'color': 'darkblue'},
             'steps': [
                 {'range': [range_min, range_max/3], 'color': 'lightgray'},
                 {'range': [range_max/3, 2*range_max/3], 'color': 'gray'},
                 {'range': [2*range_max/3, range_max], 'color': 'darkgray'}],
             'threshold': {'line': {'color': 'red', 'width': 4}, 'thickness': 0.75, 'value': value}}))

# Customize the chart title
fig.update_layout(title=""Gauge Chart"")

# Save the chart as a PDF file
fig.write_image(""gauge_chart.pdf"")

# Display the chart on the screen
fig.show()
",train
plotly,36,"Create a Python program using the 'plotly' API to generate a scatter plot with a logarithmic y-axis. The program should define data points for the x and y axes, create the scatter plot with a logarithmic y-axis, customize the chart title, X-axis label, and Y-axis label. Finally, save the chart as a JPEG image file and display it on the screen.",code/plotly/plotly_36.py,"Verify that the program creates a scatter plot with a logarithmic y-axis and displays the specified title, X-axis label, and Y-axis label.",Ensure that the program saves the chart as a JPEG image file and displays it on the screen without errors.,Test if the program correctly defines data points for the x and y axes.,,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x and y axes
x = [1, 2, 3, 4, 5]
y = [10, 100, 1000, 10000, 100000]

# Create a scatter plot with a logarithmic y-axis using Plotly Graph Objects
fig = go.Figure(data=go.Scatter(x=x, y=y, mode='markers'))
fig.update_layout(title=""Scatter Plot with Logarithmic Y-axis"", xaxis_title=""X-axis"", yaxis_title=""Y-axis"", yaxis_type=""log"")

# Save the chart as a JPEG image file
fig.write_image(""scatter_plot_with_log_y.jpeg"")

# Display the chart on the screen
fig.show()
",train
plotly,8,"Create a Python program using the 'plotly' API to generate a heatmap. The program should define data points for the x and y axes, create the heatmap, customize the heatmap title, X-axis label, and Y-axis label. Finally, save the heatmap as a PNG image file and display it on the screen.",code/plotly/plotly_8.py,"Verify that the program creates a heatmap with the specified title, X-axis label, and Y-axis label.",Ensure that the program saves the heatmap as a PNG image file and displays it on the screen without errors.,Test if the program correctly defines data points for the x and y axes.,,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x and y axes
x = ['A', 'B', 'C', 'D', 'E']
y = ['1', '2', '3', '4', '5']

# Define the data values for the heatmap
z = [[10, 16, 5, 11, 8],
     [5, 8, 3, 6, 9],
     [7, 12, 4, 9, 6],
     [3, 5, 2, 8, 10],
     [9, 6, 11, 7, 4]]

# Create a heatmap using Plotly Graph Objects
fig = go.Figure(data=go.Heatmap(x=x, y=y, z=z))
fig.update_layout(title=""Heatmap"", xaxis_title=""X-axis"", yaxis_title=""Y-axis"")

# Save the heatmap as a PNG image file
fig.write_image(""heatmap.png"")

# Display the heatmap on the screen
fig.show()
",train
plotly,15,"Create a Python program using the 'plotly' API to generate a stacked line chart. The program should define data points for the x axis, create multiple sets of data for the y axis, create the chart, customize the chart title, X-axis label, and Y-axis label. Finally, save the chart as a JPEG image file and display it on the screen.",code/plotly/plotly_15.py,"Verify that the program creates a stacked line chart with the specified title, X-axis label, and Y-axis label.",Ensure that the program saves the chart as a JPEG image file and displays it on the screen without errors.,Test if the program correctly defines data points for the x axis.,,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x axis
x = ['A', 'B', 'C', 'D', 'E']

# Define multiple sets of data for the y axis
y1 = [10, 16, 5, 11, 8]
y2 = [5, 8, 3, 6, 9]
y3 = [7, 12, 4, 9, 6]

# Create a stacked line chart using Plotly Graph Objects
fig = go.Figure()
fig.add_trace(go.Scatter(x=x, y=y1, mode='lines', stackgroup='one', name='Category 1'))
fig.add_trace(go.Scatter(x=x, y=y2, mode='lines', stackgroup='one', name='Category 2'))
fig.add_trace(go.Scatter(x=x, y=y3, mode='lines', stackgroup='one', name='Category 3'))
fig.update_layout(title=""Stacked Line Chart"", xaxis_title=""X-axis"", yaxis_title=""Y-axis"")

# Save the chart as a JPEG image file
fig.write_image(""stacked_line_chart.jpeg"")

# Display the chart on the screen
fig.show()
",train
plotly,30,"Create a Python program using the 'plotly' API to generate a polar scatter plot. The program should define data points for the theta and r axes, create the plot, customize the plot title, X-axis label, and Y-axis label. Finally, save the plot as an SVG file and display it on the screen.",code/plotly/plotly_30.py,Ensure that the program saves the plot as an SVG file and displays it on the screen without errors.,"Verify that the program creates a polar scatter plot with the specified title, X-axis label, and Y-axis label.",Test if the program correctly defines data points for the theta and r axes.,,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the theta and r axes
theta = [0, 45, 90, 135, 180, 225, 270, 315]
r = [1, 2, 3, 4, 5, 4, 3, 2]

# Create a polar scatter plot using Plotly Graph Objects
fig = go.Figure(data=go.Scatterpolar(r=r, theta=theta, mode='markers'))
fig.update_layout(title=""Polar Scatter Plot"", polar=dict(radialaxis=dict(title=""R-axis"", showticklabels=False, ticks='')),
                  showlegend=False)

# Save the plot as an SVG file
fig.write_image(""polar_scatter_plot.svg"")

# Display the plot on the screen
fig.show()
",train
plotly,28,"Create a Python program using the 'plotly' API to generate a scatter plot with a color scale. The program should define data points for the x and y axes, as well as the color values for each data point. The program should create the scatter plot with a color scale, customize the chart title, X-axis label, and Y-axis label, and display the color scale on the chart. Finally, save the chart as a PDF file and display it on the screen.",code/plotly/plotly_28.py,Ensure that the program saves the chart as a PDF file and displays it on the screen without errors.,Test if the program correctly defines data points for the x and y axes.,"Verify that the program creates a scatter plot with a color scale and displays the specified title, X-axis label, and Y-axis label.",,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x and y axes
x = [1, 2, 3, 4, 5]
y = [10, 16, 5, 11, 8]

# Define color values for each data point
color = [1, 2, 3, 4, 5]

# Create a scatter plot with a color scale using Plotly Graph Objects
fig = go.Figure(data=go.Scatter(x=x, y=y, mode='markers', marker=dict(color=color, colorscale='Viridis', showscale=True)))
fig.update_layout(title=""Scatter Plot with Color Scale"", xaxis_title=""X-axis"", yaxis_title=""Y-axis"")

# Save the chart as a PDF file
fig.write_image(""scatter_plot_with_color_scale.pdf"")

# Display the chart on the screen
fig.show()
",test
plotly,14,"Create a Python program using the 'plotly' API to generate a bubble chart. The program should define data points for the x, y, and z axes, create the chart, customize the chart title, X-axis label, Y-axis label, and size of the bubbles. Finally, save the chart as a PNG image file and display it on the screen.",code/plotly/plotly_14.py,Ensure that the program saves the chart as a PNG image file and displays it on the screen without errors.,"Verify that the program creates a bubble chart with the specified title, X-axis label, Y-axis label, and size of the bubbles.","Test if the program correctly defines data points for the x, y, and z axes.",,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x, y, and z axes
x = [1, 2, 3, 4, 5]
y = [10, 16, 5, 11, 8]
z = [7, 3, 9, 5, 2]

# Define the size of the bubbles
size = [30, 50, 20, 40, 10]

# Create a bubble chart using Plotly Graph Objects
fig = go.Figure(data=go.Scatter(x=x, y=y, mode='markers', marker=dict(size=size)))
fig.update_layout(title=""Bubble Chart"", xaxis_title=""X-axis"", yaxis_title=""Y-axis"")

# Save the chart as a PNG image file
fig.write_image(""bubble_chart.png"")

# Display the chart on the screen
fig.show()
",test
plotly,26,"Create a Python program using the 'plotly' API to generate a scatter plot with error bars. The program should define data points for the x and y axes, as well as the error values for each data point. The program should create the scatter plot with error bars, customize the chart title, X-axis label, and Y-axis label, and display the error bars on the chart. Finally, save the chart as a PDF file and display it on the screen.",code/plotly/plotly_26.py,"Verify that the program creates a scatter plot with error bars and displays the specified title, X-axis label, and Y-axis label.",Ensure that the program saves the chart as a PDF file and displays it on the screen without errors.,Test if the program correctly defines data points for the x and y axes.,,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the x and y axes
x = [1, 2, 3, 4, 5]
y = [10, 16, 5, 11, 8]

# Define error values for each data point
error = [1, 2, 0.5, 1.5, 1]

# Create a scatter plot with error bars using Plotly Graph Objects
fig = go.Figure(data=go.Scatter(x=x, y=y, mode='markers', error_y=dict(type='data', array=error, visible=True)))
fig.update_layout(title=""Scatter Plot with Error Bars"", xaxis_title=""X-axis"", yaxis_title=""Y-axis"")

# Save the chart as a PDF file
fig.write_image(""scatter_plot_with_error_bars.pdf"")

# Display the chart on the screen
fig.show()
",test
plotly,23,"Create a Python program using the 'plotly' API to generate a choropleth map. The program should define data points for the locations and values, create the map, customize the map title, and display the data points on the map. Finally, save the map as an HTML file and display it on the screen.",code/plotly/plotly_23.py,Verify that the program creates a choropleth map with the specified title and displays the data points on the map.,Ensure that the program saves the map as an HTML file and displays it on the screen without errors.,Test if the program correctly defines data points for the locations and values.,,,"#!pip install plotly
import plotly.graph_objects as go

# Define data points for the locations and values
locations = ['USA', 'Canada', 'Mexico']
values = [10, 5, 7]

# Create a choropleth map using Plotly Graph Objects
fig = go.Figure(data=go.Choropleth(locations=locations, z=values, locationmode='country names'))
fig.update_layout(title=""Choropleth Map"")

# Save the map as an HTML file
fig.write_html(""choropleth_map.html"")

# Display the map on the screen
fig.show()
",test
polars,40,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'country' column is 'Spain', select specific columns ('name', 'age', 'country'), and calculate the average age. Finally, display the resulting average age.",code/polars/polars_40.py,Verify that the program filters rows based on the 'country' column correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,Ensure that the program selects the specified columns and calculates the average age correctly.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['country'] == 'Spain')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'country'])

# Calculate the average age
average_age = selected_columns['age'].mean()

# Display the resulting average age
print(average_age)
",train
polars,27,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'score' column is less than 50, select specific columns ('name', 'score', 'grade'), and calculate the minimum score. Finally, display the resulting minimum score.",code/polars/polars_27.py,Verify that the program filters rows based on the 'score' column correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,Ensure that the program selects the specified columns and calculates the minimum score correctly.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['score'] < 50)

# Select specific columns
selected_columns = filtered_data.select(['name', 'score', 'grade'])

# Calculate the minimum score
min_score = selected_columns['score'].min()

# Display the resulting minimum score
print(min_score)
",train
polars,7,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'country' column is 'Canada', select specific columns ('name', 'age', 'country'), and calculate the minimum age. Finally, display the resulting minimum age.",code/polars/polars_7.py,Ensure that the program selects the specified columns and calculates the minimum age correctly.,Verify that the program filters rows based on the 'country' column correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['country'] == 'Canada')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'country'])

# Calculate the minimum age
min_age = selected_columns['age'].min()

# Display the resulting minimum age
print(min_age)
",train
polars,28,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'city' column is 'Sydney', select specific columns ('name', 'age', 'city'), and calculate the maximum age. Finally, display the resulting maximum age.",code/polars/polars_28.py,Verify that the program filters rows based on the 'city' column correctly.,Ensure that the program selects the specified columns and calculates the maximum age correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['city'] == 'Sydney')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'city'])

# Calculate the maximum age
max_age = selected_columns['age'].max()

# Display the resulting maximum age
print(max_age)
",train
polars,26,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'country' column is 'Germany', select specific columns ('name', 'age', 'country'), and calculate the average age. Finally, display the resulting average age.",code/polars/polars_26.py,Verify that the program filters rows based on the 'country' column correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,Ensure that the program selects the specified columns and calculates the average age correctly.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['country'] == 'Germany')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'country'])

# Calculate the average age
average_age = selected_columns['age'].mean()

# Display the resulting average age
print(average_age)
",train
polars,17,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'gender' column is 'male', select specific columns ('name', 'age', 'gender'), and calculate the sum of ages. Finally, display the resulting sum of ages.",code/polars/polars_17.py,Verify that the program filters rows based on the 'gender' column correctly.,Ensure that the program selects the specified columns and calculates the sum of ages correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['gender'] == 'male')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'gender'])

# Calculate the sum of ages
sum_of_ages = selected_columns['age'].sum()

# Display the resulting sum of ages
print(sum_of_ages)
",train
polars,9,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'city' column is 'London', select specific columns ('name', 'age', 'city'), and calculate the sum of ages. Finally, display the resulting sum of ages.",code/polars/polars_9.py,Verify that the program filters rows based on the 'city' column correctly.,Ensure that the program selects the specified columns and calculates the sum of ages correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['city'] == 'London')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'city'])

# Calculate the sum of ages
sum_of_ages = selected_columns['age'].sum()

# Display the resulting sum of ages
print(sum_of_ages)
",train
polars,16,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'city' column is 'Tokyo', select specific columns ('name', 'age', 'city'), and calculate the minimum age. Finally, display the resulting minimum age.",code/polars/polars_16.py,Verify that the program filters rows based on the 'city' column correctly.,Ensure that the program selects the specified columns and calculates the minimum age correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['city'] == 'Tokyo')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'city'])

# Calculate the minimum age
min_age = selected_columns['age'].min()

# Display the resulting minimum age
print(min_age)
",train
polars,13,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'gender' column is 'female', select specific columns ('name', 'age', 'gender'), and calculate the sum of ages. Finally, display the resulting sum of ages.",code/polars/polars_13.py,Verify that the program filters rows based on the 'gender' column correctly.,Ensure that the program selects the specified columns and calculates the sum of ages correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['gender'] == 'female')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'gender'])

# Calculate the sum of ages
sum_of_ages = selected_columns['age'].sum()

# Display the resulting sum of ages
print(sum_of_ages)
",train
polars,36,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'country' column is 'Italy', select specific columns ('name', 'age', 'country'), and calculate the average age. Finally, display the resulting average age.",code/polars/polars_36.py,Verify that the program filters rows based on the 'country' column correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,Ensure that the program selects the specified columns and calculates the average age correctly.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['country'] == 'Italy')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'country'])

# Calculate the average age
average_age = selected_columns['age'].mean()

# Display the resulting average age
print(average_age)
",train
polars,10,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'country' column is 'Germany', select specific columns ('name', 'age', 'country'), and calculate the maximum age. Finally, display the resulting maximum age.",code/polars/polars_10.py,Ensure that the program selects the specified columns and calculates the maximum age correctly.,Verify that the program filters rows based on the 'country' column correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['country'] == 'Germany')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'country'])

# Calculate the maximum age
max_age = selected_columns['age'].max()

# Display the resulting maximum age
print(max_age)
",train
polars,20,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'city' column is 'Berlin', select specific columns ('name', 'age', 'city'), and calculate the average age. Finally, display the resulting average age.",code/polars/polars_20.py,Verify that the program filters rows based on the 'city' column correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,Ensure that the program selects the specified columns and calculates the average age correctly.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['city'] == 'Berlin')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'city'])

# Calculate the average age
average_age = selected_columns['age'].mean()

# Display the resulting average age
print(average_age)
",train
polars,1,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'age' column is greater than 25, select specific columns ('name', 'city', 'age'), and aggregate data by grouping it by the 'city' column and calculating the sum of 'age'. Finally, display the resulting DataFrame.",code/polars/polars_1.py,Ensure that the program selects the specified columns and aggregates data by grouping it by 'city' while calculating the total age.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,Verify that the program filters rows based on the 'age' column correctly.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['age'] > 25)

# Select specific columns
selected_columns = filtered_data.select(['name', 'city', 'age'])

# Aggregate data
grouped_data = selected_columns.groupby('city').agg(pl.sum('age').alias('total_age'))

# Display the resulting DataFrame
print(grouped_data)
",train
polars,35,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'city' column is 'Moscow', select specific columns ('name', 'age', 'city'), and calculate the maximum age. Finally, display the resulting maximum age.",code/polars/polars_35.py,Verify that the program filters rows based on the 'city' column correctly.,Ensure that the program selects the specified columns and calculates the maximum age correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['city'] == 'Moscow')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'city'])

# Calculate the maximum age
max_age = selected_columns['age'].max()

# Display the resulting maximum age
print(max_age)
",train
polars,6,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'city' column is 'New York', select specific columns ('name', 'age', 'city'), and calculate the maximum age. Finally, display the resulting maximum age.",code/polars/polars_6.py,Verify that the program filters rows based on the 'city' column correctly.,Ensure that the program selects the specified columns and calculates the maximum age correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['city'] == 'New York')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'city'])

# Calculate the maximum age
max_age = selected_columns['age'].max()

# Display the resulting maximum age
print(max_age)
",train
polars,12,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'city' column is 'Paris', select specific columns ('name', 'age', 'city'), and calculate the average age. Finally, display the resulting average age.",code/polars/polars_12.py,Verify that the program filters rows based on the 'city' column correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,Ensure that the program selects the specified columns and calculates the average age correctly.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['city'] == 'Paris')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'city'])

# Calculate the average age
average_age = selected_columns['age'].mean()

# Display the resulting average age
print(average_age)
",train
polars,2,"Create a Python program using the 'polars' API to load a sample Parquet file into a Polars DataFrame, filter rows where the 'gender' column is 'female', select specific columns ('name', 'age', 'city'), and sort the data by the 'age' column in descending order. Finally, display the resulting DataFrame.",code/polars/polars_2.py,Verify that the program filters rows based on the 'gender' column correctly.,Ensure that the program selects the specified columns and sorts the data by the 'age' column in descending order.,Test if the program successfully loads the sample Parquet file into a Polars DataFrame.,,,"#!pip install polars
import polars as pl

# Load a sample Parquet file into a Polars DataFrame
data = pl.read_parquet(""sample_data.parquet"")

# Filter rows
filtered_data = data.filter(data['gender'] == 'female')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'city'])

# Sort the data by the 'age' column in descending order
sorted_data = selected_columns.sort('age', reverse=True)

# Display the resulting DataFrame
print(sorted_data)
",train
polars,29,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'country' column is 'France', select specific columns ('name', 'age', 'country'), and calculate the average age. Finally, display the resulting average age.",code/polars/polars_29.py,Verify that the program filters rows based on the 'country' column correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,Ensure that the program selects the specified columns and calculates the average age correctly.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['country'] == 'France')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'country'])

# Calculate the average age
average_age = selected_columns['age'].mean()

# Display the resulting average age
print(average_age)
",train
polars,22,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'country' column is 'USA', select specific columns ('name', 'age', 'country'), and calculate the sum of ages. Finally, display the resulting sum of ages.",code/polars/polars_22.py,Ensure that the program selects the specified columns and calculates the sum of ages correctly.,Verify that the program filters rows based on the 'country' column correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['country'] == 'USA')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'country'])

# Calculate the sum of ages
sum_of_ages = selected_columns['age'].sum()

# Display the resulting sum of ages
print(sum_of_ages)
",train
polars,3,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'salary' column is greater than 50000, select specific columns ('name', 'age', 'salary'), and calculate the average salary. Finally, display the resulting average salary.",code/polars/polars_3.py,Ensure that the program selects the specified columns and calculates the average salary correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,Verify that the program filters rows based on the 'salary' column correctly.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['salary'] > 50000)

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'salary'])

# Calculate the average salary
average_salary = selected_columns['salary'].mean()

# Display the resulting average salary
print(average_salary)
",train
polars,41,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'score' column is less than 40, select specific columns ('name', 'score', 'grade'), and calculate the minimum score. Finally, display the resulting minimum score.",code/polars/polars_41.py,Verify that the program filters rows based on the 'score' column correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,Ensure that the program selects the specified columns and calculates the minimum score correctly.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['score'] < 40)

# Select specific columns
selected_columns = filtered_data.select(['name', 'score', 'grade'])

# Calculate the minimum score
min_score = selected_columns['score'].min()

# Display the resulting minimum score
print(min_score)
",train
polars,38,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'city' column is 'Beijing', select specific columns ('name', 'age', 'city'), and calculate the minimum age. Finally, display the resulting minimum age.",code/polars/polars_38.py,Verify that the program filters rows based on the 'city' column correctly.,Ensure that the program selects the specified columns and calculates the minimum age correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['city'] == 'Beijing')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'city'])

# Calculate the minimum age
min_age = selected_columns['age'].min()

# Display the resulting minimum age
print(min_age)
",train
polars,4,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'country' column is 'USA', select specific columns ('name', 'age', 'country'), and count the number of rows. Finally, display the resulting row count.",code/polars/polars_4.py,Ensure that the program selects the specified columns and counts the number of rows correctly.,Verify that the program filters rows based on the 'country' column correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['country'] == 'USA')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'country'])

# Count the number of rows
row_count = selected_columns.shape[0]

# Display the resulting row count
print(row_count)
",train
polars,37,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'score' column is greater than 60, select specific columns ('name', 'score', 'grade'), and calculate the maximum score. Finally, display the resulting maximum score.",code/polars/polars_37.py,Ensure that the program selects the specified columns and calculates the maximum score correctly.,Verify that the program filters rows based on the 'score' column correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['score'] > 60)

# Select specific columns
selected_columns = filtered_data.select(['name', 'score', 'grade'])

# Calculate the maximum score
max_score = selected_columns['score'].max()

# Display the resulting maximum score
print(max_score)
",train
polars,24,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'city' column is 'Madrid', select specific columns ('name', 'age', 'city'), and calculate the minimum age. Finally, display the resulting minimum age.",code/polars/polars_24.py,Verify that the program filters rows based on the 'city' column correctly.,Ensure that the program selects the specified columns and calculates the minimum age correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['city'] == 'Madrid')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'city'])

# Calculate the minimum age
min_age = selected_columns['age'].min()

# Display the resulting minimum age
print(min_age)
",train
polars,34,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'score' column is less than 80, select specific columns ('name', 'score', 'grade'), and calculate the minimum score. Finally, display the resulting minimum score.",code/polars/polars_34.py,Verify that the program filters rows based on the 'score' column correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,Ensure that the program selects the specified columns and calculates the minimum score correctly.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['score'] < 80)

# Select specific columns
selected_columns = filtered_data.select(['name', 'score', 'grade'])

# Calculate the minimum score
min_score = selected_columns['score'].min()

# Display the resulting minimum score
print(min_score)
",train
polars,11,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'score' column is less than 60, select specific columns ('name', 'score', 'grade'), and calculate the minimum score. Finally, display the resulting minimum score.",code/polars/polars_11.py,Verify that the program filters rows based on the 'score' column correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,Ensure that the program selects the specified columns and calculates the minimum score correctly.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['score'] < 60)

# Select specific columns
selected_columns = filtered_data.select(['name', 'score', 'grade'])

# Calculate the minimum score
min_score = selected_columns['score'].min()

# Display the resulting minimum score
print(min_score)
",train
polars,23,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'score' column is greater than 70, select specific columns ('name', 'score', 'grade'), and calculate the maximum score. Finally, display the resulting maximum score.",code/polars/polars_23.py,Ensure that the program selects the specified columns and calculates the maximum score correctly.,Verify that the program filters rows based on the 'score' column correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['score'] > 70)

# Select specific columns
selected_columns = filtered_data.select(['name', 'score', 'grade'])

# Calculate the maximum score
max_score = selected_columns['score'].max()

# Display the resulting maximum score
print(max_score)
",train
polars,19,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'score' column is less than 70, select specific columns ('name', 'score', 'grade'), and calculate the minimum score. Finally, display the resulting minimum score.",code/polars/polars_19.py,Verify that the program filters rows based on the 'score' column correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,Ensure that the program selects the specified columns and calculates the minimum score correctly.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['score'] < 70)

# Select specific columns
selected_columns = filtered_data.select(['name', 'score', 'grade'])

# Calculate the minimum score
min_score = selected_columns['score'].min()

# Display the resulting minimum score
print(min_score)
",train
polars,21,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'gender' column is 'female', select specific columns ('name', 'age', 'gender'), and calculate the average age. Finally, display the resulting average age.",code/polars/polars_21.py,Verify that the program filters rows based on the 'gender' column correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,Ensure that the program selects the specified columns and calculates the average age correctly.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['gender'] == 'female')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'gender'])

# Calculate the average age
average_age = selected_columns['age'].mean()

# Display the resulting average age
print(average_age)
",train
polars,8,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'gender' column is 'male', select specific columns ('name', 'age', 'gender'), and calculate the average age. Finally, display the resulting average age.",code/polars/polars_8.py,Verify that the program filters rows based on the 'gender' column correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,Ensure that the program selects the specified columns and calculates the average age correctly.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['gender'] == 'male')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'gender'])

# Calculate the average age
average_age = selected_columns['age'].mean()

# Display the resulting average age
print(average_age)
",train
polars,15,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'score' column is greater than 90, select specific columns ('name', 'score', 'grade'), and calculate the maximum score. Finally, display the resulting maximum score.",code/polars/polars_15.py,Ensure that the program selects the specified columns and calculates the maximum score correctly.,Verify that the program filters rows based on the 'score' column correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['score'] > 90)

# Select specific columns
selected_columns = filtered_data.select(['name', 'score', 'grade'])

# Calculate the maximum score
max_score = selected_columns['score'].max()

# Display the resulting maximum score
print(max_score)
",train
polars,30,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'score' column is greater than 50, select specific columns ('name', 'score', 'grade'), and calculate the maximum score. Finally, display the resulting maximum score.",code/polars/polars_30.py,Ensure that the program selects the specified columns and calculates the maximum score correctly.,Verify that the program filters rows based on the 'score' column correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['score'] > 50)

# Select specific columns
selected_columns = filtered_data.select(['name', 'score', 'grade'])

# Calculate the maximum score
max_score = selected_columns['score'].max()

# Display the resulting maximum score
print(max_score)
",train
polars,18,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'country' column is 'Canada', select specific columns ('name', 'age', 'country'), and calculate the maximum age. Finally, display the resulting maximum age.",code/polars/polars_18.py,Ensure that the program selects the specified columns and calculates the maximum age correctly.,Verify that the program filters rows based on the 'country' column correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['country'] == 'Canada')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'country'])

# Calculate the maximum age
max_age = selected_columns['age'].max()

# Display the resulting maximum age
print(max_age)
",test
polars,14,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'country' column is 'USA', select specific columns ('name', 'age', 'country'), and calculate the average age. Finally, display the resulting average age.",code/polars/polars_14.py,Verify that the program filters rows based on the 'country' column correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,Ensure that the program selects the specified columns and calculates the average age correctly.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['country'] == 'USA')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'country'])

# Calculate the average age
average_age = selected_columns['age'].mean()

# Display the resulting average age
print(average_age)
",test
polars,5,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'score' column is greater than 80, select specific columns ('name', 'score', 'grade'), and calculate the average score. Finally, display the resulting average score.",code/polars/polars_5.py,Verify that the program filters rows based on the 'score' column correctly.,Ensure that the program selects the specified columns and calculates the average score correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['score'] > 80)

# Select specific columns
selected_columns = filtered_data.select(['name', 'score', 'grade'])

# Calculate the average score
average_score = selected_columns['score'].mean()

# Display the resulting average score
print(average_score)
",test
polars,31,"Create a Python program using the 'polars' API to load a sample CSV file into a Polars DataFrame, filter rows where the 'city' column is 'Toronto', select specific columns ('name', 'age', 'city'), and calculate the minimum age. Finally, display the resulting minimum age.",code/polars/polars_31.py,Verify that the program filters rows based on the 'city' column correctly.,Ensure that the program selects the specified columns and calculates the minimum age correctly.,Test if the program successfully loads the sample CSV file into a Polars DataFrame.,,,"#!pip install polars
import polars as pl

# Load a sample CSV file into a Polars DataFrame
data = pl.read_csv(""sample_data.csv"")

# Filter rows
filtered_data = data.filter(data['city'] == 'Toronto')

# Select specific columns
selected_columns = filtered_data.select(['name', 'age', 'city'])

# Calculate the minimum age
min_age = selected_columns['age'].min()

# Display the resulting minimum age
print(min_age)
",test
pyglove,31,"Create a Python program using the 'pyglove' API to perform web scraping. The program should access a website, scrape data (e.g., text or images) from the website, and output the scraped data. Additionally, provide a unit test that verifies the accuracy of the web scraping results.",code/pyglove/pyglove_31.py,Test that the program outputs the scraped data.,Test that the program can successfully access a website.,"Test that it correctly scrapes data (e.g., text or images) from the website.",,,"#!pip install pyglove
import pyglove as pg
import requests
from bs4 import BeautifulSoup

def scrape_website(url):
  # Replace this with your code to scrape data from a website
  # The example below accesses a sample website and scrapes text data
  response = requests.get(url)
  soup = BeautifulSoup(response.text, 'html.parser')
  scraped_data = []

  # Example: Scrape text data from website
  for element in soup.find_all('p'):
      scraped_data.append(element.get_text())

  return scraped_data

if __name__ == '__main__':
  website_url = 'https://example.com'
  scraped_data = scrape_website(website_url)

  print('Scraped Data:')
  print(scraped_data)
",train
pyglove,41,"Create a Python program using the 'pyglove' API to perform word analogy tasks. The program should load pre-trained word embeddings from a file, allow the user to input word analogies (e.g., king - man + woman = queen), and use the loaded word embeddings to find the most similar word that completes the analogy. The program should provide a function to perform the analogy task and return the result.",code/pyglove/pyglove_41.py,Test that the program can load pre-trained word embeddings from a file.,"Test the word analogy function with missing words, which should return None.",Test that it correctly performs word analogy tasks and returns the expected results.,,,"#!pip install pyglove
import pyglove
import numpy as np

# Define the path to the pre-trained word embeddings file
embedding_file_path = 'pretrained_word_embeddings.txt'

# Load pre-trained word embeddings
glove = pyglove.Glove.load(embedding_file_path)

# Function to perform word analogy
def perform_word_analogy(word1, word2, word3):
    if word1 in glove.dictionary and word2 in glove.dictionary and word3 in glove.dictionary:
        vector1 = glove.word_vectors[glove.dictionary[word1]]
        vector2 = glove.word_vectors[glove.dictionary[word2]]
        vector3 = glove.word_vectors[glove.dictionary[word3]]
        target_vector = vector2 - vector1 + vector3

        # Find the most similar word
        most_similar_word, _ = glove.most_similar(target_vector, n=1)
        return most_similar_word
    else:
        return None

# Example usage
word1 = 'king'
word2 = 'man'
word3 = 'woman'
result = perform_word_analogy(word1, word2, word3)
if result is not None:
    print(f'{word1} - {word2} + {word3} = {result}')
else:
    print('One or more words not found in the embeddings.')

# Test the word analogy function
test_result = perform_word_analogy('test1', 'test2', 'test3')
assert test_result is None, 'Word analogy should return None for missing words.'

print('Word analogy task completed successfully.')
",train
pyglove,8,"Create a Python program using the 'pyglove' API to perform time series forecasting using an LSTM (Long Short-Term Memory) neural network. The program should download and preprocess a time series dataset, split it into training and testing sets, define an LSTM model with different hyperparameters (e.g., number of LSTM layers, units, and learning rate), train the model on the training set, and output the forecasted values on the testing set. Additionally, provide a unit test that verifies the quality of the forecasted values.",code/pyglove/pyglove_8.py,Test that the program forecasts values on the testing set and outputs the RMSE (Root Mean Squared Error).,Test that it correctly defines and trains LSTM models with different hyperparameters.,Test that the program can successfully download and preprocess a time series dataset.,,,"#!pip install pyglove
import pyglove as pg
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

def download_and_preprocess_time_series():
  # Replace this with your code to download and preprocess a time series dataset
  # The example below assumes a univariate time series
  time_series = np.arange(100)
  
  # Normalize the time series
  scaler = MinMaxScaler()
  normalized_time_series = scaler.fit_transform(time_series.reshape(-1, 1))
  
  return normalized_time_series

@pg.symbolize
def build_model(hparams):
  model = tf.keras.Sequential()
  model.add(tf.keras.layers.LSTM(hparams.num_units, activation='relu', input_shape=(1, 1)))
  for _ in range(hparams.num_layers - 1):
    model.add(tf.keras.layers.LSTM(hparams.num_units, activation='relu', return_sequences=True))
  model.add(tf.keras.layers.Dense(1))
  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hparams.learning_rate),
                loss='mean_squared_error')
  return model

def train_and_forecast(model, input_data, num_epochs=10):
  time_series = input_data
  
  # Split the time series into training and testing sets
  train_size = int(len(time_series) * 0.7)
  train_data, test_data = time_series[:train_size], time_series[train_size:]
  
  # Prepare the data for training
  def prepare_data(data, look_back=1):
    data_x, data_y = [], []
    for i in range(len(data) - look_back):
      data_x.append(data[i:(i + look_back)])
      data_y.append(data[i + look_back])
    return np.array(data_x), np.array(data_y)
  
  look_back = 1
  train_x, train_y = prepare_data(train_data, look_back)
  test_x, test_y = prepare_data(test_data, look_back)
  
  # Build and train the model
  model.fit(train_x, train_y, epochs=num_epochs, batch_size=1, verbose=0)
  
  # Forecast values on the testing set
  forecasted_values = []
  for i in range(len(test_x)):
    x_input = test_x[i].reshape((1, look_back, 1))
    y_hat = model.predict(x_input, verbose=0)
    forecasted_values.append(y_hat[0, 0])
  
  return forecasted_values

def evaluate_forecast(test_data, forecasted_values):
  rmse = np.sqrt(mean_squared_error(test_data, forecasted_values))
  return rmse

if __name__ == '__main__':
  time_series = download_and_preprocess_time_series()
  hparams = pg.oneof([
    pg.Dict(num_units=pg.oneof([32, 64, 128]),
            num_layers=pg.oneof([1, 2, 3]),
            learning_rate=pg.oneof([0.001, 0.01, 0.1]))
  ])
  model = build_model(hparams)
  forecasted_values = train_and_forecast(model, time_series)
  rmse = evaluate_forecast(time_series[-len(forecasted_values):], forecasted_values)

  print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')
",train
pyglove,5,"Create a Python program using the 'pyglove' API to perform text classification on a collection of text documents. The program should download and preprocess a collection of text documents, define a text classification model with different hyperparameters (e.g., number of layers, units, and activation functions), and train the model to classify text into multiple categories. The program should output the classification accuracy.",code/pyglove/pyglove_5.py,Test that it correctly defines and trains text classification models with different hyperparameters.,Test that the program can successfully download and preprocess a collection of text documents.,Test that the program outputs the classification accuracy.,,,"#!pip install pyglove
import pyglove as pg
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder

def download_and_preprocess_documents():
  # Replace this with your code to download and preprocess a collection of text documents
  documents = [
    ""This is a positive document."",
    ""This document is negative."",
    ""A positive review."",
    ""A negative review."",
    ""Another positive document."",
    ""Another negative document.""
  ]
  labels = [""positive"", ""negative"", ""positive"", ""negative"", ""positive"", ""negative""]
  return documents, labels

@pg.symbolize
def build_model(hparams):
  model = tf.keras.Sequential()
  model.add(tf.keras.layers.Input(shape=(hparams.max_features,)))
  model.add(tf.keras.layers.Embedding(hparams.max_features, hparams.embedding_dim))
  for _ in range(hparams.num_layers):
    model.add(tf.keras.layers.LSTM(hparams.num_units, return_sequences=True))
  model.add(tf.keras.layers.GlobalMaxPooling1D())
  model.add(tf.keras.layers.Dense(len(hparams.classes), activation='softmax'))
  return model

def train_and_eval(model, input_data, num_epochs=10):
  documents, labels = input_data
  vectorizer = TfidfVectorizer(max_features=1000)
  encoder = LabelEncoder()
  dtm = vectorizer.fit_transform(documents)
  labels_encoded = encoder.fit_transform(labels)
  tr_x, te_x, tr_y, te_y = train_test_split(dtm, labels_encoded, test_size=0.2)

  model.compile(optimizer='adam',
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])
  model.fit(tr_x, tr_y, epochs=num_epochs, validation_data=(te_x, te_y))

  _, test_acc = model.evaluate(te_x, te_y, verbose=2)
  return test_acc

if __name__ == '__main__':
  documents, labels = download_and_preprocess_documents()
  hparams = pg.oneof([
    pg.Dict(max_features=pg.oneof([500, 1000, 1500]),
            embedding_dim=pg.oneof([32, 64, 128]),
            num_layers=pg.oneof([1, 2, 3]),
            num_units=pg.oneof([32, 64, 128]),
            classes=pg.oneof([2, 3, 4]))
  ])
  model = build_model(hparams)
  test_acc = train_and_eval(model, (documents, labels))

  print(f'Classification accuracy: {test_acc:.2f}')
",train
pyglove,30,"Create a Python program using the 'pyglove' API to perform database operations. The program should connect to a database, execute SQL queries, and output the results or modified data. Additionally, provide a unit test that verifies the correctness of the database operations.",code/pyglove/pyglove_30.py,Test that the program can successfully connect to a database.,Test that it correctly executes SQL queries on the database.,Test that the program outputs the results of the SQL queries or modified data.,,,"#!pip install pyglove
import pyglove as pg
import sqlite3

def connect_to_database():
  # Replace this with your code to connect to a database
  # The example below connects to a SQLite database
  conn = sqlite3.connect('sample.db')
  return conn

def execute_sql_query(connection, query):
  # Replace this with your code to execute SQL queries on the database
  cursor = connection.cursor()
  cursor.execute(query)
  
  # Example: Fetch data from the query
  data = cursor.fetchall()
  
  # Example: Modify data
  cursor.execute(""UPDATE table_name SET column_name = new_value WHERE condition"")
  
  connection.commit()
  cursor.close()
  
  return data

if __name__ == '__main__':
  connection = connect_to_database()
  sql_query = 'SELECT * FROM table_name'
  query_result = execute_sql_query(connection, sql_query)

  print('Query Result:')
  print(query_result)
",train
pyglove,15,"Create a Python program using the 'pyglove' API to perform web scraping and data extraction from a website. The program should use a web scraping library (e.g., BeautifulSoup or Scrapy) to extract data from a specific website, parse the data, and output the extracted information. Additionally, provide a unit test that verifies the correctness of the data extraction.",code/pyglove/pyglove_15.py,Test that the program outputs the extracted information.,Test that the program can successfully scrape a website and extract data.,Test that it correctly extracts and parses the data from the website.,,,"#!pip install pyglove
import pyglove as pg
from bs4 import BeautifulSoup
import requests

def scrape_website_and_extract_data():
  # Replace this with your code to scrape a website and extract data
  # The example below extracts the titles of articles from a website
  url = 'https://example.com'
  response = requests.get(url)
  soup = BeautifulSoup(response.text, 'html.parser')
  article_titles = []

  for article in soup.find_all('article'):
      title = article.find('h2').text
      article_titles.append(title)

  return article_titles

if __name__ == '__main__':
  extracted_data = scrape_website_and_extract_data()

  for i, title in enumerate(extracted_data):
      print(f'Article {i+1}: {title}')
",train
pyglove,27,"Create a Python program using the 'pyglove' API to perform natural language generation. The program should generate text or content based on a given prompt or topic. Additionally, provide a unit test that verifies the quality and relevance of the generated content.",code/pyglove/pyglove_27.py,Test that the program outputs the generated text.,Test that the generated content is of high quality and relevance to the prompt.,Test that the program can successfully generate text based on a given prompt or topic.,,,"#!pip install pyglove
import pyglove as pg
import openai

def generate_text_based_on_prompt(prompt):
  # Replace this with your code to generate text based on a prompt or topic
  # The example below uses the GPT-3 model for text generation
  openai.api_key = 'YOUR_API_KEY'   # Replace with your GPT-3 API key
  generated_text = openai.ChatCompletion.create(
    model=""gpt-3.5-turbo"",
    messages=[
      {""role"": ""user"", ""content"": prompt},
    ]
  )

  return generated_text

if __name__ == '__main__':
  prompt = 'Write a short story about a detective solving a mysterious case.'
  generated_text = generate_text_based_on_prompt(prompt)

  print('Generated Text:')
  print(generated_text)
",train
pyglove,11,"Create a Python program using the 'pyglove' API to perform text generation using a pre-trained language model. The program should download a pre-trained language model, generate text based on a given prompt, and output the generated text. Additionally, provide a unit test that checks the quality of the generated text.",code/pyglove/pyglove_11.py,Test that the program outputs the generated text.,Test that it correctly generates text based on a given prompt with different hyperparameters.,Test that the program can successfully download a pre-trained language model.,,,"#!pip install pyglove
import pyglove as pg
from transformers import GPT2LMHeadModel, GPT2Tokenizer

def download_pretrained_model():
  # Download a pre-trained GPT-2 model
  model = GPT2LMHeadModel.from_pretrained('gpt2')
  tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
  return model, tokenizer

@pg.symbolize
def generate_text(model, tokenizer, hparams, prompt='Once upon a time'):
  input_ids = tokenizer.encode(prompt, return_tensors='pt')
  max_length = hparams.max_length
  temperature = hparams.temperature
  num_return_sequences = hparams.num_return_sequences

  output = model.generate(input_ids,
                         max_length=max_length,
                         temperature=temperature,
                         num_return_sequences=num_return_sequences,
                         no_repeat_ngram_size=2,
                         top_k=50,
                         top_p=0.95)

  generated_text = []
  for sample_output in output:
      text = tokenizer.decode(sample_output, skip_special_tokens=True)
      generated_text.append(text)

  return generated_text

if __name__ == '__main__':
  model, tokenizer = download_pretrained_model()
  hparams = pg.oneof([
    pg.Dict(max_length=pg.oneof([50, 100, 150]),
            temperature=pg.oneof([0.7, 0.8, 0.9]),
            num_return_sequences=pg.oneof([1, 3, 5]))
  ])
  generated_text = generate_text(model, tokenizer, hparams)

  for i, text in enumerate(generated_text):
    print(f'Sample {i+1}: {text}')
",train
pyglove,26,"Create a Python program using the 'pyglove' API to perform deep learning on a custom dataset. The program should define a deep learning model, train it on a custom dataset, and output the model's performance in terms of loss and accuracy. Additionally, provide a unit test that verifies the performance of the deep learning model.",code/pyglove/pyglove_26.py,Test that it correctly defines and trains a deep learning model on the custom dataset.,Test that the program can successfully create a custom dataset.,"Test that the program outputs the performance of the deep learning model (e.g., loss and accuracy).",,,"#!pip install pyglove
import pyglove as pg
import tensorflow as tf
import numpy as np
from sklearn.model_selection import train_test_split

def create_custom_dataset():
  # Replace this with your code to create a custom dataset
  # The example below generates random data for illustration
  X = np.random.rand(100, 2)
  y = np.random.randint(0, 2, 100)
  
  return X, y

@pg.symbolize
def create_deep_learning_model(hparams):
  model = tf.keras.Sequential([
      tf.keras.layers.Input(shape=(2,)),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(32, activation='relu'),
      tf.keras.layers.Dense(1, activation='sigmoid')
  ])
  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hparams.learning_rate),
                loss='binary_crossentropy',
                metrics=['accuracy'])
  return model

def train_and_evaluate_deep_learning_model(model, custom_dataset, num_epochs=10):
  X, y = custom_dataset
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  
  model.fit(X_train, y_train, epochs=num_epochs, verbose=0)
  
  loss, accuracy = model.evaluate(X_test, y_test, verbose=2)
  
  return loss, accuracy

if __name__ == '__main__':
  custom_dataset = create_custom_dataset()
  hparams = pg.oneof([
    pg.Dict(learning_rate=pg.oneof([0.001, 0.01, 0.1]))
  ])
  model = create_deep_learning_model(hparams)
  loss, accuracy = train_and_evaluate_deep_learning_model(model, custom_dataset)

  print(f'Loss: {loss:.2f}')
  print(f'Accuracy: {accuracy:.2f}')
",train
pyglove,16,"Create a Python program using the 'pyglove' API to perform network analysis. The program should analyze a network (e.g., social network or computer network) by loading a dataset, applying network analysis techniques, and outputting relevant statistics or insights. Additionally, provide a unit test that verifies the accuracy of the network analysis results.",code/pyglove/pyglove_16.py,Test that the program can successfully load and analyze a network.,Test that the program outputs the accurate network analysis results.,Test that it correctly computes network statistics or insights.,,,"#!pip install pyglove
import pyglove as pg
import networkx as nx

def load_and_analyze_network():
  # Replace this with your code to load and analyze a network
  # The example below creates a simple social network and computes network statistics
  G = nx.Graph()
  G.add_edges_from([(1, 2), (2, 3), (3, 4), (4, 5), (5, 1)])

  num_nodes = G.number_of_nodes()
  num_edges = G.number_of_edges()
  average_degree = sum(dict(G.degree()).values()) / num_nodes
  network_density = nx.density(G)

  return num_nodes, num_edges, average_degree, network_density

if __name__ == '__main__':
  num_nodes, num_edges, average_degree, network_density = load_and_analyze_network()

  print(f'Number of Nodes: {num_nodes}')
  print(f'Number of Edges: {num_edges}')
  print(f'Average Degree: {average_degree:.2f}')
  print(f'Network Density: {network_density:.2f}')
",test
pyglove,4,"Create a Python program using the 'pyglove' API to perform topic modeling on a collection of text documents. The program should download and preprocess a collection of text documents, apply Latent Dirichlet Allocation (LDA) to the documents, and output the topics and their associated words. Additionally, provide a unit test that verifies the presence of topics and associated words in the output.",code/pyglove/pyglove_4.py,Test that the program can successfully download and preprocess a collection of text documents.,Test that the output contains the expected number of topics and associated words.,Test that it applies Latent Dirichlet Allocation (LDA) to the documents and extracts topics with associated words.,,,"#!pip install pyglove
import pyglove as pg
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import numpy as np

def download_and_preprocess_documents():
  # Replace this with your code to download and preprocess a collection of text documents
  documents = [
    ""This is the first document."",
    ""This document is the second document."",
    ""And this is the third one."",
    ""Is this the first document?""
  ]
  return documents

def apply_lda(documents, num_topics=5):
  # Convert documents to document-term matrix
  vectorizer = CountVectorizer()
  dtm = vectorizer.fit_transform(documents)

  # Apply Latent Dirichlet Allocation
  lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)
  lda.fit(dtm)

  return vectorizer, lda

def extract_topics_and_words(vectorizer, lda, num_words=10):
  feature_names = vectorizer.get_feature_names_out()
  topics = []
  for topic_idx, topic in enumerate(lda.components_):
    top_words_idx = topic.argsort()[:-num_words - 1:-1]
    top_words = [feature_names[i] for i in top_words_idx]
    topics.append({'topic': topic_idx, 'words': top_words})
  return topics

if __name__ == '__main__':
  documents = download_and_preprocess_documents()
  num_topics = 3
  vectorizer, lda = apply_lda(documents, num_topics)
  topics = extract_topics_and_words(vectorizer, lda)

  for topic in topics:
    print(f'Topic {topic[""topic""]}: {"", "".join(topic[""words""])}')
",test
pymc,13,"Create a Python program using the 'pymc' API to perform Bayesian inference on a dataset of observed binary outcomes. The program should define a prior distribution for the probability of success, and use a likelihood function to model the observed outcomes. After setting up the model, perform Bayesian inference to draw posterior samples using NUTS sampling.",code/pymc/pymc_13.py,Test the program with different datasets of observed binary outcomes to ensure it correctly performs Bayesian inference.,Validate the program's results by comparing the posterior samples and summary statistics to expected values.,Assess the program's performance with larger datasets of observed binary outcomes to ensure it remains efficient and accurate.,,,"#!pip install pymc arviz
from pymc import Bernoulli, Model, sample
import arviz as az
import numpy as np

rng = np.random.default_rng(8927)
size = 100
true_prob = 0.7

# Simulate observed binary outcomes
outcomes = rng.binomial(1, true_prob, size=size)

with Model() as model:
    # Define prior
    p = Bernoulli(""p"", p=0.5)

    # Define likelihood
    likelihood = Bernoulli(""likelihood"", p=p, observed=outcomes)

    # Inference!
    # draw 1000 posterior samples using NUTS sampling
    idata = sample(1000)

print(""1000 posterior samples using NUTS sampling: "")
print(az.summary(idata, round_to=2))",train
pymc,2,"Create a Python program using the 'pymc' API to perform Bayesian inference on a simple coin flipping experiment. The program should define a prior distribution for the probability of heads, and use a likelihood function to model the observed data. After setting up the model, perform Bayesian inference to draw posterior samples using Metropolis-Hastings sampling.",code/pymc/pymc_2.py,Validate the program's results by comparing the posterior samples and summary statistics to expected values.,Test the program with different coin flip sequences to ensure it correctly performs Bayesian inference.,Assess the program's performance with larger coin flip sequences to ensure it remains efficient and accurate.,,,"#!pip install pymc arviz
from pymc import Bernoulli, Model, Metropolis, sample
import arviz as az
import numpy as np

rng = np.random.default_rng(8927)
size = 100
true_prob = 0.7

# Simulate coin flips
coin_flips = rng.binomial(1, true_prob, size=size)

with Model() as model:
    # Define prior
    p = Bernoulli(""p"", p=0.5)

    # Define likelihood
    likelihood = Bernoulli(""likelihood"", p=p, observed=coin_flips)

    # Inference!
    # draw 1000 posterior samples using Metropolis-Hastings sampling
    idata = sample(1000, tune=500, step=Metropolis())

print(""1000 posterior samples using Metropolis-Hastings sampling: "")
print(az.summary(idata, round_to=2))",train
pymc,10,"Create a Python program using the 'pymc' API to perform Bayesian inference on a dataset of observed hierarchical data. The program should define a prior distribution for the parameters of a hierarchical model, such as a hierarchical normal distribution, and use a likelihood function to model the observed data. After setting up the model, perform Bayesian inference to draw posterior samples using Hamiltonian Monte Carlo (HMC) sampling.",code/pymc/pymc_10.py,Validate the program's results by comparing the posterior samples and summary statistics to expected values.,Test the program with different datasets of observed hierarchical data to ensure it correctly performs Bayesian inference.,Assess the program's performance with larger datasets of observed hierarchical data to ensure it remains efficient and accurate.,,,"#!pip install pymc arviz
from pymc import HalfCauchy, Model, Normal, sample
import arviz as az
import numpy as np

rng = np.random.default_rng(8927)
size = 100
true_group_mean = 0
true_group_std = 1
true_individual_std = 0.5

# Simulate observed hierarchical data
group_data = rng.normal(true_group_mean, true_group_std, size=size)
individual_data = rng.normal(group_data, true_individual_std)

with Model() as model:
    # Define priors
    group_mean = Normal(""group_mean"", mu=0, sigma=10)
    group_std = HalfCauchy(""group_std"", beta=10)
    individual_std = HalfCauchy(""individual_std"", beta=10)

    # Define likelihood
    likelihood = Normal(""likelihood"", mu=group_mean, sigma=group_std, observed=individual_data)

    # Inference!
    # draw 1000 posterior samples using Hamiltonian Monte Carlo (HMC) sampling
    idata = sample(1000, tune=500)

print(""1000 posterior samples using Hamiltonian Monte Carlo (HMC) sampling: "")
print(az.summary(idata, round_to=2))",train
pymc,19,"Create a Python program using the 'pymc' API to perform Bayesian inference on a dataset of observed hierarchical data. The program should define a prior distribution for the parameters of a hierarchical model, such as a hierarchical normal distribution, and use a likelihood function to model the observed data. After setting up the model, perform Bayesian inference to draw posterior samples using NUTS sampling.",code/pymc/pymc_19.py,Validate the program's results by comparing the posterior samples and summary statistics to expected values.,Test the program with different datasets of observed hierarchical data to ensure it correctly performs Bayesian inference.,Assess the program's performance with larger datasets of observed hierarchical data to ensure it remains efficient and accurate.,,,"#!pip install pymc arviz
from pymc import HalfCauchy, Model, Normal, sample
import arviz as az
import numpy as np

rng = np.random.default_rng(8927)
size = 100
true_group_mean = 0
true_group_std = 1
true_individual_std = 0.5

# Simulate observed hierarchical data
group_data = rng.normal(true_group_mean, true_group_std, size=size)
individual_data = rng.normal(group_data, true_individual_std)

with Model() as model:
    # Define priors
    group_mean = Normal(""group_mean"", mu=0, sigma=10)
    group_std = HalfCauchy(""group_std"", beta=10)
    individual_std = HalfCauchy(""individual_std"", beta=10)

    # Define likelihood
    likelihood = Normal(""likelihood"", mu=group_mean, sigma=group_std, observed=individual_data)

    # Inference!
    # draw 1000 posterior samples using NUTS sampling
    idata = sample(1000)

print(""1000 posterior samples using NUTS sampling: "")
print(az.summary(idata, round_to=2))",train
pymc,4,"Create a Python program using the 'pymc' API to perform Bayesian inference on a dataset of observed binary outcomes. The program should define a prior distribution for the probability of success, and use a likelihood function to model the observed outcomes. After setting up the model, perform Bayesian inference to draw posterior samples using Gibbs sampling.",code/pymc/pymc_4.py,Test the program with different datasets of observed binary outcomes to ensure it correctly performs Bayesian inference.,Validate the program's results by comparing the posterior samples and summary statistics to expected values.,Assess the program's performance with larger datasets of observed binary outcomes to ensure it remains efficient and accurate.,,,"#!pip install pymc arviz
from pymc import Bernoulli, Model, sample
import arviz as az
import numpy as np

rng = np.random.default_rng(8927)
size = 100
true_prob = 0.7

# Simulate observed binary outcomes
outcomes = rng.binomial(1, true_prob, size=size)

with Model() as model:
    # Define prior
    p = Bernoulli(""p"", p=0.5)

    # Define likelihood
    likelihood = Bernoulli(""likelihood"", p=p, observed=outcomes)

    # Inference!
    # draw 1000 posterior samples using Gibbs sampling
    idata = sample(1000, tune=500, step=[p])

print(""1000 posterior samples using Gibbs sampling: "")
print(az.summary(idata, round_to=2))",train
pymc,15,"Create a Python program using the 'pymc' API to perform Bayesian inference on a dataset of observed categorical outcomes. The program should define a prior distribution for the probabilities of each category, and use a likelihood function to model the observed outcomes. After setting up the model, perform Bayesian inference to draw posterior samples using NUTS sampling.",code/pymc/pymc_15.py,Assess the program's performance with larger datasets of observed categorical outcomes to ensure it remains efficient and accurate.,Validate the program's results by comparing the posterior samples and summary statistics to expected values.,Test the program with different datasets of observed categorical outcomes to ensure it correctly performs Bayesian inference.,,,"#!pip install pymc arviz
from pymc import Categorical, Dirichlet, Model, sample
import arviz as az
import numpy as np

rng = np.random.default_rng(8927)
size = 100
true_probs = [0.3, 0.4, 0.3]

# Simulate observed categorical outcomes
outcomes = rng.choice([0, 1, 2], size=size, p=true_probs)

with Model() as model:
    # Define prior
    probs = Dirichlet(""probs"", a=np.ones(3))

    # Define likelihood
    likelihood = Categorical(""likelihood"", p=probs, observed=outcomes)

    # Inference!
    # draw 1000 posterior samples using NUTS sampling
    idata = sample(1000)

print(""1000 posterior samples using NUTS sampling: "")
print(az.summary(idata, round_to=2))",train
pymc,17,"Create a Python program using the 'pymc' API to perform Bayesian inference on a dataset of observed time series data. The program should define a prior distribution for the parameters of a time series model, such as an autoregressive (AR) model or a moving average (MA) model, and use a likelihood function to model the observed data. After setting up the model, perform Bayesian inference to draw posterior samples using NUTS sampling.",code/pymc/pymc_17.py,Test the program with different datasets of observed time series data to ensure it correctly performs Bayesian inference.,Validate the program's results by comparing the posterior samples and summary statistics to expected values.,Assess the program's performance with larger datasets of observed time series data to ensure it remains efficient and accurate.,,,"#!pip install pymc arviz
from pymc import AR, Model, Normal, sample
import arviz as az
import numpy as np

rng = np.random.default_rng(8927)
size = 100
true_ar_params = [0.5, -0.2]

# Simulate observed time series data
data = np.zeros(size)
for t in range(2, size):
    data[t] = true_ar_params[0] * data[t-1] + true_ar_params[1] * data[t-2] + rng.normal(scale=0.5)

with Model() as model:
    # Define prior
    ar_params = Normal(""ar_params"", mu=0, sigma=1, shape=2)

    # Define likelihood
    likelihood = AR(""likelihood"", rho=ar_params, observed=data)

    # Inference!
    # draw 1000 posterior samples using NUTS sampling
    idata = sample(1000)

print(""1000 posterior samples using NUTS sampling: "")
print(az.summary(idata, round_to=2))",train
pymc,20,"Create a Python program using the 'pymc' API to perform Bayesian inference on a dataset of observed missing data. The program should define a prior distribution for the parameters of a model, and use a likelihood function to model the observed data, taking into account the missing values. After setting up the model, perform Bayesian inference to draw posterior samples using NUTS sampling.",code/pymc/pymc_20.py,Test the program with different datasets of observed missing data to ensure it correctly performs Bayesian inference.,Assess the program's performance with larger datasets of observed missing data to ensure it remains efficient and accurate.,Validate the program's results by comparing the posterior samples and summary statistics to expected values.,,,"#!pip install pymc arviz
from pymc import Model, Normal, sample
import arviz as az
import numpy as np

rng = np.random.default_rng(8927)
size = 100
true_mean = 0
true_std = 1

# Simulate observed data with missing values
data = rng.normal(true_mean, true_std, size=size)
missing_indices = rng.choice(size, size=int(size/10), replace=False)
data[missing_indices] = np.nan

with Model() as model:
    # Define prior
    mean = Normal(""mean"", mu=0, sigma=10)
    std = Normal(""std"", mu=1, sigma=10)

    # Define likelihood
    likelihood = Normal(""likelihood"", mu=mean, sigma=std, observed=data)

    # Inference!
    # draw 1000 posterior samples using NUTS sampling
    idata = sample(1000)

print(""1000 posterior samples using NUTS sampling: "")
print(az.summary(idata, round_to=2))",train
pymc,3,"Create a Python program using the 'pymc' API to perform Bayesian inference on a dataset of observed counts. The program should define a prior distribution for the rate parameter, and use a likelihood function to model the observed counts. After setting up the model, perform Bayesian inference to draw posterior samples using Hamiltonian Monte Carlo (HMC) sampling.",code/pymc/pymc_3.py,Assess the program's performance with larger datasets of observed counts to ensure it remains efficient and accurate.,Validate the program's results by comparing the posterior samples and summary statistics to expected values.,Test the program with different datasets of observed counts to ensure it correctly performs Bayesian inference.,,,"#!pip install pymc arviz
from pymc import Gamma, Model, Poisson, sample
import arviz as az
import numpy as np

rng = np.random.default_rng(8927)
size = 100
true_rate = 5

# Simulate observed counts
counts = rng.poisson(true_rate, size=size)

with Model() as model:
    # Define prior
    rate = Gamma(""rate"", alpha=1, beta=1)

    # Define likelihood
    likelihood = Poisson(""likelihood"", mu=rate, observed=counts)

    # Inference!
    # draw 1000 posterior samples using Hamiltonian Monte Carlo (HMC) sampling
    idata = sample(1000, tune=500)

print(""1000 posterior samples using Hamiltonian Monte Carlo (HMC) sampling: "")
print(az.summary(idata, round_to=2))",train
pymc,11,"Create a Python program using the 'pymc' API to perform Bayesian inference on a dataset of observed missing data. The program should define a prior distribution for the parameters of a model, and use a likelihood function to model the observed data, taking into account the missing values. After setting up the model, perform Bayesian inference to draw posterior samples using Metropolis-Hastings sampling.",code/pymc/pymc_11.py,Test the program with different datasets of observed missing data to ensure it correctly performs Bayesian inference.,Assess the program's performance with larger datasets of observed missing data to ensure it remains efficient and accurate.,Validate the program's results by comparing the posterior samples and summary statistics to expected values.,,,"#!pip install pymc arviz
from pymc import Model, Normal, Metropolis, sample
import arviz as az
import numpy as np

rng = np.random.default_rng(8927)
size = 100
true_mean = 0
true_std = 1

# Simulate observed data with missing values
data = rng.normal(true_mean, true_std, size=size)
missing_indices = rng.choice(size, size=int(size/10), replace=False)
data[missing_indices] = np.nan

with Model() as model:
    # Define prior
    mean = Normal(""mean"", mu=0, sigma=10)
    std = Normal(""std"", mu=1, sigma=10)

    # Define likelihood
    likelihood = Normal(""likelihood"", mu=mean, sigma=std, observed=data)

    # Inference!
    # draw 1000 posterior samples using Metropolis-Hastings sampling
    idata = sample(1000, tune=500, step=Metropolis())

print(""1000 posterior samples using Metropolis-Hastings sampling: "")
print(az.summary(idata, round_to=2))",train
pymc,21,"Create a Python program using the 'pymc' API to perform Bayesian inference on a dataset of observed count data. The program should define a prior distribution for the rate parameter, and use a likelihood function to model the observed counts. After setting up the model, perform Bayesian inference to draw posterior samples using NUTS sampling.",code/pymc/pymc_21.py,Assess the program's performance with larger datasets of observed count data to ensure it remains efficient and accurate.,Validate the program's results by comparing the posterior samples and summary statistics to expected values.,Test the program with different datasets of observed count data to ensure it correctly performs Bayesian inference.,,,"#!pip install pymc arviz
from pymc import Gamma, Model, Poisson, sample
import arviz as az
import numpy as np

rng = np.random.default_rng(8927)
size = 100
true_rate = 5

# Simulate observed counts
counts = rng.poisson(true_rate, size=size)

with Model() as model:
    # Define prior
    rate = Gamma(""rate"", alpha=1, beta=1)

    # Define likelihood
    likelihood = Poisson(""likelihood"", mu=rate, observed=counts)

    # Inference!
    # draw 1000 posterior samples using NUTS sampling
    idata = sample(1000)

print(""1000 posterior samples using NUTS sampling: "")
print(az.summary(idata, round_to=2))",train
pymc,5,"Create a Python program using the 'pymc' API to perform Bayesian inference on a dataset of observed categorical outcomes. The program should define a prior distribution for the probabilities of each category, and use a likelihood function to model the observed outcomes. After setting up the model, perform Bayesian inference to draw posterior samples using Metropolis-Hastings sampling.",code/pymc/pymc_5.py,Assess the program's performance with larger datasets of observed categorical outcomes to ensure it remains efficient and accurate.,Validate the program's results by comparing the posterior samples and summary statistics to expected values.,Test the program with different datasets of observed categorical outcomes to ensure it correctly performs Bayesian inference.,,,"#!pip install pymc arviz
from pymc import Categorical, Dirichlet, Model, Metropolis, sample
import arviz as az
import numpy as np

rng = np.random.default_rng(8927)
size = 100
true_probs = [0.3, 0.4, 0.3]

# Simulate observed categorical outcomes
outcomes = rng.choice([0, 1, 2], size=size, p=true_probs)

with Model() as model:
    # Define prior
    probs = Dirichlet(""probs"", a=np.ones(3))

    # Define likelihood
    likelihood = Categorical(""likelihood"", p=probs, observed=outcomes)

    # Inference!
    # draw 1000 posterior samples using Metropolis-Hastings sampling
    idata = sample(1000, tune=500, step=Metropolis())

print(""1000 posterior samples using Metropolis-Hastings sampling: "")
print(az.summary(idata, round_to=2))",train
pymc,14,"Create a Python program using the 'pymc' API to perform Bayesian inference on a dataset of observed continuous outcomes. The program should define a prior distribution for the mean and standard deviation, and use a likelihood function to model the observed outcomes. After setting up the model, perform Bayesian inference to draw posterior samples using NUTS sampling.",code/pymc/pymc_14.py,Test the program with different datasets of observed continuous outcomes to ensure it correctly performs Bayesian inference.,Assess the program's performance with larger datasets of observed continuous outcomes to ensure it remains efficient and accurate.,Validate the program's results by comparing the posterior samples and summary statistics to expected values.,,,"#!pip install pymc arviz
from pymc import Model, Normal, sample
import arviz as az
import numpy as np

rng = np.random.default_rng(8927)
size = 100
true_mean = 0
true_std = 1

# Simulate observed continuous outcomes
outcomes = rng.normal(true_mean, true_std, size=size)

with Model() as model:
    # Define prior
    mean = Normal(""mean"", mu=0, sigma=10)
    std = Normal(""std"", mu=1, sigma=10)

    # Define likelihood
    likelihood = Normal(""likelihood"", mu=mean, sigma=std, observed=outcomes)

    # Inference!
    # draw 1000 posterior samples using NUTS sampling
    idata = sample(1000)

print(""1000 posterior samples using NUTS sampling: "")
print(az.summary(idata, round_to=2))",train
pymc,8,"Create a Python program using the 'pymc' API to perform Bayesian inference on a dataset of observed time series data. The program should define a prior distribution for the parameters of a time series model, such as an autoregressive (AR) model or a moving average (MA) model, and use a likelihood function to model the observed data. After setting up the model, perform Bayesian inference to draw posterior samples using Hamiltonian Monte Carlo (HMC) sampling.",code/pymc/pymc_8.py,Test the program with different datasets of observed time series data to ensure it correctly performs Bayesian inference.,Validate the program's results by comparing the posterior samples and summary statistics to expected values.,Assess the program's performance with larger datasets of observed time series data to ensure it remains efficient and accurate.,,,"#!pip install pymc arviz
from pymc import AR, Model, Normal, sample
import arviz as az
import numpy as np

rng = np.random.default_rng(8927)
size = 100
true_ar_params = [0.5, -0.2]

# Simulate observed time series data
data = np.zeros(size)
for t in range(2, size):
    data[t] = true_ar_params[0] * data[t-1] + true_ar_params[1] * data[t-2] + rng.normal(scale=0.5)

with Model() as model:
    # Define prior
    ar_params = Normal(""ar_params"", mu=0, sigma=1, shape=2)

    # Define likelihood
    likelihood = AR(""likelihood"", rho=ar_params, observed=data)

    # Inference!
    # draw 1000 posterior samples using Hamiltonian Monte Carlo (HMC) sampling
    idata = sample(1000, tune=500)

print(""1000 posterior samples using Hamiltonian Monte Carlo (HMC) sampling: "")
print(az.summary(idata, round_to=2))",train
pymc,12,"Create a Python program using the 'pymc' API to perform Bayesian inference on a dataset of observed count data. The program should define a prior distribution for the rate parameter, and use a likelihood function to model the observed counts. After setting up the model, perform Bayesian inference to draw posterior samples using Hamiltonian Monte Carlo (HMC) sampling.",code/pymc/pymc_12.py,Assess the program's performance with larger datasets of observed count data to ensure it remains efficient and accurate.,Validate the program's results by comparing the posterior samples and summary statistics to expected values.,Test the program with different datasets of observed count data to ensure it correctly performs Bayesian inference.,,,"#!pip install pymc arviz
from pymc import Gamma, Model, Poisson, sample
import arviz as az
import numpy as np

rng = np.random.default_rng(8927)
size = 100
true_rate = 5

# Simulate observed counts
counts = rng.poisson(true_rate, size=size)

with Model() as model:
    # Define prior
    rate = Gamma(""rate"", alpha=1, beta=1)

    # Define likelihood
    likelihood = Poisson(""likelihood"", mu=rate, observed=counts)

    # Inference!
    # draw 1000 posterior samples using Hamiltonian Monte Carlo (HMC) sampling
    idata = sample(1000, tune=500)

print(""1000 posterior samples using Hamiltonian Monte Carlo (HMC) sampling: "")
print(az.summary(idata, round_to=2))",train
pymc,16,"Create a Python program using the 'pymc' API to perform Bayesian inference on a dataset of observed ordinal outcomes. The program should define a prior distribution for the probabilities of each category, and use a likelihood function to model the observed outcomes. After setting up the model, perform Bayesian inference to draw posterior samples using NUTS sampling.",code/pymc/pymc_16.py,Test the program with different datasets of observed ordinal outcomes to ensure it correctly performs Bayesian inference.,Assess the program's performance with larger datasets of observed ordinal outcomes to ensure it remains efficient and accurate.,Validate the program's results by comparing the posterior samples and summary statistics to expected values.,,,"#!pip install pymc arviz
from pymc import Categorical, Model, sample
import arviz as az
import numpy as np

rng = np.random.default_rng(8927)
size = 100
true_probs = [0.3, 0.4, 0.3]

# Simulate observed ordinal outcomes
outcomes = rng.choice([0, 1, 2], size=size, p=true_probs)

with Model() as model:
    # Define prior
    probs = Categorical(""probs"", p=np.ones(3) / 3, shape=3)

    # Define likelihood
    likelihood = Categorical(""likelihood"", p=probs, observed=outcomes)

    # Inference!
    # draw 1000 posterior samples using NUTS sampling
    idata = sample(1000)

print(""1000 posterior samples using NUTS sampling: "")
print(az.summary(idata, round_to=2))",train
pymc,7,"Create a Python program using the 'pymc' API to perform Bayesian inference on a dataset of observed ordinal outcomes. The program should define a prior distribution for the probabilities of each category, and use a likelihood function to model the observed outcomes. After setting up the model, perform Bayesian inference to draw posterior samples using Gibbs sampling.",code/pymc/pymc_7.py,Test the program with different datasets of observed ordinal outcomes to ensure it correctly performs Bayesian inference.,Assess the program's performance with larger datasets of observed ordinal outcomes to ensure it remains efficient and accurate.,Validate the program's results by comparing the posterior samples and summary statistics to expected values.,,,"#!pip install pymc arviz
from pymc import Categorical, Model, sample
import arviz as az
import numpy as np

rng = np.random.default_rng(8927)
size = 100
true_probs = [0.3, 0.4, 0.3]

# Simulate observed ordinal outcomes
outcomes = rng.choice([0, 1, 2], size=size, p=true_probs)

with Model() as model:
    # Define prior
    probs = Categorical(""probs"", p=np.ones(3) / 3, shape=3)

    # Define likelihood
    likelihood = Categorical(""likelihood"", p=probs, observed=outcomes)

    # Inference!
    # draw 1000 posterior samples using Gibbs sampling
    idata = sample(1000, tune=500, step=[probs])

print(""1000 posterior samples using Gibbs sampling: "")
print(az.summary(idata, round_to=2))",train
pymc,1,"Create a Python program using the 'pymc' API to perform Bayesian linear regression on a dataset. The program should define prior distributions for the model's parameters, including the intercept and slope, and use a likelihood function to model the observed data. After setting up the model, perform Bayesian inference to draw posterior samples using NUTS sampling.",code/pymc/pymc_1.py,Test the program with different datasets to ensure it correctly performs Bayesian linear regression.,Assess the program's performance with larger datasets to ensure it remains efficient and accurate.,Validate the program's results by comparing the posterior samples and summary statistics to expected values.,,,"#!pip install pymc arviz
from pymc import HalfCauchy, Model, Normal, sample
import arviz as az
import numpy as np

rng = np.random.default_rng(8927)
size = 200
true_intercept = 1
true_slope = 2

x = np.linspace(0, 1, size)
# y = a + b*x
true_regression_line = true_intercept + true_slope * x
# add noise
y = true_regression_line + rng.normal(scale=0.5, size=size)


with Model() as model:  
    # Define priors
    sigma = HalfCauchy(""sigma"", beta=10)
    intercept = Normal(""Intercept"", 0, sigma=20)
    slope = Normal(""slope"", 0, sigma=20)

    # Define likelihood
    likelihood = Normal(""y"", mu=intercept + slope * x, sigma=sigma, observed=y)

    # Inference!
    # draw 30 posterior samples using NUTS sampling
    idata = sample(30)

print(""30 posterior samples using NUTS sampling: "")
print(az.summary(idata, round_to=2))",test
pymc,6,"Create a Python program using the 'pymc' API to perform Bayesian inference on a dataset of observed continuous outcomes. The program should define a prior distribution for the mean and standard deviation, and use a likelihood function to model the observed outcomes. After setting up the model, perform Bayesian inference to draw posterior samples using Hamiltonian Monte Carlo (HMC) sampling.",code/pymc/pymc_6.py,Test the program with different datasets of observed continuous outcomes to ensure it correctly performs Bayesian inference.,Assess the program's performance with larger datasets of observed continuous outcomes to ensure it remains efficient and accurate.,Validate the program's results by comparing the posterior samples and summary statistics to expected values.,,,"#!pip install pymc arviz
from pymc import Model, Normal, sample
import arviz as az
import numpy as np

rng = np.random.default_rng(8927)
size = 100
true_mean = 0
true_std = 1

# Simulate observed continuous outcomes
outcomes = rng.normal(true_mean, true_std, size=size)

with Model() as model:
    # Define prior
    mean = Normal(""mean"", mu=0, sigma=10)
    std = Normal(""std"", mu=1, sigma=10)

    # Define likelihood
    likelihood = Normal(""likelihood"", mu=mean, sigma=std, observed=outcomes)

    # Inference!
    # draw 1000 posterior samples using Hamiltonian Monte Carlo (HMC) sampling
    idata = sample(1000, tune=500)

print(""1000 posterior samples using Hamiltonian Monte Carlo (HMC) sampling: "")
print(az.summary(idata, round_to=2))",test
pypdf,9,Create a Python program using the 'pypdf' API to password-protect a PDF file with a specified password.,code/pypdf/pypdf_9.py,Verify that the protected PDF can only be opened with the specified password.,Check for cases where the input PDF file may not exist.,Test if the program successfully password-protects the specified PDF file.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file to be protected
pdf_file_path = ""document.pdf""

# Specify the password for the PDF
password = ""my_password""

# Create a PDF writer object
pdf_writer = pypdf.PdfWriter()

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Set a password for the PDF
    pdf_reader.Info.Encrypt = f'/Standard 0 R\n{password}'

    # Add all pages to the writer
    for page in pdf_reader.pages:
        pdf_writer.add_page(page)

    # Save the protected PDF with the specified password
    output_pdf_path = ""protected_document.pdf""
    with open(output_pdf_path, ""wb"") as output_pdf:
        pdf_writer.write(output_pdf)
",train
pypdf,25,Create a Python program using the 'pypdf' API to extract and display the content of specific layers (optional content groups) in a layered PDF file.,code/pypdf/pypdf_25.py,Check for cases where the input PDF file may not be layered or may not have the specified layers.,Verify that the displayed content accurately represents the layers in the PDF.,Test if the program successfully extracts and displays the content of the specified layers (optional content groups) in the specified layered PDF file.,,,"#!pip install pypdf
import pypdf

# Specify the path of the layered PDF file
pdf_file_path = ""layered_document.pdf""

# Specify the names of the layers (optional content groups) to extract
layer_names = [""Layer1"", ""Layer2""]

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Extract and display the content of the specified layers
    for page in pdf_reader.pages:
        if '/OCProperties' in page and '/OCGs' in page['/OCProperties']:
            layers = page['/OCProperties']['/OCGs']
            for layer in layers:
                layer_name = layer['/Name']
                if layer_name in layer_names:
                    print(f""Layer: {layer_name}"")
                    page_text = page.extract_text()
                    print(page_text)
",train
pypdf,22,"Create a Python program using the 'pypdf' API to extract and display the page labels used in a PDF file (e.g., roman numerals, custom labels).",code/pypdf/pypdf_22.py,Test if the program successfully extracts and displays the page labels used in the specified PDF file.,Check for cases where the input PDF file may not have custom page labels.,Verify that the displayed page labels are accurate and represent the page numbering in the PDF.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file with page labels
pdf_file_path = ""document_with_page_labels.pdf""

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Extract page labels from the PDF
    page_labels = pdf_reader.PageLabels

    # Display the extracted page labels
    for page_num, label in enumerate(page_labels):
        print(f""Page {page_num + 1}: {label}"")
",train
pypdf,13,Create a Python program using the 'pypdf' API to extract and display the fonts used in a PDF file.,code/pypdf/pypdf_13.py,Test if the program successfully extracts and displays the fonts used in the specified PDF file.,Verify that the displayed fonts are unique and represent the fonts used in the PDF.,Check for cases where the input PDF file may not contain fonts.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file
pdf_file_path = ""document.pdf""

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Initialize a set to store the unique fonts used in the PDF
    unique_fonts = set()

    # Iterate through the pages and extract font information
    for page in pdf_reader.pages:
        font_resources = page['/Resources']['/Font'].get_object()

        # Extract font names and add them to the set
        for font_name in font_resources.keys():
            unique_fonts.add(font_name)

    # Display the unique fonts used in the PDF
    for font in unique_fonts:
        print(font)
",train
pypdf,33,"Create a Python program using the 'pypdf' API to extract and display the list of all form fields in a PDF file, including text fields, checkboxes, and radio buttons.",code/pypdf/pypdf_33.py,"Test if the program successfully extracts and displays text fields, checkboxes, and radio buttons from the specified PDF file.",Check for cases where the input PDF file may not have form fields.,Verify that the displayed form fields and values are accurate and represent the forms in the PDF.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file with form fields
pdf_file_path = ""document_with_forms.pdf""

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Initialize dictionaries to store form fields of different types
    text_fields = {}
    checkboxes = {}
    radio_buttons = {}

    # Iterate through the pages and extract form fields
    for page in pdf_reader.pages:
        if '/Annots' in page:
            for annot in page['/Annots']:
                if '/FT' in annot and annot['/FT'] == '/Tx':
                    # Text Field
                    if '/T' in annot and '/V' in annot:
                        field_name = annot['/T']
                        field_value = annot['/V']
                        text_fields[field_name] = field_value
                elif '/FT' in annot and annot['/FT'] == '/Btn':
                    # Checkbox or Radio Button
                    if '/T' in annot:
                        field_name = annot['/T']
                        if '/V' in annot and '/V' in annot:
                            field_value = annot['/V']
                        else:
                            field_value = 'Off'
                        if '/FT' in annot and annot['/FT'] == '/Btn':
                            checkboxes[field_name] = field_value
                        else:
                            radio_buttons[field_name] = field_value

    # Display the extracted form fields
    print(""Text Fields:"")
    for field_name, field_value in text_fields.items():
        print(f""{field_name}: {field_value}"")

    print(""\nCheckboxes:"")
    for field_name, field_value in checkboxes.items():
        print(f""{field_name}: {field_value}"")

    print(""\nRadio Buttons:"")
    for field_name, field_value in radio_buttons.items():
        print(f""{field_name}: {field_value}"")
",train
pypdf,10,Create a Python program using the 'pypdf' API to remove all bookmarks from a PDF file and save the modified PDF.,code/pypdf/pypdf_10.py,Check for cases where the input PDF file may not have bookmarks.,Test if the program successfully removes all bookmarks from the specified PDF file.,Verify that the modified PDF no longer contains bookmarks.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file from which to remove bookmarks
pdf_file_path = ""document_with_bookmarks.pdf""

# Create a PDF writer object
pdf_writer = pypdf.PdfWriter()

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Remove all bookmarks from the PDF
    pdf_reader.Info.Bookmarks = []

    # Add all pages to the writer (without bookmarks)
    for page in pdf_reader.pages:
        pdf_writer.add_page(page)

    # Save the modified PDF without bookmarks
    output_pdf_path = ""document_without_bookmarks.pdf""
    with open(output_pdf_path, ""wb"") as output_pdf:
        pdf_writer.write(output_pdf)
",train
pypdf,1,Create a Python program using the 'pypdf' API to extract text content from a PDF file and save it to a text file.,code/pypdf/pypdf_1.py,Test if the program successfully extracts text from the specified PDF file.,Verify that the extracted text matches the content of the PDF file.,Check that the extracted text is correctly saved to the output text file.,,,"#!pip install pypdf
import pypdf

# Specify the path to the PDF file
pdf_file_path = ""sample.pdf""

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Initialize a variable to store the extracted text
    extracted_text = """"

    # Iterate through all pages and extract text
    for page_num in range(len(pdf_reader.pages)):
        page = pdf_reader.pages[page_num]
        extracted_text += page.extract_text()

# Save the extracted text to a text file
output_file_path = ""extracted_text.txt""
with open(output_file_path, ""w"") as text_file:
    text_file.write(extracted_text)

# Display the extracted text or save it to a new text file for reference
print(extracted_text)
",train
pypdf,5,Create a Python program using the 'pypdf' API to add watermarks to each page of a PDF file and save the modified PDF.,code/pypdf/pypdf_5.py,Test if the program successfully adds watermarks to each page of the specified PDF file.,Check for cases where either the input PDF or watermark PDF may not exist.,Verify that the watermarked PDF contains the watermark on each page.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file to which you want to add a watermark
pdf_file_path = ""document.pdf""

# Specify the path of the watermark PDF
watermark_file_path = ""watermark.pdf""

# Create a PDF writer object
pdf_writer = pypdf.PdfWriter()

# Open the PDF file and watermark file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file, open(watermark_file_path, ""rb"") as watermark_file:
    # Create PDF reader objects
    pdf_reader = pypdf.PdfReader(pdf_file)
    watermark_reader = pypdf.PdfReader(watermark_file)

    # Get the first page of the watermark PDF (you can modify this based on your needs)
    watermark_page = watermark_reader.pages[0]

    # Iterate through the pages of the original PDF
    for page in pdf_reader.pages:
        # Merge the watermark page with the original page
        page.merge_page(watermark_page)
        pdf_writer.add_page(page)

# Save the modified PDF with watermarks
output_pdf_path = ""document_with_watermark.pdf""
with open(output_pdf_path, ""wb"") as output_pdf:
    pdf_writer.write(output_pdf)
",train
pypdf,17,"Create a Python program using the 'pypdf' API to extract and display the form fields from a PDF file, including text fields, checkboxes, and radio buttons.",code/pypdf/pypdf_17.py,"Test if the program successfully extracts and displays text fields, checkboxes, and radio buttons from the specified PDF file.",Check for cases where the input PDF file may not have form fields.,Verify that the displayed form fields and values are accurate and represent the forms in the PDF.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file with form fields
pdf_file_path = ""document_with_forms.pdf""

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Initialize dictionaries to store form fields of different types
    text_fields = {}
    checkboxes = {}
    radio_buttons = {}

    # Iterate through the pages and extract form fields
    for page in pdf_reader.pages:
        if '/Annots' in page:
            for annot in page['/Annots']:
                if '/FT' in annot and annot['/FT'] == '/Tx':
                    # Text Field
                    if '/T' in annot and '/V' in annot:
                        field_name = annot['/T']
                        field_value = annot['/V']
                        text_fields[field_name] = field_value
                elif '/FT' in annot and annot['/FT'] == '/Btn':
                    # Checkbox or Radio Button
                    if '/T' in annot:
                        field_name = annot['/T']
                        if '/V' in annot and '/V' in annot:
                            field_value = annot['/V']
                        else:
                            field_value = 'Off'
                        if '/FT' in annot and annot['/FT'] == '/Btn':
                            checkboxes[field_name] = field_value
                        else:
                            radio_buttons[field_name] = field_value

    # Display the extracted form fields
    print(""Text Fields:"")
    for field_name, field_value in text_fields.items():
        print(f""{field_name}: {field_value}"")

    print(""\nCheckboxes:"")
    for field_name, field_value in checkboxes.items():
        print(f""{field_name}: {field_value}"")

    print(""\nRadio Buttons:"")
    for field_name, field_value in radio_buttons.items():
        print(f""{field_name}: {field_value}"")
",train
pypdf,18,"Create a Python program using the 'pypdf' API to extract and display the document properties (metadata) of a PDF file, including title, author, and subject.",code/pypdf/pypdf_18.py,Check for cases where the input PDF file may not have document properties.,Verify that the displayed document properties match the metadata in the PDF.,Test if the program successfully extracts and displays document properties (metadata) from the specified PDF file.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file with document properties
pdf_file_path = ""document_with_metadata.pdf""

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Extract document properties (metadata) from the PDF
    document_properties = pdf_reader.Info

    # Display the document properties
    print(f""Title: {document_properties.get('/Title', 'N/A')}"")
    print(f""Author: {document_properties.get('/Author', 'N/A')}"")
    print(f""Subject: {document_properties.get('/Subject', 'N/A')}"")
",train
pypdf,6,"Create a Python program using the 'pypdf' API to split a PDF file into multiple smaller PDF files, each containing a specified number of pages.",code/pypdf/pypdf_6.py,Check for cases where the input PDF may have fewer pages than the specified number for splitting.,Verify that the split PDFs have the correct number of pages as specified.,Test if the program successfully splits the specified PDF file into smaller PDFs.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file to be split
pdf_file_path = ""large_document.pdf""

# Specify the number of pages for each split PDF
pages_per_split = 10

# Create a PDF reader object for the input PDF
with open(pdf_file_path, ""rb"") as pdf_file:
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Determine the total number of pages in the input PDF
    total_pages = len(pdf_reader.pages)

    # Calculate the number of splits required
    num_splits = total_pages // pages_per_split
    if total_pages % pages_per_split != 0:
        num_splits += 1

    # Split the PDF into smaller PDF files
    for split_num in range(num_splits):
        # Create a PDF writer for each split
        pdf_writer = pypdf.PdfWriter()

        # Determine the range of pages for the current split
        start_page = split_num * pages_per_split
        end_page = min((split_num + 1) * pages_per_split, total_pages)

        # Add pages to the current split
        for page_num in range(start_page, end_page):
            pdf_writer.add_page(pdf_reader.pages[page_num])

        # Save the split PDF as a new file
        split_pdf_path = f""split_{split_num + 1}.pdf""
        with open(split_pdf_path, ""wb"") as split_pdf_file:
            pdf_writer.write(split_pdf_file)
",train
pypdf,14,Create a Python program using the 'pypdf' API to extract all hyperlinks from a PDF file and display them.,code/pypdf/pypdf_14.py,Test if the program successfully extracts and displays hyperlinks from the specified PDF file.,Check for cases where the input PDF file may not contain hyperlinks.,Verify that the displayed hyperlinks are valid and correspond to the links in the PDF.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file with hyperlinks
pdf_file_path = ""document_with_hyperlinks.pdf""

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Initialize a list to store the extracted hyperlinks
    extracted_hyperlinks = []

    # Iterate through the pages and extract hyperlinks
    for page in pdf_reader.pages:
        if '/Annots' in page:
            for annot in page['/Annots']:
                if annot['/Subtype'] == '/Link':
                    link = annot['/A']
                    if '/URI' in link:
                        extracted_hyperlinks.append(link['/URI'])

    # Display the extracted hyperlinks
    for hyperlink in extracted_hyperlinks:
        print(hyperlink)
",train
pypdf,12,Create a Python program using the 'pypdf' API to search for specific text within a PDF file and highlight or mark the search results.,code/pypdf/pypdf_12.py,Check for cases where the input PDF file may not exist or the search text is not found.,Test if the program successfully searches for and highlights the specified text in the PDF file.,Verify that the search results are correctly highlighted in the modified PDF.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file to search
pdf_file_path = ""document.pdf""

# Specify the text to search for
search_text = ""Python""

# Specify the color for highlighting search results (e.g., red)
highlight_color = (1, 0, 0)

# Create a PDF writer object
pdf_writer = pypdf.PdfWriter()

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Iterate through the pages and search for the text
    for page in pdf_reader.pages:
        # Extract text from the page
        page_text = page.extract_text()

        # Check if the search text is present in the page
        if search_text in page_text:
            # Create an Annot object to highlight the search result
            highlight_annot = pypdf.Annot()
            highlight_annot.Page = page
            highlight_annot.Type = '/Highlight'
            highlight_annot.Rect = [0, 0, 0, 0]
            highlight_annot.OverlayText = search_text
            highlight_annot.Color = highlight_color

            # Add the highlight annotation to the page
            page.Annots.append(highlight_annot)

        # Add the page (with or without highlights) to the writer
        pdf_writer.add_page(page)

    # Save the modified PDF with search result highlights
    output_pdf_path = ""document_with_highlights.pdf""
    with open(output_pdf_path, ""wb"") as output_pdf:
        pdf_writer.write(output_pdf)
",train
pypdf,2,Create a Python program using the 'pypdf' API to extract metadata information from a PDF file and display it.,code/pypdf/pypdf_2.py,Check for cases where metadata fields may be empty or missing in the PDF.,Verify that the displayed metadata matches the actual metadata of the PDF file.,Test if the program successfully extracts metadata information from the specified PDF file.,,,"#!pip install pypdf
import pypdf

# Specify the path to the PDF file
pdf_file_path = ""sample.pdf""

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Extract metadata information from the PDF
    metadata = pdf_reader.info

# Display the extracted metadata
for key, value in metadata.items():
    print(f""{key}: {value}"")
",train
pypdf,3,Create a Python program using the 'pypdf' API to extract images from a PDF file and save them as separate image files.,code/pypdf/pypdf_3.py,Test if the program successfully extracts images from the specified PDF file.,Check for cases where the PDF may not contain any images.,Verify that the saved images match the images in the PDF file.,,,"#!pip install pypdf
import pypdf
import io
from PIL import Image

# Specify the path to the PDF file
pdf_file_path = ""sample.pdf""

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Initialize a variable to store the extracted images
    extracted_images = []

    # Iterate through all pages and extract images
    for page_num in range(len(pdf_reader.pages)):
        page = pdf_reader.pages[page_num]
        xObject = page['/Resources']['/XObject'].get_object()
        
        for obj in xObject:
            if xObject[obj]['/Subtype'] == '/Image':
                image_data = xObject[obj].get_data()
                image = Image.open(io.BytesIO(image_data))
                extracted_images.append(image)
    
    # Save the extracted images as separate image files
    for i, image in enumerate(extracted_images):
        image.save(f""extracted_image_{i}.png"")

# Display or save the extracted images for reference
for i, image in enumerate(extracted_images):
    image.show()
",train
pypdf,31,"Create a Python program using the 'pypdf' API to extract and display the list of all links (hyperlinks and internal links) in a PDF file, along with their destinations.",code/pypdf/pypdf_31.py,"Test if the program successfully extracts and displays the list of links (hyperlinks and internal links) in the specified PDF file, along with their destinations.",Verify that the displayed link information accurately represents the links in the PDF.,Check for cases where the input PDF file may not have links.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file with links
pdf_file_path = ""document_with_links.pdf""

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Initialize a list to store link information
    link_info = []

    # Iterate through the pages and extract link information
    for page_num, page in enumerate(pdf_reader.pages, start=1):
        if '/Annots' in page:
            for annot in page['/Annots']:
                if '/Subtype' in annot:
                    subtype = annot['/Subtype']
                    if subtype == '/Link':
                        link_info.append(f""Page {page_num}: External Link to '{annot['/A']['/URI']}'"")
                    elif subtype == '/Link' and '/Dest' in annot:
                        destination = annot['/Dest']
                        link_info.append(f""Page {page_num}: Internal Link to Page {destination[0] + 1}"")

    # Display the list of links and their destinations
    for info in link_info:
        print(info)
",train
pypdf,4,Create a Python program using the 'pypdf' API to merge multiple PDF files into a single PDF file.,code/pypdf/pypdf_4.py,Verify that the merged PDF contains all the pages from the input files in the correct order.,Check for cases where the input PDF files may not exist.,Test if the program successfully merges the specified PDF files.,,,"#!pip install pypdf
import pypdf

# Specify the paths of the PDF files to be merged
pdf_files_to_merge = [""file1.pdf"", ""file2.pdf"", ""file3.pdf""]

# Create a PDF writer object to generate the merged PDF
pdf_writer = pypdf.PdfWriter()

# Iterate through the input PDF files and append them to the writer
for pdf_file_path in pdf_files_to_merge:
    with open(pdf_file_path, ""rb"") as pdf_file:
        pdf_reader = pypdf.PdfReader(pdf_file)
        pdf_writer.add_page(pdf_reader.pages)

# Save the merged PDF to a new file
merged_pdf_file = ""merged.pdf""
with open(merged_pdf_file, ""wb"") as merged_pdf:
    pdf_writer.write(merged_pdf)

# Verify that the merged PDF has been created successfully
",train
pypdf,30,"Create a Python program using the 'pypdf' API to extract and display the list of all images used in a PDF file, including their formats (e.g., JPEG, PNG).",code/pypdf/pypdf_30.py,Check for cases where the input PDF file may not contain images.,"Test if the program successfully extracts and displays the list of images used in the specified PDF file, along with their formats (e.g., JPEG, PNG).",Verify that the displayed image formats accurately represent the images in the PDF.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file
pdf_file_path = ""document_with_images.pdf""

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Initialize a list to store image information
    image_info = []

    # Iterate through the pages and extract image information
    for page_num, page in enumerate(pdf_reader.pages, start=1):
        xObject = page['/Resources']['/XObject'].get_object()
        for obj in xObject:
            if xObject[obj]['/Subtype'] == '/Image':
                image_format = xObject[obj]['/Filter'][0][1:]  # Extract the image format (e.g., JPEG, PNG)
                image_info.append(f""Page {page_num}: Image format: {image_format}"")

    # Display the list of images and their formats
    for info in image_info:
        print(info)
",train
pypdf,24,Create a Python program using the 'pypdf' API to extract and display the document structure (tags and labels) of a tagged PDF file.,code/pypdf/pypdf_24.py,Test if the program successfully extracts and displays the document structure (tags and labels) of the specified tagged PDF file.,Check for cases where the input PDF file may not be tagged or have a document structure.,Verify that the displayed structure accurately represents the tagging in the PDF.,,,"#!pip install pypdf
import pypdf

# Specify the path of the tagged PDF file
pdf_file_path = ""tagged_document.pdf""

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Extract document structure (tags and labels) from the PDF
    document_structure = pdf_reader.StructTree

    # Display the extracted document structure
    def display_structure(structure, level=0):
        for item in structure:
            if isinstance(item, list):
                # If item is a list, it represents a structure with a tag and children
                tag, children = item[0], item[1:]
                print(""  "" * level + tag)
                display_structure(children, level + 1)
            else:
                # If item is a string, it represents a label at the current level
                print(""  "" * level + item)

    display_structure(document_structure, 0)
",train
pypdf,32,"Create a Python program using the 'pypdf' API to extract and display the list of all text annotations (comments) in a PDF file, including their contents and locations.",code/pypdf/pypdf_32.py,Check for cases where the input PDF file may not have text annotations.,"Test if the program successfully extracts and displays the list of text annotations (comments) in the specified PDF file, including their contents and locations.",Verify that the displayed text annotations accurately represent the comments in the PDF.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file with text annotations (comments)
pdf_file_path = ""document_with_text_annotations.pdf""

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Initialize a list to store text annotation information
    text_annotation_info = []

    # Iterate through the pages and extract text annotations
    for page_num, page in enumerate(pdf_reader.pages, start=1):
        if '/Annots' in page:
            for annot in page['/Annots']:
                if '/Subtype' in annot and annot['/Subtype'] == '/Text':
                    annotation_text = annot['/Contents']
                    annotation_location = annot['/Rect']
                    text_annotation_info.append(f""Page {page_num}: {annotation_text} (Location: {annotation_location})"")

    # Display the list of text annotations and their contents
    for info in text_annotation_info:
        print(info)
",train
pypdf,23,Create a Python program using the 'pypdf' API to extract and display the list of all named destinations (links) in a PDF file.,code/pypdf/pypdf_23.py,Test if the program successfully extracts and displays the list of named destinations (links) in the specified PDF file.,Verify that the displayed named destinations accurately represent the links in the PDF.,Check for cases where the input PDF file may not have named destinations.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file with named destinations
pdf_file_path = ""document_with_named_destinations.pdf""

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Extract named destinations (links) from the PDF
    named_destinations = pdf_reader.named_dests

    # Display the list of named destinations
    for dest_name, dest_page_num in named_destinations.items():
        print(f""{dest_name}: Page {dest_page_num + 1}"")
",train
pypdf,19,"Create a Python program using the 'pypdf' API to extract and display the comments and annotations (e.g., sticky notes) from a PDF file.",code/pypdf/pypdf_19.py,Test if the program successfully extracts and displays comments and annotations from the specified PDF file.,Verify that the displayed comments and annotations accurately represent the content of the PDF.,Check for cases where the input PDF file may not have comments or annotations.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file with comments and annotations
pdf_file_path = ""document_with_comments.pdf""

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Initialize a list to store comments and annotations
    comments_and_annotations = []

    # Iterate through the pages and extract comments and annotations
    for page in pdf_reader.pages:
        if '/Annots' in page:
            for annot in page['/Annots']:
                if '/Contents' in annot:
                    comment_text = annot['/Contents']
                    comments_and_annotations.append(comment_text)

    # Display the extracted comments and annotations
    for comment in comments_and_annotations:
        print(comment)
",train
pypdf,26,Create a Python program using the 'pypdf' API to extract and display the list of all page labels and their corresponding page numbers in a PDF file.,code/pypdf/pypdf_26.py,Verify that the displayed page labels and page numbers are accurate and represent the PDF structure.,Check for cases where the input PDF file may not have custom page labels.,Test if the program successfully extracts and displays the list of page labels and their corresponding page numbers in the specified PDF file.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file with page labels
pdf_file_path = ""document_with_page_labels.pdf""

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Extract page labels and their corresponding page numbers from the PDF
    page_labels = pdf_reader.page_labels

    # Display the list of page labels and page numbers
    for page_num, label in enumerate(page_labels):
        print(f""Page {page_num + 1}: {label}"")
",train
pypdf,7,"Create a Python program using the 'pypdf' API to rotate all pages in a PDF file by a specified angle (e.g., 90 degrees) and save the modified PDF.",code/pypdf/pypdf_7.py,Check for cases where the input PDF may not exist.,Verify that the pages in the rotated PDF have the correct rotation angle.,Test if the program successfully rotates all pages in the specified PDF file.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file to be rotated
pdf_file_path = ""document.pdf""

# Specify the rotation angle (e.g., 90 degrees for clockwise rotation)
rotation_angle = 90

# Create a PDF writer object
pdf_writer = pypdf.PdfWriter()

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Rotate each page and add it to the writer
    for page in pdf_reader.pages:
        page.Rotate = (page.Rotate + rotation_angle) % 360
        pdf_writer.add_page(page)

# Save the modified PDF with rotated pages
output_pdf_path = ""rotated_document.pdf""
with open(output_pdf_path, ""wb"") as output_pdf:
    pdf_writer.write(output_pdf)
",train
pypdf,21,"Create a Python program using the 'pypdf' API to extract and display the outline (bookmarks) of a PDF file, including hierarchical levels.",code/pypdf/pypdf_21.py,"Test if the program successfully extracts and displays the outline (bookmarks) of the specified PDF file, including hierarchical levels.",Check for cases where the input PDF file may not have an outline (bookmarks).,Verify that the displayed outline accurately represents the structure of bookmarks in the PDF.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file with bookmarks
pdf_file_path = ""document_with_bookmarks.pdf""

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Initialize a list to store the outline (bookmarks) with hierarchical levels
    outline = []

    # Extract the outline (bookmarks) from the PDF
    def extract_outline(item, level):
        for sub_item in item:
            if isinstance(sub_item, list):
                # If sub_item is a list, it represents a hierarchical level with a label and children
                label, children = sub_item[0], sub_item[1:]
                outline.append(""  "" * level + label)
                extract_outline(children, level + 1)
            else:
                # If sub_item is a string, it represents a label at the current level
                outline.append(""  "" * level + sub_item)

    extract_outline(pdf_reader.Info.Outlines, 0)

    # Display the extracted outline (bookmarks) with hierarchical levels
    for item in outline:
        print(item)
",train
pypdf,34,"Create a Python program using the 'pypdf' API to extract and display the list of all named destinations (links) in a PDF file, along with their page numbers.",code/pypdf/pypdf_34.py,Check for cases where the input PDF file may not have named destinations.,"Test if the program successfully extracts and displays the list of named destinations (links) in the specified PDF file, along with their page numbers.",Verify that the displayed named destinations accurately represent the links in the PDF.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file with named destinations
pdf_file_path = ""document_with_named_destinations.pdf""

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Extract named destinations (links) from the PDF
    named_destinations = pdf_reader.named_dests

    # Display the list of named destinations and their page numbers
    for dest_name, dest_page_num in named_destinations.items():
        print(f""{dest_name}: Page {dest_page_num + 1}"")
",train
pypdf,8,Create a Python program using the 'pypdf' API to extract text content from a specific range of pages in a PDF file and save it to a text file.,code/pypdf/pypdf_8.py,Verify that the extracted text matches the content of the specified pages in the PDF file.,Check for cases where the specified page range is out of bounds or invalid.,Test if the program successfully extracts text from the specified range of pages in the PDF file.,,,"#!pip install pypdf
import pypdf

# Specify the path to the PDF file
pdf_file_path = ""document.pdf""

# Specify the range of pages to extract text from (e.g., pages 5 to 10)
start_page = 5
end_page = 10

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Initialize a variable to store the extracted text
    extracted_text = """"

    # Iterate through the specified range of pages and extract text
    for page_num in range(start_page - 1, end_page):
        page = pdf_reader.pages[page_num]
        extracted_text += page.extract_text()

# Save the extracted text to a text file
output_file_path = ""extracted_text.txt""
with open(output_file_path, ""w"") as text_file:
    text_file.write(extracted_text)

# Display the extracted text or save it to a new text file for reference
print(extracted_text)
",train
pypdf,11,Create a Python program using the 'pypdf' API to extract the table of contents (TOC) from a PDF file and display it.,code/pypdf/pypdf_11.py,Check for cases where the input PDF may not have a table of contents.,Test if the program successfully extracts the table of contents (TOC) from the specified PDF file.,Verify that the extracted TOC matches the structure in the PDF.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file with a table of contents (TOC)
pdf_file_path = ""document_with_toc.pdf""

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Extract the table of contents (TOC) from the PDF
    toc = pdf_reader.get_outlines()

    # Display the extracted TOC
    for level, title, dest in toc:
        print(f""{'  ' * (level - 1)}{title}"")
",train
pypdf,15,Create a Python program using the 'pypdf' API to add a page number footer to each page of a PDF file and save the modified PDF.,code/pypdf/pypdf_15.py,Verify that the page numbers are correctly positioned and labeled in the modified PDF.,Check for cases where the input PDF file may not exist.,Test if the program successfully adds page numbers to each page of the specified PDF file.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file to which you want to add page numbers
pdf_file_path = ""document.pdf""

# Create a PDF writer object
pdf_writer = pypdf.PdfWriter()

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Iterate through the pages and add page numbers
    for page_num, page in enumerate(pdf_reader.pages, start=1):
        # Create a TextString to represent the page number
        page_number_text = pypdf.TextString(f""Page {page_num}"")

        # Create an Annot object for the page number
        page_number_annot = pypdf.Annot()
        page_number_annot.Page = page
        page_number_annot.Type = '/Stamp'
        page_number_annot.Rect = [10, 10, 100, 30]
        page_number_annot.Open = '/Right'
        page_number_annot.V = page_number_text

        # Add the page number annotation to the page
        page.Annots.append(page_number_annot)

        # Add the page (with page number) to the writer
        pdf_writer.add_page(page)

    # Save the modified PDF with page numbers
    output_pdf_path = ""document_with_page_numbers.pdf""
    with open(output_pdf_path, ""wb"") as output_pdf:
        pdf_writer.write(output_pdf)
",train
pypdf,29,"Create a Python program using the 'pypdf' API to extract and display the list of all fonts used in a PDF file, along with their properties (e.g., type, encoding).",code/pypdf/pypdf_29.py,Check for cases where the input PDF file may not contain font information.,Verify that the displayed font properties accurately represent the fonts in the PDF.,"Test if the program successfully extracts and displays the list of fonts used in the specified PDF file, along with their properties (type and encoding).",,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file
pdf_file_path = ""document.pdf""

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Initialize a dictionary to store font properties
    font_properties = {}

    # Iterate through the pages and extract font information
    for page in pdf_reader.pages:
        font_resources = page['/Resources']['/Font'].get_object()

        # Extract font names, types, and encodings
        for font_name, font_data in font_resources.items():
            if '/Type' in font_data and '/Encoding' in font_data:
                font_type = font_data['/Type']
                font_encoding = font_data['/Encoding']
                font_properties[font_name] = {'Type': font_type, 'Encoding': font_encoding}

    # Display the list of fonts and their properties
    for font_name, properties in font_properties.items():
        print(f""Font Name: {font_name}"")
        print(f""Type: {properties['Type']}"")
        print(f""Encoding: {properties['Encoding']}"")
        print()
",train
pypdf,16,Create a Python program using the 'pypdf' API to extract and display the list of all embedded files (attachments) in a PDF file.,code/pypdf/pypdf_16.py,Verify that the displayed file names represent the embedded files in the PDF.,Test if the program successfully extracts and displays the list of embedded files in the specified PDF file.,Check for cases where the input PDF file may not have embedded files.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file with embedded files
pdf_file_path = ""document_with_attachments.pdf""

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Initialize a list to store the names of embedded files
    embedded_files = []

    # Check if the PDF has file attachments
    if '/EmbeddedFiles' in pdf_reader.Info:
        embedded_file_dict = pdf_reader.Info.EmbeddedFiles
        for key, value in embedded_file_dict.items():
            embedded_files.append(key)

    # Display the list of embedded files
    for file_name in embedded_files:
        print(file_name)
",test
pypdf,20,Create a Python program using the 'pypdf' API to extract and display the page dimensions and orientation (portrait or landscape) of each page in a PDF file.,code/pypdf/pypdf_20.py,Verify that the displayed page information accurately represents the page dimensions and orientation in the PDF.,Check for cases where the input PDF file may not have well-defined page dimensions or orientation.,Test if the program successfully extracts and displays the page dimensions and orientation (portrait or landscape) of each page in the specified PDF file.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file to analyze page dimensions
pdf_file_path = ""document.pdf""

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Initialize a list to store page dimensions and orientation
    page_info = []

    # Iterate through the pages and extract page dimensions and orientation
    for page in pdf_reader.pages:
        page_width = page.MediaBox[2] - page.MediaBox[0]
        page_height = page.MediaBox[3] - page.MediaBox[1]
        orientation = ""Portrait"" if page_width <= page_height else ""Landscape""
        page_info.append(f""Page {page.page_number}: {page_width} x {page_height} ({orientation})"")

    # Display the extracted page information
    for info in page_info:
        print(info)
",test
pypdf,28,"Create a Python program using the 'pypdf' API to extract and display the content of a specific section (e.g., chapter) from a PDF file based on a keyword or title.",code/pypdf/pypdf_28.py,Test if the program successfully extracts and displays the content of a specific section (chapter) from the specified PDF file based on a keyword or title.,Verify that the displayed content accurately represents the section in the PDF.,Check for cases where the input PDF file may not have the specified section or keyword.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file
pdf_file_path = ""document.pdf""

# Specify the keyword or title to search for
search_keyword = ""Chapter 3""

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Initialize variables to track the start and end of the section
    section_start = None
    section_end = None

    # Iterate through the pages and find the section based on the keyword
    for page_num, page in enumerate(pdf_reader.pages, start=1):
        page_text = page.extract_text()
        if search_keyword in page_text:
            if section_start is None:
                section_start = page_num
            section_end = page_num

    # Check if the section is found
    if section_start is not None:
        # Extract and display the content of the section
        section_text = """"
        for page_num in range(section_start, section_end + 1):
            page = pdf_reader.pages[page_num - 1]
            section_text += page.extract_text()
        print(section_text)
    else:
        print(f""Section '{search_keyword}' not found."")
",test
pypdf,27,"Create a Python program using the 'pypdf' API to extract and display the content of a specific page (e.g., page 3) from a PDF file.",code/pypdf/pypdf_27.py,Test if the program successfully extracts and displays the content of the specified page from the specified PDF file.,Verify that the displayed content matches the content of the specified page in the PDF.,Check for cases where the input PDF file may not have the specified page.,,,"#!pip install pypdf
import pypdf

# Specify the path of the PDF file
pdf_file_path = ""document.pdf""

# Specify the page number to extract (e.g., page 3)
page_number_to_extract = 3

# Open the PDF file in read-binary mode
with open(pdf_file_path, ""rb"") as pdf_file:
    # Create a PDF reader object
    pdf_reader = pypdf.PdfReader(pdf_file)

    # Check if the specified page number is within the valid range
    if 1 <= page_number_to_extract <= len(pdf_reader.pages):
        # Extract and display the content of the specified page
        page = pdf_reader.pages[page_number_to_extract - 1]
        page_text = page.extract_text()
        print(page_text)
    else:
        print(""Page number is out of range."")
",test
pytorch-lightning,10,"Create a Python program using the 'pytorch-lightning' API to define a model that employs a pre-trained language model (e.g., GPT-3) for text generation. The program should demonstrate the use of the language model to generate text given a prompt and evaluate the generated text for quality.",code/pytorch-lightning/pytorch-lightning_10.py,Check if the program successfully generates text using a pre-trained language model based on a given prompt.,Ensure that the program can generate text with the specified maximum length and number of return sequences.,Verify that the generated text is decoded and printed correctly.,,,"#!pip install pytorch-lightning
import pytorch_lightning as pl
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from typing import List

# Initialize a pre-trained language model and tokenizer
model_name = ""gpt2""
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Define a function for text generation
def generate_text(prompt: str, max_length: int = 50, num_return_sequences: int = 1) -> List[str]:
    input_ids = tokenizer.encode(prompt, return_tensors=""pt"")
    outputs = model.generate(input_ids, max_length=max_length, num_return_sequences=num_return_sequences, no_repeat_ngram_size=2)
    generated_text = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]
    return generated_text

if __name__ == ""__main__"":
    prompt = ""Once upon a time, in a land far, far away""
    generated_text = generate_text(prompt)
    for i, text in enumerate(generated_text):
        print(f""Generated Text {i+1}: {text}"")",train
pytorch-lightning,38,"Create a Python program using the 'pytorch-lightning' API to define a model for object detection. The program should demonstrate the use of a pre-built data module, model, and training loop for object detection, including training and evaluation steps.",code/pytorch-lightning/pytorch-lightning_38.py,Ensure that the program can perform object detection without errors.,Verify that the model computes training and validation loss.,Check if the program successfully performs object detection using the 'ObjectDetectionDataModule'.,,,"#!pip install pytorch-lightning
import pytorch_lightning as pl
from pytorch_lightning import LightningDataModule
from torchvision.datasets import COCO
from torch.utils.data import DataLoader
import torch
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.transforms import functional as F
import os

# Define a LightningDataModule for object detection
class ObjectDetectionDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()

    def prepare_data(self):
        COCO(os.getcwd(), download=True)

    def setup(self, stage=None):
        if stage == ""fit"" or stage is None:
            self.data = COCO(os.getcwd(), train=True, download=False)

    def train_dataloader(self):
        return DataLoader(self.data, batch_size=1)

    def val_dataloader(self):
        return DataLoader(self.data, batch_size=1)

# Define a LightningModule for object detection
class ObjectDetectionModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.model = fasterrcnn_resnet50_fpn(pretrained=True)
        self.loss_fn = torch.nn.CrossEntropyLoss()

    def forward(self, x):
        return self.model(x)

    def configure_optimizers(self):
        return torch.optim.SGD(self.parameters(), lr=0.005, momentum=0.9)

    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.loss_fn(logits, y.squeeze(1))
        self.log(""train_loss"", loss)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.loss_fn(logits, y.squeeze(1))
        self.log(""val_loss"", loss)

if __name__ == ""__main__"":
    data_module = ObjectDetectionDataModule()
    model = ObjectDetectionModel()

    trainer = pl.Trainer(max_epochs=5)
    trainer.fit(model, data_module)",train
pytorch-lightning,28,"Create a Python program using the 'pytorch-lightning' API to define a model for text generation using a pre-trained language model. The program should demonstrate the use of a pre-built data module, model, and training loop for text generation, including training and evaluation steps.",code/pytorch-lightning/pytorch-lightning_28.py,Verify that the model computes training and validation loss.,Check if the program successfully trains a text generation model using the 'TextGenerationDataModule'.,Ensure that the program can train the model for text generation without errors.,,,"#!pip install pytorch-lightning
import pytorch_lightning as pl
from pytorch_lightning import LightningDataModule
import torch
from torch.utils.data import DataLoader
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Define a LightningDataModule for text generation
class TextGenerationDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()

    def prepare_data(self):
        # Download and preprocess text data
        pass

    def setup(self, stage=None):
        if stage == ""fit"" or stage is None:
            # Load and split the text dataset
            pass

    def train_dataloader(self):
        # Define a data loader for text generation
        pass

    def val_dataloader(self):
        # Define a data loader for validation
        pass

# Define a LightningModule for text generation
class TextGenerationModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.tokenizer = GPT2Tokenizer.from_pretrained(""gpt2"")
        self.model = GPT2LMHeadModel.from_pretrained(""gpt2"")
        self.loss_fn = torch.nn.CrossEntropyLoss()

    def forward(self, input_ids, attention_mask):
        return self.model(input_ids, attention_mask=attention_mask, return_dict=True)

    def training_step(self, batch, batch_idx):
        input_ids, attention_mask, labels = batch[""input_ids""], batch[""attention_mask""], batch[""labels""]
        outputs = self(input_ids, attention_mask=attention_mask)
        loss = self.loss_fn(outputs.logits.view(-1, self.model.config.vocab_size), labels.view(-1))
        self.log(""train_loss"", loss)
        return loss

    def validation_step(self, batch, batch_idx):
        input_ids, attention_mask, labels = batch[""input_ids""], batch[""attention_mask""], batch[""labels""]
        outputs = self(input_ids, attention_mask=attention_mask)
        loss = self.loss_fn(outputs.logits.view(-1, self.model.config.vocab_size), labels.view(-1))
        self.log(""val_loss"", loss)

if __name__ == ""__main__"":
    data_module = TextGenerationDataModule()
    model = TextGenerationModel()

    trainer = pl.Trainer(max_epochs=5)
    trainer.fit(model, data_module)",train
pytorch-lightning,35,"Create a Python program using the 'pytorch-lightning' API to define a model for emotion recognition in text. The program should demonstrate the use of a pre-built data module, model, and training loop for emotion recognition, including training and evaluation steps.",code/pytorch-lightning/pytorch-lightning_35.py,Ensure that the program can perform emotion recognition without errors.,Check if the program successfully performs emotion recognition using the 'EmotionRecognitionDataModule'.,Verify that the model computes training and validation loss.,,,"#!pip install pytorch-lightning
import pytorch_lightning as pl
from pytorch_lightning import LightningDataModule
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.utils.data import DataLoader
import torch

# Define a LightningDataModule for emotion recognition
class EmotionRecognitionDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()

    def prepare_data(self):
        load_dataset(""emotion"")

    def setup(self, stage=None):
        if stage == ""fit"" or stage is None:
            dataset = load_dataset(""emotion"")
            self.tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")
            self.data = dataset[""train""].map(self.encode_example, batched=True)
            self.data.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""label""])

    def encode_example(self, example):
        return self.tokenizer(
            example[""text""],
            padding=""max_length"",
            truncation=True,
        )

    def train_dataloader(self):
        return DataLoader(self.data, batch_size=2, num_workers=4)

    def val_dataloader(self):
        return DataLoader(self.data, batch_size=2, num_workers=4)

# Define a LightningModule for emotion recognition
class EmotionRecognitionModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")
        self.model = AutoModelForSequenceClassification.from_pretrained(""bert-base-uncased"")
        self.loss_fn = torch.nn.CrossEntropyLoss()

    def forward(self, input_ids, attention_mask, labels):
        return self.model(input_ids, attention_mask=attention_mask, labels=labels)

    def training_step(self, batch, batch_idx):
        input_ids, attention_mask, labels = batch[""input_ids""], batch[""attention_mask""], batch[""label""]
        outputs = self(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        self.log(""train_loss"", loss)
        return loss

    def validation_step(self, batch, batch_idx):
        input_ids, attention_mask, labels = batch[""input_ids""], batch[""attention_mask""], batch[""label""]
        outputs = self(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        self.log(""val_loss"", loss)

if __name__ == ""__main__"":
    data_module = EmotionRecognitionDataModule()
    model = EmotionRecognitionModel()

    trainer = pl.Trainer(max_epochs=5)
    trainer.fit(model, data_module)",train
pytorch-lightning,18,"Create a Python program using the 'pytorch-lightning' API to define a model for time series forecasting. The program should demonstrate the use of a pre-built data module, model, and training loop for time series forecasting, including training and evaluation steps.",code/pytorch-lightning/pytorch-lightning_18.py,Verify that the model computes training and validation metrics.,Check if the program successfully trains a time series forecasting model using the 'TimeSeriesForecastingDataModule'.,Ensure that the program can train the model for time series forecasting without errors.,,,"#!pip install pytorch-lightning
import pytorch_lightning as pl
from pytorch_lightning import LightningDataModule
import pandas as pd
from torch.utils.data import DataLoader
import torch
from torch import nn

# Define a LightningDataModule for time series forecasting
class TimeSeriesForecastingDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()

    def prepare_data(self):
        # Download and preprocess time series data
        pass

    def setup(self, stage=None):
        if stage == ""fit"" or stage is None:
            # Load and split the time series dataset
            pass

    def train_dataloader(self):
        # Define a data loader for time series forecasting
        pass

    def val_dataloader(self):
        # Define a data loader for validation
        pass

# Define a LightningModule for time series forecasting
class TimeSeriesForecastingModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        # Initialize and load a pre-trained time series forecasting model
        pass

    def forward(self, x):
        # Implement forward pass for time series forecasting
        pass

    def training_step(self, batch, batch_idx):
        # Implement training step
        pass

    def validation_step(self, batch, batch_idx):
        # Implement validation step
        pass

if __name__ == ""__main__"":
    data_module = TimeSeriesForecastingDataModule()
    model = TimeSeriesForecastingModel()

    trainer = pl.Trainer(max_epochs=5)
    trainer.fit(model, data_module)",train
pytorch-lightning,34,"Create a Python program using the 'pytorch-lightning' API to define a model for spam email classification. The program should demonstrate the use of a pre-built data module, model, and training loop for spam email classification, including training and evaluation steps.",code/pytorch-lightning/pytorch-lightning_34.py,Check if the program successfully performs spam email classification using the 'SpamEmailClassificationDataModule'.,Verify that the model computes training and validation loss.,Ensure that the program can perform spam email classification without errors.,,,"#!pip install pytorch-lightning
import pytorch_lightning as pl
from pytorch_lightning import LightningDataModule
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.utils.data import DataLoader
import torch

# Define a LightningDataModule for spam email classification
class SpamEmailClassificationDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()

    def prepare_data(self):
        load_dataset(""spam"")

    def setup(self, stage=None):
        if stage == ""fit"" or stage is None:
            dataset = load_dataset(""spam"")
            self.tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")
            self.data = dataset[""train""].map(self.encode_example, batched=True)
            self.data.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""label""])

    def encode_example(self, example):
        return self.tokenizer(
            example[""text""],
            padding=""max_length"",
            truncation=True,
        )

    def train_dataloader(self):
        return DataLoader(self.data, batch_size=2, num_workers=4)

    def val_dataloader(self):
        return DataLoader(self.data, batch_size=2, num_workers=4)

# Define a LightningModule for spam email classification
class SpamEmailClassificationModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")
        self.model = AutoModelForSequenceClassification.from_pretrained(""bert-base-uncased"")
        self.loss_fn = torch.nn.CrossEntropyLoss()

    def forward(self, input_ids, attention_mask, labels):
        return self.model(input_ids, attention_mask=attention_mask, labels=labels)

    def training_step(self, batch, batch_idx):
        input_ids, attention_mask, labels = batch[""input_ids""], batch[""attention_mask""], batch[""label""]
        outputs = self(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        self.log(""train_loss"", loss)
        return loss

    def validation_step(self, batch, batch_idx):
        input_ids, attention_mask, labels = batch[""input_ids""], batch[""attention_mask""], batch[""label""]
        outputs = self(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        self.log(""val_loss"", loss)

if __name__ == ""__main__"":
    data_module = SpamEmailClassificationDataModule()
    model = SpamEmailClassificationModel()

    trainer = pl.Trainer(max_epochs=5)
    trainer.fit(model, data_module)",train
pytorch-lightning,6,Create a Python program using the 'pytorch-lightning' API to define a custom callback that saves model checkpoints during training. The program should demonstrate the use of this custom checkpoint callback to save model checkpoints at specified intervals.,code/pytorch-lightning/pytorch-lightning_6.py,Verify that model checkpoints are saved at specified intervals during training.,Ensure that the custom checkpoint callback is correctly configured to save checkpoints at the specified path and interval.,Check if the program successfully trains the 'ExampleModel' with the custom model checkpoint callback.,,,"#!pip install pytorch-lightning
import pytorch_lightning as pl
import torch

# Define a custom callback for saving model checkpoints
class ModelCheckpointCallback(pl.Callback):
    def __init__(self, save_every_n_steps, checkpoint_path):
        super().__init__()
        self.save_every_n_steps = save_every_n_steps
        self.checkpoint_path = checkpoint_path

    def on_batch_end(self, trainer, pl_module):
        global_step = trainer.global_step
        if global_step % self.save_every_n_steps == 0:
            checkpoint_name = f""{self.checkpoint_path}/model_step_{global_step}.ckpt""
            trainer.save_checkpoint(checkpoint_name)

# Create a LightningModule for demonstration
class ExampleModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.l1 = torch.nn.Linear(10, 1)

    def forward(self, x):
        return self.l1(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = torch.nn.functional.mse_loss(y_hat, y)
        self.log(""train_loss_step"", loss)
        return loss

if __name__ == ""__main__"":
    model = ExampleModel()

    checkpoint_callback = ModelCheckpointCallback(save_every_n_steps=100, checkpoint_path=""checkpoints"")

    trainer = pl.Trainer(
        gpus=0,
        max_steps=1000,
        callbacks=[checkpoint_callback],
    )

    trainer.fit(model)",train
pytorch-lightning,20,"Create a Python program using the 'pytorch-lightning' API to define a model for image segmentation. The program should demonstrate the use of a pre-built data module, model, and training loop for image segmentation, including training and evaluation steps.",code/pytorch-lightning/pytorch-lightning_20.py,Verify that the model computes training and validation loss.,Ensure that the program can train the model for image segmentation without errors.,Check if the program successfully trains an image segmentation model using the ' ImageSegmentationDataModule'.,,,"#!pip install pytorch-lightning
import pytorch_lightning as pl
from pytorch_lightning import LightningDataModule
import torch
from torchvision.datasets import VOCSegmentation
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.models.segmentation import deeplabv3_resnet50
from torchvision.models.detection import FasterRCNN, FastRCNNPredictor, backbone_utils
import os

# Define a LightningDataModule for image segmentation
class ImageSegmentationDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()

    def prepare_data(self):
        VOCSegmentation(os.getcwd(), image_set=""train"", download=True)

    def setup(self, stage=None):
        if stage == ""fit"" or stage is None:
            self.data = VOCSegmentation(os.getcwd(), image_set=""trainval"", transform=transforms.ToTensor())

    def train_dataloader(self):
        return DataLoader(self.data, batch_size=2, num_workers=4)

    def val_dataloader(self):
        return DataLoader(self.data, batch_size=2, num_workers=4)

# Define a LightningModule for image segmentation
class ImageSegmentationModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.model = deeplabv3_resnet50(pretrained=True)
        self.model.classifier[4] = torch.nn.Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))
        self.loss_fn = torch.nn.CrossEntropyLoss()

    def forward(self, x):
        return self.model(x)[""out""]

    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.loss_fn(logits, y)
        self.log(""train_loss"", loss)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.loss_fn(logits, y)
        self.log(""val_loss"", loss)

if __name__ == ""__main__"":
    data_module = ImageSegmentationDataModule()
    model = ImageSegmentationModel()

    trainer = pl.Trainer(max_epochs=5)
    trainer.fit(model, data_module)",train
pytorch-lightning,39,"Create a Python program using the 'pytorch-lightning' API to define a model for text summarization. The program should demonstrate the use of a pre-built data module, model, and training loop for text summarization, including training and evaluation steps.",code/pytorch-lightning/pytorch-lightning_39.py,Ensure that the program can perform text summarization without errors.,Verify that the model computes training and validation loss.,Check if the program successfully performs text summarization using the 'TextSummarizationDataModule'.,,,"#!pip install pytorch-lightning
import pytorch_lightning as pl
from pytorch_lightning import LightningDataModule
from datasets import load_dataset
from transformers import T5Tokenizer, T5ForConditionalGeneration
from torch.utils.data import DataLoader
import torch

# Define a LightningDataModule for text summarization
class TextSummarizationDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()

    def prepare_data(self):
        load_dataset(""cnn_dailymail"")

    def setup(self, stage=None):
        if stage == ""fit"" or stage is None:
            dataset = load_dataset(""cnn_dailymail"")
            self.tokenizer = T5Tokenizer.from_pretrained(""t5-small"")
            self.data = dataset[""train""].map(self.encode_example, batched=True)
            self.data.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""target_ids""])

    def encode_example(self, example):
        inputs = self.tokenizer(
            example[""article""],
            padding=""max_length"",
            truncation=True,
            return_tensors=""pt"",
            max_length=512,
        )
        targets = self.tokenizer(
            example[""highlights""],
            padding=""max_length"",
            truncation=True,
            return_tensors=""pt"",
            max_length=150,
        )
        inputs[""input_ids""] = inputs[""input_ids""].squeeze()
        inputs[""attention_mask""] = inputs[""attention_mask""].squeeze()
        targets[""input_ids""] = targets[""input_ids""].squeeze()
        targets[""attention_mask""] = targets[""attention_mask""].squeeze()
        return inputs, targets

    def train_dataloader(self):
        return DataLoader(self.data, batch_size=2, num_workers=4)

    def val_dataloader(self):
        return DataLoader(self.data, batch_size=2, num_workers=4)

# Define a LightningModule for text summarization
class TextSummarizationModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.tokenizer = T5Tokenizer.from_pretrained(""t5-small"")
        self.model = T5ForConditionalGeneration.from_pretrained(""t5-small"")
        self.loss_fn = torch.nn.CrossEntropyLoss()

    def forward(self, input_ids, attention_mask, target_ids):
        return self.model(input_ids, attention_mask=attention_mask, labels=target_ids)

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.001)

    def training_step(self, batch, batch_idx):
        input_ids, attention_mask, target_ids = batch[0][""input_ids""], batch[0][""attention_mask""], batch[1][""input_ids""]
        outputs = self(input_ids, attention_mask=attention_mask, target_ids=target_ids)
        loss = outputs.loss
        self.log(""train_loss"", loss)
        return loss

    def validation_step(self, batch, batch_idx):
        input_ids, attention_mask, target_ids = batch[0][""input_ids""], batch[0][""attention_mask""], batch[1][""input_ids""]
        outputs = self(input_ids, attention_mask=attention_mask, target_ids=target_ids)
        loss = outputs.loss
        self.log(""val_loss"", loss)

if __name__ == ""__main__"":
    data_module = TextSummarizationDataModule()
    model = TextSummarizationModel()

    trainer = pl.Trainer(max_epochs=5)
    trainer.fit(model, data_module)",train
pytorch-lightning,9,"Create a Python program using the 'pytorch-lightning' API to define a model that uses a pre-trained transformer (e.g., BERT) for a natural language processing task. The program should demonstrate the use of a transformer-based model for text classification, fine-tuning the pre-trained model, and evaluating its performance on a validation dataset.",code/pytorch-lightning/pytorch-lightning_9.py,Ensure that the program can fine-tune the model on the sample data without errors.,Check if the program successfully fine-tunes a pre-trained transformer model for text classification.,Verify that the model can forward pass a batch of data and compute training and validation losses.,,,"#!pip install pytorch-lightning
import pytorch_lightning as pl
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch
from torch.utils.data import DataLoader, TensorDataset

# Initialize a pre-trained transformer model and tokenizer
model_name = ""bert-base-uncased""
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Sample data (for demonstration)
text = [""This is a positive example."", ""This is a negative example.""]
labels = [1, 0]

# Tokenize the text and convert to tensors
inputs = tokenizer(text, padding=True, truncation=True, return_tensors=""pt"")
labels = torch.tensor(labels)

# Create a dataset and dataloader
dataset = TensorDataset(inputs[""input_ids""], inputs[""attention_mask""], labels)
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

# Create a LightningModule for fine-tuning the model
class FineTuningModel(pl.LightningModule):
    def __init__(self, model, learning_rate=2e-5):
        super().__init__()
        self.model = model
        self.learning_rate = learning_rate

    def forward(self, input_ids, attention_mask):
        return self.model(input_ids, attention_mask=attention_mask).logits

    def training_step(self, batch, batch_idx):
        input_ids, attention_mask, labels = batch
        outputs = self(input_ids, attention_mask)
        loss = torch.nn.functional.cross_entropy(outputs, labels)
        self.log(""train_loss"", loss)
        return loss

    def validation_step(self, batch, batch_idx):
        input_ids, attention_mask, labels = batch
        outputs = self(input_ids, attention_mask)
        val_loss = torch.nn.functional.cross_entropy(outputs, labels)
        self.log(""val_loss"", val_loss)

if __name__ == ""__main__"":
    model = FineTuningModel(model)
    trainer = pl.Trainer(gpus=0, max_epochs=3)
    trainer.fit(model, dataloader)",train
pytorch-lightning,15,"Create a Python program using the 'pytorch-lightning' API to define a model that uses a pre-trained transformer model for a text generation task. The program should demonstrate the use of a pre-built data module, model, and training loop for text generation, including training and validation steps.",code/pytorch-lightning/pytorch-lightning_15.py,Verify that the model computes training and validation loss.,Check if the program successfully trains a text generation model using the 'TextGenerationDataModule'.,Ensure that the program can train the model for text generation without errors.,,,"#!pip install pytorch-lightning
import pytorch_lightning as pl
from pytorch_lightning import LightningDataModule
from torch.utils.data import DataLoader
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# Define a LightningDataModule for text generation
class TextGenerationDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()

    def prepare_data(self):
        # Download and preprocess text data
        pass

    def setup(self, stage=None):
        if stage == ""fit"" or stage is None:
            dataset = torch.utils.data.TensorDataset(
                torch.randint(0, 50000, (50000, 50)),  # Random input data
                torch.randint(0, 50000, (50000, 50)),  # Random target data
            )
            num_samples = len(dataset)
            train_size = int(0.9 * num_samples)
            val_size = num_samples - train_size
            self.train_data, self.val_data = torch.utils.data.random_split(dataset, [train_size, val_size])

    def train_dataloader(self):
        return DataLoader(self.train_data, batch_size=32)

    def val_dataloader(self):
        return DataLoader(self.val_data, batch_size=32)

# Define a LightningModule for text generation
class TextGenerationModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.model = GPT2LMHeadModel.from_pretrained(""gpt2"")
        self.tokenizer = GPT2Tokenizer.from_pretrained(""gpt2"")

    def forward(self, input_ids, attention_mask=None, max_length=50):
        return self.model.generate(input_ids, attention_mask=attention_mask, max_length=max_length, pad_token_id=self.tokenizer.eos_token_id)

    def training_step(self, batch, batch_idx):
        input_ids, target_ids = batch
        outputs = self.model(input_ids, labels=target_ids)
        loss = outputs.loss
        self.log(""train_loss"", loss)
        return loss

    def validation_step(self, batch, batch_idx):
        input_ids, target_ids = batch
        outputs = self.model(input_ids, labels=target_ids)
        loss = outputs.loss
        self.log(""val_loss"", loss)

if __name__ == ""__main__"":
    data_module = TextGenerationDataModule()
    model = TextGenerationModel()

    trainer = pl.Trainer(max_epochs=5)
    trainer.fit(model, data_module)",train
pytorch-lightning,26,"Create a Python program using the 'pytorch-lightning' API to define a model for named entity recognition (NER) in text data. The program should demonstrate the use of a pre-built data module, model, and training loop for NER, including training and evaluation steps.",code/pytorch-lightning/pytorch-lightning_26.py,Check if the program successfully trains a named entity recognition (NER) model using the 'NERDataModule'.,Verify that the model computes training and validation loss.,Ensure that the program can train the model for NER without errors.,,,"#!pip install pytorch-lightning
import pytorch_lightning as pl
from pytorch_lightning import LightningDataModule
from datasets import load_dataset
from transformers import AutoTokenizer
from torch.utils.data import DataLoader
import torch
from transformers import BertForTokenClassification

# Define a LightningDataModule for named entity recognition
class NERDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()

    def prepare_data(self):
        load_dataset(""conll2003"")

    def setup(self, stage=None):
        if stage == ""fit"" or stage is None:
            dataset = load_dataset(""conll2003"")
            self.tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")
            self.data = dataset[""train""].map(self.encode_example, batched=True)
            self.data.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""labels""])

    def encode_example(self, example):
        return self.tokenizer(example[""tokens""], padding=""max_length"", truncation=True)

    def train_dataloader(self):
        return DataLoader(self.data, batch_size=2, num_workers=4)

    def val_dataloader(self):
        return DataLoader(self.data, batch_size=2, num_workers=4)

# Define a LightningModule for named entity recognition
class NERModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.model = BertForTokenClassification.from_pretrained(""bert-base-uncased"")
        self.loss_fn = torch.nn.CrossEntropyLoss()

    def forward(self, input_ids, attention_mask, labels):
        return self.model(input_ids, attention_mask=attention_mask, labels=labels)

    def training_step(self, batch, batch_idx):
        input_ids, attention_mask, labels = batch[""input_ids""], batch[""attention_mask""], batch[""labels""]
        outputs = self(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        self.log(""train_loss"", loss)
        return loss

    def validation_step(self, batch, batch_idx):
        input_ids, attention_mask, labels = batch[""input_ids""], batch[""attention_mask""], batch[""labels""]
        outputs = self(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        self.log(""val_loss"", loss)

if __name__ == ""__main__"":
    data_module = NERDataModule()
    model = NERModel()

    trainer = pl.Trainer(max_epochs=5)
    trainer.fit(model, data_module)",train
pytorch-lightning,31,"Create a Python program using the 'pytorch-lightning' API to define a model for document classification using a pre-trained language model. The program should demonstrate the use of a pre-built data module, model, and training loop for document classification, including training and evaluation steps.",code/pytorch-lightning/pytorch-lightning_31.py,Ensure that the program can train the model for document classification without errors.,Check if the program successfully trains a document classification model using the 'DocumentClassificationDataModule'.,Verify that the model computes training and validation loss.,,,"#!pip install pytorch-lightning
import pytorch_lightning as pl
from pytorch_lightning import LightningDataModule
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.utils.data import DataLoader
import torch

# Define a LightningDataModule for document classification
class DocumentClassificationDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()

    def prepare_data(self):
        load_dataset(""ag_news"")

    def setup(self, stage=None):
        if stage == ""fit"" or stage is None:
            dataset = load_dataset(""ag_news"")
            self.tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")
            self.data = dataset[""train""].map(self.encode_example, batched=True)
            self.data.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""label""])

    def encode_example(self, example):
        return self.tokenizer(
            example[""text""],
            padding=""max_length"",
            truncation=True,
        )

    def train_dataloader(self):
        return DataLoader(self.data, batch_size=2, num_workers=4)

    def val_dataloader(self):
        return DataLoader(self.data, batch_size=2, num_workers=4)

# Define a LightningModule for document classification
class DocumentClassificationModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")
        self.model = AutoModelForSequenceClassification.from_pretrained(""bert-base-uncased"")
        self.loss_fn = torch.nn.CrossEntropyLoss()

    def forward(self, input_ids, attention_mask, labels):
        return self.model(input_ids, attention_mask=attention_mask, labels=labels)

    def training_step(self, batch, batch_idx):
        input_ids, attention_mask, labels = batch[""input_ids""], batch[""attention_mask""], batch[""label""]
        outputs = self(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        self.log(""train_loss"", loss)
        return loss

    def validation_step(self, batch, batch_idx):
        input_ids, attention_mask, labels = batch[""input_ids""], batch[""attention_mask""], batch[""label""]
        outputs = self(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        self.log(""val_loss"", loss)

if __name__ == ""__main__"":
    data_module = DocumentClassificationDataModule()
    model = DocumentClassificationModel()

    trainer = pl.Trainer(max_epochs=5)
    trainer.fit(model, data_module)",train
pytorch-lightning,7,Create a Python program using the 'pytorch-lightning' API to define a custom data loader that loads data from a custom data source. The program should demonstrate the use of this custom data loader in a LightningModule for training a model on the custom data.,code/pytorch-lightning/pytorch-lightning_7.py,Check if the program successfully trains the 'ExampleModel' with the custom data loader.,Ensure that the custom data loader is correctly integrated into the training process.,Verify that the program can train a model using data from the custom data source without errors.,,,"#!pip install pytorch-lightning
import pytorch_lightning as pl
import torch

# Define a custom data loader
class CustomDataLoader(torch.utils.data.Dataset):
    def __init__(self, data_source):
        self.data_source = data_source

    def __len__(self):
        return len(self.data_source)

    def __getitem__(self, index):
        return self.data_source[index]

# Create a custom data source (for demonstration)
custom_data = [torch.randn(10) for _ in range(100)]

# Initialize the custom data loader
custom_dataloader = torch.utils.data.DataLoader(CustomDataLoader(custom_data), batch_size=32, shuffle=True)

# Create a LightningModule for demonstration
class ExampleModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.l1 = torch.nn.Linear(10, 1)

    def forward(self, x):
        return self.l1(x)

    def training_step(self, batch, batch_idx):
        x = batch
        y = torch.randn(x.size(0), 1)  # Random target for demonstration
        y_hat = self(x)
        loss = torch.nn.functional.mse_loss(y_hat, y)
        self.log(""train_loss_step"", loss)
        return loss

if __name__ == ""__main__"":
    model = ExampleModel()

    trainer = pl.Trainer(gpus=0, max_steps=1000)

    trainer.fit(model, custom_dataloader)",train
pytorch-lightning,14,"Create a Python program using the 'pytorch-lightning' API to define a model that uses a pre-trained GAN (Generative Adversarial Network) for image generation. The program should demonstrate the use of a pre-built data module, generator, discriminator, loss functions, and training loop for GAN training.",code/pytorch-lightning/pytorch-lightning_14.py,Verify that the GAN model computes generator and discriminator losses.,Check if the program successfully trains a Generative Adversarial Network (GAN) with a generator and discriminator.,Ensure that the program can train the GAN on the CelebA dataset for image generation.,,,"#!pip install pytorch-lightning
import pytorch_lightning as pl
from pytorch_lightning import LightningDataModule
from torch.utils.data import DataLoader
from torchvision.datasets import CelebA
from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection import FasterRCNN
import torch
from torch import nn
import os

# Define a LightningDataModule for CelebA dataset
class CelebADataModule(LightningDataModule):
    def __init__(self):
        super().__init__()

    def prepare_data(self):
        CelebA(os.getcwd(), split=""all"", download=True)

    def setup(self, stage=None):
        if stage == ""fit"" or stage is None:
            self.data = CelebA(
                os.getcwd(),
                split=""all"",
                transform=Compose(
                    [Resize((128, 128)), CenterCrop(128), ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]
                ),
            )

    def train_dataloader(self):
        return DataLoader(self.data, batch_size=2, num_workers=4)

# Define a generator and a discriminator for GAN
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.model = torch.hub.load(
            ""pytorch/vision:v0.10.0"",
            ""resnet18"",
            pretrained=False,
        )
        self.linear = nn.Linear(1000, 3 * 128 * 128)

    def forward(self, x):
        x = self.model(x)
        x = self.linear(x)
        return x.view(-1, 3, 128, 128)

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.model = torch.hub.load(
            ""pytorch/vision:v0.10.0"",
            ""resnet18"",
            pretrained=False,
        )
        self.fc = nn.Linear(1000, 1)

    def forward(self, x):
        x = self.model(x)
        x = self.fc(x)
        return x

# Define a LightningModule for GAN
class GAN(pl.LightningModule):
    def __init__(self):
        super(GAN, self).__init__()
        self.generator = Generator()
        self.discriminator = Discriminator()
        self.loss_fn = nn.BCEWithLogitsLoss()

    def forward(self, x):
        return self.generator(x)

    def training_step(self, batch, batch_idx, optimizer_idx):
        real_images, _ = batch
        batch_size = real_images.size(0)
        z = torch.randn(batch_size, 1000)
        fake_images = self.generator(z)
        real_labels = torch.ones(batch_size, 1)
        fake_labels = torch.zeros(batch_size, 1)

        if optimizer_idx == 0:
            # Train generator
            g_loss = self.loss_fn(self.discriminator(fake_images), real_labels)
            self.log(""g_loss"", g_loss)
            return g_loss

        if optimizer_idx == 1:
            # Train discriminator
            real_loss = self.loss_fn(self.discriminator(real_images), real_labels)
            fake_loss = self.loss_fn(self.discriminator(fake_images.detach()), fake_labels)
            d_loss = real_loss + fake_loss
            self.log(""d_loss"", d_loss)
            return d_loss

    def configure_optimizers(self):
        g_optimizer = torch.optim.Adam(self.generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
        d_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))
        return [g_optimizer, d_optimizer]

if __name__ == ""__main__"":
    data_module = CelebADataModule()
    model = GAN()

    trainer = pl.Trainer(max_epochs=5)
    trainer.fit(model, data_module)",train
pytorch-lightning,1,"Create a Python program using the 'pytorch-lightning' API to define a 'DataModule' class that prepares and tokenizes data for a natural language processing task. The 'DataModule' should load a dataset using the 'datasets' and 'transformers' libraries, tokenize the data with a specific model tokenizer, and set up data loaders. The program should demonstrate the use of 'pytorch-lightning' to create a 'DataModule' instance, prepare data, set it up, and print the shape of a batch from the data loader.",code/pytorch-lightning/pytorch-lightning_1.py,Verify that the 'prepare_data' method correctly loads the dataset and the 'setup' method tokenizes and sets up the data for training and validation.,"Ensure that the program prints the shape of a batch from the data loader without errors, indicating correct data preparation and loading.",Check if the program successfully creates an instance of the 'DataModule' class.,,,"#!pip install pytorch-lightning
import torch
import pytorch_lightning as pl

from datasets import load_dataset
from transformers import AutoTokenizer


class DataModule(pl.LightningDataModule):
    def __init__(
        self,
        model_name=""google/bert_uncased_L-2_H-128_A-2"",
        batch_size=64,
        max_length=128,
    ):
        super().__init__()

        self.batch_size = batch_size
        self.max_length = max_length
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

    def prepare_data(self):
        cola_dataset = load_dataset(""glue"", ""cola"")
        self.train_data = cola_dataset[""train""]
        self.val_data = cola_dataset[""validation""]

    def tokenize_data(self, example):
        return self.tokenizer(
            example[""sentence""],
            truncation=True,
            padding=""max_length"",
            max_length=self.max_length,
        )

    def setup(self, stage=None):
        if stage == ""fit"" or stage is None:
            self.train_data = self.train_data.map(self.tokenize_data, batched=True)
            self.train_data.set_format(
                type=""torch"", columns=[""input_ids"", ""attention_mask"", ""label""]
            )

            self.val_data = self.val_data.map(self.tokenize_data, batched=True)
            self.val_data.set_format(
                type=""torch"",
                columns=[""input_ids"", ""attention_mask"", ""label""],
                output_all_columns=True,
            )

    def train_dataloader(self):
        return torch.utils.data.DataLoader(
            self.train_data, batch_size=self.batch_size, shuffle=True
        )

    def val_dataloader(self):
        return torch.utils.data.DataLoader(
            self.val_data, batch_size=self.batch_size, shuffle=False
        )


if __name__ == ""__main__"":
    data_model = DataModule()
    data_model.prepare_data()
    data_model.setup()
    print(next(iter(data_model.train_dataloader()))[""input_ids""].shape)",test
pytorch-lightning,4,Create a Python program using the 'pytorch-lightning' API to define a custom callback that logs the training loss and validation loss during training. The program should demonstrate the use of this custom callback to monitor and log losses during model training.,code/pytorch-lightning/pytorch-lightning_4.py,Ensure that the custom callback correctly monitors and logs both training and validation losses.,Verify that the losses are logged and printed at the end of each epoch during training.,Check if the program successfully trains the 'ExampleModel' with the custom loss logging callback.,,,"#!pip install pytorch-lightning
import pytorch_lightning as pl
import torch

# Define a custom callback for logging losses
class LossLoggingCallback(pl.Callback):
    def on_epoch_end(self, trainer, pl_module):
        train_loss = trainer.callback_metrics[""train_loss_step""]
        val_loss = trainer.callback_metrics[""val_loss_step""]
        print(f""Epoch {trainer.current_epoch}: Train Loss - {train_loss:.4f}, Val Loss - {val_loss:.4f}"")

# Create a LightningModule for demonstration
class ExampleModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.l1 = torch.nn.Linear(10, 1)

    def forward(self, x):
        return self.l1(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = torch.nn.functional.mse_loss(y_hat, y)
        self.log(""train_loss_step"", loss)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = torch.nn.functional.mse_loss(y_hat, y)
        self.log(""val_loss_step"", loss)

if __name__ == ""__main__"":
    model = ExampleModel()

    trainer = pl.Trainer(
        gpus=0,
        max_epochs=5,
        callbacks=[LossLoggingCallback()],
    )

    trainer.fit(model)",test
rapidfuzz,1,Create a Python program using the 'rapidfuzz' API to demonstrate fuzzy string matching. The program should take a query string and compare it to a list of target strings using the 'fuzz.ratio' scorer from 'rapidfuzz'. Print the query string and the similarity score for each target string. The goal is to find similar strings in the list based on the ratio similarity score.,code/rapidfuzz/rapidfuzz_1.py,Ensure that the program finds the most similar target string(s) based on the ratio similarity score.,Verify that the program prints the query string and the similarity scores without errors.,Test if the program correctly calculates and prints the similarity scores for the query string compared to the list of target strings using the 'fuzz.ratio' scorer.,,,"#!pip install rapidfuzz
from rapidfuzz import fuzz, process

target_strings = [""apple"", ""banana"", ""cherry"", ""date"", ""elderberry"", ""fig"", ""grape""]

query_string = ""cranberry""

print(""Query: "", query_string)

results = process.extract(query_string, target_strings, scorer=fuzz.ratio, limit=len(target_strings))

for result in results:
    target, score, _ = result
    print(f""{target}: Similarity Score {score}"")",train
rapidfuzz,9,"Create a Python program using the 'rapidfuzz' API to demonstrate fuzzy string matching. The program should take a query string and compare it to a list of target strings using the 'fuzz.WRatio' scorer from 'rapidfuzz'. Print the query string and the similarity score for each target string. Additionally, the program should calculate and print the token sort ratio similarity score for each target string. The goal is to find similar strings in the list based on both the weighted ratio and token sort ratio similarity scores.",code/rapidfuzz/rapidfuzz_9.py,Ensure that the program finds the most similar target string(s) based on both the weighted ratio and token sort ratio similarity scores.,Test if the program correctly calculates and prints the token sort ratio similarity scores for the query string compared to the list of target strings.,Test if the program correctly calculates and prints the weighted ratio similarity scores for the query string compared to the list of target strings using the 'fuzz.WRatio' scorer.,Verify that the program prints the query string and the similarity scores without errors.,,"#!pip install rapidfuzz
from rapidfuzz import fuzz, process

target_strings = [""apple"", ""banana"", ""cherry"", ""date"", ""elderberry"", ""fig"", ""grape""]

query_string = ""cranberry""

print(""Query: "", query_string)

results = process.extract(query_string, target_strings, scorer=fuzz.WRatio, limit=len(target_strings))

for result in results:
    target, score, _ = result
    token_sort_ratio_score = fuzz.token_sort_ratio(query_string, target)
    print(f""{target}: Weighted Ratio Score {score}, Token Sort Ratio Score {token_sort_ratio_score}"")",train
rapidfuzz,6,Create a Python program using the 'rapidfuzz' API to demonstrate fuzzy string matching. The program should take a query string and compare it to a list of target strings using the 'fuzz.WRatio' scorer from 'rapidfuzz'. Print the query string and the similarity score for each target string. The goal is to find similar strings in the list based on the weighted ratio similarity score.,code/rapidfuzz/rapidfuzz_6.py,Ensure that the program finds the most similar target string(s) based on the weighted ratio similarity score.,Test if the program correctly calculates and prints the similarity scores for the query string compared to the list of target strings using the 'fuzz.WRatio' scorer.,Verify that the program prints the query string and the similarity scores without errors.,,,"#!pip install rapidfuzz
from rapidfuzz import fuzz, process

target_strings = [""apple"", ""banana"", ""cherry"", ""date"", ""elderberry"", ""fig"", ""grape""]

query_string = ""cranberry""

print(""Query: "", query_string)

results = process.extract(query_string, target_strings, scorer=fuzz.WRatio, limit=len(target_strings))

for result in results:
    target, score, _ = result
    print(f""{target}: Similarity Score {score}"")",train
rapidfuzz,3,Create a Python program using the 'rapidfuzz' API to demonstrate fuzzy string matching. The program should take a query string and compare it to a list of target strings using the 'fuzz.token_sort_ratio' scorer from 'rapidfuzz'. Print the query string and the similarity score for each target string. The goal is to find similar strings in the list based on the token sort ratio similarity score.,code/rapidfuzz/rapidfuzz_3.py,Ensure that the program finds the most similar target string(s) based on the token sort ratio similarity score.,Test if the program correctly calculates and prints the similarity scores for the query string compared to the list of target strings using the 'fuzz.token_sort_ratio' scorer.,Verify that the program prints the query string and the similarity scores without errors.,,,"#!pip install rapidfuzz
from rapidfuzz import fuzz, process

target_strings = [""apple"", ""banana"", ""cherry"", ""date"", ""elderberry"", ""fig"", ""grape""]

query_string = ""cranberry""

print(""Query: "", query_string)

results = process.extract(query_string, target_strings, scorer=fuzz.token_sort_ratio, limit=len(target_strings))

for result in results:
    target, score, _ = result
    print(f""{target}: Similarity Score {score}"")",train
rapidfuzz,2,Create a Python program using the 'rapidfuzz' API to demonstrate fuzzy string matching. The program should take a query string and compare it to a list of target strings using the 'fuzz.partial_ratio' scorer from 'rapidfuzz'. Print the query string and the similarity score for each target string. The goal is to find similar strings in the list based on the partial ratio similarity score.,code/rapidfuzz/rapidfuzz_2.py,Ensure that the program finds the most similar target string(s) based on the partial ratio similarity score.,Test if the program correctly calculates and prints the similarity scores for the query string compared to the list of target strings using the 'fuzz.partial_ratio' scorer.,Verify that the program prints the query string and the similarity scores without errors.,,,"#!pip install rapidfuzz
from rapidfuzz import fuzz, process

target_strings = [""apple"", ""banana"", ""cherry"", ""date"", ""elderberry"", ""fig"", ""grape""]

query_string = ""cranberry""

print(""Query: "", query_string)

results = process.extract(query_string, target_strings, scorer=fuzz.partial_ratio, limit=len(target_strings))

for result in results:
    target, score, _ = result
    print(f""{target}: Similarity Score {score}"")",train
rapidfuzz,12,"Create a Python program using the 'rapidfuzz' API to demonstrate fuzzy string matching. The program should take a query string and compare it to a list of target strings using the 'fuzz.WRatio' scorer from 'rapidfuzz'. Print the query string and the similarity score for each target string. Additionally, the program should calculate and print the partial ratio similarity score for each target string. The goal is to find similar strings in the list based on both the weighted ratio and partial ratio similarity scores.",code/rapidfuzz/rapidfuzz_12.py,Ensure that the program finds the most similar target string(s) based on both the weighted ratio and partial ratio similarity scores.,Test if the program correctly calculates and prints the weighted ratio similarity scores for the query string compared to the list of target strings using the 'fuzz.WRatio' scorer.,Test if the program correctly calculates and prints the partial ratio similarity scores for the query string compared to the list of target strings.,Verify that the program prints the query string and the similarity scores without errors.,,"#!pip install rapidfuzz
from rapidfuzz import fuzz, process

target_strings = [""apple"", ""banana"", ""cherry"", ""date"", ""elderberry"", ""fig"", ""grape""]

query_string = ""cranberry""

print(""Query: "", query_string)

results = process.extract(query_string, target_strings, scorer=fuzz.WRatio, limit=len(target_strings))

for result in results:
    target, score, _ = result
    partial_ratio_score = fuzz.partial_ratio(query_string, target)
    print(f""{target}: Weighted Ratio Score {score}, Partial Ratio Score {partial_ratio_score}"")",train
rapidfuzz,5,Create a Python program using the 'rapidfuzz' API to demonstrate fuzzy string matching. The program should take a query string and compare it to a list of target strings using the 'fuzz.token_ratio' scorer from 'rapidfuzz'. Print the query string and the similarity score for each target string. The goal is to find similar strings in the list based on the token ratio similarity score.,code/rapidfuzz/rapidfuzz_5.py,Test if the program correctly calculates and prints the similarity scores for the query string compared to the list of target strings using the 'fuzz.token_ratio' scorer.,Ensure that the program finds the most similar target string(s) based on the token ratio similarity score.,Verify that the program prints the query string and the similarity scores without errors.,,,"#!pip install rapidfuzz
from rapidfuzz import fuzz, process

target_strings = [""apple"", ""banana"", ""cherry"", ""date"", ""elderberry"", ""fig"", ""grape""]

query_string = ""cranberry""

print(""Query: "", query_string)

results = process.extract(query_string, target_strings, scorer=fuzz.token_ratio, limit=len(target_strings))

for result in results:
    target, score, _ = result
    print(f""{target}: Similarity Score {score}"")",train
rapidfuzz,8,"Create a Python program using the 'rapidfuzz' API to demonstrate fuzzy string matching. The program should take a query string and compare it to a list of target strings using the 'fuzz.token_set_ratio' scorer from 'rapidfuzz'. Print the query string and the similarity score for each target string. Additionally, the program should calculate and print the token ratio similarity score for each target string. The goal is to find similar strings in the list based on both the token set ratio and token ratio similarity scores.",code/rapidfuzz/rapidfuzz_8.py,Test if the program correctly calculates and prints the token set ratio similarity scores for the query string compared to the list of target strings using the 'fuzz.token_set_ratio' scorer.,Test if the program correctly calculates and prints the token ratio similarity scores for the query string compared to the list of target strings.,Ensure that the program finds the most similar target string(s) based on both the token set ratio and token ratio similarity scores.,Verify that the program prints the query string and the similarity scores without errors.,,"#!pip install rapidfuzz
from rapidfuzz import fuzz, process

target_strings = [""apple"", ""banana"", ""cherry"", ""date"", ""elderberry"", ""fig"", ""grape""]

query_string = ""cranberry""

print(""Query: "", query_string)

results = process.extract(query_string, target_strings, scorer=fuzz.token_set_ratio, limit=len(target_strings))

for result in results:
    target, score, _ = result
    token_ratio_score = fuzz.token_ratio(query_string, target)
    print(f""{target}: Token Set Ratio Score {score}, Token Ratio Score {token_ratio_score}"")",train
rapidfuzz,4,Create a Python program using the 'rapidfuzz' API to demonstrate fuzzy string matching. The program should take a query string and compare it to a list of target strings using the 'fuzz.token_set_ratio' scorer from 'rapidfuzz'. Print the query string and the similarity score for each target string. The goal is to find similar strings in the list based on the token set ratio similarity score.,code/rapidfuzz/rapidfuzz_4.py,Ensure that the program finds the most similar target string(s) based on the token set ratio similarity score.,Verify that the program prints the query string and the similarity scores without errors.,Test if the program correctly calculates and prints the similarity scores for the query string compared to the list of target strings using the 'fuzz.token_set_ratio' scorer.,,,"#!pip install rapidfuzz
from rapidfuzz import fuzz, process

target_strings = [""apple"", ""banana"", ""cherry"", ""date"", ""elderberry"", ""fig"", ""grape""]

query_string = ""cranberry""

print(""Query: "", query_string)

results = process.extract(query_string, target_strings, scorer=fuzz.token_set_ratio, limit=len(target_strings))

for result in results:
    target, score, _ = result
    print(f""{target}: Similarity Score {score}"")",train
rapidfuzz,7,"Create a Python program using the 'rapidfuzz' API to demonstrate fuzzy string matching. The program should take a query string and compare it to a list of target strings using the 'fuzz.token_sort_ratio' scorer from 'rapidfuzz'. Print the query string and the similarity score for each target string. Additionally, the program should calculate and print the partial ratio similarity score for each target string. The goal is to find similar strings in the list based on both the token sort ratio and partial ratio similarity scores.",code/rapidfuzz/rapidfuzz_7.py,Ensure that the program finds the most similar target string(s) based on both the token sort ratio and partial ratio similarity scores.,Test if the program correctly calculates and prints the token sort ratio similarity scores for the query string compared to the list of target strings using the 'fuzz.token_sort_ratio' scorer.,Test if the program correctly calculates and prints the partial ratio similarity scores for the query string compared to the list of target strings.,Verify that the program prints the query string and the similarity scores without errors.,,"#!pip install rapidfuzz
from rapidfuzz import fuzz, process

target_strings = [""apple"", ""banana"", ""cherry"", ""date"", ""elderberry"", ""fig"", ""grape""]

query_string = ""cranberry""

print(""Query: "", query_string)

results = process.extract(query_string, target_strings, scorer=fuzz.token_sort_ratio, limit=len(target_strings))

for result in results:
    target, score, _ = result
    partial_ratio_score = fuzz.partial_ratio(query_string, target)
    print(f""{target}: Token Sort Ratio Score {score}, Partial Ratio Score {partial_ratio_score}"")",train
rapidfuzz,11,"Create a Python program using the 'rapidfuzz' API to demonstrate fuzzy string matching. The program should take a query string and compare it to a list of target strings using the 'fuzz.WRatio' scorer from 'rapidfuzz'. Print the query string and the similarity score for each target string. Additionally, the program should calculate and print the token ratio similarity score for each target string. The goal is to find similar strings in the list based on both the weighted ratio and token ratio similarity scores.",code/rapidfuzz/rapidfuzz_11.py,Test if the program correctly calculates and prints the token ratio similarity scores for the query string compared to the list of target strings.,Test if the program correctly calculates and prints the weighted ratio similarity scores for the query string compared to the list of target strings using the 'fuzz.WRatio' scorer.,Ensure that the program finds the most similar target string(s) based on both the weighted ratio and token ratio similarity scores.,Verify that the program prints the query string and the similarity scores without errors.,,"#!pip install rapidfuzz
from rapidfuzz import fuzz, process

target_strings = [""apple"", ""banana"", ""cherry"", ""date"", ""elderberry"", ""fig"", ""grape""]

query_string = ""cranberry""

print(""Query: "", query_string)

results = process.extract(query_string, target_strings, scorer=fuzz.WRatio, limit=len(target_strings))

for result in results:
    target, score, _ = result
    token_ratio_score = fuzz.token_ratio(query_string, target)
    print(f""{target}: Weighted Ratio Score {score}, Token Ratio Score {token_ratio_score}"")",test
rapidfuzz,10,"Create a Python program using the 'rapidfuzz' API to demonstrate fuzzy string matching. The program should take a query string and compare it to a list of target strings using the 'fuzz.WRatio' scorer from 'rapidfuzz'. Print the query string and the similarity score for each target string. Additionally, the program should calculate and print the token set ratio similarity score for each target string. The goal is to find similar strings in the list based on both the weighted ratio and token set ratio similarity scores.",code/rapidfuzz/rapidfuzz_10.py,Ensure that the program finds the most similar target string(s) based on both the weighted ratio and token set ratio similarity scores.,Test if the program correctly calculates and prints the token set ratio similarity scores for the query string compared to the list of target strings.,Test if the program correctly calculates and prints the weighted ratio similarity scores for the query string compared to the list of target strings using the 'fuzz.WRatio' scorer.,Verify that the program prints the query string and the similarity scores without errors.,,"#!pip install rapidfuzz
from rapidfuzz import fuzz, process

target_strings = [""apple"", ""banana"", ""cherry"", ""date"", ""elderberry"", ""fig"", ""grape""]

query_string = ""cranberry""

print(""Query: "", query_string)

results = process.extract(query_string, target_strings, scorer=fuzz.WRatio, limit=len(target_strings))

for result in results:
    target, score, _ = result
    token_set_ratio_score = fuzz.token_set_ratio(query_string, target)
    print(f""{target}: Weighted Ratio Score {score}, Token Set Ratio Score {token_set_ratio_score}"")",test
rich,21,Create a Python program using the 'rich' API to display a colorful and styled progress bar that shows the progress of rendering a 3D object. The program should update the progress as the rendering process continues.,code/rich/rich_21.py,Test the program by rendering different 3D objects to assess its ability to handle various rendering tasks.,Verify that the program displays the progress bar with the correct title and updates the percentage accurately as the rendering process continues.,Test the program with rendering tasks of different durations to ensure the progress bar accurately reflects the progress of rendering.,,,"# Install rich
#!pip install rich
import time
from rich.progress import Progress

# Simulate rendering a 3D object
def render_3d_object():
    for _ in range(100):
        time.sleep(0.1)

# Create a styled progress bar for rendering
with Progress() as progress:
    task = progress.add_task(""[cyan]Rendering..."", total=100)
    progress.console.show_cursor(False)
    
    # Simulate rendering and update the progress
    for _ in range(100):
        time.sleep(0.1)
        progress.update(task, completed=1)
        progress.refresh()

# Display completion message
progress.console.show_cursor(True)
print(""[green]Rendering completed!"")

",train
rich,37,"Create a Python program using the 'rich' API to display a colorful and styled countdown timer for a specific duration in days, hours, minutes, and seconds. The program should count down and update every second.",code/rich/rich_37.py,Verify the program's responsiveness and accuracy in updating the countdown every second by running it for an extended period.,"Test the program with different countdown durations in days, hours, minutes, and seconds to ensure it correctly counts down from the specified time.",Test the program by changing the title and style of the countdown timer to assess its customization options.,,,"# Install rich
#!pip install rich
import time
from rich.console import Console
from rich.panel import Panel

# Create a countdown timer for a specific duration in days, hours, minutes, and seconds
def countdown(days, hours, minutes, seconds):
    total_seconds = (days * 24 * 60 * 60) + (hours * 60 * 60) + (minutes * 60) + seconds
    console = Console()
    for i in range(total_seconds, 0, -1):
        console.clear()
        days_remaining = i // (24 * 60 * 60)
        hours_remaining = (i % (24 * 60 * 60)) // (60 * 60)
        minutes_remaining = (i % (60 * 60)) // 60
        seconds_remaining = i % 60
        panel = Panel(f""Time Remaining: {days_remaining}d {hours_remaining:02d}h {minutes_remaining:02d}m {seconds_remaining:02d}s"", style=""bold yellow on red"")
        console.print(panel)
        time.sleep(1)
    console.clear()
    panel = Panel(""Time's up!"", style=""bold red on yellow"")
    console.print(panel)

# Set the countdown time (e.g., 3 days, 4 hours, 30 minutes, and 15 seconds)
countdown(3, 4, 30, 15)

",train
rich,27,"Create a Python program using the 'rich' API to display a colorful and styled list of famous quotes. The program should include the quote text, the author, and the source of the quote.",code/rich/rich_27.py,Test the program by changing the title and style of the table to assess its customization options for famous quotes.,Verify that the program maintains readability and formatting even with longer quotes and source descriptions.,"Test the program with different sets of famous quotes to ensure it correctly displays the list with quote text, authors, and sources.",,,"# Install rich
#!pip install rich
from rich.console import Console
from rich.table import Table

# Define a list of famous quotes
quotes = [
    {
        ""text"": ""The only way to do great work is to love what you do."",
        ""author"": ""Steve Jobs"",
        ""source"": ""Stanford University commencement speech"",
    },
    {
        ""text"": ""Life is what happens when you're busy making other plans."",
        ""author"": ""John Lennon"",
        ""source"": ""Beautiful Boy (Darling Boy) song lyrics"",
    },
    {
        ""text"": ""In three words I can sum up everything I've learned about life: it goes on."",
        ""author"": ""Robert Frost"",
        ""source"": ""Caricature"",
    },
    {
        ""text"": ""Be yourself; everyone else is already taken."",
        ""author"": ""Oscar Wilde"",
        ""source"": ""The Ultimate Quotable Einstein book"",
    },
]

# Create a rich table for famous quotes
console = Console()
table = Table(title=""Famous Quotes"", style=""italic"")
table.add_column(""Quote"", style=""bold"")
table.add_column(""Author"", style=""bold"")
table.add_column(""Source"", style=""bold"")

# Populate the table with quote data
for quote in quotes:
    table.add_row(quote[""text""], quote[""author""], quote[""source""])

# Display the famous quotes
console.print(table)

",train
rich,16,Generate a Python program using the 'rich' API to display a dynamic and colorful real-time weather dashboard. The program should fetch weather data from an API and update the dashboard every few seconds.,code/rich/rich_16.py,Test the program with different cities and weather conditions to verify that it correctly fetches and displays real-time weather data.,Test the program's behavior when the API data is unavailable or returns errors to assess its error-handling capabilities.,Verify the program's responsiveness and update frequency by running it for an extended period to ensure it maintains real-time information.,,,"# Install rich
#!pip install rich
import time
import requests
from rich.console import Console

# Fetch weather data from an API (replace with a real API)
def fetch_weather_data():
    # Simulate fetching weather data (replace with API request)
    weather_data = {
        ""city"": ""New York"",
        ""temperature"": ""72F"",
        ""condition"": ""Partly cloudy"",
    }
    return weather_data

# Create a real-time weather dashboard
console = Console()

while True:
    weather_data = fetch_weather_data()
    console.clear()
    console.print(""Real-time Weather Dashboard"", style=""bold underline"")
    console.print(f""City: {weather_data['city']}"", style=""bold"")
    console.print(f""Temperature: {weather_data['temperature']}"", style=""bold"")
    console.print(f""Condition: {weather_data['condition']}"", style=""bold"")
    time.sleep(5)

",train
rich,11,Create a Python program using the 'rich' API to display a formatted and colored calendar for a specific month. The program should highlight the current date and allow navigation to other months.,code/rich/rich_11.py,Test the program with different years and months to ensure it correctly displays the calendar for the specified date.,Test the program with leap years and months with varying numbers of days to assess its accuracy in displaying the calendar.,Verify that the program highlights the current date and allows navigation to different months.,,,"# Install rich
#!pip install rich
from rich.console import Console
from rich.table import Table

# Define a calendar for a specific month
def display_calendar(year, month):
    console = Console()
    table = Table(title=f""Calendar - {year}-{month}"")
    table.add_column(""Sun"", style=""bold"", justify=""center"")
    table.add_column(""Mon"", style=""bold"", justify=""center"")
    table.add_column(""Tue"", style=""bold"", justify=""center"")
    table.add_column(""Wed"", style=""bold"", justify=""center"")
    table.add_column(""Thu"", style=""bold"", justify=""center"")
    table.add_column(""Fri"", style=""bold"", justify=""center"")
    table.add_column(""Sat"", style=""bold"", justify=""center"")

    # Sample data for days (replace with real data)
    days_data = [[f""{day}"" for day in range(1, 32)]]

    # Display the calendar
    for week in days_data:
        table.add_row(*week)
    console.print(table)

# Display the calendar for a specific year and month
display_calendar(2023, 10)

",train
rich,22,Generate a Python program using the 'rich' API to display a colorful and styled countdown timer. The program should count down from a specified time and update every second.,code/rich/rich_22.py,Test the program with different countdown durations to ensure it correctly counts down from the specified time.,Verify the program's responsiveness and accuracy in updating the countdown every second by running it for an extended period.,Test the program with different text and background colors to assess its customization options for the countdown timer.,,,"# Install rich
#!pip install rich
import time
from rich.console import Console
from rich.panel import Panel

# Create a countdown timer
def countdown(seconds):
    console = Console()
    for i in range(seconds, 0, -1):
        console.clear()
        panel = Panel(f""Time Remaining: {i} seconds"", style=""bold red on white"")
        console.print(panel)
        time.sleep(1)
    console.clear()
    panel = Panel(""Time's up!"", style=""bold white on red"")
    console.print(panel)

# Set the countdown time (e.g., 10 seconds)
countdown(10)

",train
rich,12,"Generate a Python program using the 'rich' API to create a styled and colorful text-based user interface for a to-do list application. The program should allow users to add, remove, and list tasks.",code/rich/rich_12.py,Test the program by listing tasks in the to-do list to ensure it displays them accurately.,Verify that the program removes tasks when requested and correctly handles tasks that do not exist in the list.,Test the program by adding multiple tasks to ensure it correctly adds and displays them in the to-do list.,,,"# Install rich
#!pip install rich
from rich.console import Console
from rich.panel import Panel

# Define a simple to-do list application
class ToDoList:
    def __init__(self):
        self.tasks = []

    def add_task(self, task):
        self.tasks.append(task)

    def remove_task(self, task):
        if task in self.tasks:
            self.tasks.remove(task)

    def list_tasks(self):
        return ""\n"".join(self.tasks)

# Create the to-do list interface
console = Console()

todo_list = ToDoList()

while True:
    console.clear()
    panel = Panel(""To-Do List"", title=""To-Do List"", expand=False)
    console.print(panel)
    action = input(""Choose an action: (1) Add Task (2) Remove Task (3) List Tasks (4) Quit: "")
    
    if action == ""1"":
        task = input(""Enter the task to add: "")
        todo_list.add_task(task)
    elif action == ""2"":
        task = input(""Enter the task to remove: "")
        todo_list.remove_task(task)
    elif action == ""3"":
        tasks = todo_list.list_tasks()
        console.print(tasks)
    elif action == ""4"":
        break

",train
rich,40,"Generate a Python program using the 'rich' API to display a colorful and styled countdown timer for a specific duration in weeks, days, hours, minutes, and seconds. The program should count down and update every second.",code/rich/rich_40.py,Verify the program's responsiveness and accuracy in updating the countdown every second by running it for an extended period.,"Test the program with different countdown durations in weeks, days, hours, minutes, and seconds to ensure it correctly counts down from the specified time.",Test the program by changing the title and style of the countdown timer to assess its customization options.,,,"# Install rich
#!pip install rich
import time
from rich.console import Console
from rich.panel import Panel

# Create a countdown timer for a specific duration in weeks, days, hours, minutes, and seconds
def countdown(weeks, days, hours, minutes, seconds):
    total_seconds = (weeks * 7 * 24 * 60 * 60) + (days * 24 * 60 * 60) + (hours * 60 * 60) + (minutes * 60) + seconds
    console = Console()
    for i in range(total_seconds, 0, -1):
        console.clear()
        weeks_remaining = i // (7 * 24 * 60 * 60)
        days_remaining = (i % (7 * 24 * 60 * 60)) // (24 * 60 * 60)
        hours_remaining = (i % (24 * 60 * 60)) // (60 * 60)
        minutes_remaining = (i % (60 * 60)) // 60
        seconds_remaining = i % 60
        panel = Panel(f""Time Remaining: {weeks_remaining}w {days_remaining}d {hours_remaining:02d}h {minutes_remaining:02d}m {seconds_remaining:02d}s"", style=""bold yellow on red"")
        console.print(panel)
        time.sleep(1)
    console.clear()
    panel = Panel(""Time's up!"", style=""bold red on yellow"")
    console.print(panel)

# Set the countdown time (e.g., 4 weeks, 3 days, 12 hours, 30 minutes, and 15 seconds)
countdown(4, 3, 12, 30, 15)

",train
rich,1,"Create a Python program using the 'rich' API to display a well-formatted table of product inventory data. The table should have columns for product names, prices, and quantities.",code/rich/rich_1.py,Test the table rendering performance with a large dataset to ensure it remains responsive and efficient.,Verify the program's behavior with empty or incomplete data to assess its robustness in handling different scenarios.,Test the program with various sets of product data to ensure it correctly displays the table with proper formatting and alignment.,,,"# Install rich
#!pip install rich
from rich.console import Console
from rich.table import Table

# Sample dataset
data = [
    {""name"": ""Product A"", ""price"": 10.0, ""quantity"": 50},
    {""name"": ""Product B"", ""price"": 15.0, ""quantity"": 30},
    {""name"": ""Product C"", ""price"": 12.5, ""quantity"": 40},
]

# Create a rich table
table = Table(title=""Product Inventory"")
table.add_column(""Name"", style=""bold"")
table.add_column(""Price"", style=""bold"")
table.add_column(""Quantity"", style=""bold"")

# Populate the table with data
for item in data:
    table.add_row(item[""name""], str(item[""price""]), str(item[""quantity""]))

# Render the table
console = Console()
console.print(table)",train
rich,7,"Create a Python program using the 'rich' API to display a formatted and colored list of items, each with a custom style. The list should include item names, descriptions, and prices.",code/rich/rich_7.py,Test the program with a large number of products to assess its performance and readability of the list.,"Verify that the program handles empty or incomplete product data gracefully, ensuring it does not crash or display errors.",Test the program with different sets of product data to ensure it correctly displays the list of items with the specified styles and formatting.,,,"# Install rich
#!pip install rich
from rich.console import Console
from rich.table import Table

# Sample product data
products = [
    {""name"": ""Product A"", ""description"": ""High-quality item"", ""price"": 25.0},
    {""name"": ""Product B"", ""description"": ""Affordable choice"", ""price"": 15.0},
    {""name"": ""Product C"", ""description"": ""Limited edition"", ""price"": 50.0},
]

# Create a rich table for the product list
table = Table(title=""Product List"")
table.add_column(""Name"", style=""bold"")
table.add_column(""Description"", style=""italic"")
table.add_column(""Price"", style=""bold magenta"")

# Populate the table with product data
for product in products:
    table.add_row(product[""name""], product[""description""], str(product[""price""]))

# Display the product list
console = Console()
console.print(table)
",train
rich,35,"Create a Python program using the 'rich' API to display a colorful and styled list of famous scientific quotes. The program should include the quote text, the author, and the source of the quote.",code/rich/rich_35.py,Verify that the program maintains readability and formatting even with longer quotes and source descriptions.,Test the program by changing the title and style of the table to assess its customization options for famous scientific quotes.,"Test the program with different sets of famous scientific quotes to ensure it correctly displays the list with quote text, authors, and sources.",,,"# Install rich
#!pip install rich
from rich.console import Console
from rich.table import Table

# Define a list of famous scientific quotes
quotes = [
    {
        ""text"": ""Science is the great antidote to the poison of enthusiasm and superstition."",
        ""author"": ""Adam Smith"",
        ""source"": ""The Wealth of Nations"",
    },
    {
        ""text"": ""In science there are no shortcuts to truth."",
        ""author"": ""Carl Sagan"",
        ""source"": ""Cosmos"",
    },
    {
        ""text"": ""The good thing about science is that it's true whether or not you believe in it."",
        ""author"": ""Neil deGrasse Tyson"",
        ""source"": ""The Daily Show interview"",
    },
    {
        ""text"": ""The most exciting phrase to hear in science, the one that heralds new discoveries, is not 'Eureka!' but 'That's funny...'"",
        ""author"": ""Isaac Asimov"",
        ""source"": ""The Notebooks of Lazarus Long"",
    },
]

# Create a rich table for famous scientific quotes
console = Console()
table = Table(title=""Famous Scientific Quotes"", style=""italic"")
table.add_column(""Quote"", style=""bold"")
table.add_column(""Author"", style=""bold"")
table.add_column(""Source"", style=""bold"")

# Populate the table with quote data
for quote in quotes:
    table.add_row(quote[""text""], quote[""author""], quote[""source""])

# Display the famous scientific quotes
console.print(table)

",train
rich,20,Generate a Python program using the 'rich' API to display a formatted and colored directory tree with file sizes. The program should visualize the organization of a directory and its subdirectories.,code/rich/rich_20.py,Verify that the program maintains readability and structure visualization even with deep and complex directory hierarchies.,"Test the program by changing the file sizes and units (KB, MB, GB) to assess its ability to handle different data formats.","Test the program with different directory structures, including various levels of subdirectories and files with different sizes, to ensure it correctly displays the tree.",,,"# Install rich
#!pip install rich
from rich import print
from rich.tree import Tree

# Create a directory tree structure with file sizes
directory_tree = Tree(""root"")
subdir1 = directory_tree.add(""subdir1"")
subdir1.add(""file1.txt (10 KB)"")
subdir1.add(""file2.txt (15 KB)"")
subdir2 = directory_tree.add(""subdir2"")
subdir2.add(""file3.txt (5 KB)"")
subdir2.add(""file4.txt (8 KB)"")

# Display the directory tree
print(directory_tree)

",train
rich,24,Generate a Python program using the 'rich' API to display a colorful and styled histogram of data. The program should visualize the distribution of values in a dataset.,code/rich/rich_24.py,Test the program with different datasets to ensure it correctly displays the histogram for the distribution of values.,Test the program with datasets that contain zero values to assess its ability to handle and display empty sections in the histogram.,Verify that the program maintains readability and formatting even with datasets of varying lengths and value ranges.,,,"# Install rich
#!pip install rich
from rich.console import Console
from rich.panel import Panel

# Define a dataset for the histogram
data = [2, 4, 6, 8, 6, 4, 2, 1, 2, 3, 5, 7, 9, 8, 6, 5, 4, 3, 2, 1]

# Create a styled histogram
console = Console()
panel = Panel(""Histogram"", expand=False, style=""bold green on black"")

# Generate the histogram visualization
for i in range(1, 10):
    histogram_row = [""|""]
    for value in data:
        if value >= i:
            histogram_row.append("""")
        else:
            histogram_row.append(""  "")
    panel.render(console, renderable="""".join(histogram_row))
    console.print("""")

",train
rich,8,Generate a Python program using the 'rich' API to create a dynamic and interactive progress bar with options for pausing and resuming the task. The progress bar should show the progress of a long-running task.,code/rich/rich_8.py,Verify that the program responds to pause/resume commands and displays appropriate messages for user interactions.,Test the program with tasks of varying durations to assess its adaptability to different task lengths.,Test the program with a long-running task to ensure the progress bar updates correctly and allows pausing and resuming.,,,"# Install rich
#!pip install rich
import time
from rich.progress import Progress

# Simulate a long-running task
def simulate_long_task():
    for _ in range(300):
        time.sleep(0.1)

# Create a dynamic progress bar
with Progress() as progress:
    task = progress.add_task(""[cyan]Processing..."", total=300)
    
    # Simulate the task and allow pausing and resuming
    for _ in range(300):
        time.sleep(0.1)
        progress.update(task, completed=1)
        if progress.console.is_paused:
            progress.console.print(""[yellow]Task paused. Press [cyan]P[default] to resume."")

# Display completion message
print(""[green]Task completed!"")

",train
rich,14,Generate a Python program using the 'rich' API to display a dynamic and colored log viewer. The program should show log entries with different log levels and use custom styles to highlight errors and warnings.,code/rich/rich_14.py,Test the program by changing the log level filter to assess its ability to display messages based on their severity.,"Test the program by logging messages with different log levels (debug, info, warning, error, critical) to ensure it displays them with appropriate styles and colors.",Verify that the program can handle a large number of log entries without affecting performance.,,,"# Install rich
#!pip install rich
from rich.console import Console
from rich.logging import RichHandler
import logging

# Set up the logger with rich handler
console = Console()
logger = logging.getLogger(""app"")
logger.addHandler(RichHandler(console=console, markup=True))

# Log some sample entries
logger.debug(""Debug message"")
logger.info(""Information message"")
logger.warning(""Warning message - something may be wrong"")
logger.error(""Error message - something is definitely wrong"")
logger.critical(""Critical error - this is bad"")

",train
rich,2,Generate a Python program using the 'rich' API to create a dynamic progress bar. The progress bar should show the progress of a task that takes a specific amount of time to complete. You should also include a percentage indicator and a description of the task being performed.,code/rich/rich_2.py,Test the program with different total values to ensure it handles varying task sizes properly.,Test the program with tasks of different durations to ensure the progress bar accurately reflects the progress.,Verify that the program displays the description of the task and updates the percentage correctly as the task progresses.,,,"# Install rich
#!pip install rich
import time
from rich.progress import Progress

# Simulate a task that takes time to complete
def simulate_task():
    for _ in range(100):
        time.sleep(0.1)

# Create a progress bar
with Progress() as progress:
    task = progress.add_task(""[cyan]Processing..."", total=100)
    
    # Simulate the task and update the progress
    for _ in range(100):
        time.sleep(0.1)
        progress.update(task, completed=1)

# Display completion message
print(""[green]Task completed!"")

",train
rich,30,Generate a Python program using the 'rich' API to display a colorful and styled progress bar that shows the progress of a software installation process. The program should update the progress as the installation continues.,code/rich/rich_30.py,Test the program with installation tasks of different durations to ensure the progress bar accurately reflects the progress of the installation.,Verify that the program displays the progress bar with the correct title and updates the percentage accurately as the installation process continues.,Test the program by installing different software packages to assess its ability to handle various installation tasks.,,,"# Install rich
#!pip install rich
import time
from rich.progress import Progress

# Simulate a software installation process
def install_software():
    for _ in range(100):
        time.sleep(0.1)

# Create a styled progress bar for installation
with Progress() as progress:
    task = progress.add_task(""[cyan]Installing..."", total=100)
    progress.console.show_cursor(False)
    
    # Simulate installation and update the progress
    for _ in range(100):
        time.sleep(0.1)
        progress.update(task, completed=1)
        progress.refresh()

# Display completion message
progress.console.show_cursor(True)
print(""[green]Installation completed!"")

",train
rich,5,Create a Python program using the 'rich' API to display a colored and formatted progress bar with custom styling. The progress bar should show the progress of a file download and include details such as download speed and remaining time.,code/rich/rich_5.py,Test the program with various start values to assess its behavior when the download task starts from different points.,Test the program with different download speeds to ensure the progress bar displays the correct speed information.,Verify that the program correctly estimates and displays the remaining time for the download.,,,"# Install rich
#!pip install rich
import time
from rich.progress import Progress

# Simulate a file download
def simulate_download():
    for _ in range(100):
        time.sleep(0.1)

# Create a styled progress bar
with Progress() as progress:
    task = progress.add_task(""[cyan]Downloading..."", total=100, start=True)
    progress.console.show_cursor(False)

    while not progress.finished:
        simulate_download()
        progress.update(task, completed=1, speed=""1.5MB/s"")
        progress.refresh()

# Display completion message
progress.console.show_cursor(True)
print(""[green]Download completed!"")

",train
rich,39,Create a Python program using the 'rich' API to display a colorful and styled progress bar that shows the progress of a software compilation process. The program should update the progress as the compilation continues.,code/rich/rich_39.py,Test the program by compiling different software projects to assess its ability to handle various compilation tasks.,Verify that the program displays the progress bar with the correct title and updates the percentage accurately as the compilation process continues.,Test the program with software compilation tasks of different durations to ensure the progress bar accurately reflects the progress of the compilation.,,,"# Install rich
#!pip install rich
import time
from rich.progress import Progress

# Simulate a software compilation process
def compile_software():
    for _ in range(100):
        time.sleep(0.1)

# Create a styled progress bar for software compilation
with Progress() as progress:
    task = progress.add_task(""[cyan]Compiling..."", total=100)
    progress.console.show_cursor(False)
    
    # Simulate compilation and update the progress
    for _ in range(100):
        time.sleep(0.1)
        progress.update(task, completed=1)
        progress.refresh()

# Display completion message
progress.console.show_cursor(True)
print(""[green]Compilation completed!"")

",train
rich,6,Generate a Python program using the 'rich' API to display a formatted and colored traceback for an intentionally raised exception. The traceback should be easy to read and visually appealing.,code/rich/rich_6.py,Verify that the program maintains the readability of tracebacks for longer and more complex error scenarios.,Test the program by raising different types of exceptions to see how it handles and displays the tracebacks.,Test the program with various custom exceptions to assess its ability to display customized error messages and details.,,,"# Install rich
#!pip install rich
from rich.traceback import install

# Raise a custom exception
class CustomError(Exception):
    pass

def trigger_exception():
    raise CustomError(""This is a custom exception raised for testing purposes."")

# Install rich traceback
install()

# Trigger the exception
trigger_exception()
",train
rich,38,"Generate a Python program using the 'rich' API to display a colorful and styled list of famous literary quotes. The program should include the quote text, the author, and the source of the quote.",code/rich/rich_38.py,Test the program by changing the title and style of the table to assess its customization options for famous literary quotes.,Verify that the program maintains readability and formatting even with longer quotes and source descriptions.,"Test the program with different sets of famous literary quotes to ensure it correctly displays the list with quote text, authors, and sources.",,,"# Install rich
#!pip install rich
from rich.console import Console
from rich.table import Table

# Define a list of famous literary quotes
quotes = [
    {
        ""text"": ""To be yourself in a world that is constantly trying to make you something else is the greatest accomplishment."",
        ""author"": ""Ralph Waldo Emerson"",
        ""source"": ""Self-Reliance"",
    },
    {
        ""text"": ""The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars."",
        ""author"": ""Jack Kerouac"",
        ""source"": ""On the Road"",
    },
    {
        ""text"": ""We are all fools in love."",
        ""author"": ""Jane Austen"",
        ""source"": ""Pride and Prejudice"",
    },
    {
        ""text"": ""All the world is made of faith, and trust, and pixie dust."",
        ""author"": ""J.M. Barrie"",
        ""source"": ""Peter Pan"",
    },
]

# Create a rich table for famous literary quotes
console = Console()
table = Table(title=""Famous Literary Quotes"", style=""italic"")
table.add_column(""Quote"", style=""bold"")
table.add_column(""Author"", style=""bold"")
table.add_column(""Source"", style=""bold"")

# Populate the table with quote data
for quote in quotes:
    table.add_row(quote[""text""], quote[""author""], quote[""source""])

# Display the famous literary quotes
console.print(table)

",train
rich,29,"Create a Python program using the 'rich' API to display a visually appealing and colored table of contents for a book. The program should include chapter names, page numbers, and a navigation option.",code/rich/rich_29.py,Test the program by changing the title and style of the table to assess its customization options for the table of contents.,Verify that the program maintains readability and formatting even with longer chapter names and page numbers.,Test the program with different sets of table of contents data to ensure it correctly displays chapter names and page numbers.,,,"# Install rich
#!pip install rich
from rich.table import Table
from rich.console import Console
from rich.panel import Panel

# Define a table of contents for a book
table_of_contents = [
    {""chapter"": ""Introduction"", ""page"": 1},
    {""chapter"": ""Chapter 1: Getting Started"", ""page"": 5},
    {""chapter"": ""Chapter 2: Advanced Topics"", ""page"": 20},
    {""chapter"": ""Conclusion"", ""page"": 40},
]

# Create a styled table of contents
console = Console()
panel = Panel(""Table of Contents"", expand=False, style=""bold white on blue"")
table = Table.grid(expand=True, padding=(1, 2, 1, 2))
table.add_column(""Chapter"", style=""bold"", no_wrap=True)
table.add_column(""Page"", style=""bold"", no_wrap=True)

# Populate the table with table of contents data
for entry in table_of_contents:
    table.add_row(entry[""chapter""], str(entry[""page""]))

# Display the table of contents
panel.render(console, renderable=table)

",train
rich,33,Create a Python program using the 'rich' API to display a visually appealing and colored progress bar that shows the progress of a data transfer process. The program should update the progress as the data transfer continues.,code/rich/rich_33.py,Verify that the program displays the progress bar with the correct title and updates the percentage accurately as the data transfer process continues.,Test the program with data transfer tasks of different durations to ensure the progress bar accurately reflects the progress of the data transfer.,Test the program by transferring different types of data to assess its ability to handle various data transfer tasks.,,,"# Install rich
#!pip install rich
import time
from rich.progress import Progress

# Simulate a data transfer process
def transfer_data():
    for _ in range(100):
        time.sleep(0.1)

# Create a styled progress bar for data transfer
with Progress() as progress:
    task = progress.add_task(""[cyan]Transferring data..."", total=100)
    progress.console.show_cursor(False)
    
    # Simulate data transfer and update the progress
    for _ in range(100):
        time.sleep(0.1)
        progress.update(task, completed=1)
        progress.refresh()

# Display completion message
progress.console.show_cursor(True)
print(""[green]Data transfer completed!"")

",train
rich,13,"Create a Python program using the 'rich' API to display a formatted and colorful list of popular programming languages. The list should include language names, descriptions, and their associated icons.",code/rich/rich_13.py,Test the program by changing the order of the languages in the list to ensure it correctly organizes and displays them.,"Test the program with different sets of programming languages to ensure it correctly displays the list with language names, descriptions, and icons.",Verify that the program maintains readability and formatting even with longer descriptions and language names.,,,"# Install rich
#!pip install rich
from rich.table import Table
from rich.console import Console
from rich.panel import Panel

# Define a list of popular programming languages
languages = [
    {""name"": ""Python"", ""description"": ""High-level, versatile language"", ""icon"": """"},
    {""name"": ""JavaScript"", ""description"": ""Client-side scripting language"", ""icon"": """"},
    {""name"": ""Java"", ""description"": ""Cross-platform, object-oriented"", ""icon"": """"},
    {""name"": ""C++"", ""description"": ""High-performance language"", ""icon"": ""++""},
    {""name"": ""Ruby"", ""description"": ""Dynamic, object-oriented"", ""icon"": """"},
]

# Create a rich table to display the languages
console = Console()
table = Table(title=""Popular Programming Languages"")
table.add_column(""Name"", style=""bold"")
table.add_column(""Description"", style=""italic"")
table.add_column(""Icon"", style=""bold"")

# Populate the table with language data
for lang in languages:
    table.add_row(lang[""name""], lang[""description""], lang[""icon""])

# Display the table
console.print(table)

",train
rich,28,Generate a Python program using the 'rich' API to display a styled and colorful digital clock that shows the current time in a 12-hour format with AM/PM. The program should update the time every second.,code/rich/rich_28.py,Verify the program's responsiveness and update frequency by running it for an extended period to ensure it maintains real-time accuracy.,Test the program by running it and checking the accuracy of the displayed time compared to the system time.,Test the program with different text and background colors to assess its customization options for the digital clock.,,,"# Install rich
#!pip install rich
import time
from rich.console import Console
from rich.panel import Panel

# Create a real-time digital clock in 12-hour format with AM/PM
console = Console()

while True:
    console.clear()
    current_time = time.strftime(""%I:%M:%S %p"")
    panel = Panel(current_time, style=""bold green on black"")
    console.print(panel)
    time.sleep(1)

",train
rich,23,"Create a Python program using the 'rich' API to display a visually appealing and colored list of top movie recommendations. The program should include movie titles, release years, and brief descriptions.",code/rich/rich_23.py,Verify that the program maintains readability and formatting even with longer movie titles and descriptions.,Test the program by changing the title and style of the table to assess its customization options for movie recommendations.,"Test the program with different sets of movie recommendations to ensure it correctly displays the list with movie titles, release years, and descriptions.",,,"# Install rich
#!pip install rich
from rich.console import Console
from rich.table import Table

# Define a list of top movie recommendations
movie_recommendations = [
    {""title"": ""Inception"", ""year"": 2010, ""description"": ""Mind-bending science fiction thriller""},
    {""title"": ""The Shawshank Redemption"", ""year"": 1994, ""description"": ""Drama about hope and redemption""},
    {""title"": ""The Dark Knight"", ""year"": 2008, ""description"": ""Superhero crime drama""},
    {""title"": ""Pulp Fiction"", ""year"": 1994, ""description"": ""Quirky and violent crime film""},
    {""title"": ""Schindler's List"", ""year"": 1993, ""description"": ""Historical drama about a businessman who saved lives during the Holocaust""},
]

# Create a rich table for movie recommendations
console = Console()
table = Table(title=""Top Movie Recommendations"")
table.add_column(""Title"", style=""bold"")
table.add_column(""Year"", style=""bold"")
table.add_column(""Description"", style=""italic"")

# Populate the table with movie data
for movie in movie_recommendations:
    table.add_row(movie[""title""], str(movie[""year""]), movie[""description""])

# Display the movie recommendations
console.print(table)

",train
rich,31,Create a Python program using the 'rich' API to display a colorful and styled countdown timer for a specific duration in minutes and seconds. The program should count down and update every second.,code/rich/rich_31.py,Verify the program's responsiveness and accuracy in updating the countdown every second by running it for an extended period.,Test the program by changing the title and style of the countdown timer to assess its customization options.,Test the program with different countdown durations in minutes and seconds to ensure it correctly counts down from the specified time.,,,"# Install rich
#!pip install rich
import time
from rich.console import Console
from rich.panel import Panel

# Create a countdown timer for a specific duration in minutes and seconds
def countdown(minutes, seconds):
    total_seconds = minutes * 60 + seconds
    console = Console()
    for i in range(total_seconds, 0, -1):
        console.clear()
        minutes_remaining = i // 60
        seconds_remaining = i % 60
        panel = Panel(f""Time Remaining: {minutes_remaining:02d}:{seconds_remaining:02d}"", style=""bold yellow on red"")
        console.print(panel)
        time.sleep(1)
    console.clear()
    panel = Panel(""Time's up!"", style=""bold red on yellow"")
    console.print(panel)

# Set the countdown time (e.g., 5 minutes and 30 seconds)
countdown(5, 30)

",train
rich,9,Create a Python program using the 'rich' API to display a styled and colored hierarchical tree structure. The program should visualize the organization of a directory with subdirectories and files.,code/rich/rich_9.py,"Test the program with different directory structures, including various levels of subdirectories and files, to ensure it correctly displays the tree.",Test the program with long file and directory names to assess its handling of text wrapping and alignment.,Verify that the program maintains readability and structure visualization even with deep and complex directory hierarchies.,,,"# Install rich
#!pip install rich
from rich.console import Console
from rich.tree import Tree

# Create a directory tree structure
directory_tree = Tree(""root"")
subdir1 = directory_tree.add(""subdir1"")
subdir1.add(""file1.py"")
subdir1.add(""file2.py"")
subdir2 = directory_tree.add(""subdir2"")
subdir2.add(""file3.txt"")
subdir2.add(""file4.txt"")

# Display the directory tree
console = Console()
console.print(directory_tree)

",train
rich,26,Generate a Python program using the 'rich' API to display a colorful and styled countdown timer for a specific duration. The program should count down and update every second.,code/rich/rich_26.py,Test the program with different countdown durations to ensure it correctly counts down from the specified time.,Verify the program's responsiveness and accuracy in updating the countdown every second by running it for an extended period.,Test the program by changing the title and style of the countdown timer to assess its customization options.,,,"# Install rich
#!pip install rich
import time
from rich.console import Console
from rich.panel import Panel

# Create a countdown timer for a specific duration
def countdown(seconds):
    console = Console()
    for i in range(seconds, 0, -1):
        console.clear()
        panel = Panel(f""Time Remaining: {i} seconds"", style=""bold yellow on red"")
        console.print(panel)
        time.sleep(1)
    console.clear()
    panel = Panel(""Time's up!"", style=""bold red on yellow"")
    console.print(panel)

# Set the countdown time (e.g., 60 seconds)
countdown(60)

",train
rich,10,"Generate a Python program using the 'rich' API to display a live updating table of stock market data. The table should include columns for stock symbols, prices, and price changes, and it should refresh every few seconds to show real-time data.",code/rich/rich_10.py,Verify the program's performance and responsiveness by running it for an extended period to ensure it maintains real-time data visualization.,Test the program with real stock market data to verify that it correctly displays and updates the table with actual stock prices and changes.,Test the program's ability to handle different numbers of stocks and display them in an organized table.,,,"# Install rich
#!pip install rich
import random
import time
from rich.console import Console
from rich.table import Table

# Simulate live stock market data
def get_stock_data():
    stocks = [
        {""symbol"": ""AAPL"", ""price"": round(random.uniform(140, 145), 2), ""change"": round(random.uniform(-1, 1), 2)},
        {""symbol"": ""GOOGL"", ""price"": round(random.uniform(2700, 2750), 2), ""change"": round(random.uniform(-10, 10), 2)},
        {""symbol"": ""TSLA"", ""price"": round(random.uniform(800, 820), 2), ""change"": round(random.uniform(-5, 5), 2)},
    ]
    return stocks

# Create a live stock market table
console = Console()
table = Table(title=""Stock Market"")
table.add_column(""Symbol"", style=""bold"")
table.add_column(""Price"", style=""bold"")
table.add_column(""Change"", style=""bold"")

while True:
    stocks = get_stock_data()
    table.clear_rows()
    for stock in stocks:
        table.add_row(stock[""symbol""], str(stock[""price""]), str(stock[""change""]))
    console.print(table)
    time.sleep(5)

",train
rich,19,"Create a Python program using the 'rich' API to display a colorful and styled table of contents for a document. The program should include section names, page numbers, and a navigation option.",code/rich/rich_19.py,Test the program with different sets of table of contents data to ensure it correctly displays section names and page numbers.,Verify that the program maintains readability and formatting even with longer section names and page numbers.,Test the program by changing the title and style of the table of contents to assess its customization options.,,,"# Install rich
#!pip install rich
from rich.console import Console
from rich.table import Table

# Define a table of contents
table_of_contents = [
    {""section"": ""Introduction"", ""page"": 1},
    {""section"": ""Chapter 1: Getting Started"", ""page"": 5},
    {""section"": ""Chapter 2: Advanced Topics"", ""page"": 20},
    {""section"": ""Conclusion"", ""page"": 40},
]

# Create a styled table of contents
console = Console()
table = Table(title=""Table of Contents"", style=""bold underline"")
table.add_column(""Section"", style=""bold"")
table.add_column(""Page"", style=""bold"")

# Populate the table with table of contents data
for entry in table_of_contents:
    table.add_row(entry[""section""], str(entry[""page""]))

# Display the table of contents
console.print(table)
",train
rich,34,"Generate a Python program using the 'rich' API to display a colorful and styled countdown timer for a specific duration in hours, minutes, and seconds. The program should count down and update every second.",code/rich/rich_34.py,Verify the program's responsiveness and accuracy in updating the countdown every second by running it for an extended period.,"Test the program with different countdown durations in hours, minutes, and seconds to ensure it correctly counts down from the specified time.",Test the program by changing the title and style of the countdown timer to assess its customization options.,,,"# Install rich
#!pip install rich
import time
from rich.console import Console
from rich.panel import Panel

# Create a countdown timer for a specific duration in hours, minutes, and seconds
def countdown(hours, minutes, seconds):
    total_seconds = hours * 3600 + minutes * 60 + seconds
    console = Console()
    for i in range(total_seconds, 0, -1):
        console.clear()
        hours_remaining = i // 3600
        minutes_remaining = (i % 3600) // 60
        seconds_remaining = i % 60
        panel = Panel(f""Time Remaining: {hours_remaining:02d}:{minutes_remaining:02d}:{seconds_remaining:02d}"", style=""bold yellow on red"")
        console.print(panel)
        time.sleep(1)
    console.clear()
    panel = Panel(""Time's up!"", style=""bold red on yellow"")
    console.print(panel)

# Set the countdown time (e.g., 1 hour, 30 minutes, and 15 seconds)
countdown(1, 30, 15)

",train
rich,41,"Create a Python program using the 'rich' API to display a colorful and styled list of famous historical quotes. The program should include the quote text, the author, and the source of the quote.",code/rich/rich_41.py,Verify that the program maintains readability and formatting even with longer quotes and source descriptions.,Test the program by changing the title and style of the table to assess its customization options for famous historical quotes.,"Test the program with different sets of famous historical quotes to ensure it correctly displays the list with quote text, authors, and sources.",,,"# Install rich
#!pip install rich
from rich.console import Console
from rich.table import Table

# Define a list of famous historical quotes
quotes = [
    {
        ""text"": ""Those who cannot remember the past are condemned to repeat it."",
        ""author"": ""George Santayana"",
        ""source"": ""The Life of Reason"",
    },
    {
        ""text"": ""In the end, we will remember not the words of our enemies, but the silence of our friends."",
        ""author"": ""Martin Luther King Jr."",
        ""source"": ""Strength to Love"",
    },
    {
        ""text"": ""Never interrupt your enemy when he is making a mistake."",
        ""author"": ""Napoleon Bonaparte"",
        ""source"": ""Letter to Charles-Maurice Talleyrand-Perigord"",
    },
    {
        ""text"": ""We must, indeed, all hang together or, most assuredly, we shall all hang separately."",
        ""author"": ""Benjamin Franklin"",
        ""source"": ""In the Continental Congress just before signing the Declaration of Independence"",
    },
]

# Create a rich table for famous historical quotes
console = Console()
table = Table(title=""Famous Historical Quotes"", style=""italic"")
table.add_column(""Quote"", style=""bold"")
table.add_column(""Author"", style=""bold"")
table.add_column(""Source"", style=""bold"")

# Populate the table with quote data
for quote in quotes:
    table.add_row(quote[""text""], quote[""author""], quote[""source""])

# Display the famous historical quotes
console.print(table)

",test
rich,18,Generate a Python program using the 'rich' API to display a stylish and colored digital clock that shows the current time. The program should update the time every second.,code/rich/rich_18.py,Verify the program's responsiveness and update frequency by running it for an extended period to ensure it maintains real-time accuracy.,Test the program by running it and checking the accuracy of the displayed time compared to the system time.,Test the program with different text and background colors to assess its customization options for the digital clock.,,,"# Install rich
#!pip install rich
import time
from rich.console import Console

# Create a real-time digital clock
console = Console()
while True:
    console.clear()
    current_time = time.strftime(""%H:%M:%S"")
    console.print(current_time, style=""bold yellow on green"")
    time.sleep(1)

",test
rich,32,"Generate a Python program using the 'rich' API to display a styled and colorful list of inspirational quotes. The program should include the quote text, the author, and the source of the quote.",code/rich/rich_32.py,"Test the program with different sets of inspirational quotes to ensure it correctly displays the list with quote text, authors, and sources.",Verify that the program maintains readability and formatting even with longer quotes and source descriptions.,Test the program by changing the title and style of the table to assess its customization options for inspirational quotes.,,,"# Install rich
#!pip install rich
from rich.console import Console
from rich.table import Table

# Define a list of inspirational quotes
quotes = [
    {
        ""text"": ""The future belongs to those who believe in the beauty of their dreams."",
        ""author"": ""Eleanor Roosevelt"",
        ""source"": ""You Learn by Living: Eleven Keys for a More Fulfilling Life"",
    },
    {
        ""text"": ""The only limit to our realization of tomorrow will be our doubts of today."",
        ""author"": ""Franklin D. Roosevelt"",
        ""source"": ""Great Speeches"",
    },
    {
        ""text"": ""Believe you can and you're halfway there."",
        ""author"": ""Theodore Roosevelt"",
        ""source"": ""Quoted in Half-Hearted Enemies: Nova Scotia Texts"",
    },
    {
        ""text"": ""The greatest glory in living lies not in never falling, but in rising every time we fall."",
        ""author"": ""Nelson Mandela"",
        ""source"": ""Long Walk to Freedom: The Autobiography of Nelson Mandela"",
    },
]

# Create a rich table for inspirational quotes
console = Console()
table = Table(title=""Inspirational Quotes"", style=""italic"")
table.add_column(""Quote"", style=""bold"")
table.add_column(""Author"", style=""bold"")
table.add_column(""Source"", style=""bold"")

# Populate the table with quote data
for quote in quotes:
    table.add_row(quote[""text""], quote[""author""], quote[""source""])

# Display the inspirational quotes
console.print(table)

",test
rich,36,Generate a Python program using the 'rich' API to display a colorful and styled progress bar that shows the progress of a video rendering process. The program should update the progress as the rendering continues.,code/rich/rich_36.py,Test the program with video rendering tasks of different durations to ensure the progress bar accurately reflects the progress of video rendering.,Verify that the program displays the progress bar with the correct title and updates the percentage accurately as the video rendering process continues.,Test the program by rendering different types of videos to assess its ability to handle various video rendering tasks.,,,"# Install rich
#!pip install rich
import time
from rich.progress import Progress

# Simulate video rendering process
def render_video():
    for _ in range(100):
        time.sleep(0.1)

# Create a styled progress bar for video rendering
with Progress() as progress:
    task = progress.add_task(""[cyan]Rendering video..."", total=100)
    progress.console.show_cursor(False)
    
    # Simulate video rendering and update the progress
    for _ in range(100):
        time.sleep(0.1)
        progress.update(task, completed=1)
        progress.refresh()

# Display completion message
progress.console.show_cursor(True)
print(""[green]Video rendering completed!"")

",test
rustworkx,9,"Create a Python program using the 'rustworkx' API to define an undirected graph and find the bridges. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish edges between these nodes and print the indices of the added edges. Then, use the tarjan_bridges function to find and print the bridges of the graph.",code/rustworkx/rustworkx_9.py,Test that the program correctly creates nodes and edges in the graph.,Test that the program accurately finds and prints the bridges of the graph.,Test that the indices of the added edges are printed as expected.,,,"#!pip install rustworkx
import rustworkx as rx

graph = rx.PyGraph()

# Each time add node is called, it returns a new node index
a = graph.add_node(""A"")
b = graph.add_node(""B"")
c = graph.add_node(""C"")
d = graph.add_node(""D"")

# add_edges_from takes tuples of node indices and weights,
# and returns edge indices
edge_indices = graph.add_edges_from([(a, b), (b, c), (c, d)])
print(edge_indices)

# Returns the bridges of the graph
bridges = rx.tarjan_bridges(graph)
print(bridges)",train
rustworkx,6,"Create a Python program using the 'rustworkx' API to define a directed graph and find the minimum cut. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish directed edges between these nodes and print the indices of the added edges. Then, use the stoer_wagner_min_cut function to find and print the minimum cut of the graph.",code/rustworkx/rustworkx_6.py,Test that the program correctly creates nodes and edges in the graph.,Test that the program accurately finds and prints the minimum cut of the graph.,Test that the indices of the added edges are printed as expected.,,,"#!pip install rustworkx
import rustworkx as rx

graph = rx.PyDiGraph()

# Each time add node is called, it returns a new node index
a = graph.add_node(""A"")
b = graph.add_node(""B"")
c = graph.add_node(""C"")
d = graph.add_node(""D"")

# add_edges_from takes tuples of node indices and weights,
# and returns edge indices
edge_indices = graph.add_edges_from([(a, b), (b, c), (c, d), (d, a)])
print(edge_indices)

# Returns the minimum cut of the graph
min_cut = rx.stoer_wagner_min_cut(graph)
print(min_cut)",train
rustworkx,4,"Create a Python program using the 'rustworkx' API to define a directed graph and find the strongly connected components. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish directed edges between these nodes and print the indices of the added edges. Then, use the kosaraju_strongly_connected_components function to find and print the strongly connected components of the graph.",code/rustworkx/rustworkx_4.py,Test that the program correctly creates nodes and edges in the graph.,Test that the program accurately finds and prints the strongly connected components of the graph.,Test that the indices of the added edges are printed as expected.,,,"#!pip install rustworkx
import rustworkx as rx

graph = rx.PyDiGraph()

# Each time add node is called, it returns a new node index
a = graph.add_node(""A"")
b = graph.add_node(""B"")
c = graph.add_node(""C"")
d = graph.add_node(""D"")

# add_edges_from takes tuples of node indices and weights,
# and returns edge indices
edge_indices = graph.add_edges_from([(a, b), (b, c), (c, d), (d, a)])
print(edge_indices)

# Returns the strongly connected components of the graph
strongly_connected_components = rx.kosaraju_strongly_connected_components(graph)
print(strongly_connected_components)",train
rustworkx,14,"Create a Python program using the 'rustworkx' API to define an undirected graph and find the minimum vertex cover. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish edges between these nodes and print the indices of the added edges. Then, use the min_vertex_cover function to find and print the minimum vertex cover of the graph.",code/rustworkx/rustworkx_14.py,Test that the program correctly creates nodes and edges in the graph.,Test that the program accurately finds and prints the minimum vertex cover of the graph.,Test that the indices of the added edges are printed as expected.,,,"#!pip install rustworkx
import rustworkx as rx

graph = rx.PyGraph()

# Each time add node is called, it returns a new node index
a = graph.add_node(""A"")
b = graph.add_node(""B"")
c = graph.add_node(""C"")
d = graph.add_node(""D"")

# add_edges_from takes tuples of node indices and weights,
# and returns edge indices
edge_indices = graph.add_edges_from([(a, b), (b, c), (c, d)])
print(edge_indices)

# Returns the minimum vertex cover of the graph
min_vertex_cover = rx.min_vertex_cover(graph)
print(min_vertex_cover)",train
rustworkx,17,"Create a Python program using the 'rustworkx' API to define an undirected graph and find the maximum clique. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish edges between these nodes and print the indices of the added edges. Then, use the max_clique function to find and print the maximum clique of the graph.",code/rustworkx/rustworkx_17.py,Test that the program correctly creates nodes and edges in the graph.,Test that the program accurately finds and prints the maximum clique of the graph.,Test that the indices of the added edges are printed as expected.,,,"#!pip install rustworkx
import rustworkx as rx

graph = rx.PyGraph()

# Each time add node is called, it returns a new node index
a = graph.add_node(""A"")
b = graph.add_node(""B"")
c = graph.add_node(""C"")
d = graph.add_node(""D"")

# add_edges_from takes tuples of node indices and weights,
# and returns edge indices
edge_indices = graph.add_edges_from([(a, b), (b, c), (c, d)])
print(edge_indices)

# Returns the maximum clique of the graph
max_clique = rx.max_clique(graph)
print(max_clique)",train
rustworkx,16,"Create a Python program using the 'rustworkx' API to define an undirected graph and find the minimum dominating set. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish edges between these nodes and print the indices of the added edges. Then, use the min_dominating_set function to find and print the minimum dominating set of the graph.",code/rustworkx/rustworkx_16.py,Test that the program correctly creates nodes and edges in the graph.,Test that the program accurately finds and prints the minimum dominating set of the graph.,Test that the indices of the added edges are printed as expected.,,,"#!pip install rustworkx
import rustworkx as rx

graph = rx.PyGraph()

# Each time add node is called, it returns a new node index
a = graph.add_node(""A"")
b = graph.add_node(""B"")
c = graph.add_node(""C"")
d = graph.add_node(""D"")

# add_edges_from takes tuples of node indices and weights,
# and returns edge indices
edge_indices = graph.add_edges_from([(a, b), (b, c), (c, d)])
print(edge_indices)

# Returns the minimum dominating set of the graph
min_dominating_set = rx.min_dominating_set(graph)
print(min_dominating_set)",train
rustworkx,12,"Create a Python program using the 'rustworkx' API to define an undirected graph and find the eccentricity of each node. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish edges between these nodes and print the indices of the added edges. Then, use the eccentricity function to find and print the eccentricity of each node in the graph.",code/rustworkx/rustworkx_12.py,Test that the program correctly creates nodes and edges in the graph.,Test that the program accurately finds and prints the eccentricity of each node in the graph.,Test that the indices of the added edges are printed as expected.,,,"#!pip install rustworkx
import rustworkx as rx

graph = rx.PyGraph()

# Each time add node is called, it returns a new node index
a = graph.add_node(""A"")
b = graph.add_node(""B"")
c = graph.add_node(""C"")
d = graph.add_node(""D"")

# add_edges_from takes tuples of node indices and weights,
# and returns edge indices
edge_indices = graph.add_edges_from([(a, b), (b, c), (c, d)])
print(edge_indices)

# Returns the eccentricity of each node in the graph
eccentricity = rx.eccentricity(graph)
print(eccentricity)",train
rustworkx,3,"Create a Python program using the 'rustworkx' API to define a directed graph and find the topological sorting of the nodes. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish directed edges between these nodes and print the indices of the added edges. Then, use the topological_sort function to find and print the topological sorting of the nodes in the graph.",code/rustworkx/rustworkx_3.py,Test that the program correctly creates nodes and edges in the graph.,Test that the program accurately finds and prints the topological sorting of the nodes in the graph.,Test that the indices of the added edges are printed as expected.,,,"#!pip install rustworkx
import rustworkx as rx

graph = rx.PyDiGraph()

# Each time add node is called, it returns a new node index
a = graph.add_node(""A"")
b = graph.add_node(""B"")
c = graph.add_node(""C"")
d = graph.add_node(""D"")

# add_edges_from takes tuples of node indices and weights,
# and returns edge indices
edge_indices = graph.add_edges_from([(a, b), (b, c), (c, d)])
print(edge_indices)

# Returns the topological sorting of the nodes
topological_sorting = rx.topological_sort(graph)
print(topological_sorting)",train
rustworkx,10,"Create a Python program using the 'rustworkx' API to define an undirected graph and find the connected components. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish edges between these nodes and print the indices of the added edges. Then, use the connected_components function to find and print the connected components of the graph.",code/rustworkx/rustworkx_10.py,Test that the program correctly creates nodes and edges in the graph.,Test that the program accurately finds and prints the connected components of the graph.,Test that the indices of the added edges are printed as expected.,,,"#!pip install rustworkx
import rustworkx as rx

graph = rx.PyGraph()

# Each time add node is called, it returns a new node index
a = graph.add_node(""A"")
b = graph.add_node(""B"")
c = graph.add_node(""C"")
d = graph.add_node(""D"")

# add_edges_from takes tuples of node indices and weights,
# and returns edge indices
edge_indices = graph.add_edges_from([(a, b), (b, c), (c, d)])
print(edge_indices)

# Returns the connected components of the graph
connected_components = rx.connected_components(graph)
print(connected_components)",train
rustworkx,18,"Create a Python program using the 'rustworkx' API to define an undirected graph and find the minimum feedback vertex set. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish edges between these nodes and print the indices of the added edges. Then, use the min_feedback_vertex_set function to find and print the minimum feedback vertex set of the graph.",code/rustworkx/rustworkx_18.py,Test that the program correctly creates nodes and edges in the graph.,Test that the program accurately finds and prints the minimum feedback vertex set of the graph.,Test that the indices of the added edges are printed as expected.,,,"#!pip install rustworkx
import rustworkx as rx

graph = rx.PyGraph()

# Each time add node is called, it returns a new node index
a = graph.add_node(""A"")
b = graph.add_node(""B"")
c = graph.add_node(""C"")
d = graph.add_node(""D"")

# add_edges_from takes tuples of node indices and weights,
# and returns edge indices
edge_indices = graph.add_edges_from([(a, b), (b, c), (c, d)])
print(edge_indices)

# Returns the minimum feedback vertex set of the graph
min_feedback_vertex_set = rx.min_feedback_vertex_set(graph)
print(min_feedback_vertex_set)",train
rustworkx,5,"Create a Python program using the 'rustworkx' API to define a directed graph and find the transitive closure. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish directed edges between these nodes and print the indices of the added edges. Then, use the transitive_closure function to find and print the transitive closure of the graph.",code/rustworkx/rustworkx_5.py,Test that the program correctly creates nodes and edges in the graph.,Test that the program accurately finds and prints the transitive closure of the graph.,Test that the indices of the added edges are printed as expected.,,,"#!pip install rustworkx
import rustworkx as rx

graph = rx.PyDiGraph()

# Each time add node is called, it returns a new node index
a = graph.add_node(""A"")
b = graph.add_node(""B"")
c = graph.add_node(""C"")
d = graph.add_node(""D"")

# add_edges_from takes tuples of node indices and weights,
# and returns edge indices
edge_indices = graph.add_edges_from([(a, b), (b, c), (c, d)])
print(edge_indices)

# Returns the transitive closure of the graph
transitive_closure = rx.transitive_closure(graph)
print(transitive_closure)",train
rustworkx,13,"Create a Python program using the 'rustworkx' API to define an undirected graph and find the center. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish edges between these nodes and print the indices of the added edges. Then, use the center function to find and print the center of the graph.",code/rustworkx/rustworkx_13.py,Test that the program correctly creates nodes and edges in the graph.,Test that the program accurately finds and prints the center of the graph.,Test that the indices of the added edges are printed as expected.,,,"#!pip install rustworkx
import rustworkx as rx

graph = rx.PyGraph()

# Each time add node is called, it returns a new node index
a = graph.add_node(""A"")
b = graph.add_node(""B"")
c = graph.add_node(""C"")
d = graph.add_node(""D"")

# add_edges_from takes tuples of node indices and weights,
# and returns edge indices
edge_indices = graph.add_edges_from([(a, b), (b, c), (c, d)])
print(edge_indices)

# Returns the center of the graph
center = rx.center(graph)
print(center)",train
rustworkx,8,"Create a Python program using the 'rustworkx' API to define an undirected graph and find the articulation points. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish edges between these nodes and print the indices of the added edges. Then, use the tarjan_articulation_points function to find and print the articulation points of the graph.",code/rustworkx/rustworkx_8.py,Test that the program correctly creates nodes and edges in the graph.,Test that the program accurately finds and prints the articulation points of the graph.,Test that the indices of the added edges are printed as expected.,,,"#!pip install rustworkx
import rustworkx as rx

graph = rx.PyGraph()

# Each time add node is called, it returns a new node index
a = graph.add_node(""A"")
b = graph.add_node(""B"")
c = graph.add_node(""C"")
d = graph.add_node(""D"")

# add_edges_from takes tuples of node indices and weights,
# and returns edge indices
edge_indices = graph.add_edges_from([(a, b), (b, c), (c, d)])
print(edge_indices)

# Returns the articulation points of the graph
articulation_points = rx.tarjan_articulation_points(graph)
print(articulation_points)",train
rustworkx,11,"Create a Python program using the 'rustworkx' API to define an undirected graph and find the diameter. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish edges between these nodes and print the indices of the added edges. Then, use the diameter function to find and print the diameter of the graph.",code/rustworkx/rustworkx_11.py,Test that the program correctly creates nodes and edges in the graph.,Test that the program accurately finds and prints the diameter of the graph.,Test that the indices of the added edges are printed as expected.,,,"#!pip install rustworkx
import rustworkx as rx

graph = rx.PyGraph()

# Each time add node is called, it returns a new node index
a = graph.add_node(""A"")
b = graph.add_node(""B"")
c = graph.add_node(""C"")
d = graph.add_node(""D"")

# add_edges_from takes tuples of node indices and weights,
# and returns edge indices
edge_indices = graph.add_edges_from([(a, b), (b, c), (c, d)])
print(edge_indices)

# Returns the diameter of the graph
diameter = rx.diameter(graph)
print(diameter)",train
rustworkx,15,"Create a Python program using the 'rustworkx' API to define an undirected graph and find the maximum independent set. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish edges between these nodes and print the indices of the added edges. Then, use the max_independent_set function to find and print the maximum independent set of the graph.",code/rustworkx/rustworkx_15.py,Test that the program correctly creates nodes and edges in the graph.,Test that the program accurately finds and prints the maximum independent set of the graph.,Test that the indices of the added edges are printed as expected.,,,"#!pip install rustworkx
import rustworkx as rx

graph = rx.PyGraph()

# Each time add node is called, it returns a new node index
a = graph.add_node(""A"")
b = graph.add_node(""B"")
c = graph.add_node(""C"")
d = graph.add_node(""D"")

# add_edges_from takes tuples of node indices and weights,
# and returns edge indices
edge_indices = graph.add_edges_from([(a, b), (b, c), (c, d)])
print(edge_indices)

# Returns the maximum independent set of the graph
max_independent_set = rx.max_independent_set(graph)
print(max_independent_set)",train
rustworkx,7,"Create a Python program using the 'rustworkx' API to define a directed graph and find the maximum flow. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish directed edges between these nodes and print the indices of the added edges. Then, use the edmonds_karp_max_flow function to find and print the maximum flow of the graph.",code/rustworkx/rustworkx_7.py,Test that the program correctly creates nodes and edges in the graph.,Test that the program accurately finds and prints the maximum flow of the graph.,Test that the indices of the added edges are printed as expected.,,,"#!pip install rustworkx
import rustworkx as rx

graph = rx.PyDiGraph()

# Each time add node is called, it returns a new node index
a = graph.add_node(""A"")
b = graph.add_node(""B"")
c = graph.add_node(""C"")
d = graph.add_node(""D"")

# add_edges_from takes tuples of node indices and weights,
# and returns edge indices
edge_indices = graph.add_edges_from([(a, b, 3), (a, c, 2), (b, c, 2), (b, d, 1), (c, d, 3)])
print(edge_indices)

# Returns the maximum flow of the graph
max_flow = rx.edmonds_karp_max_flow(graph, a, d, weight_fn=int)
print(max_flow)",train
rustworkx,1,"Create a Python program using the 'rustworkx' API to define a directed graph and find the shortest path between two nodes. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish weighted edges between these nodes and print the indices of the added edges. Then, use Dijkstra's shortest path algorithm to find and print the shortest path from node 'A' to node 'C'",code/rustworkx/rustworkx_1.py,Test that the program correctly creates nodes and edges in the graph.,Test that the program accurately finds and prints the shortest path from node 'A' to node 'C' using Dijkstra's algorithm.,Test that the indices of the added edges are printed as expected.,,,"#!pip install rustworkx
import rustworkx as rx

graph = rx.PyGraph()

# Each time add node is called, it returns a new node index
a = graph.add_node(""A"")
b = graph.add_node(""B"")
c = graph.add_node(""C"")
d = graph.add_node(""D"")

# add_edges_from takes tuples of node indices and weights,
# and returns edge indices
edge_indices = graph.add_edges_from([(a, b, 1.5), (a, d, 2.0), (b, c, 2.5),(d, c, 1.5)])
print(edge_indices)

# Returns the path 
path_mapping = rx.dijkstra_shortest_paths(graph, a, c, weight_fn=float)
print(path_mapping)",test
rustworkx,2,"Create a Python program using the 'rustworkx' API to define an undirected graph and find the minimum spanning tree. Initialize an empty graph and add nodes 'A,' 'B,' 'C,' and 'D' to it. Establish weighted edges between these nodes and print the indices of the added edges. Then, use Kruskal's algorithm to find and print the minimum spanning tree of the graph.",code/rustworkx/rustworkx_2.py,Test that the program correctly creates nodes and edges in the graph.,Test that the program accurately finds and prints the minimum spanning tree of the graph using Kruskal's algorithm.,Test that the indices of the added edges are printed as expected.,,,"#!pip install rustworkx
import rustworkx as rx

graph = rx.PyGraph()

# Each time add node is called, it returns a new node index
a = graph.add_node(""A"")
b = graph.add_node(""B"")
c = graph.add_node(""C"")
d = graph.add_node(""D"")

# add_edges_from takes tuples of node indices and weights,
# and returns edge indices
edge_indices = graph.add_edges_from([(a, b, 1.5), (a, d, 2.0), (b, c, 2.5),(d, c, 1.5)])
print(edge_indices)

# Returns the minimum spanning tree
mst = rx.minimum_spanning_tree(graph, weight_fn=float)
print(mst)",test
scikit-learn,10,"Create a Python program that uses the 'scikit-learn' API to load the Wine Quality dataset, split it into training and testing sets, train a Random Forest Regressor, make predictions, calculate the mean squared error, and save the trained model.",code/scikit-learn/scikit-learn_10.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Measure and record the time taken to train the model on a larger dataset for performance testing.,Test the program with different random states for dataset splitting to ensure consistency in mean squared error calculations.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_openml
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

#Load the Wine Quality dataset
wine_quality = fetch_openml('wine-quality-red', version=1)
X, y = wine_quality.data, wine_quality.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Random Forest Regressor
rf_regressor = RandomForestRegressor(n_estimators=100)

#Train the regressor
rf_regressor.fit(X_train, y_train)

#Make predictions
y_pred = rf_regressor.predict(X_test)

#Calculate mean squared error
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse:.2f}')

#Serialize and save the model
joblib.dump(rf_regressor, 'wine_quality_model.pkl')
",train
scikit-learn,12,"Create a Python program that uses the 'scikit-learn' API to load the Breast Cancer dataset, split it into training and testing sets, train a Logistic Regression classifier, make predictions, calculate accuracy, and save the trained model.",code/scikit-learn/scikit-learn_12.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in accuracy calculations.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression

#Load the Breast Cancer dataset
breast_cancer = load_breast_cancer()
X, y = breast_cancer.data, breast_cancer.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Logistic Regression classifier
logistic_regression = LogisticRegression()

#Train the classifier
logistic_regression.fit(X_train, y_train)

#Make predictions
y_pred = logistic_regression.predict(X_test)

#Calculate accuracy
accuracy = (y_pred == y_test).mean()
print(f'Accuracy: {accuracy:.2f}')

#Serialize and save the model
joblib.dump(logistic_regression, 'breast_cancer_classifier.pkl')
",train
scikit-learn,23,"Create a Python program that uses the 'scikit-learn' API to load the Diabetes dataset, split it into training and testing sets, train a K-Means clustering model, make predictions, and save the trained model.",code/scikit-learn/scikit-learn_23.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in clustering results.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_diabetes
from sklearn.cluster import KMeans

#Load the Diabetes dataset
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the K-Means clustering model
kmeans = KMeans(n_clusters=2)

#Train the model
kmeans.fit(X_train)

#Make predictions
y_pred = kmeans.predict(X_test)

#Serialize and save the model
joblib.dump(kmeans, 'diabetes_kmeans_model.pkl')
",train
scikit-learn,25,"Create a Python program that uses the 'scikit-learn' API to load the LFW (Labeled Faces in the Wild) dataset, split it into training and testing sets, train a K-Means clustering model, make predictions, and save the trained model.",code/scikit-learn/scikit-learn_25.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in clustering results.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_lfw_people
from sklearn.cluster import KMeans

#Load the LFW dataset
lfw = fetch_lfw_people(min_faces_per_person=70, resize=0.4)
X, y = lfw.data, lfw.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the K-Means clustering model
kmeans = KMeans(n_clusters=10)

#Train the model
kmeans.fit(X_train)

#Make predictions
y_pred = kmeans.predict(X_test)

#Serialize and save the model
joblib.dump(kmeans, 'lfw_kmeans_model.pkl')
",train
scikit-learn,15,"Create a Python program that uses the 'scikit-learn' API to load the MNIST dataset, split it into training and testing sets, train a Logistic Regression classifier, make predictions, calculate accuracy, and save the trained model.",code/scikit-learn/scikit-learn_15.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in accuracy calculations.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_openml
from sklearn.linear_model import LogisticRegression

#Load the MNIST dataset
mnist = fetch_openml('mnist_784', version=1)
X, y = mnist.data, mnist.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Logistic Regression classifier
logistic_regression = LogisticRegression()

#Train the classifier
logistic_regression.fit(X_train, y_train)

#Make predictions
y_pred = logistic_regression.predict(X_test)

#Calculate accuracy
accuracy = (y_pred == y_test).mean()
print(f'Accuracy: {accuracy:.2f}')

#Serialize and save the model
joblib.dump(logistic_regression, 'mnist_classifier.pkl')
",train
scikit-learn,1,"Create a Python program that uses the 'scikit-learn' API to load the Iris dataset, split it into training and testing sets, train a K-Nearest Neighbors classifier, make predictions, calculate accuracy, and save the trained model.",code/scikit-learn/scikit-learn_1.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in accuracy calculations.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier

#Load the iris dataset
iris = load_iris()
X, y = iris.data, iris.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the K-Nearest Neighbors classifier
knn_classifier = KNeighborsClassifier(n_neighbors=3)

#Train the classifier
knn_classifier.fit(X_train, y_train)

#Make predictions
y_pred = knn_classifier.predict(X_test)

#Calculate accuracy
accuracy = (y_pred == y_test).mean()
print(f'Accuracy: {accuracy:.2f}')

#Serialize and save the model
joblib.dump(knn_classifier, 'iris_classifier.pkl')
",train
scikit-learn,6,"Create a Python program that uses the 'scikit-learn' API to load the MNIST dataset, split it into training and testing sets, train a Support Vector Machine classifier, make predictions, calculate accuracy, and save the trained model.",code/scikit-learn/scikit-learn_6.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in accuracy calculations.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_openml
from sklearn.svm import SVC

#Load the MNIST dataset
mnist = fetch_openml('mnist_784', version=1)
X, y = mnist.data, mnist.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Support Vector Machine classifier
svm_classifier = SVC()

#Train the classifier
svm_classifier.fit(X_train, y_train)

#Make predictions
y_pred = svm_classifier.predict(X_test)

#Calculate accuracy
accuracy = (y_pred == y_test).mean()
print(f'Accuracy: {accuracy:.2f}')

#Serialize and save the model
joblib.dump(svm_classifier, 'mnist_classifier.pkl')
",train
scikit-learn,19,"Create a Python program that uses the 'scikit-learn' API to load the Iris dataset, split it into training and testing sets, train a Logistic Regression classifier, make predictions, calculate accuracy, and save the trained model.",code/scikit-learn/scikit-learn_19.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in accuracy calculations.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression

#Load the iris dataset
iris = load_iris()
X, y = iris.data, iris.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Logistic Regression classifier
logistic_regression = LogisticRegression()

#Train the classifier
logistic_regression.fit(X_train, y_train)

#Make predictions
y_pred = logistic_regression.predict(X_test)

#Calculate accuracy
accuracy = (y_pred == y_test).mean()
print(f'Accuracy: {accuracy:.2f}')

#Serialize and save the model
joblib.dump(logistic_regression, 'iris_classifier.pkl')
",train
scikit-learn,7,"Create a Python program that uses the 'scikit-learn' API to load the California Housing dataset, split it into training and testing sets, train a Gradient Boosting Regressor, make predictions, calculate the mean squared error, and save the trained model.",code/scikit-learn/scikit-learn_7.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Measure and record the time taken to train the model on a larger dataset for performance testing.,Test the program with different random states for dataset splitting to ensure consistency in mean squared error calculations.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_california_housing
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error

#Load the California Housing dataset
california_housing = fetch_california_housing()
X, y = california_housing.data, california_housing.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Gradient Boosting Regressor
gb_regressor = GradientBoostingRegressor()

#Train the regressor
gb_regressor.fit(X_train, y_train)

#Make predictions
y_pred = gb_regressor.predict(X_test)

#Calculate mean squared error
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse:.2f}')

#Serialize and save the model
joblib.dump(gb_regressor, 'california_housing_model.pkl')
",train
scikit-learn,16,"Create a Python program that uses the 'scikit-learn' API to load the California Housing dataset, split it into training and testing sets, train a Decision Tree Regressor, make predictions, calculate the mean squared error, and save the trained model.",code/scikit-learn/scikit-learn_16.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Measure and record the time taken to train the model on a larger dataset for performance testing.,Test the program with different random states for dataset splitting to ensure consistency in mean squared error calculations.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_california_housing
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

#Load the California Housing dataset
california_housing = fetch_california_housing()
X, y = california_housing.data, california_housing.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Decision Tree Regressor
dt_regressor = DecisionTreeRegressor()

#Train the regressor
dt_regressor.fit(X_train, y_train)

#Make predictions
y_pred = dt_regressor.predict(X_test)

#Calculate mean squared error
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse:.2f}')

#Serialize and save the model
joblib.dump(dt_regressor, 'california_housing_model.pkl')
",train
scikit-learn,14,"Create a Python program that uses the 'scikit-learn' API to load the Wine Quality dataset, split it into training and testing sets, train a Support Vector Machine classifier, make predictions, calculate accuracy, and save the trained model.",code/scikit-learn/scikit-learn_14.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in accuracy calculations.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_openml
from sklearn.svm import SVC

#Load the Wine Quality dataset
wine_quality = fetch_openml('wine-quality-red', version=1)
X, y = wine_quality.data, wine_quality.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Support Vector Machine classifier
svm_classifier = SVC()

#Train the classifier
svm_classifier.fit(X_train, y_train)

#Make predictions
y_pred = svm_classifier.predict(X_test)

#Calculate accuracy
accuracy = (y_pred == y_test).mean()
print(f'Accuracy: {accuracy:.2f}')

#Serialize and save the model
joblib.dump(svm_classifier, 'wine_quality_classifier.pkl')
",train
scikit-learn,29,"Create a Python program that uses the 'scikit-learn' API to load the Wine dataset, split it into training and testing sets, train a Support Vector Machine classifier, make predictions, calculate accuracy, and save the trained model.",code/scikit-learn/scikit-learn_29.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in accuracy calculations.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_wine
from sklearn.svm import SVC

#Load the Wine dataset
wine = load_wine()
X, y = wine.data, wine.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Support Vector Machine classifier
svm_classifier = SVC()

#Train the classifier
svm_classifier.fit(X_train, y_train)

#Make predictions
y_pred = svm_classifier.predict(X_test)

#Calculate accuracy
accuracy = (y_pred == y_test).mean()
print(f'Accuracy: {accuracy:.2f}')

#Serialize and save the model
joblib.dump(svm_classifier, 'wine_classifier.pkl')
",train
scikit-learn,3,"Create a Python program that uses the 'scikit-learn' API to load the Breast Cancer dataset, split it into training and testing sets, train a Support Vector Machine classifier, make predictions, calculate accuracy, and save the trained model.",code/scikit-learn/scikit-learn_3.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in accuracy calculations.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_breast_cancer
from sklearn.svm import SVC

#Load the Breast Cancer dataset
breast_cancer = load_breast_cancer()
X, y = breast_cancer.data, breast_cancer.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Support Vector Machine classifier
svm_classifier = SVC()

#Train the classifier
svm_classifier.fit(X_train, y_train)

#Make predictions
y_pred = svm_classifier.predict(X_test)

#Calculate accuracy
accuracy = (y_pred == y_test).mean()
print(f'Accuracy: {accuracy:.2f}')

#Serialize and save the model
joblib.dump(svm_classifier, 'breast_cancer_classifier.pkl')
",train
scikit-learn,4,"Create a Python program that uses the 'scikit-learn' API to load the Wine dataset, split it into training and testing sets, train a Random Forest classifier, make predictions, calculate accuracy, and save the trained model.",code/scikit-learn/scikit-learn_4.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in accuracy calculations.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_wine
from sklearn.ensemble import RandomForestClassifier

#Load the Wine dataset
wine = load_wine()
X, y = wine.data, wine.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100)

#Train the classifier
rf_classifier.fit(X_train, y_train)

#Make predictions
y_pred = rf_classifier.predict(X_test)

#Calculate accuracy
accuracy = (y_pred == y_test).mean()
print(f'Accuracy: {accuracy:.2f}')

#Serialize and save the model
joblib.dump(rf_classifier, 'wine_classifier.pkl')
",train
scikit-learn,35,"Create a Python program that uses the 'scikit-learn' API to load the Digits dataset, split it into training and testing sets, train a Decision Tree classifier, make predictions, calculate accuracy, and save the trained model.",code/scikit-learn/scikit-learn_35.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in accuracy calculations.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_digits
from sklearn.tree import DecisionTreeClassifier

#Load the Digits dataset
digits = load_digits()
X, y = digits.data, digits.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Decision Tree classifier
dt_classifier = DecisionTreeClassifier()

#Train the classifier
dt_classifier.fit(X_train, y_train)

#Make predictions
y_pred = dt_classifier.predict(X_test)

#Calculate accuracy
accuracy = (y_pred == y_test).mean()
print(f'Accuracy: {accuracy:.2f}')

#Serialize and save the model
joblib.dump(dt_classifier, 'digits_classifier.pkl')
",train
scikit-learn,5,"Create a Python program that uses the 'scikit-learn' API to load the Diabetes dataset, split it into training and testing sets, train a Decision Tree classifier, make predictions, calculate accuracy, and save the trained model.",code/scikit-learn/scikit-learn_5.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in accuracy calculations.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_diabetes
from sklearn.tree import DecisionTreeClassifier

#Load the Diabetes dataset
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Decision Tree classifier
dt_classifier = DecisionTreeClassifier()

#Train the classifier
dt_classifier.fit(X_train, y_train)

#Make predictions
y_pred = dt_classifier.predict(X_test)

#Calculate accuracy
accuracy = (y_pred == y_test).mean()
print(f'Accuracy: {accuracy:.2f}')

#Serialize and save the model
joblib.dump(dt_classifier, 'diabetes_classifier.pkl')
",train
scikit-learn,33,"Create a Python program that uses the 'scikit-learn' API to load the Wine dataset, split it into training and testing sets, train a Decision Tree classifier, make predictions, calculate accuracy, and save the trained model.",code/scikit-learn/scikit-learn_33.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in accuracy calculations.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_wine
from sklearn.tree import DecisionTreeClassifier

#Load the Wine dataset
wine = load_wine()
X, y = wine.data, wine.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Decision Tree classifier
dt_classifier = DecisionTreeClassifier()

#Train the classifier
dt_classifier.fit(X_train, y_train)

#Make predictions
y_pred = dt_classifier.predict(X_test)

#Calculate accuracy
accuracy = (y_pred == y_test).mean()
print(f'Accuracy: {accuracy:.2f}')

#Serialize and save the model
joblib.dump(dt_classifier, 'wine_classifier.pkl')
",train
scikit-learn,28,"Create a Python program that uses the 'scikit-learn' API to load the Breast Cancer dataset, split it into training and testing sets, train a Random Forest classifier, make predictions, calculate accuracy, and save the trained model.",code/scikit-learn/scikit-learn_28.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in accuracy calculations.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier

#Load the Breast Cancer dataset
breast_cancer = load_breast_cancer()
X, y = breast_cancer.data, breast_cancer.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100)

#Train the classifier
rf_classifier.fit(X_train, y_train)

#Make predictions
y_pred = rf_classifier.predict(X_test)

#Calculate accuracy
accuracy = (y_pred == y_test).mean()
print(f'Accuracy: {accuracy:.2f}')

#Serialize and save the model
joblib.dump(rf_classifier, 'breast_cancer_classifier.pkl')
",train
scikit-learn,37,"Create a Python program that uses the 'scikit-learn' API to load the California Housing dataset, split it into training and testing sets, train a Random Forest Regressor, make predictions, calculate the mean squared error, and save the trained model.",code/scikit-learn/scikit-learn_37.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Measure and record the time taken to train the model on a larger dataset for performance testing.,Test the program with different random states for dataset splitting to ensure consistency in mean squared error calculations.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_california_housing
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

#Load the California Housing dataset
california_housing = fetch_california_housing()
X, y = california_housing.data, california_housing.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Random Forest Regressor
rf_regressor = RandomForestRegressor(n_estimators=100)

#Train the regressor
rf_regressor.fit(X_train, y_train)

#Make predictions
y_pred = rf_regressor.predict(X_test)

#Calculate mean squared error
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse:.2f}')

#Serialize and save the model
joblib.dump(rf_regressor, 'california_housing_model.pkl')
",train
scikit-learn,26,"Create a Python program that uses the 'scikit-learn' API to load the California Housing dataset, split it into training and testing sets, train a K-Means clustering model, make predictions, and save the trained model.",code/scikit-learn/scikit-learn_26.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in clustering results.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_california_housing
from sklearn.cluster import KMeans

#Load the California Housing dataset
california_housing = fetch_california_housing()
X, y = california_housing.data, california_housing.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the K-Means clustering model
kmeans = KMeans(n_clusters=3)

#Train the model
kmeans.fit(X_train)

#Make predictions
y_pred = kmeans.predict(X_test)

#Serialize and save the model
joblib.dump(kmeans, 'california_housing_kmeans_model.pkl')
",train
scikit-learn,22,"Create a Python program that uses the 'scikit-learn' API to load the Wine dataset, split it into training and testing sets, train a K-Means clustering model, make predictions, and save the trained model.",code/scikit-learn/scikit-learn_22.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in clustering results.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_wine
from sklearn.cluster import KMeans

#Load the Wine dataset
wine = load_wine()
X, y = wine.data, wine.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the K-Means clustering model
kmeans = KMeans(n_clusters=3)

#Train the model
kmeans.fit(X_train)

#Make predictions
y_pred = kmeans.predict(X_test)

#Serialize and save the model
joblib.dump(kmeans, 'wine_kmeans_model.pkl')
",train
scikit-learn,30,"Create a Python program that uses the 'scikit-learn' API to load the Iris dataset, split it into training and testing sets, train a Decision Tree classifier, make predictions, calculate accuracy, and save the trained model.",code/scikit-learn/scikit-learn_30.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in accuracy calculations.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

#Load the iris dataset
iris = load_iris()
X, y = iris.data, iris.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Decision Tree classifier
dt_classifier = DecisionTreeClassifier()

#Train the classifier
dt_classifier.fit(X_train, y_train)

#Make predictions
y_pred = dt_classifier.predict(X_test)

#Calculate accuracy
accuracy = (y_pred == y_test).mean()
print(f'Accuracy: {accuracy:.2f}')

#Serialize and save the model
joblib.dump(dt_classifier, 'iris_classifier.pkl')
",train
scikit-learn,8,"Create a Python program that uses the 'scikit-learn' API to load the Digits dataset, split it into training and testing sets, train a Random Forest classifier, make predictions, calculate accuracy, and save the trained model.",code/scikit-learn/scikit-learn_8.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in accuracy calculations.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_digits
from sklearn.ensemble import RandomForestClassifier

#Load the Digits dataset
digits = load_digits()
X, y = digits.data, digits.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100)

#Train the classifier
rf_classifier.fit(X_train, y_train)

#Make predictions
y_pred = rf_classifier.predict(X_test)

#Calculate accuracy
accuracy = (y_pred == y_test).mean()
print(f'Accuracy: {accuracy:.2f}')

#Serialize and save the model
joblib.dump(rf_classifier, 'digits_classifier.pkl')
",train
scikit-learn,24,"Create a Python program that uses the 'scikit-learn' API to load the Digits dataset, split it into training and testing sets, train a K-Means clustering model, make predictions, and save the trained model.",code/scikit-learn/scikit-learn_24.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in clustering results.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_digits
from sklearn.cluster import KMeans

#Load the Digits dataset
digits = load_digits()
X, y = digits.data, digits.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the K-Means clustering model
kmeans = KMeans(n_clusters=10)

#Train the model
kmeans.fit(X_train)

#Make predictions
y_pred = kmeans.predict(X_test)

#Serialize and save the model
joblib.dump(kmeans, 'digits_kmeans_model.pkl')
",train
scikit-learn,40,"Create a Python program that uses the 'scikit-learn' API to load the California Housing dataset, split it into training and testing sets, train a Support Vector Machine regressor, make predictions, calculate the mean squared error, and save the trained model.",code/scikit-learn/scikit-learn_40.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Measure and record the time taken to train the model on a larger dataset for performance testing.,Test the program with different random states for dataset splitting to ensure consistency in mean squared error calculations.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_california_housing
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

#Load the California Housing dataset
california_housing = fetch_california_housing()
X, y = california_housing.data, california_housing.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Support Vector Machine regressor
svm_regressor = SVR()

#Train the regressor
svm_regressor.fit(X_train, y_train)

#Make predictions
y_pred = svm_regressor.predict(X_test)

#Calculate mean squared error
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse:.2f}')

#Serialize and save the model
joblib.dump(svm_regressor, 'california_housing_model.pkl')
",train
scikit-learn,9,"Create a Python program that uses the 'scikit-learn' API to load the LFW (Labeled Faces in the Wild) dataset, split it into training and testing sets, train a Support Vector Machine classifier, make predictions, calculate accuracy, and save the trained model.",code/scikit-learn/scikit-learn_9.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in accuracy calculations.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_lfw_people
from sklearn.svm import SVC

#Load the LFW dataset
lfw = fetch_lfw_people(min_faces_per_person=70, resize=0.4)
X, y = lfw.data, lfw.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Support Vector Machine classifier
svm_classifier = SVC()

#Train the classifier
svm_classifier.fit(X_train, y_train)

#Make predictions
y_pred = svm_classifier.predict(X_test)

#Calculate accuracy
accuracy = (y_pred == y_test).mean()
print(f'Accuracy: {accuracy:.2f}')

#Serialize and save the model
joblib.dump(svm_classifier, 'lfw_classifier.pkl')
",train
scikit-learn,13,"Create a Python program that uses the 'scikit-learn' API to load the Diabetes dataset, split it into training and testing sets, train a Random Forest Regressor, make predictions, calculate the mean squared error, and save the trained model.",code/scikit-learn/scikit-learn_13.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Measure and record the time taken to train the model on a larger dataset for performance testing.,Test the program with different random states for dataset splitting to ensure consistency in mean squared error calculations.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_diabetes
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

#Load the Diabetes dataset
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Random Forest Regressor
rf_regressor = RandomForestRegressor(n_estimators=100)

#Train the regressor
rf_regressor.fit(X_train, y_train)

#Make predictions
y_pred = rf_regressor.predict(X_test)

#Calculate mean squared error
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse:.2f}')

#Serialize and save the model
joblib.dump(rf_regressor, 'diabetes_model.pkl')
",train
scikit-learn,17,"Create a Python program that uses the 'scikit-learn' API to load the Digits dataset, split it into training and testing sets, train a Support Vector Machine classifier, make predictions, calculate accuracy, and save the trained model.",code/scikit-learn/scikit-learn_17.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in accuracy calculations.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_digits
from sklearn.svm import SVC

#Load the Digits dataset
digits = load_digits()
X, y = digits.data, digits.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Support Vector Machine classifier
svm_classifier = SVC()

#Train the classifier
svm_classifier.fit(X_train, y_train)

#Make predictions
y_pred = svm_classifier.predict(X_test)

#Calculate accuracy
accuracy = (y_pred == y_test).mean()
print(f'Accuracy: {accuracy:.2f}')

#Serialize and save the model
joblib.dump(svm_classifier, 'digits_classifier.pkl')
",train
scikit-learn,34,"Create a Python program that uses the 'scikit-learn' API to load the Diabetes dataset, split it into training and testing sets, train a Support Vector Machine classifier, make predictions, calculate accuracy, and save the trained model.",code/scikit-learn/scikit-learn_34.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in accuracy calculations.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_diabetes
from sklearn.svm import SVC

#Load the Diabetes dataset
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Support Vector Machine classifier
svm_classifier = SVC()

#Train the classifier
svm_classifier.fit(X_train, y_train)

#Make predictions
y_pred = svm_classifier.predict(X_test)

#Calculate accuracy
accuracy = (y_pred == y_test).mean()
print(f'Accuracy: {accuracy:.2f}')

#Serialize and save the model
joblib.dump(svm_classifier, 'diabetes_classifier.pkl')
",train
scikit-learn,39,"Create a Python program that uses the 'scikit-learn' API to load the LFW (Labeled Faces in the Wild) dataset, split it into training and testing sets, train a Decision Tree classifier, make predictions, calculate accuracy, and save the trained model.",code/scikit-learn/scikit-learn_39.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in accuracy calculations.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_lfw_people
from sklearn.tree import DecisionTreeClassifier

#Load the LFW dataset
lfw = fetch_lfw_people(min_faces_per_person=70, resize=0.4)
X, y = lfw.data, lfw.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Decision Tree classifier
dt_classifier = DecisionTreeClassifier()

#Train the classifier
dt_classifier.fit(X_train, y_train)

#Make predictions
y_pred = dt_classifier.predict(X_test)

#Calculate accuracy
accuracy = (y_pred == y_test).mean()
print(f'Accuracy: {accuracy:.2f}')

#Serialize and save the model
joblib.dump(dt_classifier, 'lfw_classifier.pkl')
",test
scikit-learn,18,"Create a Python program that uses the 'scikit-learn' API to load the LFW (Labeled Faces in the Wild) dataset, split it into training and testing sets, train a Random Forest classifier, make predictions, calculate accuracy, and save the trained model.",code/scikit-learn/scikit-learn_18.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in accuracy calculations.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_lfw_people
from sklearn.ensemble import RandomForestClassifier

#Load the LFW dataset
lfw = fetch_lfw_people(min_faces_per_person=70, resize=0.4)
X, y = lfw.data, lfw.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100)

#Train the classifier
rf_classifier.fit(X_train, y_train)

#Make predictions
y_pred = rf_classifier.predict(X_test)

#Calculate accuracy
accuracy = (y_pred == y_test).mean()
print(f'Accuracy: {accuracy:.2f}')

#Serialize and save the model
joblib.dump(rf_classifier, 'lfw_classifier.pkl')
",test
scikit-learn,32,"Create a Python program that uses the 'scikit-learn' API to load the Breast Cancer dataset, split it into training and testing sets, train a K-Nearest Neighbors classifier, make predictions, calculate accuracy, and save the trained model.",code/scikit-learn/scikit-learn_32.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in accuracy calculations.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_breast_cancer
from sklearn.neighbors import KNeighborsClassifier

#Load the Breast Cancer dataset
breast_cancer = load_breast_cancer()
X, y = breast_cancer.data, breast_cancer.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the K-Nearest Neighbors classifier
knn_classifier = KNeighborsClassifier(n_neighbors=3)

#Train the classifier
knn_classifier.fit(X_train, y_train)

#Make predictions
y_pred = knn_classifier.predict(X_test)

#Calculate accuracy
accuracy = (y_pred == y_test).mean()
print(f'Accuracy: {accuracy:.2f}')

#Serialize and save the model
joblib.dump(knn_classifier, 'breast_cancer_classifier.pkl')
",test
scikit-learn,21,"Create a Python program that uses the 'scikit-learn' API to load the Breast Cancer dataset, split it into training and testing sets, train a K-Means clustering model, make predictions, and save the trained model.",code/scikit-learn/scikit-learn_21.py,Load the saved model and validate its correctness by making predictions on a test dataset and comparing the results.,Test the program with different random states for dataset splitting to ensure consistency in clustering results.,Measure and record the time taken to train the model on a larger dataset for performance testing.,,,"#!pip install scikit-learn
import joblib
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_breast_cancer
from sklearn.cluster import KMeans

#Load the Breast Cancer dataset
breast_cancer = load_breast_cancer()
X, y = breast_cancer.data, breast_cancer.target

#Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Initialize the K-Means clustering model
kmeans = KMeans(n_clusters=2)

#Train the model
kmeans.fit(X_train)

#Make predictions
y_pred = kmeans.predict(X_test)

#Serialize and save the model
joblib.dump(kmeans, 'breast_cancer_kmeans_model.pkl')
",test
scipy,24,"Create a Python program using the 'scipy' API to perform numerical optimization with inequality constraints. The program should define an objective function to be minimized or maximized, specify the inequality constraints, and use the 'scipy.optimize.minimize' function with the 'constraints' parameter to find the optimal solution.",code/scipy/scipy_24.py,Test the program with different objective functions and inequality constraints to validate its ability to handle various input scenarios.,Ensure that the program correctly defines the objective function to be minimized or maximized and the inequality constraints.,Verify that the program accurately finds the optimal solution with inequality constraints using the scipy.optimize.minimize function and displays the result.,,,"import scipy as sp
from scipy.optimize import minimize

# Define the objective function
def objective(x):
    return x[0]**2 + x[1]**2

# Define the inequality constraints
def constraint(x):
    return x[0] + x[1] - 1

# Specify the optimization method
method = 'SLSQP'

# Specify the initial guess for the variables
x0 = [0, 0]

# Specify the constraints
constraints = [{'type': 'ineq', 'fun': constraint}]

# Find the optimal solution with inequality constraints
result = minimize(objective, x0, method=method, constraints=constraints)

# Display the optimal solution
print(""Optimal solution:"", result.x)",train
scipy,4,"Create a Python program using the 'scipy' API to solve a system of linear equations. The program should define the coefficients and constants of the equations, solve the system using the 'scipy.linalg.solve' function, and display the solution.",code/scipy/scipy_4.py,Verify that the program accurately solves the system of linear equations using the scipy.linalg.solve function and displays the solution.,Test the program with different systems of linear equations to validate its ability to handle various input data.,Ensure that the program correctly defines the coefficients and constants of the equations.,,,"import scipy as sp
import scipy.linalg

# Define the coefficients and constants of the equations
A = sp.array([[2, 3], [4, 5]])
b = sp.array([6, 7])

# Solve the system of linear equations
x = sp.linalg.solve(A, b)

# Display the solution
print(""Solution:"", x)",train
scipy,13,"Create a Python program using the 'scipy' API to perform numerical optimization with constraints. The program should define an objective function to be minimized or maximized, specify the constraints, and use the 'scipy.optimize.minimize' function with the 'constraints' parameter to find the optimal solution.",code/scipy/scipy_13.py,Test the program with different objective functions and constraints to validate its ability to handle various input scenarios.,Ensure that the program correctly defines the objective function to be minimized or maximized and the constraints.,Verify that the program accurately finds the optimal solution with constraints using the scipy.optimize.minimize function and displays the result.,,,"import scipy as sp
from scipy.optimize import minimize

# Define the objective function
def objective(x):
    return x[0]**2 + x[1]**2

# Define the constraints
def constraint(x):
    return x[0] + x[1] - 1

# Specify the optimization method
method = 'SLSQP'

# Specify the initial guess for the variables
x0 = [0, 0]

# Specify the constraints
constraints = [{'type': 'eq', 'fun': constraint}]

# Find the optimal solution with constraints
result = minimize(objective, x0, method=method, constraints=constraints)

# Display the optimal solution
print(""Optimal solution:"", result.x)",train
scipy,9,"Create a Python program using the 'scipy' API to solve a system of nonlinear equations. The program should define the equations and initial guesses for the variables, solve the system using the 'scipy.optimize.root' function, and display the solution.",code/scipy/scipy_9.py,Ensure that the program correctly defines the equations and initial guesses for the variables.,Verify that the program accurately solves the system of nonlinear equations using the scipy.optimize.root function and displays the solution.,Test the program with different systems of nonlinear equations and initial guesses to validate its ability to handle various input scenarios.,,,"import scipy as sp
from scipy.optimize import root

# Define the equations
def equations(x):
    eq1 = x[0]**2 + x[1]**2 - 1
    eq2 = x[0] - x[1]**2
    return [eq1, eq2]

# Specify the initial guesses for the variables
x0 = [0, 0]

# Solve the system of nonlinear equations
result = root(equations, x0)

# Display the solution
print(""Solution:"", result.x)",train
scipy,16,"Create a Python program using the 'scipy' API to perform numerical optimization with bounds. The program should define an objective function to be minimized or maximized, specify the bounds for the variables, and use the 'scipy.optimize.minimize' function with the 'bounds' parameter to find the optimal solution.",code/scipy/scipy_16.py,Verify that the program accurately finds the optimal solution with bounds using the scipy.optimize.minimize function and displays the result.,Test the program with different objective functions and bounds to validate its ability to handle various input scenarios.,Ensure that the program correctly defines the objective function to be minimized or maximized and the bounds for the variables.,,,"import scipy as sp
from scipy.optimize import minimize

# Define the objective function
def objective(x):
    return x[0]**2 + x[1]**2

# Specify the bounds for the variables
bounds = [(0, 1), (0, 1)]

# Specify the optimization method
method = 'L-BFGS-B'

# Specify the initial guess for the variables
x0 = [0, 0]

# Find the optimal solution with bounds
result = minimize(objective, x0, method=method, bounds=bounds)

# Display the optimal solution
print(""Optimal solution:"", result.x)",train
scipy,6,"Create a Python program using the 'scipy' API to perform interpolation on a given dataset. The program should read the dataset from a CSV file, preprocess the data if necessary, apply an interpolation method (e.g., linear interpolation) to the data, and visualize the interpolated curve using a line plot.",code/scipy/scipy_6.py,Test the program with different datasets to validate its ability to handle various input data.,"Verify that the program accurately applies an interpolation method (e.g., linear interpolation) to the data and visualizes the interpolated curve using a line plot.",Ensure that the program correctly reads the dataset from the CSV file and preprocesses the data if necessary.,,,"#!pip install scipy pandas matplotlib
import scipy as sp
import pandas as pd
import matplotlib.pyplot as plt

# Read the dataset from a CSV file
data = pd.read_csv(""dataset.csv"")

# Preprocess the data if necessary

# Apply an interpolation method to the data
x = data.iloc[:, 0]
y = data.iloc[:, 1]
interp_func = sp.interpolate.interp1d(x, y, kind='linear')

# Generate points for the interpolated curve
x_interp = sp.linspace(x.min(), x.max(), 100)
y_interp = interp_func(x_interp)

# Visualize the interpolated curve using a line plot
plt.plot(x_interp, y_interp)
plt.scatter(x, y, color='red')
plt.xlabel(""X"")
plt.ylabel(""Y"")
plt.title(""Interpolation"")
plt.show()",train
scipy,30,"Create a Python program using the 'scipy' API to perform numerical optimization with linear equality constraints. The program should define an objective function to be minimized or maximized, specify the linear equality constraints, and use the 'scipy.optimize.linprog' function to find the optimal solution.",code/scipy/scipy_30.py,Test the program with different objective functions and linear equality constraints to validate its ability to handle various input scenarios.,Verify that the program accurately finds the optimal solution with linear equality constraints using the scipy.optimize.linprog function and displays the result.,Ensure that the program correctly defines the objective function to be minimized or maximized and the linear equality constraints.,,,"import scipy as sp
from scipy.optimize import linprog

# Define the objective function
c = [1, 1]

# Define the linear equality constraints
A_eq = [[1, 1]]
b_eq = [1]

# Find the optimal solution with linear equality constraints
result = linprog(c, A_eq=A_eq, b_eq=b_eq)

# Display the optimal solution
print(""Optimal solution:"", result.x)",train
scipy,26,"Create a Python program using the 'scipy' API to perform numerical optimization with nonlinear equality constraints. The program should define an objective function to be minimized or maximized, specify the nonlinear equality constraints, and use the 'scipy.optimize.minimize' function with the 'constraints' parameter to find the optimal solution.",code/scipy/scipy_26.py,Test the program with different objective functions and nonlinear equality constraints to validate its ability to handle various input scenarios.,Verify that the program accurately finds the optimal solution with nonlinear equality constraints using the scipy.optimize.minimize function and displays the result.,Ensure that the program correctly defines the objective function to be minimized or maximized and the nonlinear equality constraints.,,,"import scipy as sp
from scipy.optimize import minimize

# Define the objective function
def objective(x):
    return x[0]**2 + x[1]**2

# Define the nonlinear equality constraints
def constraint(x):
    return x[0]**2 + x[1]**2 - 1

# Specify the optimization method
method = 'SLSQP'

# Specify the initial guess for the variables
x0 = [0, 0]

# Specify the constraints
constraints = [{'type': 'eq', 'fun': constraint}]

# Find the optimal solution with nonlinear equality constraints
result = minimize(objective, x0, method=method, constraints=constraints)

# Display the optimal solution
print(""Optimal solution:"", result.x)",train
scipy,20,"Create a Python program using the 'scipy' API to perform numerical optimization with nonlinear constraints. The program should define an objective function to be minimized or maximized, specify the nonlinear constraints, and use the 'scipy.optimize.minimize' function with the 'constraints' parameter to find the optimal solution.",code/scipy/scipy_20.py,Test the program with different objective functions and nonlinear constraints to validate its ability to handle various input scenarios.,Verify that the program accurately finds the optimal solution with nonlinear constraints using the scipy.optimize.minimize function and displays the result.,Ensure that the program correctly defines the objective function to be minimized or maximized and the nonlinear constraints.,,,"import scipy as sp
from scipy.optimize import minimize

# Define the objective function
def objective(x):
    return x[0]**2 + x[1]**2

# Define the nonlinear constraints
def constraint(x):
    return x[0]**2 + x[1]**2 - 1

# Specify the optimization method
method = 'SLSQP'

# Specify the initial guess for the variables
x0 = [0, 0]

# Specify the constraints
constraints = [{'type': 'eq', 'fun': constraint}]

# Find the optimal solution with nonlinear constraints
result = minimize(objective, x0, method=method, constraints=constraints)

# Display the optimal solution
print(""Optimal solution:"", result.x)",train
scipy,5,"Create a Python program using the 'scipy' API to perform numerical integration. The program should define a function to be integrated, specify the integration limits, and use the 'scipy.integrate.quad' function to compute the definite integral.",code/scipy/scipy_5.py,Test the program with different functions and integration limits to validate its ability to handle various input scenarios.,Verify that the program accurately computes the definite integral using the scipy.integrate.quad function and displays the result.,Ensure that the program correctly defines the function to be integrated.,,,"import scipy as sp
from scipy.integrate import quad

# Define the function to be integrated
def f(x):
    return x**2

# Specify the integration limits
a = 0
b = 1

# Compute the definite integral
result, error = quad(f, a, b)

# Display the result
print(""Definite integral:"", result)",train
scipy,14,"Create a Python program using the 'scipy' API to perform numerical interpolation on a given dataset. The program should read the dataset from a CSV file, preprocess the data if necessary, apply an interpolation method (e.g., cubic spline) to the data, and visualize the interpolated curve using a line plot.",code/scipy/scipy_14.py,Test the program with different datasets to validate its ability to handle various input data.,Ensure that the program correctly reads the dataset from the CSV file and preprocesses the data if necessary.,"Verify that the program accurately applies an interpolation method (e.g., cubic spline) to the data and visualizes the interpolated curve using a line plot.",,,"#!pip install scipy pandas matplotlib
import scipy as sp
import pandas as pd
import matplotlib.pyplot as plt

# Read the dataset from a CSV file
data = pd.read_csv(""dataset.csv"")

# Preprocess the data if necessary

# Apply an interpolation method to the data
x = data.iloc[:, 0]
y = data.iloc[:, 1]
interp_func = sp.interpolate.interp1d(x, y, kind='cubic')

# Generate points for the interpolated curve
x_interp = sp.linspace(x.min(), x.max(), 100)
y_interp = interp_func(x_interp)

# Visualize the interpolated curve using a line plot
plt.plot(x_interp, y_interp)
plt.scatter(x, y, color='red')
plt.xlabel(""X"")
plt.ylabel(""Y"")
plt.title(""Interpolation"")
plt.show()",train
scipy,32,"Create a Python program using the 'scipy' API to perform numerical optimization with linear inequality constraints. The program should define an objective function to be minimized or maximized, specify the linear inequality constraints, and use the 'scipy.optimize.linprog' function to find the optimal solution.",code/scipy/scipy_32.py,Ensure that the program correctly defines the objective function to be minimized or maximized and the linear inequality constraints.,Test the program with different objective functions and linear inequality constraints to validate its ability to handle various input scenarios.,Verify that the program accurately finds the optimal solution with linear inequality constraints using the scipy.optimize.linprog function and displays the result.,,,"import scipy as sp
from scipy.optimize import linprog

# Define the objective function
c = [1, 1]

# Define the linear inequality constraints
A_ub = [[-1, 1], [1, 2]]
b_ub = [1, 2]

# Find the optimal solution with linear inequality constraints
result = linprog(c, A_ub=A_ub, b_ub=b_ub)

# Display the optimal solution
print(""Optimal solution:"", result.x)",train
scipy,7,"Create a Python program using the 'scipy' API to perform numerical optimization. The program should define an objective function to be minimized or maximized, specify the optimization method, and use the 'scipy.optimize.minimize' function to find the optimal solution.",code/scipy/scipy_7.py,Test the program with different objective functions and optimization methods to validate its ability to handle various input scenarios.,Ensure that the program correctly defines the objective function to be minimized or maximized.,Verify that the program accurately finds the optimal solution using the scipy.optimize.minimize function and displays the result.,,,"import scipy as sp
from scipy.optimize import minimize

# Define the objective function
def objective(x):
    return x**2 + 3*x + 2

# Specify the optimization method
method = 'BFGS'

# Find the optimal solution
result = minimize(objective, x0=0, method=method)

# Display the optimal solution
print(""Optimal solution:"", result.x)",train
scipy,18,"Create a Python program using the 'scipy' API to perform numerical optimization with linear constraints. The program should define an objective function to be minimized or maximized, specify the linear constraints, and use the 'scipy.optimize.linprog' function to find the optimal solution.",code/scipy/scipy_18.py,Verify that the program accurately finds the optimal solution with linear constraints using the scipy.optimize.linprog function and displays the result.,Ensure that the program correctly defines the objective function to be minimized or maximized and the linear constraints.,Test the program with different objective functions and linear constraints to validate its ability to handle various input scenarios.,,,"import scipy as sp
from scipy.optimize import linprog

# Define the objective function
c = [1, 1]

# Define the linear constraints
A = [[-1, 1], [1, 2]]
b = [1, 2]

# Find the optimal solution with linear constraints
result = linprog(c, A_ub=A, b_ub=b)

# Display the optimal solution
print(""Optimal solution:"", result.x)",train
scipy,12,"Create a Python program using the 'scipy' API to perform numerical integration of a system of ordinary differential equations (ODEs). The program should define the ODEs, specify the initial conditions and integration method, and use the 'scipy.integrate.solve_ivp' function to solve the ODEs and plot the solution.",code/scipy/scipy_12.py,Test the program with different ODEs and integration methods to validate its ability to handle various input scenarios.,Ensure that the program correctly defines the ODEs and initial conditions.,Verify that the program accurately solves the ODEs using the scipy.integrate.solve_ivp function and plots the solution.,,,"import scipy as sp
from scipy.integrate import solve_ivp
import matplotlib.pyplot as plt

# Define the ODEs
def odes(t, y):
    dydt = [y[1], -y[0]]
    return dydt

# Specify the initial conditions
t0 = 0
y0 = [1, 0]

# Solve the ODEs
solution = solve_ivp(odes, [t0, 10], y0)

# Plot the solution
plt.plot(solution.t, solution.y[0], label='y1')
plt.plot(solution.t, solution.y[1], label='y2')
plt.xlabel('t')
plt.ylabel('y')
plt.legend()
plt.title('Solution of ODEs')
plt.show()",train
scipy,15,"Create a Python program using the 'scipy' API to perform numerical integration of a system of partial differential equations (PDEs). The program should define the PDEs, specify the initial conditions and integration method, and use the 'scipy.integrate.solve_ivp' function to solve the PDEs and visualize the solution.",code/scipy/scipy_15.py,Test the program with different PDEs and integration methods to validate its ability to handle various input scenarios.,Verify that the program accurately solves the PDEs using the scipy.integrate.solve_ivp function and visualizes the solution.,Ensure that the program correctly defines the PDEs and initial conditions.,,,"import scipy as sp
from scipy.integrate import solve_ivp
import matplotlib.pyplot as plt

# Define the PDEs
def pdes(t, u):
    du_dx = u[1]
    du_dt = u[0]
    return [du_dx, du_dt]

# Specify the initial conditions
t0 = 0
u0 = [1, 0]

# Solve the PDEs
solution = solve_ivp(pdes, [t0, 10], u0)

# Visualize the solution
plt.plot(solution.t, solution.y[0], label='u1')
plt.plot(solution.t, solution.y[1], label='u2')
plt.xlabel('t')
plt.ylabel('u')
plt.legend()
plt.title('Solution of PDEs')
plt.show()",train
scipy,22,"Create a Python program using the 'scipy' API to perform numerical optimization with equality constraints. The program should define an objective function to be minimized or maximized, specify the equality constraints, and use the 'scipy.optimize.minimize' function with the 'constraints' parameter to find the optimal solution.",code/scipy/scipy_22.py,Verify that the program accurately finds the optimal solution with equality constraints using the scipy.optimize.minimize function and displays the result.,Ensure that the program correctly defines the objective function to be minimized or maximized and the equality constraints.,Test the program with different objective functions and equality constraints to validate its ability to handle various input scenarios.,,,"import scipy as sp
from scipy.optimize import minimize

# Define the objective function
def objective(x):
    return x[0]**2 + x[1]**2

# Define the equality constraints
def constraint(x):
    return x[0] + x[1] - 1

# Specify the optimization method
method = 'SLSQP'

# Specify the initial guess for the variables
x0 = [0, 0]

# Specify the constraints
constraints = [{'type': 'eq', 'fun': constraint}]

# Find the optimal solution with equality constraints
result = minimize(objective, x0, method=method, constraints=constraints)

# Display the optimal solution
print(""Optimal solution:"", result.x)",train
scipy,10,"Create a Python program using the 'scipy' API to perform statistical analysis on a given dataset. The program should read the dataset from a CSV file, calculate descriptive statistics (e.g., mean, median, standard deviation) for each column, and display the results.",code/scipy/scipy_10.py,Test the program with different datasets to validate its ability to handle various input data.,"Verify that the program accurately calculates the mean, median, and standard deviation for each column and displays the results.",Ensure that the program correctly reads the dataset from the CSV file and calculates the descriptive statistics for each column.,,,"#!pip install scipy pandas
import scipy as sp
import pandas as pd

# Read the dataset from a CSV file
data = pd.read_csv(""dataset.csv"")

# Calculate descriptive statistics for each column
statistics = {
    ""mean"": data.mean(),
    ""median"": data.median(),
    ""std"": data.std()
}

# Display the results
for column, stats in statistics.items():
    print(""Column:"", column)
    print(""Mean:"", stats[""mean""])
    print(""Median:"", stats[""median""])
    print(""Standard Deviation:"", stats[""std""])
    print()",train
scipy,1,Create a Python program using the 'scipy' API to apply various image transformations to a sample image. The program should perform operations like shifting and rotating the image and display the original and transformed images.,code/scipy/scipy_1.py,Ensure that the program correctly applies the specified image transformations (shifting and rotating) to the sample image and displays the images side by side.,Verify that the program can display the original and transformed images using matplotlib.,Test the program with different shift and rotation parameters to validate its flexibility in handling various transformation scenarios.,,,"#!pip install scipy matplotlib
import scipy as sp
import matplotlib.pyplot as plt

face = sp.datasets.face(gray=True)

shifted_face = sp.ndimage.shift(face, (50, 50))
rotated_face = sp.ndimage.rotate(face, 30)

plt.figure(figsize=(15, 3))
plt.subplot(151)
plt.imshow(face, cmap=plt.cm.gray)
plt.axis(""off"")

plt.subplot(152)
plt.imshow(shifted_face, cmap=plt.cm.gray)
plt.axis(""off"")

plt.subplot(153)
plt.imshow(rotated_face, cmap=plt.cm.gray)
plt.axis(""off"")

plt.subplots_adjust(wspace=0.05, left=0.01, bottom=0.01, right=0.99, top=0.99)

plt.show()",test
scipy,28,"Create a Python program using the 'scipy' API to perform numerical optimization with nonlinear inequality constraints. The program should define an objective function to be minimized or maximized, specify the nonlinear inequality constraints, and use the 'scipy.optimize.minimize' function with the 'constraints' parameter to find the optimal solution.",code/scipy/scipy_28.py,Test the program with different objective functions and nonlinear inequality constraints to validate its ability to handle various input scenarios.,Verify that the program accurately finds the optimal solution with nonlinear inequality constraints using the scipy.optimize.minimize function and displays the result.,Ensure that the program correctly defines the objective function to be minimized or maximized and the nonlinear inequality constraints.,,,"import scipy as sp
from scipy.optimize import minimize

# Define the objective function
def objective(x):
    return x[0]**2 + x[1]**2

# Define the nonlinear inequality constraints
def constraint(x):
    return x[0]**2 + x[1]**2 - 1

# Specify the optimization method
method = 'SLSQP'

# Specify the initial guess for the variables
x0 = [0, 0]

# Specify the constraints
constraints = [{'type': 'ineq', 'fun': constraint}]

# Find the optimal solution with nonlinear inequality constraints
result = minimize(objective, x0, method=method, constraints=constraints)

# Display the optimal solution
print(""Optimal solution:"", result.x)",test
spacy,1,Create a Python program using the 'spacy' API to analyze text and label its sentiment based on the presence of positive and negative emoji. The program should identify and label positive and negative sentiments in a text.,code/spacy/spacy_1.py,Verify the program's behavior with text that does not contain any emoji to assess its robustness in handling different scenarios.,Test the program with various input texts containing positive and negative emoji to ensure it correctly identifies and labels the sentiment.,Test the program's performance with long texts to ensure it remains responsive and efficient.,,,"#!pip install spacy
from spacy.lang.en import English
from spacy.matcher import Matcher

nlp = English()  
matcher = Matcher(nlp.vocab)

pos_emoji = ["""", """", """", """", """", """"]  # Positive emoji
neg_emoji = ["""", """", """", """", """", """"]  # Negative emoji

# Add patterns to match one or more emoji tokens
pos_patterns = [[{""ORTH"": emoji}] for emoji in pos_emoji]
neg_patterns = [[{""ORTH"": emoji}] for emoji in neg_emoji]

# Function to label the sentiment
def label_sentiment(matcher, doc, i, matches):
    match_id, start, end = matches[i]
    if doc.vocab.strings[match_id] == ""HAPPY"": 
        doc.sentiment += 0.1  
    elif doc.vocab.strings[match_id] == ""SAD"":
        doc.sentiment -= 0.1  

matcher.add(""HAPPY"", pos_patterns, on_match=label_sentiment) 
matcher.add(""SAD"", neg_patterns, on_match=label_sentiment)  

# Add pattern for valid hashtag, i.e. '#' plus any ASCII token
matcher.add(""HASHTAG"", [[{""ORTH"": ""#""}, {""IS_ASCII"": True}]])

doc = nlp(""Hello world  #MondayMotivation"")
matches = matcher(doc)
for match_id, start, end in matches:
    string_id = doc.vocab.strings[match_id]  
    span = doc[start:end]
    print(string_id, span.text)",train
spacy,9,Create a Python program using the 'spacy' API to perform lemmatization on a given text. The program should convert each word in the text to its base or dictionary form.,code/spacy/spacy_9.py,Test the program's performance with long texts to ensure it remains responsive and efficient.,Verify the program's behavior with text that does not contain any words to assess its robustness in handling different scenarios.,Test the program with different texts to ensure it correctly lemmatizes each word.,,,"#!pip install spacy
import spacy

nlp = spacy.load(""en_core_web_sm"")

def lemmatize_text(text):
    doc = nlp(text)
    lemmas = []
    for token in doc:
        lemmas.append(token.lemma_)
    return lemmas

text = ""I am running in the park.""
lemmas = lemmatize_text(text)
for lemma in lemmas:
    print(lemma)",train
spacy,6,Create a Python program using the 'spacy' API to perform text classification on a given text. The program should classify the text into predefined categories based on its content.,code/spacy/spacy_6.py,Test the program's performance with long texts to ensure it remains responsive and efficient.,Verify the program's behavior with text that does not belong to any predefined category to assess its robustness in handling different scenarios.,Test the program with different texts to ensure it correctly classifies them into the predefined categories.,,,"#!pip install spacy
import spacy

nlp = spacy.load(""en_core_web_sm"")

def text_classification(text):
    doc = nlp(text)
    category = ""Unknown""
    # Perform classification based on the content of the text
    # Add your classification logic here
    return category

text = ""This is a sample text.""
category = text_classification(text)
print(category)",train
spacy,3,Create a Python program using the 'spacy' API to perform part-of-speech tagging on a given text. The program should identify and label the parts of speech for each word in the text.,code/spacy/spacy_3.py,Verify the program's behavior with text that does not contain any words to assess its robustness in handling different scenarios.,Test the program with different texts containing various parts of speech to ensure it correctly identifies and labels them.,Test the program's performance with long texts to ensure it remains responsive and efficient.,,,"#!pip install spacy
import spacy

nlp = spacy.load(""en_core_web_sm"")

def pos_tagging(text):
    doc = nlp(text)
    pos_tags = []
    for token in doc:
        pos_tags.append((token.text, token.pos_))
    return pos_tags

text = ""I love eating pizza.""
pos_tags = pos_tagging(text)
for tag in pos_tags:
    print(tag)",train
spacy,2,"Create a Python program using the 'spacy' API to extract named entities from a given text. The program should identify and label different types of named entities such as persons, organizations, and locations.",code/spacy/spacy_2.py,Verify the program's behavior with text that does not contain any named entities to assess its robustness in handling different scenarios.,Test the program's performance with long texts to ensure it remains responsive and efficient.,Test the program with different texts containing various types of named entities to ensure it correctly identifies and labels them.,,,"#!pip install spacy
import spacy

nlp = spacy.load(""en_core_web_sm"")

def extract_named_entities(text):
    doc = nlp(text)
    entities = []
    for ent in doc.ents:
        entities.append((ent.text, ent.label_))
    return entities

text = ""Apple Inc. is planning to open a new store in New York City. Tim Cook, the CEO of Apple, announced the news yesterday.""
entities = extract_named_entities(text)
for entity in entities:
    print(entity)",train
spacy,34,"Create a Python program using the 'spacy' API to analyze text and extract entities. The program should identify and label different types of entities such as persons, organizations, and locations.",code/spacy/spacy_34.py,Test the program with different texts containing various types of entities to ensure it correctly identifies and labels them.,Verify the program's behavior with text that does not contain any entities to assess its robustness in handling different scenarios.,Test the program's performance with long texts to ensure it remains responsive and efficient.,,,"#!pip install spacy
import spacy

nlp = spacy.load(""en_core_web_sm"")

def extract_entities(text):
    doc = nlp(text)
    entities = []
    for ent in doc.ents:
        entities.append((ent.text, ent.label_))
    return entities

text = ""Apple Inc. is a technology company headquartered in Cupertino, California. Tim Cook is the CEO of Apple.""
entities = extract_entities(text)
for entity in entities:
    print(entity)",train
spacy,5,"Create a Python program using the 'spacy' API to perform named entity recognition on a given text. The program should identify and label different types of named entities such as persons, organizations, and locations.",code/spacy/spacy_5.py,Verify the program's behavior with text that does not contain any named entities to assess its robustness in handling different scenarios.,Test the program's performance with long texts to ensure it remains responsive and efficient.,Test the program with different texts containing various types of named entities to ensure it correctly identifies and labels them.,,,"#!pip install spacy
import spacy

nlp = spacy.load(""en_core_web_sm"")

def named_entity_recognition(text):
    doc = nlp(text)
    entities = []
    for ent in doc.ents:
        entities.append((ent.text, ent.label_))
    return entities

text = ""Apple Inc. is a technology company headquartered in Cupertino, California. Tim Cook is the CEO of Apple.""
entities = named_entity_recognition(text)
for entity in entities:
    print(entity)",train
spacy,8,Create a Python program using the 'spacy' API to perform sentence segmentation on a given text. The program should split the text into individual sentences.,code/spacy/spacy_8.py,Verify the program's behavior with text that does not contain any sentences to assess its robustness in handling different scenarios.,Test the program with different texts to ensure it correctly segments them into individual sentences.,Test the program's performance with long texts to ensure it remains responsive and efficient.,,,"#!pip install spacy
import spacy

nlp = spacy.load(""en_core_web_sm"")

def segment_sentences(text):
    doc = nlp(text)
    sentences = []
    for sent in doc.sents:
        sentences.append(sent.text)
    return sentences

text = ""This is the first sentence. This is the second sentence. And this is the third sentence.""
sentences = segment_sentences(text)
for sentence in sentences:
    print(sentence)",train
spacy,32,Create a Python program using the 'spacy' API to analyze text and extract noun phrases. The program should identify and extract noun phrases from a given text.,code/spacy/spacy_32.py,Test the program with different texts to ensure it correctly extracts noun phrases.,Verify the program's behavior with text that does not contain any noun phrases to assess its robustness in handling different scenarios.,Test the program's performance with long texts to ensure it remains responsive and efficient.,,,"#!pip install spacy
import spacy

nlp = spacy.load(""en_core_web_sm"")

def extract_noun_phrases(text):
    doc = nlp(text)
    noun_phrases = []
    for chunk in doc.noun_chunks:
        noun_phrases.append(chunk.text)
    return noun_phrases

text = ""I love eating pizza.""
noun_phrases = extract_noun_phrases(text)
for phrase in noun_phrases:
    print(phrase)",train
spacy,4,Create a Python program using the 'spacy' API to perform dependency parsing on a given text. The program should identify the syntactic relationships between words in the text and label them accordingly.,code/spacy/spacy_4.py,Verify the program's behavior with text that does not contain any words to assess its robustness in handling different scenarios.,Test the program with different texts containing various syntactic relationships to ensure it correctly identifies and labels them.,Test the program's performance with long texts to ensure it remains responsive and efficient.,,,"#!pip install spacy
import spacy

nlp = spacy.load(""en_core_web_sm"")

def dependency_parsing(text):
    doc = nlp(text)
    dependencies = []
    for token in doc:
        dependencies.append((token.text, token.dep_))
    return dependencies

text = ""The cat sat on the mat.""
dependencies = dependency_parsing(text)
for dep in dependencies:
    print(dep)",train
spacy,7,"Create a Python program using the 'spacy' API to perform tokenization on a given text. The program should split the text into individual tokens, including words, punctuation marks, and special characters.",code/spacy/spacy_7.py,Test the program with different texts to ensure it correctly tokenizes them into individual tokens.,Verify the program's behavior with text that does not contain any words or punctuation marks to assess its robustness in handling different scenarios.,Test the program's performance with long texts to ensure it remains responsive and efficient.,,,"#!pip install spacy
import spacy

nlp = spacy.load(""en_core_web_sm"")

def tokenize_text(text):
    doc = nlp(text)
    tokens = []
    for token in doc:
        tokens.append(token.text)
    return tokens

text = ""Hello, world! This is a sample text.""
tokens = tokenize_text(text)
for token in tokens:
    print(token)",train
spacy,33,"Create a Python program using the 'spacy' API to analyze text and extract named entities. The program should identify and label different types of named entities such as persons, organizations, and locations.",code/spacy/spacy_33.py,Verify the program's behavior with text that does not contain any named entities to assess its robustness in handling different scenarios.,Test the program's performance with long texts to ensure it remains responsive and efficient.,Test the program with different texts containing various types of named entities to ensure it correctly identifies and labels them.,,,"#!pip install spacy
import spacy

nlp = spacy.load(""en_core_web_sm"")

def extract_named_entities(text):
    doc = nlp(text)
    entities = []
    for ent in doc.ents:
        entities.append((ent.text, ent.label_))
    return entities

text = ""Apple Inc. is a technology company headquartered in Cupertino, California. Tim Cook is the CEO of Apple.""
entities = extract_named_entities(text)
for entity in entities:
    print(entity)",test
spacy,11,"Create a Python program using the 'spacy' API to perform sentiment analysis on a given text. The program should analyze the sentiment of the text and classify it as positive, negative, or neutral.",code/spacy/spacy_11.py,Test the program with different texts to ensure it correctly analyzes and classifies the sentiment.,Verify the program's behavior with text that does not convey any sentiment to assess its robustness in handling different scenarios.,Test the program's performance with long texts to ensure it remains responsive and efficient.,,,"#!pip install spacy
import spacy

nlp = spacy.load(""en_core_web_sm"")

def sentiment_analysis(text):
    doc = nlp(text)
    sentiment = ""Neutral""
    # Perform sentiment analysis based on the content of the text
    # Add your sentiment analysis logic here
    return sentiment

text = ""I love this movie!""
sentiment = sentiment_analysis(text)
print(sentiment)",test
stumpy,11,"Create a Python program using the 'stumpy' API to find the top-k discord motifs in a given time series. The program should allow the user to specify the value of k, and it should return the indices and corresponding distances of the top-k discord motifs.",code/stumpy/stumpy_11.py,Verify that the program correctly identifies the indices and distances of the top-k discord motifs.,Test the program with different time series data to verify that it correctly finds the top-k discord motifs.,Assess the program's performance with larger time series data and different values of k to ensure it remains efficient.,,,"#!pip install stumpy
import stumpy
import numpy as np

# Generate a random time series
your_time_series = np.random.rand(1000)

# Specify the value of k (number of discord motifs to find)
k = 5

# Find the top-k discord motifs
motif_indices, motif_distances = stumpy.top_k_motifs(your_time_series, k)

# Print the indices and distances of the top-k discord motifs
print(f""Top {k} Discord Motifs Indices: {motif_indices}"")
print(f""Top {k} Discord Motifs Distances: {motif_distances}"")",train
stumpy,27,"Create a Python program using the 'st umpy' API to find the top-k discord motifs in a given time series. The program should allow the user to specify the value of k, and it should return the indices and corresponding distances of the top-k discord motifs.",code/stumpy/stumpy_27.py,Verify that the program correctly identifies the indices and distances of the top-k discord motifs.,Test the program with different time series data to verify that it correctly finds the top-k discord motifs.,Assess the program's performance with larger time series data and different values of k to ensure it remains efficient.,,,"#!pip install stumpy
import stumpy
import numpy as np

# Generate a random time series
your_time_series = np.random.rand(1000)

# Specify the value of k (number of discord motifs to find)
k = 5

# Find the top-k discord motifs
motif_indices, motif_distances = stumpy.top_k_motifs(your_time_series, k)

# Print the indices and distances of the top-k discord motifs
print(f""Top {k} Discord Motifs Indices: {motif_indices}"")
print(f""Top {k} Discord Motifs Distances: {motif_distances}"")",train
stumpy,3,Create a Python program using the 'stumpy' API to perform motif discovery in a given time series. The program should allow the user to specify the desired motif length and find the motifs within the time series. It should return the indices of all identified motifs.,code/stumpy/stumpy_3.py,Test the program with different time series data and motif lengths to verify that it correctly identifies motifs.,Check the program with a long time series to ensure it remains efficient.,Verify that the program correctly identifies and returns the indices of motifs within the time series.,,,"#!pip install stumpy
import stumpy
import numpy as np

# Generate a random time series
your_time_series = np.random.rand(1000)

# Specify the motif length
motif_length = 50

# Perform motif discovery
motif_indices = stumpy.motif(your_time_series, motif_length)

# Print the indices of identified motifs
print(f""Motif Indices: {motif_indices}"")",train
stumpy,2,"Develop a Python program using the 'stumpy' API to find the top-k discord motifs in a given time series. The program should allow the user to specify the value of k, and it should return the indices and corresponding distances of the top-k discord motifs.",code/stumpy/stumpy_2.py,Check the program's performance with larger time series data and different values of k to ensure it remains efficient.,Test the program with different time series data to verify that it correctly finds the top-k discord motifs.,Verify that the program correctly identifies the indices and distances of the top-k discord motifs.,,,"#!pip install stumpy
import stumpy
import numpy as np

# Generate a random time series
your_time_series = np.random.rand(1000)

# Specify the value of k (number of discord motifs to find)
k = 5

# Find the top-k discord motifs
motif_indices, motif_distances = stumpy.top_k_motifs(your_time_series, k)

# Print the indices and distances of the top-k discord motifs
print(f""Top {k} Discord Motifs Indices: {motif_indices}"")
print(f""Top {k} Discord Motifs Distances: {motif_distances}"")",train
stumpy,10,Develop a Python program using the 'stumpy' API to perform semantic segmentation of a given time series. The program should allow the user to input a set of change points and segment the time series into distinct regions based on those change points.,code/stumpy/stumpy_10.py,Test the program with different time series data and change point configurations to verify that it correctly segments the time series.,Verify that the program correctly segments the time series into distinct regions based on the provided change points.,Check the program with a long time series and multiple change points to ensure it remains efficient.,,,"#!pip install stumpy
import stumpy
import numpy as np

# Generate a random time series
your_time_series = np.random.rand(1000)

# Specify a set of change points (indices where the segmentation should occur)
change_points = [100, 300, 700]

# Perform semantic segmentation
segments = stumpy.segment(your_time_series, change_points)

# Print the segments of the time series
for i, segment in enumerate(segments):
    print(f""Segment {i + 1}: {segment}"")",train
stumpy,5,Create a Python program using the 'stumpy' API to perform semantic segmentation of a given time series. The program should allow the user to input a set of change points and segment the time series into distinct regions based on those change points.,code/stumpy/stumpy_5.py,Test the program with different time series data and change point configurations to verify that it correctly segments the time series.,Verify that the program correctly segments the time series into distinct regions based on the provided change points.,Check the program with a long time series and multiple change points to ensure it remains efficient.,,,"#!pip install stumpy
import stumpy
import numpy as np

# Generate a random time series
your_time_series = np.random.rand(1000)

# Specify a set of change points (indices where the segmentation should occur)
change_points = [100, 300, 700]

# Perform semantic segmentation
segments = stumpy.segment(your_time_series, change_points)

# Print the segments of the time series
for i, segment in enumerate(segments):
    print(f""Segment {i + 1}: {segment}"")",train
stumpy,9,Create a Python program using the 'stumpy' API to detect anomalies in a given time series. The program should allow the user to specify a z-normalized threshold and identify data points in the time series that exceed this threshold as anomalies.,code/stumpy/stumpy_9.py,Test the program with different time series data and threshold values to verify that it correctly detects anomalies.,Check the program with a long time series to ensure it remains efficient.,Verify that the program correctly returns the indices of detected anomalies based on the specified threshold.,,,"#!pip install stumpy
import stumpy
import numpy as np

# Generate a random time series with anomalies
your_time_series = np.random.rand(1000)
your_time_series[300:350] += 3.0  # Introduce anomalies

# Specify the z-normalized threshold for anomaly detection
threshold = 2.0

# Detect anomalies in the time series
anomalies = stumpy.anom(your_time_series, threshold=threshold)

# Print the indices of detected anomalies
print(f""Anomaly Indices: {anomalies}"")",train
stumpy,4,"Develop a Python program using the 'stumpy' API to perform subsequence search in a given time series. The program should allow the user to input a query subsequence and find all occurrences of the query subsequence in the time series, along with their starting indices.",code/stumpy/stumpy_4.py,Test the program with different time series data and query subsequences to verify that it correctly identifies matches.,Check the program with a long time series and various query subsequences to ensure it remains efficient.,Verify that the program correctly returns the starting indices of matching subsequences.,,,"#!pip install stumpy
import stumpy
import numpy as np

# Generate a random time series
your_time_series = np.random.rand(1000)

# Define a query subsequence (e.g., a pattern you want to search for)
query_subsequence = np.array([0.2, 0.3, 0.4, 0.5])

# Perform subsequence search
matches = stumpy.subsequence(your_time_series, query_subsequence)

# Print the starting indices of matching subsequences
print(f""Starting Indices of Matching Subsequences: {matches}"")",train
stumpy,7,Create a Python program using the 'stumpy' API to find the nearest neighbors of a query subsequence within a given time series. The program should allow the user to input a query subsequence and specify the number of nearest neighbors to find.,code/stumpy/stumpy_7.py,"Test the program with different time series data, query subsequences, and values of k to verify that it correctly finds nearest neighbors.",Check the program with a long time series to ensure it remains efficient.,Verify that the program correctly returns the nearest neighbors and their distances for the query subsequence.,,,"#!pip install stumpy
import stumpy
import numpy as np

# Generate a random time series
your_time_series = np.random.rand(1000)

# Define a query subsequence (e.g., a pattern you want to find nearest neighbors for)
query_subsequence = np.array([0.2, 0.3, 0.4, 0.5])

# Specify the number of nearest neighbors to find
k_neighbors = 3

# Find the k nearest neighbors of the query subsequence
nearest_neighbors = stumpy.nearest_neighbors(your_time_series, query_subsequence, k=k_neighbors)

# Print the nearest neighbors and their distances
print(f""Nearest Neighbors: {nearest_neighbors}"")",train
stumpy,6,Develop a Python program using the 'stumpy' API to perform motif discovery in a given time series. The program should allow the user to specify the desired motif length and find the motifs within the time series. It should return the indices of all identified motifs.,code/stumpy/stumpy_6.py,Test the program with different time series data and motif lengths to verify that it correctly identifies motifs.,Check the program with a long time series to ensure it remains efficient.,Verify that the program correctly identifies and returns the indices of motifs within the time series.,,,"#!pip install stumpy
import stumpy
import numpy as np

# Generate a random time series
your_time_series = np.random.rand(1000)

# Specify the motif length
motif_length = 30

# Perform motif discovery
motif_indices = stumpy.motif(your_time_series, motif_length)

# Print the indices of identified motifs
print(f""Motif Indices: {motif_indices}"")",test
stumpy,1,"Develop a Python program using the 'stumpy' API to generate a matrix profile for a given random time series data. The program should compute the e z-normalized matrix profile, and the resulting profile should be saved to a text file.",code/stumpy/stumpy_1.py,Assess the program's performance with larger time series data to ensure it remains efficient.,Examine the generated output files to confirm that the matrix profile is saved accurately and can be used for further analysis.,Test the program with different random time series data to verify that it correctly computes the matrix profile.,,,"#!pip install stumpy
import stumpy
import numpy as np

# Generate a random time series
your_time_series = np.random.rand(10000)
window_size = 50  

# Compute the e z-normalized matrix profile 
matrix_profile = stumpy.stump(your_time_series, m=window_size)

output_file_path = ""matrix_profile.txt""
np.savetxt(output_file_path, matrix_profile)",test
supervision,6,"Develop a Python program that utilizes the 'supervision' API to perform face recognition on a collection of images. Load a set of reference images, perform face recognition on a target image using a pre-trained face recognition model, and display the recognized faces along with their names.",code/supervision/supervision_6.py,Ensure the program correctly loads the reference images.,Ensure that the program correctly recognizes and displays the names of the faces in the target image along with their confidence scores.,Verify that the target image is correctly loaded and processed for face recognition.,,,"#!pip install supervision opencv-python
import cv2
import supervision as sv

# Load the reference images
reference_images = [
    sv.Image.from_file('reference1.jpg'),
    sv.Image.from_file('reference2.jpg'),
    sv.Image.from_file('reference3.jpg')
]

# Load the target image
target_image = sv.Image.from_file('target.jpg')

# Perform face recognition on the target image
face_recognizer = sv.FaceRecognizer()
face_recognizer.load_reference_images(reference_images)
recognized_faces = face_recognizer.recognize_faces(target_image)

# Display the recognized faces along with their names
for face in recognized_faces:
    name = face['name']
    confidence = face['confidence']
    bounding_box = face['bounding_box']

    # Draw the bounding box around the recognized face
    target_image = sv.draw_box(target_image, bounding_box)

    # Display the name and confidence score
    print(f""Name: {name}"")
    print(f""Confidence: {confidence}"")

# Display the target image with the bounding boxes
target_image.show()",train
supervision,17,"Develop a Python program that utilizes the 'supervision' API to perform sentiment analysis on an input text document. Load a text document, perform sentiment analysis using a pre-trained sentiment analysis model, and display the predicted sentiment along with the confidence score.",code/supervision/supervision_17.py,Ensure the program correctly loads the pre-trained sentiment analysis model.,Ensure that the program correctly predicts the sentiment and confidence score for the input text document.,Verify that the input text document is correctly loaded and processed for sentiment analysis.,,,"#!pip install supervision transformers
import torch
from transformers import pipeline
import supervision as sv

# Load the pre-trained sentiment analysis model
model_name = 'textattack/bert-base-uncased-imdb'
sentiment_analysis = pipeline('sentiment-analysis', model=model_name)

# Load the input text document
text_path = 'document.txt'
with open(text_path, 'r') as file:
    text = file.read()

# Perform sentiment analysis on the document
result = sentiment_analysis(text)[0]

# Get the predicted sentiment and confidence score
sentiment = result['label']
confidence = result['score']

# Display the predicted sentiment and confidence score
print(f""Sentiment: {sentiment}"")
print(f""Confidence: {confidence}"")",train
supervision,16,"Develop a Python program that utilizes the 'supervision' API to perform text classification on an input text document. Load a text document, perform text classification using a pre-trained classification model, and display the predicted class label along with the confidence score.",code/supervision/supervision_16.py,Ensure that the program correctly predicts the class label and confidence score for the input text document.,Ensure the program correctly loads the pre-trained classification model.,Verify that the input text document is correctly loaded and processed for text classification.,,,"#!pip install supervision transformers
import torch
from transformers import pipeline
import supervision as sv

# Load the pre-trained classification model
model_name = 'textattack/bert-base-uncased-imdb'
text_classification = pipeline('text-classification', model=model_name)

# Load the input text document
text_path = 'document.txt'
with open(text_path, 'r') as file:
    text = file.read()

# Perform text classification on the document
result = text_classification(text)[0]

# Get the predicted class label and confidence score
label = result['label']
score = result['score']

# Display the predicted class label and confidence score
print(f""Label: {label}"")
print(f""Score: {score}"")",train
supervision,14,"Develop a Python program that utilizes the 'supervision' API to perform face detection on an input image. Load an image, perform face detection using a pre-trained face detection model, and display the image with bounding boxes around the detected faces.",code/supervision/supervision_14.py,Ensure that the program correctly displays the image with bounding boxes around the detected faces.,Ensure the program correctly loads the pre-trained face detection model.,Verify that the input image is correctly loaded and processed for face detection.,,,"#!pip install supervision opencv-python
import cv2
import supervision as sv

# Load the pre-trained face detection model
model = sv.FaceDetector.from_pretrained('retinaface_resnet50')

# Load the input image
image = sv.Image.from_file('image.jpg')

# Perform face detection on the image
detections = model.detect(image)

# Draw bounding boxes around the detected faces
image = sv.draw_boxes(image, detections)

# Display the image with the bounding boxes
image.show()",train
supervision,9,"Develop a Python program that utilizes the 'supervision' API to perform object detection on a video file. Load a video file, perform object detection using a pre-trained object detection model, and display the video frames with bounding boxes around the detected objects.",code/supervision/supervision_9.py,Ensure that the program correctly performs object detection on each frame of the video and displays the frames with bounding boxes around the detected objects.,Ensure the program correctly loads the video file.,Verify that the pre-trained object detection model is correctly loaded.,,,"#!pip install supervision opencv-python
import cv2
import supervision as sv

# Load the video file
video_path = 'video.mp4'
cap = cv2.VideoCapture(video_path)

# Load the pre-trained object detection model
model = sv.ObjectDetector.from_pretrained('yolov5s')

while True:
    # Read a frame from the video
    ret, frame = cap.read()
    if not ret:
        break

    # Perform object detection on the frame
    detections = model.detect(frame)

    # Draw bounding boxes around the detected objects
    frame = sv.draw_boxes(frame, detections)

    # Display the frame with the bounding boxes
    cv2.imshow('Object Detection', frame)

    # Break the loop if 'q' is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the video capture and close the window
cap.release()
cv2.destroyAllWindows()",train
supervision,11,"Develop a Python program that utilizes the 'supervision' API to perform text classification on a collection of text documents. Load multiple text documents, perform text classification using a pre-trained classification model, and display the predicted class labels along with the confidence scores for each document.",code/supervision/supervision_11.py,Ensure that the program correctly predicts the class labels and confidence scores for each text document.,Ensure the program correctly loads the pre-trained classification model.,Verify that the text documents are correctly loaded and processed for text classification.,,,"#!pip install supervision transformers
import torch
from transformers import pipeline
import supervision as sv

# Load the pre-trained classification model
model_name = 'textattack/bert-base-uncased-imdb'
text_classification = pipeline('text-classification', model=model_name)

# Load the text documents
text_paths = ['document1.txt', 'document2.txt', 'document3.txt']
texts = []
for text_path in text_paths:
    with open(text_path, 'r') as file:
        text = file.read()
        texts.append(text)

# Perform text classification on each document
results = []
for text in texts:
    result = text_classification(text)[0]
    results.append(result)

# Display the predicted class labels and confidence scores for each document
for i, result in enumerate(results):
    label = result['label']
    score = result['score']
    print(f""Document {i+1}:"")
    print(f""Label: {label}"")
    print(f""Score: {score}"")",train
supervision,3,"Develop a Python program that utilizes the 'supervision' API to perform text classification using a pre-trained BERT model. Load a text document, classify it using the BERT model, and display the predicted class label along with the confidence score.",code/supervision/supervision_3.py,Ensure that the program correctly predicts the class label and confidence score for the text document.,Verify that the text document is correctly loaded and tokenized using the BERT tokenizer.,Ensure the program correctly loads the pre-trained BERT model and tokenizer.,,,"#!pip install supervision transformers
import torch
from transformers import BertTokenizer, BertForSequenceClassification
import supervision as sv

# Load the pre-trained BERT model and tokenizer
model_name = 'bert-base-uncased'
model = BertForSequenceClassification.from_pretrained(model_name)
tokenizer = BertTokenizer.from_pretrained(model_name)

# Load the text document
text_path = 'document.txt'
with open(text_path, 'r') as file:
    text = file.read()

# Tokenize the text
tokens = tokenizer.encode_plus(
    text,
    add_special_tokens=True,
    max_length=512,
    padding='max_length',
    truncation=True,
    return_tensors='pt'
)

# Perform the classification
with torch.no_grad():
    outputs = model(**tokens)

# Get the predicted class label and confidence score
predicted_class = torch.argmax(outputs.logits, dim=1).item()
confidence = torch.nn.functional.softmax(outputs.logits, dim=1)[0, predicted_class].item()

# Display the predicted class label and confidence score
print(f""Predicted class: {predicted_class}"")
print(f""Confidence: {confidence}"")",train
supervision,18,Develop a Python program that utilizes the 'supervision' API to perform object tracking on a video stream from a webcam. Display the video stream with bounding boxes around the tracked objects.,code/supervision/supervision_18.py,Verify that the webcam feed is correctly captured and processed for object detection and tracking.,Ensure the program correctly loads the pre-trained object detection model.,Ensure that the program correctly displays the video stream with bounding boxes around the tracked objects.,,,"#!pip install supervision opencv-python
import cv2
import supervision as sv

# Load the pre-trained object detection model
model = sv.ObjectDetector.from_pretrained('yolov5s')

# Initialize the object tracker
tracker = sv.ObjectTracker()

# Open the webcam
cap = cv2.VideoCapture(0)

while True:
    # Read a frame from the webcam
    ret, frame = cap.read()
    if not ret:
        break

    # Perform object detection on the frame
    detections = model.detect(frame)

    # Update the object tracker with the new detections
    tracker.update(detections)

    # Get the tracked objects from the tracker
    tracked_objects = tracker.get_tracked_objects()

    # Draw bounding boxes around the tracked objects
    frame = sv.draw_boxes(frame, tracked_objects)

    # Display the frame with the bounding boxes
    cv2.imshow('Object Tracking', frame)

    # Break the loop if 'q' is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the webcam and close the window
cap.release()
cv2.destroyAllWindows()",train
supervision,5,"Develop a Python program that utilizes the 'supervision' API to perform object tracking on a video file. Load a video file, track a specific object using a pre-trained object detection model, and display the bounding box around the tracked object in each frame.",code/supervision/supervision_5.py,Ensure the program correctly loads the video file.,Ensure that the program correctly tracks and displays the bounding box around the specified object in each frame of the video.,Verify that the pre-trained object detection model is correctly loaded.,,,"#!pip install supervision opencv-python
import cv2
import supervision as sv

# Load the video file
video_path = 'video.mp4'
cap = cv2.VideoCapture(video_path)

# Load the pre-trained object detection model
model = sv.ObjectDetector.from_pretrained('yolov5s')

# Initialize the object tracker
tracker = sv.ObjectTracker()

while True:
    # Read a frame from the video
    ret, frame = cap.read()
    if not ret:
        break

    # Perform object detection on the frame
    detections = model.detect(frame)

    # Update the object tracker with the new detections
    tracker.update(detections)

    # Get the tracked objects from the tracker
    tracked_objects = tracker.get_tracked_objects()

    # Draw bounding boxes around the tracked objects
    frame = sv.draw_boxes(frame, tracked_objects)

    # Display the frame with the bounding boxes
    cv2.imshow('Object Tracking', frame)

    # Break the loop if 'q' is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the video capture and close the window
cap.release()
cv2.destroyAllWindows()",train
supervision,8,"Develop a Python program that utilizes the 'supervision' API to perform image segmentation on an input image. Load an image, perform image segmentation using a pre-trained segmentation model, and display the segmented image with different colors assigned to different segments.",code/supervision/supervision_8.py,Ensure that the program correctly displays the segmented image with different colors assigned to different segments.,Verify that the input image is correctly loaded and processed for image segmentation.,Ensure the program correctly loads the pre-trained segmentation model.,,,"#!pip install supervision opencv-python
import cv2
import supervision as sv

# Load the pre-trained segmentation model
model = sv.SegmentationModel.from_pretrained('deeplabv3_resnet50')

# Load the input image
image = sv.Image.from_file('image.jpg')

# Perform image segmentation
segmentation = model.segment(image)

# Display the segmented image
segmentation.show()",train
supervision,12,"Develop a Python program that utilizes the 'supervision' API to perform object detection on an input image. Load an image, perform object detection using a pre-trained object detection model, and display the image with bounding boxes around the detected objects.",code/supervision/supervision_12.py,Verify that the input image is correctly loaded and processed for object detection.,Ensure the program correctly loads the pre-trained object detection model.,Ensure that the program correctly displays the image with bounding boxes around the detected objects.,,,"#!pip install supervision opencv-python
import cv2
import supervision as sv

# Load the pre-trained object detection model
model = sv.ObjectDetector.from_pretrained('yolov5s')

# Load the input image
image = sv.Image.from_file('image.jpg')

# Perform object detection on the image
detections = model.detect(image)

# Draw bounding boxes around the detected objects
image = sv.draw_boxes(image, detections)

# Display the image with the bounding boxes
image.show()",train
supervision,15,"Develop a Python program that utilizes the 'supervision' API to perform image classification on an input image. Load an image, perform image classification using a pre-trained classification model, and display the predicted class label along with the confidence score.",code/supervision/supervision_15.py,Ensure that the program correctly predicts the class label and confidence score for the input image.,Ensure the program correctly loads the pre-trained classification model.,Verify that the input image is correctly loaded and processed for image classification.,,,"#!pip install supervision opencv-python
import cv2
import supervision as sv

# Load the pre-trained classification model
model = sv.ClassificationModel.from_pretrained('resnet50')

# Load the input image
image = sv.Image.from_file('image.jpg')

# Perform image classification
result = model.classify(image)

# Get the predicted class label and confidence score
class_label = result['class_label']
confidence = result['confidence']

# Display the predicted class label and confidence score
print(f""Class Label: {class_label}"")
print(f""Confidence: {confidence}"")",train
supervision,4,"Develop a Python program that utilizes the 'supervision' API to perform sentiment analysis on a collection of text documents. Load multiple text documents, perform sentiment analysis using a pre-trained sentiment analysis model, and display the predicted sentiment for each document along with the confidence score.",code/supervision/supervision_4.py,Ensure the program correctly loads the pre-trained sentiment analysis model.,Ensure that the program correctly predicts the sentiment and confidence score for each text document.,Verify that the text documents are correctly loaded and processed for sentiment analysis.,,,"#!pip install supervision transformers
import torch
from transformers import pipeline
import supervision as sv

# Load the pre-trained sentiment analysis model
model_name = 'textattack/bert-base-uncased-imdb'
sentiment_analysis = pipeline('sentiment-analysis', model=model_name)

# Load the text documents
text_paths = ['document1.txt', 'document2.txt', 'document3.txt']
texts = []
for text_path in text_paths:
    with open(text_path, 'r') as file:
        text = file.read()
        texts.append(text)

# Perform sentiment analysis on each text document
results = []
for text in texts:
    result = sentiment_analysis(text)[0]
    results.append(result)

# Display the predicted sentiment and confidence score for each document
for i, result in enumerate(results):
    sentiment = result['label']
    confidence = result['score']
    print(f""Document {i+1}:"")
    print(f""Sentiment: {sentiment}"")
    print(f""Confidence: {confidence}"")",train
supervision,7,Develop a Python program that utilizes the 'supervision' API to perform object detection on a video stream from a webcam. Display the video stream with bounding boxes around the detected objects and labels indicating the object classes.,code/supervision/supervision_7.py,Ensure the program correctly loads the pre-trained object detection model.,Verify that the webcam feed is correctly captured and processed for object detection.,Ensure that the program correctly displays the video stream with bounding boxes around the detected objects and labels indicating the object classes.,,,"#!pip install supervision opencv-python
import cv2
import supervision as sv

# Load the pre-trained object detection model
model = sv.ObjectDetector.from_pretrained('yolov5s')

# Open the webcam
cap = cv2.VideoCapture(0)

while True:
    # Read a frame from the webcam
    ret, frame = cap.read()
    if not ret:
        break

    # Perform object detection on the frame
    detections = model.detect(frame)

    # Draw bounding boxes and labels on the frame
    frame = sv.draw_boxes(frame, detections)

    # Display the frame with the bounding boxes and labels
    cv2.imshow('Object Detection', frame)

    # Break the loop if 'q' is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the webcam and close the window
cap.release()
cv2.destroyAllWindows()",train
supervision,1,Develop a Python program that utilizes the 'supervision' API for object detection using a YOLO model on a webcam feed. Annotate detected objects with labels and draw a red polygon zone on the video frame. Use the Ultralytics YOLO model for object detection.,code/supervision/supervision_1.py,Verify that detected objects are annotated with labels on the video feed.,Ensure that the red polygon zone is correctly displayed on the video frame.,Ensure the program captures the webcam feed and performs object detection.,,,"#!pip install ultralytics supervision opencv-python
import cv2
from ultralytics import YOLO
import supervision as sv
import numpy as np

ZONE_POLYGON = np.array([
    [0, 0],
    [0.5, 0],
    [0.5, 1],
    [0, 1]
])
webcam_resolution = (1280, 720)

frame_width, frame_height = webcam_resolution

cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, frame_width)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, frame_height)

model = YOLO(""yolov8s.pt"")

box_annotator = sv.BoxAnnotator(
        thickness=2,
        text_thickness=2,
        text_scale=1
    )

zone_polygon = (ZONE_POLYGON * np.array(webcam_resolution)).astype(int)
zone = sv.PolygonZone(polygon=zone_polygon, frame_resolution_wh=tuple(webcam_resolution))
zone_annotator = sv.PolygonZoneAnnotator(
        zone=zone, 
        color=sv.Color.red(),
        thickness=2,
        text_thickness=4,
        text_scale=2
    )

while True:
  _, frame = cap.read()

  result = model(frame, agnostic_nms=True)[0]
  detections = sv.Detections.from_ultralytics(result)
  print(detections)
  labels = [
            f""{model.model.names[class_id]} {confidence:0.2f}""
            for _, _, confidence, class_id, _
            in detections
        ]
  frame = box_annotator.annotate(
            scene=frame, 
            detections=detections, 
            labels=labels
        )

  zone.trigger(detections=detections)
  frame = zone_annotator.annotate(scene=frame)      
        
  cv2.imshow(""yolov8"", frame)

  if (cv2.waitKey(30) == 27):
      break",test
supervision,2,"Develop a Python program that utilizes the 'supervision' API to perform image classification using a pre-trained ResNet model. Load an image from a file, classify it using the ResNet model, and display the predicted class label along with the confidence score.",code/supervision/supervision_2.py,Verify that the image is correctly loaded and transformed using the defined transformation pipeline.,Ensure that the program correctly predicts the class label and confidence score for the image.,Ensure the program correctly loads the pre-trained ResNet model.,,,"#!pip install supervision torchvision
import torch
import torchvision.transforms as transforms
import supervision as sv

# Load the pre-trained ResNet model
model = torch.hub.load('pytorch/vision:v0.9.0', 'resnet18', pretrained=True)
model.eval()

# Define the image transformation pipeline
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Load the image from file
image_path = 'image.jpg'
image = sv.Image.from_file(image_path)

# Apply the transformation pipeline to the image
input_tensor = transform(image)

# Add a batch dimension to the input tensor
input_tensor = input_tensor.unsqueeze(0)

# Perform the classification
with torch.no_grad():
    output = model(input_tensor)

# Get the predicted class label and confidence score
_, predicted_class = torch.max(output, 1)
confidence = torch.nn.functional.softmax(output, dim=1)[0, predicted_class]

# Display the predicted class label and confidence score
print(f""Predicted class: {predicted_class.item()}"")
print(f""Confidence: {confidence.item()}"")",test
tensorflow,10,"Create a Python program that uses the 'tensorflow' API to implement a text generation model using recurrent neural networks (RNNs). The program should load a text corpus, preprocess it, build an RNN model, train it, and generate new text based on a seed phrase.",code/tensorflow/tensorflow_10.py,Test the program with different text corpora and verify that the RNN model can generate coherent and contextually relevant text.,Experiment with different seed phrases and observe how they affect the style and content of the generated text.,"Modify the model architecture, embedding dimension, or sequence length to investigate their impact on text generation quality.",,,"#pip install tensorflow
import tensorflow as tf
import numpy as np
import random

# Sample text corpus
text = ""This is a sample text corpus used for text generation. Text generation is a fun and creative task.""

# Create character-level mapping
chars = sorted(set(text))
char_to_int = {c: i for i, c in enumerate(chars)}
int_to_char = {i: c for i, c in enumerate(chars)}

# Convert text to sequences of integers
max_sequence_length = 50
sequences = []
next_chars = []
for i in range(0, len(text) - max_sequence_length, 1):
    sequences.append([char_to_int[c] for c in text[i:i + max_sequence_length]])
    next_chars.append(char_to_int[text[i + max_sequence_length]])

sequences = np.array(sequences)
next_chars = np.array(next_chars)

# Build a text generation RNN model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=len(chars), output_dim=128, input_length=max_sequence_length),
    tf.keras.layers.LSTM(256, return_sequences=True),
    tf.keras.layers.LSTM(256),
    tf.keras.layers.Dense(len(chars), activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy')

# Preprocess the sequences for training
X = sequences / len(chars)
y = tf.keras.utils.to_categorical(next_chars)

# Train the text generation model
model.fit(X, y, epochs=10)

# Generate new text based on a seed phrase
seed_phrase = ""Text generation is""
generated_text = seed_phrase
num_characters = 100

for _ in range(num_characters):
    input_sequence = [char_to_int[c] for c in seed_phrase]
    input_sequence = np.array(input_sequence) / len(chars)
    predicted_probabilities = model.predict(np.array([input_sequence]))[0]
    predicted_char = int_to_char[np.random.choice(len(chars), p=predicted_probabilities)]
    generated_text += predicted_char
    seed_phrase = seed_phrase[1:] + predicted_char

print(""Generated text:\n"", generated_text)",train
tensorflow,23,"Create a Python program that uses the 'tensorflow' API to implement a sentiment analysis model for text data. The program should preprocess text data, build a model, train it on labeled text data, and evaluate its performance.",code/tensorflow/tensorflow_23.py,Test the program by using different labeled text data for sentiment analysis and verify that the sentiment analysis model can correctly predict sentiment labels.,Evaluate the model's performance by comparing its predictions to ground truth sentiment labels for test data.,Experiment with different model architectures and hyperparameters to observe how they affect sentiment analysis accuracy.,,,"#pip install tensorflow
import tensorflow as tf
import numpy as np

# Sample labeled text data for sentiment analysis (text, sentiment labels)
data = [(""I love this product!"", ""positive""),
        (""Terrible experience, do not buy."", ""negative""),
        (""Great service and fast delivery."", ""positive"")]

# Tokenize the text data
tokenizer = tf.keras.layers.TextVectorization(max_tokens=1000, output_sequence_length=10)
tokenizer.adapt([text for text, _ in data])

sequences = tokenizer([text for text, _ in data])
vocab_size = len(tokenizer.get_vocabulary())

# Build an NLP model for sentiment analysis
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=64, input_length=10),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),
    tf.keras.layers.Dense(len(set(label for _, label in data)), activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Preprocess the sequences and labels for training
X = sequences
y = np.array([label for _, label in data])

# Train the sentiment analysis model
model.fit(X, y, epochs=5)

# Evaluate the model's performance
test_texts = [""Excellent product, highly recommended!"", ""Not satisfied with the service.""]
test_sequences = tokenizer(test_texts)
predictions = model.predict(test_sequences)
predicted_labels = [np.argmax(prediction, axis=-1) for prediction in predictions]
print(""Sentiment predictions:"", predicted_labels)",train
tensorflow,22,"Create a Python program that uses the 'tensorflow' API to implement a natural language processing (NLP) model for named entity recognition (NER). The program should preprocess text data, build a model, train it on labeled text data, and evaluate its performance.",code/tensorflow/tensorflow_22.py,Test the program by using different labeled text data for named entity recognition and verify that the NER model can correctly predict entity labels.,Evaluate the model's performance by comparing its predictions to ground truth entity labels for test data.,Experiment with different model architectures and hyperparameters to observe how they affect NER accuracy.,,,"#pip install tensorflow
import tensorflow as tf
import numpy as np

# Sample labeled text data for NER (text, entity labels)
data = [(""Apple Inc. is located in Cupertino, California."", [""ORG"", ""O"", ""O"", ""O"", ""LOC"", ""LOC""]),
        (""John Smith works at Microsoft in Seattle."", [""PER"", ""PER"", ""O"", ""O"", ""ORG"", ""LOC""])]

# Tokenize the text data
tokenizer = tf.keras.layers.TextVectorization(max_tokens=1000, output_sequence_length=10)
tokenizer.adapt([text for text, _ in data])

sequences = tokenizer([text for text, _ in data])
vocab_size = len(tokenizer.get_vocabulary())

# Build an NLP model for NER
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=64, input_length=10),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),
    tf.keras.layers.Dense(len(set(label for _, labels in data for label in labels)), activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Preprocess the sequences and labels for training
X = sequences
y = np.array([label for _, labels in data for label in labels])

# Train the NER model
model.fit(X, y, epochs=5)

# Evaluate the model's performance
test_texts = [""Tim Cook is the CEO of Apple Inc."", ""Seattle is a city in Washington.""]
test_sequences = tokenizer(test_texts)
predictions = model.predict(test_sequences)
predicted_labels = [np.argmax(prediction, axis=-1) for prediction in predictions]
print(""NER predictions:"", predicted_labels)",train
tensorflow,17,"Create a Python program that uses the 'tensorflow' API to implement a generative adversarial network (GAN) for generating images. The program should define a generator and a discriminator, train the GAN, and generate new images with the trained model.",code/tensorflow/tensorflow_17.py,Load the saved generator model and generate data to ensure it produces convincing results.,Test the program by training the GAN on different datasets and verify that it can generate realistic data samples.,Experiment with different generator and discriminator architectures to observe their impact on the generated data.,,,"#pip install tensorflow
import tensorflow as tf
import numpy as np

# Generate synthetic data
data_size = 1000
data = np.random.rand(data_size, 64)

# Define a generator model
generator = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(64,)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='sigmoid')
])

# Define a discriminator model
discriminator = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(64,)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Combine generator and discriminator into a GAN
discriminator.compile(optimizer='adam', loss='binary_crossentropy')
discriminator.trainable = False
gan = tf.keras.Sequential([generator, discriminator])
gan.compile(optimizer='adam', loss='binary_crossentropy')

# Train the GAN
batch_size = 32
epochs = 100
for _ in range(epochs):
    for _ in range(data_size // batch_size):
        noise = np.random.rand(batch_size, 64)
        generated_data = generator.predict(noise)
        real_data = data[np.random.choice(data_size, batch_size)]
        x = np.concatenate([real_data, generated_data])
        y = np.array([1] * batch_size + [0] * batch_size)
        discriminator.train_on_batch(x, y)
        noise = np.random.rand(batch_size, 64)
        gan.train_on_batch(noise, np.array([1] * batch_size))

# Generate new data with the trained GAN
new_data_size = 10
noise = np.random.rand(new_data_size, 64)
generated_data = generator.predict(noise)

print(""Generated data:\n"", generated_data)",train
tensorflow,13,"Create a Python program that uses the 'tensorflow' API to implement a sequence-to-sequence model for machine translation. The program should load parallel text data in two languages, preprocess the data, build an encoder-decoder model, train it, and generate translations for new sentences.",code/tensorflow/tensorflow_13.py,Test the program by using different parallel text data for machine translation and ensure the model can accurately translate sentences between languages.,Verify the quality of translations by comparing the generated translations to human translations or reference translations.,Experiment with different model architectures and hyperparameters to investigate their impact on translation quality.,,,"#pip install tensorflow
import tensorflow as tf
import numpy as np

# Sample parallel text data (English to French)
english_sentences = [""I love machine learning."", ""TensorFlow is amazing."", ""Translate this sentence.""]
french_sentences = [""J'adore l'apprentissage automatique."", ""TensorFlow est incroyable."", ""Traduisez cette phrase.""]

# Tokenize the text data
english_tokenizer = tf.keras.layers.TextVectorization(max_tokens=1000, output_sequence_length=10)
english_tokenizer.adapt(english_sentences)
french_tokenizer = tf.keras.layers.TextVectorization(max_tokens=1000, output_sequence_length=10)
french_tokenizer.adapt(french_sentences)

english_sequences = english_tokenizer(english_sentences)
french_sequences = french_tokenizer(french_sentences)

# Build a sequence-to-sequence model
encoder = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=len(english_tokenizer.get_vocabulary()), output_dim=64),
    tf.keras.layers.LSTM(128)
])

decoder = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=len(french_tokenizer.get_vocabulary()), output_dim=64),
    tf.keras.layers.LSTM(128, return_sequences=True),
    tf.keras.layers.Dense(len(french_tokenizer.get_vocabulary()), activation='softmax')
])

# Compile the model
model = tf.keras.Sequential([encoder, decoder])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

# Preprocess the sequences for training
X = english_sequences
y = french_sequences

# Train the sequence-to-sequence model
model.fit(X, y, epochs=5)

# Generate translations for new sentences
new_sentences = [""Machine learning is fascinating."", ""Translate more sentences.""]
new_sequences = english_tokenizer(new_sentences)
translated_sequences = model.predict(new_sequences)

# Decode the translated sequences
translated_sentences = french_tokenizer.detokenize(translated_sequences)
print(""Translated sentences:"", translated_sentences)",train
tensorflow,15,"Create a Python program that uses the 'tensorflow' API to implement an anomaly detection model. The program should load a dataset, preprocess it, build an autoencoder model, train it on normal data, and use it to detect anomalies in new data.",code/tensorflow/tensorflow_15.py,Experiment with different anomaly detection thresholds and observe how they affect the detection performance.,Modify the autoencoder model architecture and hyperparameters to see how they influence anomaly detection quality.,Test the program with different datasets containing normal and anomalous data and confirm that the autoencoder model can effectively detect anomalies based on reconstruction error.,,,"#pip install tensorflow
import tensorflow as tf
import numpy as np

# Generate synthetic normal data
data_size = 1000
normal_data = np.random.normal(0, 1, (data_size, 4))

# Create a noisy version of the normal data
noisy_normal_data = normal_data + 0.1 * np.random.normal(0, 1, normal_data.shape)

# Build an autoencoder model for anomaly detection
model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(4,)),
    tf.keras.layers.Dense(2, activation='relu'),
    tf.keras.layers.Dense(4, activation='linear')
])

# Compile the model
model.compile(optimizer='adam', loss='mse')

# Train the autoencoder on normal data
model.fit(noisy_normal_data, normal_data, epochs=10)

# Generate new data, including anomalies
new_data_size = 100
new_data = np.random.normal(0, 1, (new_data_size, 4))

# Create anomalies in the new data
anomalies = np.random.normal(5, 2, (new_data_size // 10, 4))
anomalous_indices = np.random.choice(new_data_size, new_data_size // 10, replace=False)
new_data[anomalous_indices] = anomalies

# Use the autoencoder to detect anomalies
reconstructed_data = model.predict(new_data)
mse = np.mean(np.square(new_data - reconstructed_data), axis=1)

# Detect anomalies based on reconstruction error
anomaly_threshold = 1.0
anomaly_indices = np.where(mse > anomaly_threshold)[0]

print(""Detected anomalies:"", anomaly_indices)",train
tensorflow,7,"Create a Python program that uses the 'tensorflow' API to implement a recommendation system using matrix factorization. The program should load user-item interaction data, build a recommendation model, train it, and make personalized recommendations for users.",code/tensorflow/tensorflow_7.py,Experiment with different embedding dimensions and training epochs to observe their impact on recommendation quality.,Verify that the model's recommendations for users are relevant and effective in suggesting items they might like.,Test the program by using different user-item interaction data and checking if the matrix factorization model can provide accurate recommendations.,,,"#pip install tensorflow
import tensorflow as tf
import numpy as np

# Sample user-item interaction data (user, item, rating)
user_item_data = np.array([
    [1, 1, 5],
    [1, 2, 4],
    [2, 2, 3],
    [2, 3, 2],
    [3, 1, 4],
    [3, 4, 1]
])

# Number of users and items
num_users = max(user_item_data[:, 0])
num_items = max(user_item_data[:, 1])

# Create a matrix factorization model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=num_users, output_dim=8, input_length=1),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Embedding(input_dim=num_items, output_dim=8, input_length=1),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dot(axes=1)
])

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Prepare data for training
X_user = user_item_data[:, 0]
X_item = user_item_data[:, 1]
y = user_item_data[:, 2]

# Train the matrix factorization model
model.fit([X_user, X_item], y, epochs=5)

# Make personalized recommendations for users
user_id = 1
items_to_recommend = [item_id for item_id in range(1, num_items + 1) if item_id not in user_item_data[user_item_data[:, 0] == user_id][:, 1]]
predictions = model.predict([np.array([user_id] * len(items_to_recommend)), np.array(items_to_recommend)])
recommended_items = [items_to_recommend[i] for i in np.argsort(predictions, axis=0).flatten()]
print(""Recommended items for user"", user_id, "":"", recommended_items)",train
tensorflow,41,"Create a Python program that utilizes the 'tensorflow' API to build a recurrent neural network (RNN) for sequence generation. The program should use a text dataset, preprocess it, build an RNN model, train the model, and generate sequences.",code/tensorflow/tensorflow_41.py,"Ensure that the model's architecture, training process, and sequence generation are correct by examining the output and making sure there are no errors.",Verify the program's ability to learn and generate coherent sequences by examining the generated text and checking if it resembles the training text.,Test the program by training the RNN model on the given text data and generate sequences from different starting points to see the diversity of generated text.,,,"#pip install tensorflow
import tensorflow as tf
import numpy as np

# Sample text data
text_data = ""This is a sample text used for training an RNN. It will generate sequences.""

# Create a vocabulary and map characters to integers
chars = sorted(list(set(text_data)))
char_to_int = {char: i for i, char in enumerate(chars)}
int_to_char = {i: char for i, char in enumerate(chars)}
text_data_int = [char_to_int[char] for char in text_data]

# Create sequences for training
sequence_length = 100
sequences = []
next_chars = []
for i in range(0, len(text_data_int) - sequence_length):
    sequences.append(text_data_int[i:i+sequence_length])
    next_chars.append(text_data_int[i+sequence_length])

# Reshape and preprocess the sequences
sequences = np.array(sequences)
sequences = tf.one_hot(sequences, len(chars))
next_chars = np.array(next_chars)

# Build an RNN model for sequence generation
model = tf.keras.Sequential([
    tf.keras.layers.LSTM(128, input_shape=(sequence_length, len(chars))),
    tf.keras.layers.Dense(len(chars), activation='softmax')
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy')

# Train the model on the sequences
model.fit(sequences, next_chars, epochs=10)

# Generate a sequence
start_index = np.random.randint(0, len(text_data_int) - sequence_length)
seed_sequence = text_data_int[start_index:start_index+sequence_length]
generated_text = [int_to_char[char] for char in seed_sequence]

for _ in range(200):
    input_sequence = np.array(seed_sequence)
    input_sequence = tf.one_hot(input_sequence, len(chars))
    input_sequence = np.expand_dims(input_sequence, axis=0)
    predicted_probs = model.predict(input_sequence)[0]
    predicted_char = int_to_char[np.random.choice(len(chars), p=predicted_probs)]
    generated_text.append(predicted_char)
    seed_sequence = seed_sequence[1:] + [char_to_int[predicted_char]]

print(''.join(generated_text))",train
tensorflow,9,"Create a Python program that uses the 'tensorflow' API to implement an object detection model using a pre-trained neural network. The program should load an image, process it, use a pre-trained model to detect objects, and display the results with bounding boxes.",code/tensorflow/tensorflow_9.py,Experiment with different pre-trained models and see how they perform on object detection tasks.,Test the program by using different images with various objects and confirm that the object detection model can accurately identify and annotate them.,Adjust the minimum confidence threshold and make sure the program correctly filters out low-confidence detections.,,,"#pip install tensorflow
import tensorflow as tf
import cv2
import numpy as np

# Load a pre-trained object detection model (e.g., MobileNetSSD)
model = tf.saved_model.load(""ssd_mobilenet_v2_coco/saved_model"")

# Load an image
image = cv2.imread(""image.jpg"")

# Preprocess the image
input_tensor = tf.convert_to_tensor(image)
input_tensor = tf.image.resize(input_tensor, (300, 300))
input_tensor = input_tensor[tf.newaxis, ...]

# Use the model for object detection
detections = model(input_tensor)

# Extract detection results
boxes = detections['detection_boxes'][0].numpy()
classes = detections['detection_classes'][0].numpy().astype(int)
scores = detections['detection_scores'][0].numpy()

# Set a minimum confidence threshold
min_confidence = 0.5
selected_indices = np.where(scores >= min_confidence)[0]
boxes = boxes[selected_indices]
classes = classes[selected_indices]
scores = scores[selected_indices]

# Display the results with bounding boxes
for i in range(len(boxes)):
    box = boxes[i]
    ymin, xmin, ymax, xmax = box
    x, y, w, h = xmin * image.shape[1], ymin * image.shape[0], (xmax - xmin) * image.shape[1], (ymax - ymin) * image.shape[0]
    class_id = classes[i]
    class_name = ""Class "" + str(class_id)
    score = scores[i]

    cv2.rectangle(image, (int(x), int(y)), (int(x + w), int(y + h)), (0, 255, 0), 2)
    cv2.putText(image, class_name + f"" ({score:.2f})"", (int(x), int(y) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

# Save or display the annotated image
cv2.imwrite(""annotated_image.jpg"", image)",train
tensorflow,12,"Create a Python program that uses the 'tensorflow' API to implement a style transfer model. The program should load a content image and a style image, preprocess the data, build a model that combines the style of the style image with the content of the content image, and generate a stylized output image.",code/tensorflow/tensorflow_12.py,Experiment with different style and content weights to observe how they affect the resulting stylized images.,"Modify the style transfer model by using a different base network (e.g., ResNet) and investigate the impact on style transfer quality.",Test the program by using different content and style images for style transfer and verify that the generated stylized output images capture the desired style.,,,"#pip install tensorflow
import tensorflow as tf
import cv2

# Load a content image and a style image
content_image = cv2.imread(""content_image.jpg"")
style_image = cv2.imread(""style_image.jpg"")

# Preprocess the images
content_image = cv2.resize(content_image, (224, 224))
style_image = cv2.resize(style_image, (224, 224))
content_image = tf.keras.applications.vgg19.preprocess_input(content_image)
style_image = tf.keras.applications.vgg19.preprocess_input(style_image)

# Build a style transfer model
model = tf.keras.applications.VGG19(include_top=False, weights='imagenet')

content_layers = ['block5_conv2']
style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']

num_content_layers = len(content_layers)
num_style_layers = len(style_layers)

model.trainable = False
outputs = [model.get_layer(name).output for name in style_layers + content_layers]
feature_extractor = tf.keras.Model(inputs=model.input, outputs=outputs)

def gram_matrix(tensor):
    result = tf.linalg.einsum('bijc,bijd->bcd', tensor, tensor)
    input_shape = tf.shape(tensor)
    num_locations = tf.cast(input_shape[1] * input_shape[2], tf.float32)
    return result / num_locations

style_weight = 1e-2
content_weight = 1e4

style_target = feature_extractor(style_image)['block4_conv1']
style_target_gram = gram_matrix(style_target)

content_target = feature_extractor(content_image)['block5_conv2']

image = tf.Variable(content_image)

optimizer = tf.optimizers.Adam(learning_rate=2, beta_1=0.99, epsilon=1e-1)
iterations = 1000

for i in range(1, iterations + 1):
    with tf.GradientTape() as tape:
        x = tf.keras.applications.vgg19.preprocess_input(image)
        target_features = feature_extractor(x)
        content_features = target_features['block5_conv2']
        style_features = target_features['block4_conv1']

        style_loss = tf.add_n([tf.reduce_mean((style_features[name] - style_target_gram) ** 2) for name in style_features.keys()])
        content_loss = tf.reduce_mean((content_features - content_target) ** 2)
        loss = style_weight * style_loss + content_weight * content_loss

    grad = tape.gradient(loss, image)
    optimizer.apply_gradients([(grad, image)])

# Generate the stylized output image
output_image = image.numpy()
output_image = tf.keras.applications.vgg19.preprocess_input(output_image)
output_image = tf.clip_by_value(output_image[0], -1, 1)
output_image = (output_image + 1) * 127.5

# Save or display the stylized output image
cv2.imwrite(""stylized_image.jpg"", output_image)",train
tensorflow,16,"Create a Python program that uses the 'tensorflow' API to implement a recurrent neural network (RNN) for natural language generation. The program should preprocess text data, build an RNN model, train it, and generate new text based on a seed phrase.",code/tensorflow/tensorflow_16.py,Test the program with different text datasets and verify that the RNN model can generate coherent and contextually relevant text.,Experiment with different seed phrases and observe how they affect the style and content of the generated text.,"Modify the model architecture, embedding dimension, or sequence length to investigate their impact on text generation quality.",,,"#pip install tensorflow
import tensorflow as tf
import numpy as np

# Sample text data
text = ""This is a sample text used for text generation. Text generation is a creative task.""

# Tokenize the text data
tokenizer = tf.keras.layers.TextVectorization(max_tokens=1000, output_sequence_length=10)
tokenizer.adapt([text])

sequences = tokenizer([text])
vocab_size = len(tokenizer.get_vocabulary())

# Build a text generation RNN model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=64, input_length=10),
    tf.keras.layers.LSTM(128, return_sequences=True),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(vocab_size, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

# Preprocess the sequences for training
X = sequences[:, :-1]
y = sequences[:, 1:]

# Train the text generation model
model.fit(X, y, epochs=10)

# Generate new text based on a seed phrase
seed_phrase = ""Text generation is""
generated_text = seed_phrase
num_tokens = 50

for _ in range(num_tokens):
    input_sequence = tokenizer([seed_phrase]).numpy()
    predicted_probabilities = model.predict(input_sequence)[0][-1]
    predicted_token = np.random.choice(vocab_size, p=predicted_probabilities)
    predicted_word = tokenizer.get_vocabulary()[predicted_token]
    generated_text += "" "" + predicted_word
    seed_phrase = "" "".join(generated_text.split()[-10:])

print(""Generated text:\n"", generated_text)",train
tensorflow,21,"Create a Python program that uses the 'tensorflow' API to implement a face recognition model. The program should load a pre-trained neural network for face recognition, process images, and identify faces in the images.",code/tensorflow/tensorflow_21.py,"Experiment with different pre-trained models (e.g., VGGFace) and observe their performance on face recognition tasks.",Test the program with different images containing faces and confirm that the face recognition model can correctly identify and produce embeddings for faces.,"To fully validate the program, you would compare the generated face embeddings with a database of known faces and identify individuals.",,,"#pip install tensorflow
import tensorflow as tf
import cv2

# Load a pre-trained face recognition model (e.g., FaceNet)
model = tf.saved_model.load(""facenet/saved_model"")

# Load an image with faces
image = cv2.imread(""image.jpg"")

# Preprocess the image
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
image = cv2.resize(image, (160, 160))
image = tf.keras.applications.facenet.preprocess_input(image)

# Use the model for face recognition
embeddings = model(image)

# Identify faces in the image
# You would typically compare the embeddings with a database of known faces to identify individuals.",train
tensorflow,8,"Create a Python program that uses the 'tensorflow' API to implement a time series forecasting model using recurrent neural networks (RNNs). The program should load time series data, preprocess it, build an RNN model, train it, and make predictions for future time steps.",code/tensorflow/tensorflow_8.py,Experiment with different sequence lengths and RNN architectures to observe their impact on time series forecasting performance.,Test the program by using different time series data and verifying the RNN can accurately forecast future time steps.,Compare the model's predictions to the actual values for future time steps and check the accuracy of the forecasts.,,,"#pip install tensorflow
import tensorflow as tf
import numpy as np

# Generate synthetic time series data
time_steps = 100
time_series = np.sin(np.arange(0, 10 * np.pi, 0.1)) + np.random.normal(0, 0.1, time_steps)

# Preprocess the data into sequences
sequence_length = 10
sequences = []
next_values = []
for i in range(time_steps - sequence_length):
    sequences.append(time_series[i:i + sequence_length])
    next_values.append(time_series[i + sequence_length])

sequences = np.array(sequences)
next_values = np.array(next_values)

# Build a time series forecasting RNN model
model = tf.keras.Sequential([
    tf.keras.layers.LSTM(64

, input_shape=(sequence_length, 1)),
    tf.keras.layers.Dense(1)
])

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the RNN on the time series data
model.fit(sequences, next_values, epochs=10)

# Make predictions for future time steps
future_steps = 10
future_predictions = []

for i in range(time_steps - sequence_length, time_steps - sequence_length + future_steps):
    input_sequence = time_series[i:i + sequence_length]
    prediction = model.predict(np.array([input_sequence]))
    future_predictions.append(prediction[0][0])

print(""Future time series predictions:"", future_predictions)",train
tensorflow,11,"Create a Python program that uses the 'tensorflow' API to implement an image segmentation model for semantic segmentation. The program should load an image and its corresponding segmentation mask, preprocess the data, build a segmentation model, and generate pixel-wise segmentations for the image.",code/tensorflow/tensorflow_11.py,"Experiment with different model architectures, image sizes, or loss functions to study their impact on segmentation quality.",Test the program by using different image and mask pairs for semantic segmentation and confirm that the model can accurately segment objects in the images.,Compare the generated segmentations to the ground truth masks to evaluate the accuracy of the segmentations.,,,"#pip install tensorflow
import tensorflow as tf
import cv2
import numpy as np

# Load an image and its corresponding segmentation mask
image = cv2.imread(""image.jpg"")
mask = cv2.imread(""segmentation_mask.png"", cv2.IMREAD_GRAYSCALE)

# Preprocess the image and mask
image = cv2.resize(image, (224, 224))
mask = cv2.resize(mask, (224, 224))
image = image / 255.0
mask = mask / 255.0

# Build an image segmentation model
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(224 * 224, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy')

# Train the segmentation model
model.fit(image[np.newaxis, ...], mask[np.newaxis, ...], epochs=5)

# Generate pixel-wise segmentations for the image
segmentation = model.predict(image[np.newaxis, ...])[0]
segmentation = (segmentation > 0.5).astype(np.uint8)

# Save or display the segmentation
cv2.imwrite(""segmentation_result.png"", segmentation * 255)",train
tensorflow,1,"Create a Python program that uses the 'tensorflow' API to build a simple feedforward neural network for binary classification. The program should generate synthetic data, compile the model, train it on the data, and evaluate its performance.",code/tensorflow/tensorflow_1.py,Test the program's evaluation step by calculating the loss and accuracy on a separate set of synthetic data.,Test the program by training the model on different synthetic datasets and verifying that it achieves reasonable accuracy.,"Verify that the model's predictions fall within the range [0, 1] by checking if 'np.all(predictions >= 0) and np.all(predictions <= 1)' is True.",,,"#pip install tensorflow
import tensorflow as tf
import numpy as np

# Generate synthetic data for binary classification
data_size = 1000
data_x = np.random.rand(data_size, 2)
data_y = (data_x[:, 0] + data_x[:, 1] > 1).astype(int)

# Build a simple feedforward neural network
model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(2,)),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model on the synthetic data
print(""Training............"")
model.fit(data_x, data_y, epochs=10)


print(""\nEvaluation............"")
loss, accuracy = model.evaluate(data_x, data_y)
print(""Accuracy: "",accuracy)
print(""Loss: "",loss)

predictions = model.predict(data_x)
print(""Are all predictions between 0 and 1?"",np.all(predictions >= 0) and np.all(predictions <= 1))",test
tensorflow,3,"Create a Python program that uses the 'tensorflow' API to implement a recurrent neural network (RNN) for sequence prediction. The program should use a sequence dataset, preprocess it, build an RNN model, train it, and generate predictions on new sequences.",code/tensorflow/tensorflow_3.py,Change the architecture of the RNN or the dataset and make sure that the program still works correctly.,Test the program by generating predictions on various new sequences and confirming the accuracy of the predictions.,Test the program by creating different synthetic sequence datasets and verifying that the RNN can predict the sum of sequences accurately.,,,"#pip install tensorflow
import tensorflow as tf
import numpy as np

# Generate a synthetic sequence dataset
seq_length = 100
sequences = [np.random.randint(0, 10, size=seq_length) for _ in range(1000)]
targets = [np.sum(seq) for seq in sequences]

# Preprocess the data
sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post')
targets = np.array(targets)

# Build a simple recurrent neural network
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=10, output_dim=16, input_length=seq_length),
    tf.keras.layers.LSTM(64, activation='relu', return_sequences=False),
    tf.keras.layers.Dense(1)
])

# Compile the model
model.compile(optimizer='adam',
              loss='mean_squared_error')

# Train the model on the synthetic sequence data
model.fit(sequences, targets, epochs=5)

# Generate predictions on new sequences
new_sequence = np.array([np.random.randint(0, 10, size=seq_length)])
prediction = model.predict(new_sequence)
print(""Predicted sum of the new sequence:"", prediction[0][0])",test
validators,1,"Create a Python program using the 'validators' API to validate user-provided email addresses, checking if they adhere to the standard email format.",code/validators/validators_1.py,"Verify the program's behavior when invalid email addresses are provided, ensuring it properly identifies them as invalid.",Test the program with valid email addresses to ensure it correctly identifies them as valid.,Test the program with various valid and invalid email formats to assess its robustness and accuracy in validation.,,,"#!pip install validators
import validators

def validate_email(email):
    if validators.email(email):
        return ""Valid email address.""
    else:
        return ""Invalid email address.""

# Get user inputs
user_email = input(""Enter an email address: "")

# Validate and display results
print(validate_email(user_email))
",train
validators,16,"Create a Python program using the 'validators' API to validate user-provided URLs, checking if they adhere to the standard URL format. Additionally, the program should also check if the URL is reachable by making a request to it and display the response status code.",code/validators/validators_16.py,"Test the program with valid URLs to ensure it correctly identifies them as valid and reachable, and displays the correct response status code.",Test the program with various valid and invalid URL formats to assess its robustness and accuracy in validation.,"Verify the program's behavior when invalid URLs are provided, ensuring it properly identifies them as invalid.",,,"#!pip install validators
import validators
import requests

def validate_url(url):
    if validators.url(url):
        try:
            response = requests.get(url)
            if response.status_code == 200:
                return ""Valid URL and reachable. Response status code: "" + str(response.status_code)
            else:
                return ""Valid URL but unreachable. Response status code: "" + str(response.status_code)
        except requests.exceptions.RequestException:
            return ""Valid URL but unreachable.""
    else:
        return ""Invalid URL.""

# Get user inputs
user_url = input(""Enter a URL: "")

# Validate and display results
print(validate_url(user_url))
",train
validators,8,"Create a Python program using the 'validators' API to validate user-provided UUIDs, checking if they adhere to the standard UUID format.",code/validators/validators_8.py,Test the program with valid UUIDs to ensure it correctly identifies them as valid.,"Verify the program's behavior when invalid UUIDs are provided, ensuring it properly identifies them as invalid.",Test the program with various valid and invalid UUID formats to assess its robustness and accuracy in validation.,,,"#!pip install validators
import validators

def validate_uuid(uuid):
    if validators.uuid(uuid):
        return ""Valid UUID.""
    else:
        return ""Invalid UUID.""

# Get user inputs
user_uuid = input(""Enter a UUID: "")

# Validate and display results
print(validate_uuid(user_uuid))
",train
validators,3,"Create a Python program using the 'validators' API to validate user-provided IP addresses, checking if they adhere to the standard IP address format.",code/validators/validators_3.py,Test the program with valid IP addresses to ensure it correctly identifies them as valid.,Test the program with various valid and invalid IP address formats to assess its robustness and accuracy in validation.,"Verify the program's behavior when invalid IP addresses are provided, ensuring it properly identifies them as invalid.",,,"#!pip install validators
import validators

def validate_ip(ip):
    if validators.ip_address.ipv4(ip) or validators.ip_address.ipv6(ip):
        return ""Valid IP address.""
    else:
        return ""Invalid IP address.""

# Get user inputs
user_ip = input(""Enter an IP address: "")

# Validate and display results
print(validate_ip(user_ip))
",train
validators,2,"Create a Python program using the 'validators' API to validate user-provided URLs, checking if they adhere to the standard URL format.",code/validators/validators_2.py,Test the program with various valid and invalid URL formats to assess its robustness and accuracy in validation.,"Verify the program's behavior when invalid URLs are provided, ensuring it properly identifies them as invalid.",Test the program with valid URLs to ensure it correctly identifies them as valid.,,,"#!pip install validators
import validators

def validate_url(url):
    if validators.url(url):
        return ""Valid URL.""
    else:
        return ""Invalid URL.""

# Get user inputs
user_url = input(""Enter a URL: "")

# Validate and display results
print(validate_url(user_url))
",train
validators,38,Create a Python program using the 'validators' API to validate whether a given string is a valid UUID (Universally Unique Identifier).,code/validators/validators_38.py,Test the program with various string inputs that are not UUIDs to assess its robustness and accuracy in validation.,Test the program with a valid UUID to ensure it correctly identifies it as a valid UUID.,"Verify the program's behavior when an invalid UUID is provided, ensuring it identifies it as an invalid UUID.",,,"#!pip install validators
import validators

def validate_uuid(uuid_str):
    if validators.uuid(uuid_str):
        return ""Valid UUID.""
    else:
        return ""Invalid UUID.""

# Get user input
user_uuid = input(""Enter a UUID: "")

# Validate and display results
print(validate_uuid(user_uuid))
",train
validators,7,"Create a Python program using the 'validators' API to validate user-provided MAC addresses, checking if they adhere to the standard MAC address format.",code/validators/validators_7.py,Test the program with various valid and invalid MAC address formats to assess its robustness and accuracy in validation.,"Verify the program's behavior when invalid MAC addresses are provided, ensuring it properly identifies them as invalid.",Test the program with valid MAC addresses to ensure it correctly identifies them as valid.,,,"#!pip install validators
import validators

def validate_mac(mac):
    if validators.mac_address(mac):
        return ""Valid MAC address.""
    else:
        return ""Invalid MAC address.""

# Get user inputs
user_mac = input(""Enter a MAC address: "")

# Validate and display results
print(validate_mac(user_mac))
",train
validators,15,"Create a Python program using the 'validators' API to validate user-provided IP addresses, checking if they adhere to the standard IP address format. Additionally, the program should also check if the IP address is reachable by making a ping request to it.",code/validators/validators_15.py,"Verify the program's behavior when invalid IP addresses are provided, ensuring it properly identifies them as invalid.",Test the program with various valid and invalid IP address formats to assess its robustness and accuracy in validation.,Test the program with valid IP addresses to ensure it correctly identifies them as valid and reachable.,,,"#!pip install validators
import validators
import subprocess

def validate_ip(ip):
    if validators.ip_address.ipv4(ip) or validators.ip_address.ipv6(ip):
        try:
            response = subprocess.call(['ping', '-c', '1', ip])
            if response == 0:
                return ""Valid IP address and reachable.""
            else:
                return ""Valid IP address but unreachable.""
        except subprocess.CalledProcessError:
            return ""Valid IP address but unreachable.""
    else:
        return ""Invalid IP address.""

# Get user inputs
user_ip = input(""Enter an IP address: "")

# Validate and display results
print(validate_ip(user_ip))
",train
validators,18,"Create a Python program using the 'validators' API to validate user-provided URLs, checking if they adhere to the standard URL format. Additionally, the program should also check if the URL is reachable by making a request to it and display the response status code. If the URL is not reachable, the program should display an appropriate error message.",code/validators/validators_18.py,"Test the program with valid URLs to ensure it correctly identifies them as valid and reachable, and displays the correct response status code.",Test the program with various valid and invalid URL formats to assess its robustness and accuracy in validation.,"Verify the program's behavior when invalid URLs are provided, ensuring it properly identifies them as invalid.",,,"#!pip install validators
import validators
import requests

def validate_url(url):
    if validators.url(url):
        try:
            response = requests.get(url)
            if response.status_code == 200:
                return ""Valid URL and reachable. Response status code: "" + str(response.status_code)
            else:
                return ""Valid URL but unreachable. Response status code: "" + str(response.status_code)
        except requests.exceptions.RequestException:
            return ""Valid URL but unreachable.""
    else:
        return ""Invalid URL.""

# Get user inputs
user_url = input(""Enter a URL: "")

# Validate and display results
print(validate_url(user_url))
",train
validators,4,"Create a Python program using the 'validators' API to validate user-provided domain names, checking if they adhere to the standard domain name format.",code/validators/validators_4.py,Test the program with various valid and invalid domain name formats to assess its robustness and accuracy in validation.,Test the program with valid domain names to ensure it correctly identifies them as valid.,"Verify the program's behavior when invalid domain names are provided, ensuring it properly identifies them as invalid.",,,"#!pip install validators
import validators

def validate_domain(domain):
    if validators.domain(domain):
        return ""Valid domain name.""
    else:
        return ""Invalid domain name.""

# Get user inputs
user_domain = input(""Enter a domain name: "")

# Validate and display results
print(validate_domain(user_domain))
",train
validators,14,"Create a Python program using the 'validators' API to validate user-provided URLs, checking if they adhere to the standard URL format. Additionally, the program should also check if the URL is reachable by making a request to it.",code/validators/validators_14.py,Test the program with valid URLs to ensure it correctly identifies them as valid and reachable.,Test the program with various valid and invalid URL formats to assess its robustness and accuracy in validation.,"Verify the program's behavior when invalid URLs are provided, ensuring it properly identifies them as invalid.",,,"#!pip install validators
import validators
import requests

def validate_url(url):
    if validators.url(url):
        try:
            response = requests.get(url)
            if response.status_code == 200:
                return ""Valid URL and reachable.""
            else:
                return ""Valid URL but unreachable.""
        except requests.exceptions.RequestException:
            return ""Valid URL but unreachable.""
    else:
        return ""Invalid URL.""

# Get user inputs
user_url = input(""Enter a URL: "")

# Validate and display results
print(validate_url(user_url))
",train
validators,19,"Create a Python program using the 'validators' API to validate user-provided URLs, checking if they adhere to the standard URL format. Additionally, the program should also check if the URL is reachable by making a request to it and display the response status code. If the URL is not reachable, the program should display an appropriate error message. Finally, the program should also check if the URL is secure (HTTPS) and display a message indicating its security status.",code/validators/validators_19.py,Test the program with various valid and invalid URL formats to assess its robustness and accuracy in validation.,"Verify the program's behavior when invalid URLs are provided, ensuring it properly identifies them as invalid.","Test the program with valid URLs to ensure it correctly identifies them as valid, reachable, and secure (HTTPS), and displays the correct response status code.",,,"#!pip install validators
import validators
import requests

def validate_url(url):
    if validators.url(url):
        try:
            response = requests.get(url)
            if response.status_code == 200:
                if validators.url(url, public=True, schemes=['https']):
                    return ""Valid URL, reachable, and secure (HTTPS). Response status code: "" + str(response.status_code)
                else:
                    return ""Valid URL, reachable, but not secure (HTTP). Response status code: "" + str(response.status_code)
            else:
                return ""Valid URL but unreachable. Response status code: "" + str(response.status_code)
        except requests.exceptions.RequestException:
            return ""Valid URL but unreachable.""
    else:
        return ""Invalid URL.""

# Get user inputs
user_url = input(""Enter a URL: "")

# Validate and display results
print(validate_url(user_url))
",test
validators,17,"Create a Python program using the 'validators' API to validate user-provided email addresses, checking if they adhere to the standard email format. Additionally, the program should also check if the email address is deliverable by making a request to the email server.",code/validators/validators_17.py,"Verify the program's behavior when invalid email addresses are provided, ensuring it properly identifies them as invalid.",Test the program with various valid and invalid email address formats to assess its robustness and accuracy in validation.,Test the program with valid email addresses to ensure it correctly identifies them as valid and deliverable.,,,"#!pip install validators
import validators
import smtplib

def validate_email(email):
    if validators.email(email):
        try:
            server = smtplib.SMTP('smtp.gmail.com', 587)
            server.starttls()
            server.login(""your_email_address"", ""your_email_password"")
            response = server.sendmail(""your_email_address"", email, ""Test message"")
            server.quit()
            if response == {}:
                return ""Valid email address and deliverable.""
            else:
                return ""Valid email address but undeliverable.""
        except smtplib.SMTPException:
            return ""Valid email address but undeliverable.""
    else:
        return ""Invalid email address.""

# Get user inputs
user_email = input(""Enter an email address: "")

# Validate and display results
print(validate_email(user_email))
",test
xarray,17,"Create a Python program using the 'xarray' API to perform data concatenation and rechunking on multiple multi-dimensional data arrays. The program should read multiple NetCDF files, select a specific variable from each file, concatenate the selected variables along a specified dimension, and then rechunk the concatenated data to a different chunk size. Finally, the program should save the rechunked data to a new NetCDF file.",code/xarray/xarray_17.py,Validate that the program correctly rechunks the concatenated data to the specified chunk size and saves the rechunked data to a new NetCDF file with the expected name and format.,Ensure that the program successfully reads the NetCDF files and selects the specified variables.,Test the program with different combinations of NetCDF files and verify that it correctly concatenates the selected variables along the specified dimension.,,,"#!pip install xarray
import xarray as xr

# Read the NetCDF files
ds1 = xr.open_dataset('input1.nc')
ds2 = xr.open_dataset('input2.nc')

# Select the variables
var1 = ds1['temperature']
var2 = ds2['precipitation']

# Concatenate the selected variables along the specified dimension
concatenated_data = xr.concat([var1, var2], dim='time')

# Rechunk the concatenated data to a different chunk size
rechunked_data = concatenated_data.chunk({'time': 10, 'lat': 50, 'lon': 50})

# Save the rechunked data to a new NetCDF file
rechunked_data.to_netcdf('output.nc')",train
xarray,3,"Create a Python program using the 'xarray' API to perform statistical analysis on a multi-dimensional data array. The program should read a NetCDF file, select a specific variable, calculate the mean, standard deviation, and maximum value along a specified dimension, and print the results.",code/xarray/xarray_3.py,Validate that the program correctly prints the calculated statistics.,"Test the program with different NetCDF files and verify that it correctly calculates the mean, standard deviation, and maximum value along the specified dimension.",Ensure that the program successfully reads the NetCDF file and selects the specified variable.,,,"#!pip install xarray
import xarray as xr

# Read the NetCDF file
ds = xr.open_dataset('input.nc')

# Select the variable
var = ds['temperature']

# Calculate the mean, standard deviation, and maximum value along the specified dimension
mean = var.mean(dim='time')
std_dev = var.std(dim='time')
max_value = var.max(dim='time')

# Print the results
print(""Mean: "", mean)
print(""Standard Deviation: "", std_dev)
print(""Maximum Value: "", max_value)",train
xarray,10,"Create a Python program using the 'xarray' API to perform data regridding on a multi-dimensional data array. The program should read a NetCDF file, select a specific variable, regrid the data to a different grid resolution, and save the regridded data to a new NetCDF file.",code/xarray/xarray_10.py,Validate that the regridded data is saved to a new NetCDF file with the expected name and format.,Ensure that the program successfully reads the NetCDF file and selects the specified variable.,Test the program with different NetCDF files and verify that it correctly regrids the data to the specified grid resolution.,,,"#!pip install xarray
import xarray as xr

# Read the NetCDF file
ds = xr.open_dataset('input.nc')

# Select the variable
var = ds['temperature']

# Regrid the data to a different grid resolution
var_regridded = var.coarsen(lat=2, lon=2).mean()

# Save the regridded data to a new NetCDF file
var_regridded.to_netcdf('output.nc')",train
xarray,7,"Create a Python program using the 'xarray' API to perform data slicing on a multi-dimensional data array. The program should read a NetCDF file, select a specific variable, slice the data along one or more dimensions, and print the sliced data.",code/xarray/xarray_7.py,Test the program with different NetCDF files and verify that it correctly slices the data along the specified dimensions.,Ensure that the program successfully reads the NetCDF file and selects the specified variable.,Validate that the program correctly prints the sliced data.,,,"#!pip install xarray
import xarray as xr

# Read the NetCDF file
ds = xr.open_dataset('input.nc')

# Select the variable
var = ds['temperature']

# Slice the data along one or more dimensions
sliced_data = var.sel(time=slice('2000-01-01', '2000-12-31'), lat=slice(0, 90))

# Print the sliced data
print(""Sliced Data: "", sliced_data)",train
xarray,13,"Create a Python program using the 'xarray' API to perform data masking and interpolation on a multi-dimensional data array. The program should read a NetCDF file, select a specific variable, apply a mask based on a condition, interpolate the masked data along a specified dimension, and save the interpolated data to a new NetCDF file.",code/xarray/xarray_13.py,Validate that the interpolated data is saved to a new NetCDF file with the expected name and format.,Ensure that the program successfully reads the NetCDF file and selects the specified variable.,Test the program with different NetCDF files and verify that it correctly applies the mask based on the specified condition and interpolates the masked data along the specified dimension.,,,"#!pip install xarray
import xarray as xr

# Read the NetCDF file
ds = xr.open_dataset('input.nc')

# Select the variable
var = ds['temperature']

# Apply a mask based on a condition
mask = var > 25
masked_data = var.where(mask)

# Interpolate the masked data along the specified dimension
interpolated_data = masked_data.interp(time=[1, 2, 3, 4])

# Save the interpolated data to a new NetCDF file
interpolated_data.to_netcdf('output.nc')",train
xarray,5,"Create a Python program using the 'xarray' API to perform data aggregation on a multi-dimensional data array. The program should read a NetCDF file, select a specific variable, aggregate the data along a specified dimension using a specified aggregation function, and print the aggregated data.",code/xarray/xarray_5.py,Validate that the program correctly prints the aggregated data.,Ensure that the program successfully reads the NetCDF file and selects the specified variable.,Test the program with different NetCDF files and verify that it correctly aggregates the data along the specified dimension using the specified aggregation function.,,,"#!pip install xarray
import xarray as xr

# Read the NetCDF file
ds = xr.open_dataset('input.nc')

# Select the variable
var = ds['temperature']

# Aggregate the data along the specified dimension using the specified aggregation function
aggregated_data = var.sum(dim='time')

# Print the aggregated data
print(""Aggregated Data: "", aggregated_data)",train
xarray,20,"Create a Python program using the 'xarray' API to perform data merging, regridding, and resampling on multiple multi-dimensional data arrays. The program should read multiple NetCDF files, select a specific variable from each file, merge the selected variables along a specified dimension, regrid the merged data to a different grid resolution, and then resample the regridded data to a different time frequency. Finally, the program should save the resampled data to a new NetCDF file.",code/xarray/xarray_20.py,Ensure that the program successfully reads the NetCDF files and selects the specified variables.,Test the program with different combinations of NetCDF files and verify that it correctly merges the selected variables along the specified dimension.,"Validate that the program correctly regrids the merged data to the specified grid resolution, resamples the regridded data to the specified time frequency, and saves the resampled data to a new NetCDF file with the expected name and format.",,,"#!pip install xarray
import xarray as xr

# Read the NetCDF files
ds1 = xr.open_dataset('input1.nc')
ds2 = xr.open_dataset('input2.nc')

# Select the variables
var1 = ds1['temperature']
var2 = ds2['precipitation']

# Merge the selected variables along the specified dimension
merged_data = xr.merge([var1, var2], join='outer')

# Regrid the merged data to a different grid resolution
regridded_data = merged_data.coarsen(lat=2, lon=2).mean()

# Resample the regridded data to a different time frequency
resampled_data = regridded_data.resample(time='1M').mean()

# Save the resampled data to a new NetCDF file
resampled_data.to_netcdf('output.nc')",train
xarray,18,"Create a Python program using the 'xarray' API to perform data masking, interpolation, and aggregation on a multi-dimensional data array. The program should read a NetCDF file, select a specific variable, apply a mask based on a condition, interpolate the masked data along a specified dimension, and then calculate the mean, standard deviation, and maximum value of the interpolated data along another specified dimension. Finally, the program should print the calculated statistics.",code/xarray/xarray_18.py,"Validate that the program correctly calculates the mean, standard deviation, and maximum value of the interpolated data along the specified dimension and prints the calculated statistics.",Ensure that the program successfully reads the NetCDF file and selects the specified variable.,Test the program with different NetCDF files and verify that it correctly applies the mask based on the specified condition and interpolates the masked data along the specified dimension.,,,"#!pip install xarray
import xarray as xr

# Read the NetCDF file
ds = xr.open_dataset('input.nc')

# Select the variable
var = ds['temperature']

# Apply a mask based on a condition
mask = var > 25
masked_data = var.where(mask)

# Interpolate the masked data along the specified dimension
interpolated_data = masked_data.interp(time=[1, 2, 3, 4])

# Calculate the mean, standard deviation, and maximum value of the interpolated data along another specified dimension
mean = interpolated_data.mean(dim='time')
std_dev = interpolated_data.std(dim='time')
max_value = interpolated_data.max(dim='time')

# Print the calculated statistics
print(""Mean: "", mean)
print(""Standard Deviation: "", std_dev)
print(""Maximum Value: "", max_value)",train
xarray,15,"Create a Python program using the 'xarray' API to perform data slicing and masking on a multi-dimensional data array. The program should read a NetCDF file, select a specific variable, slice the data along one or more dimensions, apply a mask based on a condition, and print the masked and sliced data.",code/xarray/xarray_15.py,Validate that the program correctly prints the masked and sliced data.,Ensure that the program successfully reads the NetCDF file and selects the specified variable.,Test the program with different NetCDF files and verify that it correctly slices the data along the specified dimensions and applies the mask based on the specified condition.,,,"#!pip install xarray
import xarray as xr

# Read the NetCDF file
ds = xr.open_dataset('input.nc')

# Select the variable
var = ds['temperature']

# Slice the data along one or more dimensions
sliced_data = var.sel(time=slice('2000-01-01', '2000-12-31'), lat=slice(0, 90))

# Apply a mask based on a condition
mask = sliced_data > 25
masked_data = sliced_data.where(mask)

# Print the masked and sliced data
print(""Masked and Sliced Data: "", masked_data)",train
xarray,4,"Create a Python program using the 'xarray' API to perform data interpolation on a multi-dimensional data array. The program should read a NetCDF file, select a specific variable, interpolate the data along a specified dimension, and save the interpolated data to a new NetCDF file.",code/xarray/xarray_4.py,Validate that the interpolated data is saved to a new NetCDF file with the expected name and format.,Test the program with different NetCDF files and verify that it correctly interpolates the data along the specified dimension.,Ensure that the program successfully reads the NetCDF file and selects the specified variable.,,,"#!pip install xarray
import xarray as xr

# Read the NetCDF file
ds = xr.open_dataset('input.nc')

# Select the variable
var = ds['temperature']

# Interpolate the data along the specified dimension
var_interpolated = var.interp(time=[1, 2, 3, 4])

# Save the interpolated data to a new NetCDF file
var_interpolated.to_netcdf('output.nc')",train
xarray,11,"Create a Python program using the 'xarray' API to perform data concatenation on multiple multi-dimensional data arrays. The program should read multiple NetCDF files, select a specific variable from each file, concatenate the selected variables along a specified dimension, and save the concatenated data to a new NetCDF file.",code/xarray/xarray_11.py,Validate that the concatenated data is saved to a new NetCDF file with the expected name and format.,Ensure that the program successfully reads the NetCDF files and selects the specified variables.,Test the program with different combinations of NetCDF files and verify that it correctly concatenates the selected variables along the specified dimension.,,,"#!pip install xarray
import xarray as xr

# Read the NetCDF files
ds1 = xr.open_dataset('input1.nc')
ds2 = xr.open_dataset('input2.nc')

# Select the variables
var1 = ds1['temperature']
var2 = ds2['precipitation']

# Concatenate the selected variables along the specified dimension
concatenated_data = xr.concat([var1, var2], dim='time')

# Save the concatenated data to a new NetCDF file
concatenated_data.to_netcdf('output.nc')",train
xarray,21,"Create a Python program using the 'xarray' API to perform data concatenation, rechunking, and resampling on multiple multi-dimensional data arrays. The program should read multiple NetCDF files, select a specific variable from each file, concatenate the selected variables along a specified dimension, rechunk the concatenated data to a different chunk size, and then resample the rechunked data to a different time frequency. Finally, the program should save the resampled data to a new NetCDF file.",code/xarray/xarray_21.py,Ensure that the program successfully reads the NetCDF files and selects the specified variables.,Test the program with different combinations of NetCDF files and verify that it correctly concatenates the selected variables along the specified dimension.,"Validate that the program correctly rechunks the concatenated data to the specified chunk size, resamples the rechunked data to the specified time frequency, and saves the resampled data to a new NetCDF file with the expected name and format.",,,"#!pip install xarray
import xarray as xr

# Read the NetCDF files
ds1 = xr.open_dataset('input1.nc')
ds2 = xr.open_dataset('input2.nc')

# Select the variables
var1 = ds1['temperature']
var2 = ds2['precipitation']

# Concatenate the selected variables along the specified dimension
concatenated_data = xr.concat([var1, var2], dim='time')

# Rechunk the concatenated data to a different chunk size
rechunked_data = concatenated_data.chunk({'time': 10, 'lat': 50, 'lon': 50})

# Resample the rechunked data to a different time frequency
resampled_data = rechunked_data.resample(time='1M').mean()

# Save the resampled data to a new NetCDF file
resampled_data.to_netcdf('output.nc')",train
xarray,6,"Create a Python program using the 'xarray' API to perform data resampling on a multi-dimensional data array. The program should read a NetCDF file, select a specific variable, resample the data to a different time frequency, and save the resampled data to a new NetCDF file.",code/xarray/xarray_6.py,Test the program with different NetCDF files and verify that it correctly resamples the data to the specified time frequency.,Validate that the resampled data is saved to a new NetCDF file with the expected name and format.,Ensure that the program successfully reads the NetCDF file and selects the specified variable.,,,"#!pip install xarray
import xarray as xr

# Read the NetCDF file
ds = xr.open_dataset('input.nc')

# Select the variable
var = ds['temperature']

# Resample the data to a different time frequency
var_resampled = var.resample(time='1M').mean()

# Save the resampled data to a new NetCDF file
var_resampled.to_netcdf('output.nc')",train
xarray,14,"Create a Python program using the 'xarray' API to perform data aggregation and resampling on a multi-dimensional data array. The program should read a NetCDF file, select a specific variable, aggregate the data along a specified dimension using a specified aggregation function, and then resample the aggregated data to a different time frequency. Finally, the program should save the resampled data to a new NetCDF file.",code/xarray/xarray_14.py,Validate that the program correctly resamples the aggregated data to the specified time frequency and saves the resampled data to a new NetCDF file with the expected name and format.,Ensure that the program successfully reads the NetCDF file and selects the specified variable.,Test the program with different NetCDF files and verify that it correctly aggregates the data along the specified dimension using the specified aggregation function.,,,"#!pip install xarray
import xarray as xr

# Read the NetCDF file
ds = xr.open_dataset('input.nc')

# Select the variable
var = ds['temperature']

# Aggregate the data along the specified dimension using the specified aggregation function
aggregated_data = var.sum(dim='time')

# Resample the aggregated data to a different time frequency
resampled_data = aggregated_data.resample(time='1M').mean()

# Save the resampled data to a new NetCDF file
resampled_data.to_netcdf('output.nc')",train
xarray,9,"Create a Python program using the 'xarray' API to perform data masking on a multi-dimensional data array. The program should read a NetCDF file, select a specific variable, apply a mask based on a condition, and print the masked data.",code/xarray/xarray_9.py,Test the program with different NetCDF files and verify that it correctly applies the mask based on the specified condition.,Validate that the program correctly prints the masked data.,Ensure that the program successfully reads the NetCDF file and selects the specified variable.,,,"#!pip install xarray
import xarray as xr

# Read the NetCDF file
ds = xr.open_dataset('input.nc')

# Select the variable
var = ds['temperature']

# Apply a mask based on a condition
mask = var > 25
masked_data = var.where(mask)

# Print the masked data
print(""Masked Data: "", masked_data)",train
xarray,12,"Create a Python program using the 'xarray' API to perform data rechunking on a multi-dimensional data array. The program should read a NetCDF file, select a specific variable, rechunk the data to a different chunk size, and save the rechunked data to a new NetCDF file.",code/xarray/xarray_12.py,Validate that the rechunked data is saved to a new NetCDF file with the expected name and format.,Test the program with different NetCDF files and verify that it correctly rechunks the data to the specified chunk size.,Ensure that the program successfully reads the NetCDF file and selects the specified variable.,,,"#!pip install xarray
import xarray as xr

# Read the NetCDF file
ds = xr.open_dataset('input.nc')

# Select the variable
var = ds['temperature']

# Rechunk the data to a different chunk size
var_rechunked = var.chunk({'time': 10, 'lat': 50, 'lon': 50})

# Save the rechunked data to a new NetCDF file
var_rechunked.to_netcdf('output.nc')",train
xarray,16,"Create a Python program using the 'xarray' API to perform data merging and regridding on multiple multi-dimensional data arrays. The program should read multiple NetCDF files, select a specific variable from each file, merge the selected variables along a specified dimension, and then regrid the merged data to a different grid resolution. Finally, the program should save the regridded data to a new NetCDF file.",code/xarray/xarray_16.py,Ensure that the program successfully reads the NetCDF files and selects the specified variables.,Test the program with different combinations of NetCDF files and verify that it correctly merges the selected variables along the specified dimension.,Validate that the program correctly regrids the merged data to the specified grid resolution and saves the regridded data to a new NetCDF file with the expected name and format.,,,"#!pip install xarray
import xarray as xr

# Read the NetCDF files
ds1 = xr.open_dataset('input1.nc')
ds2 = xr.open_dataset('input2.nc')

# Select the variables
var1 = ds1['temperature']
var2 = ds2['precipitation']

# Merge the selected variables along the specified dimension
merged_data = xr.merge([var1, var2], join='outer')

# Regrid the merged data to a different grid resolution
regridded_data = merged_data.coarsen(lat=2, lon=2).mean()

# Save the regridded data to a new NetCDF file
regridded_data.to_netcdf('output.nc')",train
xarray,8,"Create a Python program using the 'xarray' API to perform data merging on multiple multi-dimensional data arrays. The program should read multiple NetCDF files, select a specific variable from each file, merge the selected variables along a specified dimension, and save the merged data to a new NetCDF file.",code/xarray/xarray_8.py,Ensure that the program successfully reads the NetCDF files and selects the specified variables.,Test the program with different combinations of NetCDF files and verify that it correctly merges the selected variables along the specified dimension.,Validate that the merged data is saved to a new NetCDF file with the expected name and format.,,,"#!pip install xarray
import xarray as xr

# Read the NetCDF files
ds1 = xr.open_dataset('input1.nc')
ds2 = xr.open_dataset('input2.nc')

# Select the variables
var1 = ds1['temperature']
var2 = ds2['precipitation']

# Merge the selected variables along the specified dimension
merged_data = xr.merge([var1, var2], join='outer')

# Save the merged data to a new NetCDF file
merged_data.to_netcdf('output.nc')",train
xarray,2,"Create a Python program using the 'xarray' API to read and manipulate a NetCDF file. The program should read a NetCDF file, extract a specific variable, perform a mathematical operation on the variable, and save the modified variable back to a new NetCDF file.",code/xarray/xarray_2.py,Validate that the modified variable is saved to a new NetCDF file with the expected name and format.,Test the program with different NetCDF files and verify that it correctly performs the mathematical operation on the variable.,Ensure that the program successfully reads the NetCDF file and extracts the specified variable.,,,"#!pip install xarray
import xarray as xr

# Read the NetCDF file
ds = xr.open_dataset('input.nc')

# Extract the variable
var = ds['temperature']

# Perform a mathematical operation on the variable
var_modified = var * 2

# Save the modified variable to a new NetCDF file
var_modified.to_netcdf('output.nc')",test
xarray,19,"Create a Python program using the 'xarray' API to perform data slicing, masking, and aggregation on a multi-dimensional data array. The program should read a NetCDF file, select a specific variable, slice the data along one or more dimensions, apply a mask based on a condition, and then calculate the mean, standard deviation, and maximum value of the masked and sliced data along another specified dimension. Finally, the program should print the calculated statistics.",code/xarray/xarray_19.py,"Validate that the program correctly calculates the mean, standard deviation, and maximum value of the masked and sliced data along the specified dimension and prints the calculated statistics.",Ensure that the program successfully reads the NetCDF file and selects the specified variable.,Test the program with different NetCDF files and verify that it correctly slices the data along the specified dimensions and applies the mask based on the specified condition.,,,"#!pip install xarray
import xarray as xr

# Read the NetCDF file
ds = xr.open_dataset('input.nc')

# Select the variable
var = ds['temperature']

# Slice the data along one or more dimensions
sliced_data = var.sel(time=slice('2000-01-01', '2000-12-31'), lat=slice(0, 90))

# Apply a mask based on a condition
mask = sliced_data > 25
masked_data = sliced_data.where(mask)

# Calculate the mean, standard deviation, and maximum value of the masked and sliced data along another specified dimension
mean = masked_data.mean(dim='time')
std_dev = masked_data.std(dim='time')
max_value = masked_data.max(dim='time')

# Print the calculated statistics
print(""Mean: "", mean)
print(""Standard Deviation: "", std_dev)
print(""Maximum Value: "", max_value)",test
ydata-profiling,11,"Create a Python program that employs the 'ydata-profiling' API to profile a JSONL file named 'data.jsonl'. The profiling should include a count of records, a summary of the data, and a table of unique values for a column named 'category'. Save the profiling report as an HTML file named 'jsonl_data_profile.html'.",code/ydata-profiling/ydata-profiling_11.py,"Check if the ""jsonl_data_profile.html"" file is created after running the program.","Verify that the ""data.jsonl"" file is successfully loaded as a JSONL dataset into a Pandas DataFrame.","Ensure that the profiling report includes a count of records, a data summary, and a table of unique values for the ""category"" column.",,,"#! pip install ydata-profiling
import pandas as pd
from ydata_profiling import ProfileReport

# Load the JSONL data
data = pd.read_json('data.jsonl', lines=True)

# Perform data profiling
profiling_report = ProfileReport(data)

# Save the report as an HTML file
profiling_report.to_file('jsonl_data_profile.html')",train
ydata-profiling,12,"Create a Python program that uses the 'ydata-profiling' API to profile an SQLite database named 'mydb.db'. The profiling should include a list of tables in the database, data types of columns in a specific table named 'employees', and basic statistics for numeric columns in the 'sales' table. Save the profiling report as a PDF file named 'sqlite_db_profile.pdf'.",code/ydata-profiling/ydata-profiling_12.py,"Ensure that the SQLite database connection to ""mydb.db"" is successfully established.","Check if the ""sqlite_db_profile.pdf"" file is created after running the program.","Confirm that the profiling reports include a list of tables, data types of columns in the ""employees"" table, and basic statistics for numeric columns in the ""sales"" table.",,,"#! pip install ydata-profiling
import sqlite3
import pandas as pd
from ydata_profiling import ProfileReport

# Connect to the SQLite database 'mydb.db'
conn = sqlite3.connect('mydb.db')

# Retrieve data from a specific table
employees_data = pd.read_sql_query('SELECT * FROM employees', conn)

# Perform data profiling on the 'employees' table
profiling_report_employees = ProfileReport(employees_data)

# Retrieve data from the 'sales' table
sales_data = pd.read_sql_query('SELECT * FROM sales', conn)

# Perform data profiling on the 'sales' table
profiling_report_sales = ProfileReport(sales_data)

# Save the report as a PDF file
profiling_report_sales.to_file('sqlite_db_profile.pdf')",train
ydata-profiling,3,"Create a Python program that employs the 'ydata-profiling' API to profile an Excel file named 'data.xlsx'. The profiling should include a summary of the data, a list of missing values, and an interactive report saved as an HTML file named 'data_summary.html'.",code/ydata-profiling/ydata-profiling_3.py,"Verify that the ""data.xlsx"" file is successfully loaded into a Pandas DataFrame.",Ensure that the profiling report contains a summary of the data and a list of missing values.,"Check if the ""data_summary.html"" file is created after running the program.",,,"#! pip install ydata-profiling
import pandas as pd
from ydata_profiling import ProfileReport

# Load the Excel data
data = pd.read_excel('data.xlsx')

# Perform data profiling
profiling_report = ProfileReport(data)

# Save the summary report as an HTML file
profiling_report.to_file('data_summary.html')",train
ydata-profiling,2,"Create a Python program that utilizes the ""ydata-profiling"" API to profile a JSON file named 'data.json'. The profiling should include statistics for both numeric and categorical columns, as well as a correlation matrix. Save the profiling report as a JSON file named 'data_profile.json'.",code/ydata-profiling/ydata-profiling_2.py,"Ensure that the ""data.json"" file is successfully loaded into a Pandas DataFrame.","Check if the ""data_profile.json"" file is created after running the program.",Confirm that the profiling report contains statistics for both numeric and categorical columns.,,,"#! pip install ydata-profiling
import pandas as pd
from ydata_profiling import ProfileReport

# Load the JSON data
data = pd.read_json('data.json')

# Perform data profiling
profiling_report = ProfileReport(data)

# Save the report as a JSON file
profiling_report.to_file('data_profile.json')",train
ydata-profiling,10,"Create a Python program that uses the 'ydata-profiling' API to profile a Parquet file named 'data.parquet'. The profiling should include a summary of the data, data types of columns, and a list of unique values for a column named 'product_name'. Save the profiling report as an HTML file named 'parquet_data_summary.html'.",code/ydata-profiling/ydata-profiling_10.py,"Confirm that the profiling report contains a data summary, data types of columns, and a list of unique values for the ""product_name"" column.","Check if the ""parquet_data_summary.html"" file is created after running the program.","Ensure that the ""data.parquet"" file is successfully loaded into a Pandas DataFrame.",,,"#! pip install ydata-profiling
import pandas as pd
from ydata_profiling import ProfileReport

# Load the Parquet data
data = pd.read_parquet('data.parquet')

# Perform data profiling
profiling_report = ProfileReport(data)

# Save the summary report as an HTML file
profiling_report.to_file('parquet_data_summary.html')",train
ydata-profiling,6,Create a Python program that uses the 'ydata-profiling' API to profile a Parquet file named 'data.parquet'. The profiling should include basic statistics for all columns and a summary of the data. Save the profiling report as an HTML file named 'parquet_data_summary.html'.,code/ydata-profiling/ydata-profiling_6.py,Confirm that the profiling report contains basic statistics for all columns and a data summary.,"Check if the ""parquet_data_summary.html"" file is created after running the program.","Ensure that the ""data.parquet"" file is successfully loaded into a Pandas DataFrame.",,,"#! pip install ydata-profiling
import pandas as pd
from ydata_profiling import ProfileReport

# Load the Parquet data
data = pd.read_parquet('data.parquet')

# Perform data profiling
profiling_report = ProfileReport(data)

# Save the summary report as an HTML file
profiling_report.to_file('parquet_data_summary.html')",train
ydata-profiling,9,"Create a Python program that utilizes the 'ydata-profiling' API to profile a URL pointing to an Excel file, 'http://example.com/data.xlsx'. The profiling should include data types of columns and a summary of the data. Save the profiling report as a JSON file named 'url_excel_profile.json'.",code/ydata-profiling/ydata-profiling_9.py,Ensure that the profiling report includes data types of columns and a data summary.,Verify that the data is successfully loaded from the URL into a Pandas DataFrame.,"Check if the ""url_excel_profile.json"" file is created after running the program.",,,"#! pip install ydata-profiling
import pandas as pd
from ydata_profiling import ProfileReport

# Load data from a URL
url = 'http://example.com/data.xlsx'
data = pd.read_excel(url)

# Perform data profiling
profiling_report = ProfileReport(data)

# Save the report as a JSON file
profiling_report.to_file('url_excel_profile.json')",train
ydata-profiling,5,"Create a Python program that utilizes the 'ydata-profiling' API to profile a URL pointing to a CSV file, 'http://example.com/data.csv'. The profiling should include basic statistics for all columns and a visualization of the data distribution. Save the profiling report as a JSON file named 'url_data_profile.json'.",code/ydata-profiling/ydata-profiling_5.py,Verify that the data is successfully loaded from the URL into a Pandas DataFrame.,Confirm that the profiling report contains basic statistics for all columns and data distribution visualization.,"Check if the ""url_data_profile.json"" file is created after running the program.",,,"#! pip install ydata-profiling
import pandas as pd
from ydata_profiling import ProfileReport

# Load data from a URL
url = 'http://example.com/data.csv'
data = pd.read_csv(url)

# Perform data profiling
profiling_report = ProfileReport(data)

# Save the report as a JSON file
profiling_report.to_file('url_data_profile.json')",train
ydata-profiling,8,"Create a Python program that uses the 'ydata-profiling' API to profile an SQLite database named 'mydb.db'. The profiling should include a list of tables in the database, data types of columns in a specific table named 'customers', and basic statistics for numeric columns in the 'orders' table. Save the profiling report as a PDF file named 'sqlite_db_profile.pdf'.",code/ydata-profiling/ydata-profiling_8.py,"Ensure that the SQLite database connection to ""mydb.db"" is successfully established.","Check if the ""sqlite_db_profile.pdf"" file is created after running the program.","Confirm that the profiling reports include a list of tables, data types of columns in the ""customers"" table, and basic statistics for numeric columns in the ""orders"" table.",,,"#! pip install ydata-profiling
import sqlite3
import pandas as pd
from ydata_profiling import ProfileReport

# Connect to the SQLite database 'mydb.db'
conn = sqlite3.connect('mydb.db')

# Retrieve data from a specific table
customers_data = pd.read_sql_query('SELECT * FROM customers', conn)

# Perform data profiling on the 'customers' table
profiling_report_customers = ProfileReport(customers_data)

# Retrieve data from the 'orders' table
orders_data = pd.read_sql_query('SELECT * FROM orders', conn)

# Perform data profiling on the 'orders' table
profiling_report_orders = ProfileReport(orders_data)

# Save the report as a PDF file
profiling_report_orders.to_file('sqlite_db_profile.pdf')",train
ydata-profiling,7,"Create a Python program that employs the 'ydata-profiling' API to profile a JSONL file named 'data.jsonl'. The profiling should include a count of records, data types of columns, and a table of unique values for a column named 'user_id'. Save the profiling report as an HTML file named 'jsonl_data_profile.html'.",code/ydata-profiling/ydata-profiling_7.py,"Check if the ""jsonl_data_profile.html"" file is created after running the program.","Verify that the ""data.jsonl"" file is successfully loaded as a JSONL dataset into a Pandas DataFrame.","Ensure that the profiling report includes a count of records, data types of columns, and a table of unique values for the ""user_id"" column.",,,"#! pip install ydata-profiling
import pandas as pd
from ydata_profiling import ProfileReport

# Load the JSONL data
data = pd.read_json('data.jsonl', lines=True)

# Perform data profiling
profiling_report = ProfileReport(data)

# Save the report as an HTML file
profiling_report.to_file('jsonl_data_profile.html')",test
ydata-profiling,1,"Create a Python program that uses the ""ydata-profiling"" API to perform data profiling on a CSV file named ""data.csv."" The profiling should include statistics like mean, median, and histograms for numeric columns. Save the profiling report as an HTML file named ""data_profile.html.""",code/ydata-profiling/ydata-profiling_1.py,"Test that the ""data.csv"" file is successfully loaded into a Pandas DataFrame.",Verify that the profiling report contains statistics such as mean and median.,"Check if the ""data_profile.html"" file is created after running the program.",,,"#! pip install ydata-profiling
import pandas as pd
from ydata_profiling import ProfileReport

# Load the CSV data
data = pd.read_csv(""data.csv"")

# Perform data profiling
profiling_report = ProfileReport(data)

# Save the report as an HTML file
profiling_report.to_file(""data_profile.html"")",test
