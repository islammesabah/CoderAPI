{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a40d8e2",
   "metadata": {},
   "source": [
    "### Scraping APIs documentation websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "223d3f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: markdownify in /home/mesabah/.local/lib/python3.10/site-packages (0.11.6)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.9 in /home/mesabah/.local/lib/python3.10/site-packages (from markdownify) (4.12.2)\n",
      "Requirement already satisfied: six<2,>=1.15 in /usr/lib/python3/dist-packages (from markdownify) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/mesabah/.local/lib/python3.10/site-packages (from beautifulsoup4<5,>=4.9->markdownify) (2.4.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install markdownify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e0c40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import markdownify\n",
    "from urllib.parse import urldefrag, urlparse\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eab3c266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_internal_path(tree, relative_path_type=True, name = \"\"):\n",
    "    links = []\n",
    "    for ele in tree:\n",
    "        try: link,_ = urldefrag(ele['href'])\n",
    "        except: continue\n",
    "        if relative_path_type:\n",
    "            if ((link not in links) and \n",
    "                (not (link.startswith('http'))) and\n",
    "                (not (link.startswith('/'))) and\n",
    "                (not (link.startswith('./'))) and\n",
    "                (not (link.startswith('../'))) and\n",
    "                len(link)>1 and\n",
    "                link != \"index.html\"\n",
    "               ):\n",
    "                links.append(link)\n",
    "        else:\n",
    "            if name: name+'/'\n",
    "            if ((link not in links) and \n",
    "                ((link.startswith('/docs/'+name)) or (link.startswith('/'+name)))):\n",
    "                links.append(link)\n",
    "    return links\n",
    "\n",
    "def get_html(url, name, relative_path_type = True, cls = ''):\n",
    "    response = requests.get(url)\n",
    "    article, tree = None,[]\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        if cls == 'main':\n",
    "            article = soup.find('main')\n",
    "        elif cls:\n",
    "            article = soup.find('div',class_=cls)\n",
    "        else:\n",
    "            article = soup.find('article')\n",
    "        if not article:\n",
    "            article = soup.find('div',class_='body')\n",
    "        if not article:\n",
    "            article = soup.find('body')\n",
    "        if not article:\n",
    "            article = soup\n",
    "        for s in article.select('script'):\n",
    "            s.extract()\n",
    "        for s in article.select('aside'):\n",
    "            s.extract()\n",
    "        for s in article.select('style'):\n",
    "            s.extract()\n",
    "        tree = soup.find_all('a')\n",
    "    if 'huggingface' in url:\n",
    "        links = get_internal_path(tree, False, name)\n",
    "    else:\n",
    "        links = get_internal_path(tree, relative_path_type)\n",
    "    return article, links\n",
    "\n",
    "def get_docs_relative(url,name,cls=''):\n",
    "    print(url,name,cls)\n",
    "    article, main_links = get_html(url,name,cls=cls)\n",
    "    print(main_links)\n",
    "    url = url[:url.rfind('/')]+'/'\n",
    "    main_page = markdownify.markdownify(str(article), strip=['a'],heading_style=\"ATX\")\n",
    "    if not os.path.exists('data/'+name): os.makedirs('data/'+name)\n",
    "    c = 0\n",
    "    with open('data/'+name+'/index_'+str(c)+'.txt', 'w') as f: f.write(main_page)\n",
    "    print(\"Number of files to Collect: \",len(main_links), \" Files\")\n",
    "    for link in main_links:\n",
    "        c+=1\n",
    "        file_path = 'data/'+name+'/'+(link[:link.rfind('.')].replace('/','_'))+('_'+str(c)+'.txt')\n",
    "        print(file_path)\n",
    "        in_url = url+link\n",
    "        m_url = in_url\n",
    "        if os.path.exists(file_path):\n",
    "            continue\n",
    "        article, links= get_html(in_url,name,cls=cls)\n",
    "        content = markdownify.markdownify(str(article), strip=['a'],heading_style=\"ATX\")\n",
    "        in_url = in_url[:in_url.rfind('/')]+'/'\n",
    "        for lnk in links:\n",
    "            if (not (lnk in main_links)) and ((in_url+lnk) != m_url):\n",
    "                article, _ = get_html(in_url+lnk,name,cls=cls)\n",
    "                content += \"\\n\\n\"+markdownify.markdownify(str(article), strip=['a'],heading_style=\"ATX\")\n",
    "        with open(file_path, 'w') as f: f.write(content)\n",
    "\n",
    "def get_docs_absolute(url,name,cls=''):\n",
    "    _, main_links = get_html(url, name, False,cls=cls)\n",
    "    if not os.path.exists('data/'+name): os.makedirs('data/'+name)\n",
    "    files = {}\n",
    "    pth = ''\n",
    "    if 'huggingface' in url: pth = \"/\" + name\n",
    "    for l in main_links:\n",
    "        f_name = (l.replace('/docs'+pth,'')[1:]).split('/')[0]\n",
    "        if f_name in files:\n",
    "                files[f_name].append(l)\n",
    "        else:\n",
    "            files[f_name] = [l]\n",
    "    base_url = urlparse(url)\n",
    "    base_url = str(base_url.scheme)+\"://\"+str(base_url.netloc)\n",
    "    c = 0\n",
    "    for key in files:\n",
    "        links = files[key]\n",
    "        content = \"\"\n",
    "        for l in links:\n",
    "            html, _ = get_html(base_url+l, name, False, cls=cls)\n",
    "            content += \"\\n\\n\"+markdownify.markdownify(str(html), strip=['a'],heading_style=\"ATX\")\n",
    "        with open('data/'+name+'/'+key+'_'+str(c)+'.txt', 'w') as f: f.write(content)\n",
    "        c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16fae289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_lines(text_str):\n",
    "    l = text_str.split(\"\\n\\n\")\n",
    "    temp = []\n",
    "    for x in l:\n",
    "        if x not in temp:\n",
    "            temp.append(x)\n",
    "    return '\\n\\n'.join(temp)\n",
    "\n",
    "def clean_files(path):\n",
    "    paths = []\n",
    "    for p in glob.glob(path+\"/*\"): \n",
    "        with open(p) as f:\n",
    "            file_content = f.read()\n",
    "            file_content = remove_duplicate_lines(file_content)\n",
    "            with open(p, 'w') as f: \n",
    "                f.write(file_content)\n",
    "            \n",
    "\n",
    "def combine_files(path):\n",
    "    paths = []\n",
    "    for p in glob.glob(path+\"/*\"): \n",
    "        paths.append(p.replace(path+'/',''))\n",
    "    files = []\n",
    "    for p in paths:\n",
    "        f = p.replace('.txt','').split('_')\n",
    "        f_name = f[0]\n",
    "        try: f_num = int(f[-1])\n",
    "        except: continue\n",
    "        files.append((f_num,f_name,p))\n",
    "    files.sort(key=lambda x: x[0])\n",
    "    total_content = \"\"\n",
    "    while files:\n",
    "        i = files[0]\n",
    "        file_content = \"\"\n",
    "        with open(path+'/'+i[2]) as f: file_content += \"\\n\\n\" + f.read()\n",
    "        os.remove(path+'/'+i[2])\n",
    "        files.remove(i)\n",
    "        j = 0\n",
    "        while j < len(files):\n",
    "            c=files[j]\n",
    "            if (i[1]==c[1]):\n",
    "                with open(path+'/'+c[2]) as f: file_content += \"\\n\\n\" + f.read()\n",
    "                os.remove(path+'/'+c[2])\n",
    "                files.remove(c)\n",
    "            else:\n",
    "                j+=1\n",
    "        with open(path+'/'+i[1]+'.txt', 'w') as f: f.write(file_content)\n",
    "        total_content += file_content\n",
    "    with open(path+'/total_content.txt', 'w') as f: f.write(total_content)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "605060d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num</th>\n",
       "      <th>Name</th>\n",
       "      <th>Use Case</th>\n",
       "      <th>Testing_Type</th>\n",
       "      <th>Type</th>\n",
       "      <th>Install command</th>\n",
       "      <th>Version</th>\n",
       "      <th>Last_version_release_date</th>\n",
       "      <th>Start_version</th>\n",
       "      <th>Start_version_release_date</th>\n",
       "      <th>Pypi url</th>\n",
       "      <th>Documentation</th>\n",
       "      <th>Github</th>\n",
       "      <th>Docs Type</th>\n",
       "      <th>API_Ref_File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>stumpy</td>\n",
       "      <td>computes something called the matrix profile ...</td>\n",
       "      <td>Manual</td>\n",
       "      <td>New_Version</td>\n",
       "      <td>pip install stumpy</td>\n",
       "      <td>1.12.0</td>\n",
       "      <td>2023-08-21</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://pypi.org/project/stumpy/</td>\n",
       "      <td>https://stumpy.readthedocs.io/en/latest/#</td>\n",
       "      <td>https://github.com/TDAmeritrade/stumpy.git</td>\n",
       "      <td>article</td>\n",
       "      <td>api.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Num    Name                                           Use Case  \\\n",
       "42   43  stumpy   computes something called the matrix profile ...   \n",
       "\n",
       "   Testing_Type         Type     Install command Version  \\\n",
       "42       Manual  New_Version  pip install stumpy  1.12.0   \n",
       "\n",
       "   Last_version_release_date Start_version Start_version_release_date  \\\n",
       "42                2023-08-21           0.1                 2019-05-04   \n",
       "\n",
       "                            Pypi url  \\\n",
       "42  https://pypi.org/project/stumpy/   \n",
       "\n",
       "                                Documentation  \\\n",
       "42  https://stumpy.readthedocs.io/en/latest/#   \n",
       "\n",
       "                                        Github Docs Type API_Ref_File  \n",
       "42  https://github.com/TDAmeritrade/stumpy.git   article      api.txt  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apis_list = pd.read_csv(\"APIs List.csv\")\n",
    "apis_list.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e94bd7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pymc ........ Downloading\n",
      "https://www.pymc.io/projects/docs/en/stable/learn.html pymc \n",
      "['api.html', 'contributing/index.html', 'installation.html', 'learn/core_notebooks/index.html', 'learn/core_notebooks/pymc_overview.html', 'learn/core_notebooks/GLM_linear.html', 'learn/core_notebooks/model_comparison.html', 'learn/core_notebooks/posterior_predictive.html', 'learn/core_notebooks/dimensionality.html', 'learn/core_notebooks/pymc_pytensor.html', 'learn/core_notebooks/Gaussian_Processes.html', 'learn/books.html', 'learn/videos_and_podcasts.html', 'learn/consulting.html', 'glossary.html', '_sources/learn.md.txt']\n",
      "Number of files to Collect:  16  Files\n",
      "data/pymc/api_1.txt\n",
      "data/pymc/contributing_index_2.txt\n",
      "data/pymc/installation_3.txt\n",
      "data/pymc/learn_core_notebooks_index_4.txt\n",
      "data/pymc/learn_core_notebooks_pymc_overview_5.txt\n",
      "data/pymc/learn_core_notebooks_GLM_linear_6.txt\n",
      "data/pymc/learn_core_notebooks_model_comparison_7.txt\n",
      "data/pymc/learn_core_notebooks_posterior_predictive_8.txt\n",
      "data/pymc/learn_core_notebooks_dimensionality_9.txt\n",
      "data/pymc/learn_core_notebooks_pymc_pytensor_10.txt\n",
      "data/pymc/learn_core_notebooks_Gaussian_Processes_11.txt\n",
      "data/pymc/learn_books_12.txt\n",
      "data/pymc/learn_videos_and_podcasts_13.txt\n",
      "data/pymc/learn_consulting_14.txt\n",
      "data/pymc/glossary_15.txt\n",
      "data/pymc/_sources_learn.md_16.txt\n",
      "pymc ........ Cleaning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:27<00:00,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pymc ........ Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0,len(apis_list))):\n",
    "    api = apis_list.iloc[[i]]\n",
    "    name = api['Name'][i]\n",
    "    print(name+\" ........ Downloading\")\n",
    "    url = api['Documentation'][i]\n",
    "    docs_type = api['Docs Type'][i].split('_')\n",
    "    cls = docs_type[0]\n",
    "    if cls == 'article': cls = ''\n",
    "    if len(docs_type)<2: \n",
    "        get_docs_relative(url,name,cls)\n",
    "    else:\n",
    "        get_docs_absolute(url,name,cls)\n",
    "    \n",
    "    print(name+\" ........ Cleaning\")\n",
    "    combine_files('data/'+name)\n",
    "    clean_files('data/'+name)\n",
    "    \n",
    "    print(name+\" ........ Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d85ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
